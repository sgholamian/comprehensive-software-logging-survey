"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Measuring the forensic-ability of audit logs for nonrepudiation","J. King","Department of Computer Science, North Carolina State University, 890 Oval Drive, Raleigh, NC, USA, 27695","2013 35th International Conference on Software Engineering (ICSE)","26 Sep 2013","2013","","","1419","1422","Forensic analysis of software log files is used to extract user behavior profiles, detect fraud, and check compliance with policies and regulations. Software systems maintain several types of log files for different purposes. For example, a system may maintain logs for debugging, monitoring application performance, and/or tracking user access to system resources. The objective of my research is to develop and validate a minimum set of log file attributes and software security metrics for user nonrepudiation by measuring the degree to which a given audit log file captures the data necessary to allow for meaningful forensic analysis of user behavior within the software system. For a log to enable user nonrepudiation, the log file must record certain data fields, such as a unique user identifier. The log must also record relevant user activity, such as creating, viewing, updating, and deleting system resources, as well as software security events, such as the addition or revocation of user privileges. Using a grounded theory method, I propose a methodology for observing the current state of activity logging mechanisms in healthcare, education, and finance, then I quantify differences between activity logs and logs not specifically intended to capture user activity. I will then propose software security metrics for quantifying the forensic-ability of log files. I will evaluate my work with empirical analysis by comparing the performance of my metrics on several types of log files, including both activity logs and logs not directly intended to record user activity. My research will help software developers strengthen user activity logs for facilitating forensic analysis for user nonrepudiation.","1558-1225","978-1-4673-3076-3","10.1109/ICSE.2013.6606732","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6606732","forensics;metric;security;nonrepudiation;logging;software logs;grounded theory","Forensics;Measurement;Security;Software systems;Standards;Medical services","auditing;digital forensics;educational computing;financial data processing;fraud;health care;software metrics;system monitoring","finance;education;healthcare;activity logging mechanism;grounded theory method;user privilege revocation;software security events;unique user identifier;data field;user nonrepudiation;software security metrics;log file attribute;system resources;user access tracking;application performance monitoring;debugging;software system;compliance checking;fraud detection;user behavior profile extraction;software log files;forensic analysis;audit log;forensic ability measurement","","1","","15","","26 Sep 2013","","","IEEE","IEEE Conferences"
"Abstracting log lines to log event types for mining software system logs","M. Nagappan; M. A. Vouk","Department of Computer Science, North Carolina State University, Raleigh, USA; Department of Computer Science, North Carolina State University, Raleigh, USA","2010 7th IEEE Working Conference on Mining Software Repositories (MSR 2010)","13 May 2010","2010","","","114","117","Log files contain valuable information about the execution of a system. This information is often used for debugging, operational profiling, finding anomalies, detecting security threats, measuring performance etc. The log files are usually too big for extracting this valuable information manually, even though manual perusal is still one of the more widely used techniques. Recently a variety of data mining and machine learning algorithms are being used to analyze the information in the log files. A major road block for the efficient use of these algorithms is the inherent variability present in every log line of a log file. Each log line is a combination of a static message type field and a variable parameter field. Even though both these fields are required, the analyses algorithm often requires that these be separated out, in order to find correlations in the repeating log event types. This disentangling of the message and parameter fields to find the event types is called abstraction of log lines. Each log line is abstracted to a unique ID or event type and the dynamic parameter value is extracted to give an insight on the current state of the system. In this paper we present a technique based on a clustering technique used in the Simple Log file Clustering Tool for log file abstraction. This solution is especially useful when we don't have access to the source code of the application or when the lines in the log file do not conform to a rigid structure. We evaluated our implementation on log files from the Virtual Computing Lab, a cloud computer management system at North Carolina State University, and abstracted it to 727 unique event types.","2160-1860","978-1-4244-6803-4","10.1109/MSR.2010.5463281","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5463281","log file abstraction;clustering","Software systems;Data mining;Machine learning algorithms;Algorithm design and analysis;Cloud computing;Debugging;Data security;Information security;Manuals;Information analysis","abstracting;computer debugging;data mining;file organisation;learning (artificial intelligence);software maintenance;system monitoring","software system logs;system execution;debugging;security threats;information extraction;manual perusal;data mining;machine learning algorithms;static message type field;variable parameter field;log event types;log line abstraction;unique ID;parameter value;clustering technique;simple log file clustering tool;log file abstraction;source code access;virtual computing lab;cloud computer management system;North Carolina State University;message disentangling","","26","8","9","","13 May 2010","","","IEEE","IEEE Conferences"
"IEEE Standard for eXtensible Event Stream (XES) for Achieving Interoperability in Event Logs and Event Streams","",,"IEEE Std 1849-2016","10 Nov 2016","2016","","","1","50","A grammar for a tag-based language whose aim is to provide designers of information systems with a unified and extensible methodology for capturing systems behaviors by means of event logs and event streams is defined in the XES standard. An XML Schema describing the structure of an XES event log/stream and a XML Schema describing the structure of an extension of such a log/stream are included in this standard. Moreover, a basic collection of so-called XES extension prototypes that provide semantics to certain attributes as recorded in the event log/stream is included in this standard.;A grammar for a tag-based language whose aim is to provide designers of information systems with a unified and extensible methodology for capturing systems behaviors by means of event logs and event streams is defined in the XES standard. An XML Schema describing the structure of an XES event log/stream and a XML Schema describing the structure of an extension of such a log/stream are included in this standard. Moreover, a basic collection of so-called XES extension prototypes that provide semantics to certain attributes as recorded in the event log/stream is included in this standard.","","978-1-5044-2421-9","10.1109/IEEESTD.2016.7740858","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7740858","event log;event stream;extensions;IEEE 1849(TM);system behavior;XML","IEEE Standards;Event recognition;Behavioral sciences;XML;Information systems;Semantics;Grammar","grammars;IEEE standards;open systems;XML","IEEE standard;extensible event stream;interoperability;event logs;event streams;IEEE Std 1849-2016;grammar;tag-based language;information systems;XML schema;XES extension prototypes","","1","","34","","10 Nov 2016","","","IEEE","IEEE Standards"
"Which log level should developers choose for a new logging statement? (journal-first abstract)","H. Li; W. Shang; A. E. Hassan","Software Analysis and Intelligence Lab (SAIL), Queen's University, Canada; Department of Computer Science and Software Engineering, Concordia University, Canada; Software Analysis and Intelligence Lab (SAIL), Queen's University, Canada","2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)","5 Apr 2018","2018","","","468","468","This is an extended abstract of a paper published in the Empirical Software Engineering journal. The original paper is communicated by Mark Grechanik. The paper empirically studied how developers assign log levels to their logging statements and proposed an automated approach to help developers determine the most appropriate log level when they add a new logging statement. We analyzed the development history of four open source projects (Hadoop, Directory Server, Hama, and Qpid). We found that our automated approach can accurately suggest the levels of logging statements with an AUC of 0.75 to 0.81. We also found that the characteristics of the containing block of a newly-added logging statement, the existing logging statements in the containing source code file, and the content of the newly-added logging statement play important roles in determining the appropriate log level for that logging statement.","","978-1-5386-4969-5","10.1109/SANER.2018.8330234","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8330234","software logging;log level;ordinal regression model","","","","","","","","","5 Apr 2018","","","IEEE","IEEE Conferences"
"Automatic Real-Time Mining Software Process Activities From SVN Logs Using a Naive Bayes Classifier","R. Zhu; Y. Dai; T. Li; Z. Ma; M. Zheng; Y. Tang; J. Yuan; Y. Huang","School of Software, Yunnan University, Kunming, China; College of Computer, National University of Defense Technology, Changsha, China; Key Laboratory in Software Engineering of Yunnan Province, Kunming, China; School of Software, Yunnan University, Kunming, China; School of Software, Yunnan University, Kunming, China; School of Software, Yunnan University, Kunming, China; School of Software, Yunnan University, Kunming, China; School of Software, Yunnan University, Kunming, China","IEEE Access","16 Oct 2019","2019","7","","146403","146415","The abundance of event data in current software configuration management systems makes it possible to discover software process models automatically by using actual observed behavior. However, traditional process mining algorithms cannot be applied to event logs recorded in software configuration management (SCM) systems, such as SVN, because of missing activity attributes. To address this problem, a software process activity classifier is proposed to build event-activity mapping relationships from software development event streams, revealing activity attributes and associating the activity to the original SVN log. The proposed approach extracts activity from the SVN log based on semantic features and introduces a novel technique based on a naive Bayes approach to associate event activities dynamically. The approach has been applied to two real-world software development process logs, ArgoUML and jEdit, consisting of more than 80,000 events, covering development information from 1998 to 2015. With the application of our approach to such data, activities can be extracted from event logs and a classifier can be constructed for adding activity attributes to new events. The results of the classification are evaluated in terms of precision rate, recall rate, and the F-measure. Overall, two real-world software development process logs are used to validate the method, and the experimental results show that the approach can mine software process activities from SVN log events automatically and in real-time.","2169-3536","","10.1109/ACCESS.2019.2945608","National Natural Science Foundation of China; Natural Science Foundation of Yunnan Province; Yunnan Provincial Department of Education; Yunnan University; Yunnan University Dong Lu Young-backbone Teacher Training Program; Yunnan University Education Department Science Research Fund Graduate Program; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8864026","Activity classifier;machine learning;software process activity;SVN log","Software;Data mining;Semantics;Business;Capability maturity model;Feature extraction","Bayes methods;configuration management;data mining;pattern classification","SVN log;software configuration management systems;event activities;software development event streams;event-activity mapping relationships;software process activity classifier;missing activity attributes;software process models;event data;naive Bayes classifier;SVN logs;automatic real-time mining software process activities;event logs;real-world software development process logs","","1","","29","CCBY","11 Oct 2019","","","IEEE","IEEE Journals"
"An Approach to Recommendation of Verbosity Log Levels Based on Logging Intention","H. Anu; J. Chen; W. Shi; J. Hou; B. Liang; B. Qin",Renmin University of China; Renmin University of China; Renmin University of China; Renmin University of China; Renmin University of China; Renmin University of China,"2019 IEEE International Conference on Software Maintenance and Evolution (ICSME)","5 Dec 2019","2019","","","125","134","Verbosity levels of logs are designed to discriminate highly diverse runtime events, which facilitates system failure identification through simple keyword search (e.g., fatal, error). Verbosity levels should be properly assigned to logging statements, as inappropriate verbosity levels would confuse users and cause a lot of redundant maintenance effort. However, to achieve such a goal is not an easy task due to the lack of practical specifications and guidelines towards verbosity log level usages. The existing research has built a classification model on log related quantitative metrics such as log density to improve logging level practice. Though such quantitative metrics can reveal logging characteristics, their contributions on logging level decision are limited, since valuable logging intention information buried in logging code context can not be captured. In this paper, we propose an automatic approach to help developers determine the appropriate verbosity log levels. More specially, our approach discriminates different verbosity log level usages based on code context features that contain underlying logging intention. To validate our approach, we implement a prototype tool, VerbosityLevelDirector, and perform a case study to measure its effectiveness on four well-known open source software projects. Evaluation results show that VerbosityLevelDirector achieves high performance on verbosity level discrimination and outperforms the baseline approaches on all those projects. Furthermore, through applying noise handling technique, our approach can detect previously unknown inappropriate verbosity level configurations in the code repository. We have reported 21 representative logging level errors with modification advice to issue tracking platforms of the examined software projects and received positive feedback from their developers. The above results confirm that our work can help developers make a better logging level decision in real-world engineering.","2576-3148","978-1-7281-3094-1","10.1109/ICSME.2019.00022","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8919094","logging level decision;logging intention;logging context feature;static code analysis;noise handling technique","Measurement;Tools;Software;Feature extraction;Runtime;Task analysis;Libraries","feature extraction;program diagnostics;public domain software;software maintenance;software metrics","logging statements;inappropriate verbosity levels;verbosity log level usages;log related quantitative metrics;log density;logging level practice;logging characteristics;logging level decision;valuable logging intention information;appropriate verbosity log levels;underlying logging intention;verbosity level discrimination;unknown inappropriate verbosity level configurations;21 representative logging level errors","","1","","35","","5 Dec 2019","","","IEEE","IEEE Conferences"
"Examining the Stability of Logging Statements","S. Kabinna; W. Shang; C. Bezemer; A. E. Hassan","Software Anal. & Intell. Lab., Queen's Univ., Kingston, ON, Canada; Dept. of Comput. Sci. & Software Eng, Concordia Univ., Montreal, QC, Canada; Software Anal. & Intell. Lab., Queen's Univ., Kingston, ON, Canada; Software Anal. & Intell. Lab., Queen's Univ., Kingston, ON, Canada","2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)","23 May 2016","2016","1","","326","337","Logging statements produce logs that assist in understanding system behavior, monitoring choke-points and debugging. Prior research demonstrated the importance of logging statements in operating, understanding and improving software systems. The importance of logs has lead to a new market of log management and processing tools. However, logs are often unstable, i.e., the logging statements that generate logs are often changed without the consideration of other stakeholders, causing misleading results and failures of log processing tools. In order to proactively mitigate such issues that are caused by unstable logging statements, in this paper we empirically study the stability of logging statements in four open source applications namely:Liferay, ActiveMQ, Camel and Cloud Stack. We find that 20-45% of the logging statements in our studied applications change throughout their lifetime. The median number of days between the introduction of a logging statement and the first change to that statement is between 1 and 17 in our studied applications. These numbers show that in order to reduce maintenance effort, developers of log processing tools must be careful when selecting the logging statements on which they will let their tools depend. In this paper, we make an important first step towards assisting developers of log processing tools in determining whether a logging statement is likely to remain unchanged in the future. Using random forest classifiers, we examine which metrics are important for understanding whether a logging statement will change. We show that our classifiers achieve 83%-91% precision and 65%-85% recall in the four studied applications. We find that file ownership, developer experience, log density and SLOC are important metrics for determining whether a logging statement will change in the future. Developers can use this knowledge to build more robust log processing tools, by making those tools depend on logs that are generated by logging statements that are likely to remain unchanged.","","978-1-5090-1855-0","10.1109/SANER.2016.29","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7476654","Logging Statements;Random Forest;classifier;developer experience;ownership","Java;Libraries;History;Data mining;Monitoring;Measurement;Context","checkpointing;pattern classification;program debugging;public domain software;software metrics;software process improvement;system monitoring","logging statement stability;choke-point monitoring;debugging;software system improvement;log management;log processing tools;open source applications;Liferay;ActiveMQ;Camel;CloudStack;random forest classifiers;SLOC","","17","","37","","23 May 2016","","","IEEE","IEEE Conferences"
"Improving State-of-the-art Compression Techniques for Log Management Tools","K. Yao; M. Sayagh; W. Shang; A. E. Hassan","School of Computing, Queen's University, 4257 Kingston, Ontario, Canada, (e-mail: kundi@cs.queensu.ca); School of Computing, Queen's University, 4257 Kingston, Ontario, Canada, (e-mail: msayagh@cs.queensu.ca); Department of Computer Science and Software Engineering, Concordia University, Montreal, Quebec, Canada, (e-mail: shang@encs.concordia.ca); School of Computing, Queen's University, Kingston, Ontario, Canada, K7L 3N6 (e-mail: ahmed@cs.queensu.ca)","IEEE Transactions on Software Engineering","","2021","PP","99","1","1","Log data records important runtime information about the running of a software system for different purposes including performance assurance, capacity planning, and anomaly detection. Log management tools such as ELK Stack and Splunk are widely adopted to manage and leverage log data in order to assist DevOps in real-time log analytics and decision making. To enable fast queries and to save storage space, such tools split log data into small blocks (e.g., 16KB), then index and compress each block separately. Previous log compression studies focus on improving the compression of either large-sized log files or log streams, without considering improving the compression of small log blocks (the actual compression need by modern log management tools). The evaluation of four state-of-the-art compression approaches (e.g., Logzip, a variation of Logzip by pre-extracting log templates named Logzip-E, LogArchive and Cowic) indicates that these approaches do not perform well on small log blocks. In fact, the compressed blocks that are preprocessed using Logzip, Logzip-E, LogArchive or Cowic are even larger (on median 1.3 times, 1.5 times, 0.2 times or 6.6 times) than the compressed blocks without any preprocessing. Hence, we propose an approach named LogBlock to preprocess small log blocks before compressing them with a general compressor such as gzip, deflate and lz4, which are widely adopted by log management tools. Logblock reduces the repetitiveness of logs by preprocessing the log headers and rearranging the log content leading to an improved compression ratio for a log file. Our evaluation on 16 log files shows that, for 16KB to 128KB block sizes, the compressed blocks by LogBlock are on median 5% to 21% smaller than the same compressed blocks without preprocessing (outperforming the state-of-the-art compression approaches). LogBlock achieves both a higher compression ratio (a median of 1.7 to 8.4 times, 1.9 to 10.0 times, 1.3 to 1.9 times and 6.2 to 11.4 times) and a faster compression speed (a median of 30.8 to 49.7 times, 42.6 to 53.8 times, 4.5 to 6.0 times and 2.5 to 4.0 times) than Logzip, Logzip-E, LogArchive and Cowic. LogBlock can help improve the storage efficiency of log management tools.","1939-3520","","10.1109/TSE.2021.3069958","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9392377","Software log compression;Software logging;Log management tools F","Tools;Indexes;IP networks;Software systems;Runtime;Monitoring;Message systems","","","","","","","IEEE","31 Mar 2021","","","IEEE","IEEE Early Access Articles"
"ISO/IEC/IEEE International Standard - Systems and software engineering -- Vocabulary","",,"ISO/IEC/IEEE 24765:2010(E)","8 Feb 2018","2010","","","1","418","The systems and software engineering disciplines are continuing to mature while information technology advances. This International Standard was prepared to collect and standardize terminology. Its purpose is to identify terms currently in use in the field and standard definitions for these terms. It is intended to serve as a useful reference for those in the Information Technology field, and to encourage the use of systems and software engineering standards prepared by ISO and liaison organizations IEEE Computer Society and Project Management Institute (PMI). This International Standard replaces IEEE Std 610.12-1990, IEEE Standard Glossary of Software Engineering Terminology, which was contributed by the IEEE as a source document. The approach and lexical exactitude of IEEE Std 610.12-1990 served as a model for this International Standard. Nevertheless, approximately two thirds of the definitions in this International Standard are new since IEEE Std 610.12 was last updated in 1990, a reflection of the continued evolution in the field.;ISO/IEC/IEEE 24765:2010 provides a common vocabulary applicable to all systems and software engineering work. It was prepared to collect and standardize terminology. ISO/IEC/IEEE 24765:2010 is intended to serve as a useful reference for those in the information technology field, and to encourage the use of systems and software engineering standards prepared by ISO and liaison organizations IEEE Computer Society and Project Management Institute. ISO/IEC/IEEE 24765:2010 includes references to the active source standards for each definition so that the use of the term can be further explored.","","978-0-7381-6205-8","10.1109/IEEESTD.2010.5733835","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5733835","computer;dictionary;information technology;software engineering;systems engineering;terminology;vocabulary","IEEE standards;ISO standards;IEC standards;Software engineering;Dictionaries","IEEE standards;ISO standards;software engineering","systems engineering;International Standard;information technology field;IEEE Computer Society;Project Management Institute;IEEE Std 610.12-1990;IEEE Standard Glossary of Software Engineering Terminology","","24","","128","","8 Feb 2018","","","IEEE","IEEE Standards"
"An Exploratory Study of the Evolution of Communicated Information about the Execution of Large Software Systems","W. Shang; Z. M. Jiang; B. Adams; A. E. Hassan; M. W. Godfrey; M. Nasser; P. Flora","Sch. of Comput., Queen's Univ., Kingston, ON, Canada; Sch. of Comput., Queen's Univ., Kingston, ON, Canada; Sch. of Comput., Queen's Univ., Kingston, ON, Canada; Sch. of Comput., Queen's Univ., Kingston, ON, Canada; David R. Cheriton Sch. of Comput. Sci., Univ. of Waterloo, Waterloo, ON, Canada; Performance Eng. Res. In Motion (RIM), Waterloo, ON, Canada; Performance Eng. Res. In Motion (RIM), Waterloo, ON, Canada","2011 18th Working Conference on Reverse Engineering","17 Nov 2011","2011","","","335","344","A great deal of research in software engineering focuses on understanding the dynamic nature of software systems. Such research makes use of automated instrumentation and profiling techniques after fact, i.e., without considering domain knowledge. In this paper, we turn our attention to another source of dynamic information, i.e., the Communicated Information (CI) about the execution of a software system. Major examples of CI are execution logs and system events. They are generated from statements that are inserted intentionally by domain experts (e.g., developers or administrators) to convey crucial points of interest. The accessibility and domain-driven nature of the CI make it a valuable source for studying the evolution of a software system. In a case study on one large open source and one industrial software system, we explore the concept of CI and its evolution by mining the execution logs of these systems. Our study illustrates the need for better trace ability techniques between CI and the Log Processing Apps that analyze the CI. In particular, we find that the CI changes at a rather high rate across versions, leading to fragile Log Processing Apps. 40% to 60% of these changes can be avoided and the impact of 15% to 50% of the changes can be controlled through the use of the robust analysis techniques by Log Processing Apps. We also find that Log Processing Apps that track implementation-level CI (e.g., performance analysis) are more fragile than Log Processing Apps that track domain-level CI (e.g., workload modeling), because the implementation-level CI is often short-lived.","2375-5369","978-1-4577-1948-6","10.1109/WCRE.2011.48","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6079859","Reverse engineering;Software evolution;Communicated information;Execution log analysis","Context;Software systems;Maintenance engineering;Monitoring;History;Educational institutions","program diagnostics;public domain software;recording;reverse engineering","communicated information;large software system execution;software engineering;automated instrumentation;profiling technique;execution logs;domain expert;domain-driven nature;open source system;industrial software system;execution log mining;traceability technique;log processing apps;domain-level CI change","","18","1","33","","17 Nov 2011","","","IEEE","IEEE Conferences"
"Logram: Efficient Log Parsing Using n-Gram Dictionaries","H. Dai; H. Li; C. S. Chen; W. Shang; T. Chen","Department of Computer Science and Software Engineering, Concordia University, 5618 Montreal, Quebec, Canada, (e-mail: he_da@encs.concordia.ca); School of Computing, Queen's University, Kingston, Ontario, Canada, (e-mail: hengli@cs.queensu.ca); Department of Computer Science and Software Engineering, Concordia University, 5618 Montreal, Quebec, Canada, (e-mail: c_chesha@encs.concordia.ca); Department of Computer Science and Software Engineering, Concordia University, Montreal, Quebec, Canada, (e-mail: shang@encs.concordia.ca); Computer Science and Software Engineering, Concordia University, 5618 Montreal, Quebec, Canada, H3G 2W1 (e-mail: peterc@encs.concordia.ca)","IEEE Transactions on Software Engineering","","2020","PP","99","1","1","Software systems usually record important runtime information in their logs. Logs help practitioners understand system runtime behaviors and diagnose field failures. As logs are usually very large in size, automated log analysis is needed to assist practitioners in their software operation and maintenance efforts. Typically, the first step of automated log analysis is log parsing, i.e.,converting unstructured raw logs into structured data. However, log parsing is challenging, because logs are produced by static templates in the source code (i.e., logging statements) yet the templates are usually inaccessible when parsing logs. Prior work proposed automated log parsing approaches that have achieved high accuracy. However, as the volume of logs grows rapidly in the era of cloud computing, efficiency becomes a major concern in log parsing. In this work, we propose an automated log parsing approach, Logram, which leverages-gram dictionaries to achieve efficient log parsing. We evaluated Logram on 16 public log datasets and compared Logram with five state-of-the-art log parsing approaches. We found that Logram achieves a higher parsing accuracy than the best existing approaches (i.e., at least 10% higher, on average) and also outperforms these approaches in efficiency (i.e., 1.8 to 5.1 times faster than the second-fastest approaches in terms of end-to-end parsing time). Furthermore, we deployed Logram on Spark and we found that Logram scales out efficiently with the number of Spark nodes (e.g., with near-linear scalability for some logs) without sacrificing parsing accuracy. In addition, we demonstrated that Logram can support effective online parsing of logs, achieving similar parsing results and efficiency to the offline mode.","1939-3520","","10.1109/TSE.2020.3007554","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9134790","Log parsing;Log analysis;N-gram","Dictionaries;Runtime;Data mining;Cows;Sparks;Software systems;Moon","","","","3","","","","7 Jul 2020","","","IEEE","IEEE Early Access Articles"
"Studying and Suggesting Logging Locations in Code Blocks","Z. Li","Concordia University,Montreal,Quebec,Canada","2020 IEEE/ACM 42nd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)","1 Dec 2020","2020","","","125","127","Developers write logging statements to generate logs and record system execution behaviors to assist in debugging and software maintenance. However, there exists no practical guidelines on where to write logging statements. On one hand, adding too many logging statements may introduce superfluously trivial logs and performance overheads. On the other hand, logging too little may miss necessary runtime information. Thus, properly deciding the logging location is a challenging task and a finer-grained under-standing of where to write logging statements is needed to assist developers in making logging decisions. In this paper, we conduct a comprehensive study to uncover guidelines on logging locations at the code block level. We analyze logging statements and their surrounding code by combining both deep learning techniques and manual investigations. From our preliminary results, we find that our deep learning models achieve over 90% in precision and recall when trained using the syntactic (e.g., nodes in abstract syntax tree) and semantic (e.g., variable names) features. However, cross-system models trained using semantic features only have 45.6% in precision and 73.2% in recall, while models trained using syntactic features still have over 90% precision and recall. Our current progress high-lights that there is an implicit syntactic logging guideline across systems, and such information may be leveraged to uncover general logging guidelines.","2574-1926","978-1-4503-7122-3","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9270355","log;deep learning;where to log","Syntactics;Guidelines;Software engineering;Semantics;Manuals;Deep learning;Analytical models","data loggers;learning (artificial intelligence);neural nets;program debugging;software maintenance","logging decisions;logging location;logging statements;implicit syntactic logging guideline;code blocks;software maintenance;debugging;deep learning","","","","23","","1 Dec 2020","","","IEEE","IEEE Conferences"
"Comparing Constraints Mined From Execution Logs to Understand Software Evolution","T. Krismayer; M. Vierhauser; R. Rabiser; P. Grünbacher","Johannes Kepler University Linz, Christian Doppler Laboratory MEVSS, Institute for Software Systems Engineering, Austria; Johannes Kepler University Linz, Institute for Software Systems Engineering, Austria; Johannes Kepler University Linz, Christian Doppler Laboratory MEVSS, Institute for Software Systems Engineering, Austria; Johannes Kepler University Linz, Christian Doppler Laboratory MEVSS, Institute for Software Systems Engineering, Austria","2019 IEEE International Conference on Software Maintenance and Evolution (ICSME)","5 Dec 2019","2019","","","491","495","Complex software systems evolve frequently, e.g., when introducing new features or fixing bugs during maintenance. However, understanding the impact of such changes on system behavior is often difficult. Many approaches have thus been proposed that analyze systems before and after changes, e.g., by comparing source code, model-based representations, or system execution logs. In this paper, we propose an approach for comparing run-time constraints, synthesized by a constraint mining algorithm, based on execution logs recorded before and after changes. Specifically, automatically mined constraints define the expected timing and order of recurring events and the values of data elements attached to events. Our approach presents the differences of the mined constraints to users, thereby providing a higher-level view on software evolution and supporting the analysis of the impact of changes on system behavior. We present a motivating example and a preliminary evaluation based on a cyber-physical system controlling unmanned aerial vehicles. The results of our preliminary evaluation show that our approach can help to analyze changed behavior and thus contributes to understanding software evolution.","2576-3148","978-1-7281-3094-1","10.1109/ICSME.2019.00082","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8919165","software evolution;constraint mining;dynamic analysis","Data mining;Drones;Software systems;Timing;Control systems","data mining;software architecture;software maintenance;software metrics;source code (software)","execution logs;software evolution;complex software systems;system behavior;source code;model-based representations;system execution;run-time constraints;constraint mining algorithm;automatically mined constraints;expected timing;cyber-physical system;changed behavior;system execution logs;unmanned aerial vehicles","","","","16","","5 Dec 2019","","","IEEE","IEEE Conferences"
"An Automatic Approach to Validating Log Levels in Java","T. Kim; S. Kim; C. Yoo; S. Cho; S. Park","Dept. of Software Eng., CAIIT Chonbuk Nat. Univ., Jeonju, South Korea; Dept. of Software Eng., CAIIT Chonbuk Nat. Univ., Jeonju, South Korea; Dept. of Software Eng., CAIIT Chonbuk Nat. Univ., Jeonju, South Korea; Dept. of Comput. Sci. & Eng., Sogang Univ., Seoul, South Korea; Dept. of Comput. Sci. & Eng., Sogang Univ., Seoul, South Korea","2018 25th Asia-Pacific Software Engineering Conference (APSEC)","23 May 2019","2018","","","623","627","A log statement is used to record important runtime behavior of software systems for diverse reasons, which is inevitable to develop most of the software systems. However, developers do not tend to deeply consider an appropriate log level in their source code. In order to address the issues, this paper proposes an automatic approach to validating log levels in Java in consideration of the syntactic as well as semantic features. We first build up the Word2Vec model and generate semantic and syntactic log feature vectors, then train the machine learning classifiers to automatically validate the log levels. For the evaluation, we collected six open source projects of the message-oriented middleware domain, and obtained the 88% precision and the 87% recall respectively.","2640-0715","978-1-7281-1970-0","10.1109/APSEC.2018.00078","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8719571","log statement;log level;Word2Vec;log feature vector;classification model","Semantics;Syntactics;Feature extraction;Java;Frequency measurement;Software systems;Machine learning","Java;learning (artificial intelligence);middleware;pattern classification;public domain software;system monitoring","automatic approach;Java;semantic features;log statement;software systems;runtime behavior;log levels;source code;Word2Vec model;syntactic log feature vectors;semantic log feature vectors;machine learning classifiers;open source projects;message-oriented middleware domain;recall;precision","","","","18","","23 May 2019","","","IEEE","IEEE Conferences"
"Design and development of generic web based framework for log analysis","R. Rastogi; S. Akash; G. Shobha; G. Poonam; D. Pratiba; A. Singh","Department of CSE, R.V. College of Engineering, Bengaluru, India; Department of CSE, R.V. College of Engineering, Bengaluru, India; Department of CSE, R.V. College of Engineering, Bengaluru, India; Faculty, Department of CSE, R.V. College of Engineering, Bengaluru, India; Faculty, Department of CSE, R.V. College of Engineering, Bengaluru, India; Senior Software Engineer 2, Citrix R&D, India","2016 IEEE Region 10 Conference (TENCON)","9 Feb 2017","2016","","","232","236","Large scale software systems keep on generating logs for the events carried out in the past. The information recorded in these log files is very useful in debugging operation as well as for regression testing. Now days, companies are required to review their log records on regular intervals to detect and analyze the anomalies, faults or any unwanted activity that is not normal. However, when the system is complex, these log files become huge and are almost impossible to read. Often, entries are irrelevant, so combining and correlating events in huge logs is difficult, time consuming process and requires enormous computational resources. Thus this paper aims at development of generic web based framework to analyze the log files provided by the user. The built tool will parse the log files based on user selected text phrases. The developed prototype based on the assumption that a log file generally records different events based on timestamps. And each event will have its corresponding entity and pattern pairs. An entity is the attribute name given to particular entity present in similar events. A pattern is basically a value for the attribute corresponding to each entity and it is the actual point of interest. In the proposed framework timestamp is considered as the metadata for the log file and the user is required to highlight the entity and any pattern corresponding to that entity. The entity and its corresponding value are searched in the entire log file by generating regular expression dynamically. Finally, the proposed log analysis tool in this paper visualizes the highlighted entity against time using Google charts. The proposed web enabled tool is light-weight framework supporting data streaming capabilities. It is different from the existing log analysis tools in three ways. Firstly, it supports the feature of highlighting the entity-pattern pair and provides the visualizations in terms of graphs, listings, etc for the highlighted entity-pattern pair. Secondly, the tool supports generation of Regular Expressions dynamically for the highlighted entity-pattern pair. Lastly, to print and save the visualization reports as JPeg images for latter reference.","2159-3450","978-1-5090-2597-8","10.1109/TENCON.2016.7847996","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7847996","Regular Expressions;Events;Regular Expressions;Log Analysis","Google;Data mining;Servers;Software;Data visualization;Organizations;Dictionaries","Internet;program debugging;Web design","generic Web;log analysis;large scale software systems;information recorded;log files;debugging operation;regression testing;computational resources;framework timestamp;metadata;Google charts;data streaming;log analysis tools","","3","","14","","9 Feb 2017","","","IEEE","IEEE Conferences"
"Where Shall We Log? Studying and Suggesting Logging Locations in Code Blocks","Z. Li; T. -H. Chen; W. Shang","Concordia University,Montreal,Quebec,Canada; Concordia University,Montreal,Quebec,Canada; Concordia University,Montreal,Quebec,Canada","2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)","24 Dec 2020","2020","","","361","372","Developers write logging statements to generate logs and record system execution behaviors to assist in debugging and software maintenance. However, deciding where to insert logging statements is a crucial yet challenging task. On one hand, logging too little may increase the maintenance difficulty due to missing important system execution information. On the other hand, logging too much may introduce excessive logs that mask the real problems and cause significant performance overhead. Prior studies provide recommendations on logging locations, but such recommendations are only for limited situations (e.g., exception logging) or at a coarse-grained level (e.g., method level). Thus, properly helping developers decide finer-grained logging locations for different situations remains an unsolved challenge. In this paper, we tackle the challenge by first conducting a comprehensive manual study on the characteristics of logging locations in seven open-source systems. We uncover six categories of logging locations and find that developers usually insert logging statements to record execution information in various types of code blocks. Based on the observed patterns, we then propose a deep learning framework to automatically suggest logging locations at the block level. We model the source code at the code block level using the syntactic and semantic information. We find that: 1) our models achieve an average of 80.1% balanced accuracy when suggesting logging locations in blocks; 2) our cross-system logging suggestion results reveal that there might be an implicit logging guideline across systems. Our results show that we may accurately provide finer-grained suggestions on logging locations, and such suggestions may be shared across systems.","2643-1572","978-1-4503-6768-4","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9286119","log, logging location, deep learning, neural network, empirical study","Deep learning;Software maintenance;Semantics;Manuals;Syntactics;Task analysis;Software engineering","information retrieval;learning (artificial intelligence);program debugging;public domain software;software maintenance;system monitoring","suggesting logging locations;code blocks;excessive logs;exception logging;cross-system logging suggestion results;implicit logging guideline","","","","78","","24 Dec 2020","","","IEEE","IEEE Conferences"
"An Empirical Study on Leveraging Logs for Debugging Production Failures","A. R. Chen","Concordia University, Canada","2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)","19 Aug 2019","2019","","","126","128","In modern software development, maintenance is one of the most expensive processes. When end-users encounter software defects, they report the bug to developers by specifying the expected behavior and error messages (e.g., log message). Then, they wait for a bug fix from the developers. However, on the developers' side, it can be very challenging and expensive to debug the problem. To fix the bugs, developers often have to play the role of detectives: seeking clues in the user-reported logs files or stack trace in a snapshot of specific system execution. This debugging process may take several hours or even days. In this paper, we first look at the usefulness of the user-reported logs. Then, we propose an automated approach to assist the debugging process by reconstructing the execution path. Through the analysis, our investigation shows that 31% of the time, developer further requests logs from the reporter. Moreover, our preliminary results show that the reconducted path illustrates the user's execution. We believe that our approach proposes a novel solution in debugging production failures.","2574-1934","978-1-7281-1764-5","10.1109/ICSE-Companion.2019.00055","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8802657","Mining Software Repository Data;Events Log;Static Analysis;Production Errors","Computer bugs;Debugging;Production;Tools;Static analysis;Open source software","program debugging;software reliability","production failures;expensive processes;software defects;expected behavior;error messages;bug fix;user-reported logs files;debugging process;software development","","1","","13","","19 Aug 2019","","","IEEE","IEEE Conferences"
"Applied process mining in software development","M. L. Sebu; H. Ciocârlie","Computer and Software Engineering Department, Politehnica University of Timisoara, Romania; Computer and Software Engineering Department, Politehnica University of Timisoara, Romania","2014 IEEE 9th IEEE International Symposium on Applied Computational Intelligence and Informatics (SACI)","19 Jun 2014","2014","","","55","60","Current study uses the event process logs of a custom software development process used internally in the IT department of a large automotive company. The target is to extract hidden information about the process, difficult to identify due to the wideness and complexity of the input data. The log file contains all incident records tracked during one release of a software product. This analysis was performed using process mining techniques and tools included in ProM Framework, an academic project of the Eindhoven Technical University. The paper describes the steps followed to extract the working process model, organizational network and statistical information. Based on the obtained results, an action plan is put in practice with the scope of improving the development process and the release result.","","978-1-4799-4694-5","10.1109/SACI.2014.6840098","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6840098","process mining;process model;event log;heuristic miner","Data mining;Software;Analytical models;Companies;Computational intelligence;Informatics","data mining;software engineering;statistical analysis","applied process mining;event process logs;custom software development process;IT department;automotive company;hidden information extraction;input data complexity;log file;ProM framework;Eindhoven technical university;organizational network;statistical information","","6","","7","","19 Jun 2014","","","IEEE","IEEE Conferences"
"Access control for cloud forensics through secure logging services","B. C. Sekhar; G. Murali","Computer Science and Engineering JNTUACEP, Pulivendula, A.P, India; Computer Science and Engineering JNTUACEP, Pulivendula, A.P, India","2017 International Conference on Energy, Communication, Data Analytics and Soft Computing (ICECDS)","21 Jun 2018","2017","","","3527","3532","Cloud Forensics can be termed as the application of digital forensics in cloud computing platform. In the world of digital forensic we can estimate the position of “Cloud Forensics”. At this point, we must assess what logs the forensics investigator required to find out who was behind the attack. Basically, cloud forensic is an interdisciplinary concept in between cloud computing and Digital forensics. We often do now not have get entry to the records that we are after and must approach the CSP to furnish the information we need. The problem with such type of data is that we must trust the CSP to give us the required right information. They might give us false information or hold back some very important information. For Securing Cloud and making a Investigation A Log Management is a record of all the activities and events occurring and processed within an industry, organizations, application or system or software or network. This work proposes an enhanced Secure Log Management for securing user activities from malicious attacks. Log records play a prominent role in digital forensic analysis of systems. To maintain log security and provide protecting from attackers we design a integrated novel log security algorithm which provides security to log files at all times. As the log files contain confidential information we require confidentiality and privacy of log infromation is an important. This work proposed Delegating log management provides cost saving measure. The project identifies a novel frame work for a challenging secure cloud based log management service by multiple encryption mechanism and effective authentication. This work proposes to Extend and design a integrated solution for storing and maintaining log records in a server maintained in a cloud-based environment. To the best of our knowledge, This is the first work to provide a complete solution to the cloud based secure log management problem.","","978-1-5386-1887-5","10.1109/ICECDS.2017.8390116","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8390116","Cloud Forensics;Logging;Cloud service providers (CSP);Log Management;Multiple encryption mechanism","Cloud computing;Protocols;Digital forensics;Encryption;Servers","cloud computing;data privacy;digital forensics","digital forensics;cloud computing platform;cloud forensic;log security;log files;log infromation;log management service;secure log management problem;secure logging services;cloud forensics;investigation a log management;secure log management","","1","","13","","21 Jun 2018","","","IEEE","IEEE Conferences"
"A Business Log System to Display Related Logs Based on Past Cyclic Events of Business","Y. Yoshida; M. Niibori; M. Kamada","Grad. Sch. of Sci. & Eng., Ibaraki Univ., Hitachi, Japan; Grad. Sch. of Sci. & Eng., Ibaraki Univ., Hitachi, Japan; Dept. of Comput. & Inf. Sci., Ibaraki Univ., Hitachi, Japan","2014 17th International Conference on Network-Based Information Systems","29 Jan 2015","2014","","","545","548","Business logs on information systems have greatly improved business efficiency. Log records can be easily found by keyword search if we only know right keywords. Further efficiency may be achieved by a system that automatically shows log records in the past related to what the user is currently doing or what is likely to happen in the near future even if the user does not notice their existence. In this paper, we develop a business log system that presents related log records of similar events taking place in a cycle in the past log.","2157-0426","978-1-4799-4224-4","10.1109/NBiS.2014.50","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7024009","business log system;cycles of business events;groupware","Business;Estimation;Libraries;Information systems;Educational institutions;Collaborative software;Collaborative work","business data processing;data loggers;data recording;groupware;information systems","business log system;logs display;business cyclic events;information systems;business efficiency;log records;keyword search;groupware","","","","8","","29 Jan 2015","","","IEEE","IEEE Conferences"
"Tracing Back Log Data to its Log Statement: From Research to Practice","D. Schipper; M. Aniche; A. van Deursen",Adyen N.V.; Delft University of Technology; Delft University of Technology,"2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)","29 Aug 2019","2019","","","545","549","Logs are widely used as a source of information to understand the activity of computer systems and to monitor their health and stability. However, most log analysis techniques require the link between the log messages in the raw log file and the log statements in the source code that produce them. Several solutions have been proposed to solve this non-trivial challenge, of which the approach based on static analysis reaches the highest accuracy. We, at Adyen, implemented the state-of-the-art research on log parsing in our logging environment and evaluated their accuracy and performance. Our results show that, with some adaptation, the current static analysis techniques are highly efficient and performant. In other words, ready for use.","2574-3864","978-1-7281-3412-3","10.1109/MSR.2019.00081","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8816773","software engineering;runtime monitoring;log parsing","Static analysis;Runtime;Indexes;Production;Manuals","program diagnostics;source code (software);system monitoring","log messages;raw log file;log statement;source code;log parsing;logging environment;static analysis;back log data tracing;Adyen","","2","","32","","29 Aug 2019","","","IEEE","IEEE Conferences"
"Characterizing and Detecting Anti-Patterns in the Logging Code","B. Chen; Z. M. Jiang","AnaLytics & Evaluation (SCALE) Lab., York Univ., Toronto, ON, Canada; AnaLytics & Evaluation (SCALE) Lab., York Univ., Toronto, ON, Canada","2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE)","20 Jul 2017","2017","","","71","81","Snippets of logging code are output statements (e.g., LOG.info or System.out.println) that developers insert into a software system. Although more logging code can provide more execution context of the system's behavior during runtime, it is undesirable to instrument the system with too much logging code due to maintenance overhead. Furthermore, excessive logging may cause unexpected side-effects like performance slow-down or high disk I/O bandwidth. Recent studies show that there are no well-defined coding guidelines for performing effective logging. Previous research on the logging code mainly tackles the problems of where-to-log and what-to-log. There are very few works trying to address the problem of how-to-log (developing and maintaining high-quality logging code). In this paper, we study the problem of how-to-log by characterizing and detecting the anti-patterns in the logging code. As the majority of the logging code is evolved together with the feature code, the remaining set of logging code changes usually contains the fixes to the anti-patterns. We have manually examined 352 pairs of independently changed logging code snippets from three well-maintenance open source systems: ActiveMQ, Hadoop and Maven. Our analysis has resulted in six different anti-patterns in the logging code. To demonstrate the value of our findings, we have encoded these anti-patterns into a static code analysis tool, LCAnalyzer. Case studies show that LCAnalyzer has an average recall of 95% and precision of 60% and can be used to automatically detect previously unknown anti-patterns in the source code. To gather feedback, we have filed 64 representative instances of the logging code anti-patterns from the most recent releases of ten open source software systems. Among them, 46 instances (72%) have already been accepted by their developers.","1558-1225","978-1-5386-3868-2","10.1109/ICSE.2017.15","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7985651","anti-patterns;logging code;logging practices;empirical studies;software maintenance","Tools;Open source software;Software systems;Runtime;Computer crashes;Data mining","data handling;parallel processing;program diagnostics;public domain software;software maintenance;source code (software)","anti-pattern detection;anti-pattern characterization;system behavior;maintenance overhead;high disk I/O bandwidth;logging code snippets;ActiveMQ;Hadoop;Maven;static code analysis tool;LCAnalyzer;source code;open source software systems;how-to-log problem","","18","","47","","20 Jul 2017","","","IEEE","IEEE Conferences"
"Automatic Discovery of Behavioral Models From Software Execution Data","C. Liu","Department of Computer Science and Technology, Shandong University of Science and Technology, Qingdao, China","IEEE Transactions on Automation Science and Engineering","4 Oct 2018","2018","15","4","1897","1908","During the execution of a software system, tremendous amounts of data are recorded, and such data provide valuable information on software runtime behavior analysis. This paper presents an approach on how to utilize process mining as an enabler to discover software behavioral models. To achieve this, we formally define the software event log and its transformation from original software execution data. Essentially, a software event log consists of a set of cases that each is a manifestation of an independent software run. A case is represented as an ordered sequence of events that each refers to a method call. Given the observation that software usually has a hierarchical structure, we first propose an approach to construct a hierarchical software event log from the original flat one by the recursively applying method calling relation detection. Next, using extended process discovery techniques, we discover a software behavioral model, which is represented as a kind of hierarchical Petri net with components, from the hierarchical software event log. We have implemented the proposed approach in the open source process mining toolkit ProM. By using two synthetic software event logs, we show that our approach can deal with infrequent behavior. Moreover, a validation with one real-life software event log shows that our approach can help visualize actual software runtime behavior in an easy-to-understand manner.","1558-3783","","10.1109/TASE.2018.2844725","National Natural Science Foundation of China; Shanghai Science and Technology Development Fund; Taishan Scholar Foundation of Shandong Province; SDUST Research Fund; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8453824","Behavioral model;software dynamic analysis;software execution data;software process mining","Data mining;Software systems;Data models;Runtime;Analytical models;Software architecture","data mining;Petri nets;public domain software;software engineering;system monitoring","software system;software runtime behavior analysis;software behavioral model;software event log;extended process discovery techniques;open source process mining toolkit ProM;software execution data;automatic discovery;hierarchical software event log;method calling relation detection;hierarchical Petri net;synthetic software event logs","","8","","42","","2 Sep 2018","","","IEEE","IEEE Journals"
"Software Analytics for Digital Games","T. Zimmermann","Microsoft Res., Redmond, WA, USA","2015 IEEE/ACM 4th International Workshop on Games and Software Engineering","30 Jul 2015","2015","","","1","2","Software and its development generates an inordinate amount of data. Development activities such as check-ins, work items, bug reports, code reviews, and test executions are recorded in software repositories. User interactions that reflect how customers experience software are recorded in telemetry data, run-time traces, and log files and helps to track application and feature usage and expose performance and reliability. Software analytics takes this data and turns it into actionable insight to better inform decisions related to software. In this talk, I will summarize our efforts in the area of software analytics with a special focus on digital games. I will present several examples of games studies, which we have worked on at Microsoft Research such as how players are engaged in Project Gotham Racing, how skill develops over time in Halo Reach and Forza Motor sports, and the initial experience of game play. I will also point out important differences between games development and traditional software development. The work presented in this talk has been done by Nachi Nagappan, myself, and many others who have visited our group over the past years.","","978-1-4673-7046-2","10.1109/GAS.2015.8","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7169461","software analytics;digital games","Games;Software;Software engineering;Telemetry;Software reliability;Data mining;Committees","computer games;software engineering","software analytics;digital games;check-ins;work items;bug reports;code reviews;test executions;software repositories;telemetry data;run-time traces;log files;Microsoft research;Project Gotham Racing;Halo Reach;Forza Motorsports;software development;Nachi Nagappan","","1","","10","","30 Jul 2015","","","IEEE","IEEE Conferences"
"Semantic Interpretation of Structured Log Files","P. Nimbalkar; V. Mulwad; N. Puranik; A. Joshi; T. Finin","Univ. of Maryland, Baltimore County, Baltimore, MD, USA; GE Global Res., Niskayuna, NY, USA; Univ. of Maryland, Baltimore County, Baltimore, MD, USA; Univ. of Maryland, Baltimore County, Baltimore, MD, USA; Univ. of Maryland, Baltimore County, Baltimore, MD, USA","2016 IEEE 17th International Conference on Information Reuse and Integration (IRI)","19 Dec 2016","2016","","","549","555","Data from computer log files record traces of events involving user activity, applications, system software and network traffic. Logs are usually intended for diagnostic and debugging purposes, but their data can be extremely useful in system audits and forensic investigations. Logs created by intrusion detection systems, Web servers, antivirus and anti-malware systems, firewalls and network devices have information that can reconstruct the activities of malware or a malicious agent, help plan for remediation and prevent attacks by revealing probes or intrusions before damage has been done. While existing tools like Splunk can help analyze logs with known schemas, understanding log whose format is unfamiliar or associated with new device or custom application can be challenging. We describe a framework for analyzing logs and automatically generating a semantic description of their schema and content in RDF. The framework begins by normalizing the log into columns and rows using regular expression-based and dictionary-based classifiers. Leveraging our existing work on inferring the semantics of tables, we associate semantic types with columns and, when possible, map them to concepts in general knowledge-bases (e.g. DBpedia) and domain specific ones (e.g., Unified Cybersecurity Ontology). We link cell values to known type instances (e.g., an IP address) and suggest relationships between columns. Converting large and verbose log files into such semantic representations reveals their meaning and supports search, integration and reasoning over the data.","","978-1-5090-3207-5","10.1109/IRI.2016.81","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7785790","log files;cybersecurity;knowledge graph;linked-data","Resource description framework;Ontologies;Semantics;Computer security;Web servers","firewalls;invasive software;pattern classification","semantic interpretation;structured log files;computer log files;anti-malware systems;antivirus;intrusion detection systems;Web servers;firewalls;network devices;Splunk tool;semantic description;RDF;dictionary-based classifiers;regular expression-based classifiers","","7","1","16","","19 Dec 2016","","","IEEE","IEEE Conferences"
"Towards Providing Automated Supports to Developers on Writing Logging Statements","Z. Li","Concordia University,Montreal,Quebec,Canada","2020 IEEE/ACM 42nd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)","1 Dec 2020","2020","","","198","201","Developers write logging statements to generate logs and record system execution behaviors. Such logs are widely used for a variety of tasks, such as debugging, testing, program comprehension, and performance analysis. However, there exists no practical guidelines on how to write logging statements; hence, making the logging decision a very challenging task. There are two main challenges that developers are facing while making logging decisions: 1) Difficult to accurately and succinctly record execution behaviors; and 2) Hard to decide where to write logging statements. This thesis proposes a series of approaches to address the problems and help developers make logging decisions in two aspects: assist in making decisions on logging contents and on logging locations. Through case studies on large-scale open source and commercial systems, we anticipate that our study will provide useful suggestions and supports to developers for writing better logging statements.","2574-1926","978-1-4503-7122-3","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9270339","log;deep learning;empirical study","Software engineering;Software;Writing;Guidelines;Task analysis;Semantics;Tools","program debugging;program testing;public domain software","logging statements;record system execution behaviors;logging decision;logging contents;logging locations;large-scale open source","","","","17","","1 Dec 2020","","","IEEE","IEEE Conferences"
"A Parallel Approach of Weighted Edit Distance Calculation for Log Parsing","X. Ren; L. Zhang; K. Xie; Q. Dong","National Marine Data and Information Service,Tianjin,China; Nankai University,College of Computer Science,Tianjin,China; Nankai University,College of Computer Science,Tianjin,China; Nankai University,College of Computer Science,Tianjin,China","2019 IEEE 2nd International Conference on Computer and Communication Engineering Technology (CCET)","10 Feb 2020","2019","","","101","104","For modern software systems, larger numbers of log massages have been generated every day. By analyzing these log messages with vital information such as exception reports, developers can manage and monitor software systems efficiently. Each log message in the log file consists of a fixed part (template) and a variable part, and the fixed parts of log messages with one event type are the same, while the variable part are different. LKE (Log Key Extraction), a widely used log parser for analyzing log messages, can find the fixed parts efficiently, due to the cluster strategy base on the calculation of weighted edit distance between log messages. However, it is time-consuming to calculate the weighted edit distance for large scale log files. In this paper, we proposed a parallel approach using a unique hierarchical index structure to calculate the weighted edit distance on GPU (Graph Processing Unit). GPU has an advantage of high parallelism and is suitable for intensive computing, therefore, the time required to process large-scale logs could be reduced by this approach. Experiments show that LKE parser using GPU to calculate the weighted edit distance has high efficiency and accuracy in the HDFS data set and the marine information data set.","","978-1-7281-2871-9","10.1109/CCET48361.2019.8989069","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8989069","weighted edit distance;log parsing;GPU;LKE","","parallel processing;pattern clustering;software development management;system monitoring","parallel approach;large-scale logs;weighted edit distance calculation;Log parsing;log messages;log file;Log Key Extraction;log parser;graph processing unit;GPU;HDFS data set;marine information data set;software system monitoring","","","","10","","10 Feb 2020","","","IEEE","IEEE Conferences"
"A knowledge management system for analysis of organisational log files","C. Teixeira; J. B. de Vasconcelos; G. Pestana","Universidade Europeia, Laureate International Universities, Lisboa, Portugal; Universidade Europeia, Laureate International Universities, Lisboa, Portugal; Universidade Europeia, Laureate International Universities, Lisboa, Portugal","2018 13th Iberian Conference on Information Systems and Technologies (CISTI)","28 Jun 2018","2018","","","1","4","This paper presents a research approach for the analysis of organisational log files. The purpose of log file analysis is to provide information about how the client uses an app (software application) and to detect anomalies that are not perceive by the user so those anomalies can be corrected before the escalation of a problem. The outcome of this research aims to define an architecture to find software flows and apply datamining techniques to measure and obtain knowledge to be recorded in KMS.","","978-989-98434-8-6","10.23919/CISTI.2018.8399229","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8399229","Knowledge Management;Artificial Intelligence techniques;Data Mining;Log file analysis","Software;Data mining;Knowledge management;Artificial intelligence;Organizations;Computer architecture","data mining;knowledge management","knowledge management system;organisational log files;research approach;log file analysis;software application;data mining techniques","","","","11","","28 Jun 2018","","","IEEE","IEEE Conferences"
"Event Indexing and Searching for High Volumes of Event Streams in the Cloud","M. Wang; V. Holub; J. Murphy; P. O'Sullivan","Performance Eng. Lab., Univ. Coll. Dublin, Dublin, Ireland; Performance Eng. Lab., Univ. Coll. Dublin, Dublin, Ireland; Performance Eng. Lab., Univ. Coll. Dublin, Dublin, Ireland; Collaboration Solutions Syst. Test, IBM Software Group, Dublin, Ireland","2012 IEEE 36th Annual Computer Software and Applications Conference","10 Nov 2012","2012","","","405","415","Deployed software applications use log files to keep a record of system events. Log analysis provides support for system administrators to gain the knowledge of system health and behavior. As a result, the ability to efficiently search for patterns in historical events has become a major requirement for timely analysis. Enterprise systems today produce high volumes of log data, regularly in the order of thousands of events per second, which requires to build inverted indexes for quick data retrieval. However, current inverted indexing techniques are rarely designed to handle high volumes of dynamic stream data and often resource consuming. We propose an efficient indexing solution, which reduces the necessary resources by employing bloom filter techniques. The solution builds a generic indexing engine for the Run Time Correlation Engine logging framework to achieve efficient monitoring in the Cloud. In particular, our solution is able to deliver significant performance improvement over existing indexing engines.","0730-3157","978-0-7695-4736-7","10.1109/COMPSAC.2012.60","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6340190","Event Indexing;Event Searching;Event Stream","Silicon;Indexing;Engines;Arrays;Data analysis","cloud computing;data analysis;data mining;indexing;information filters;information retrieval;pattern clustering;search engines","event indexing;event searching;event stream;cloud computing;software application deployement;log file analysis;historical event;pattern searching;enterprise system;data retrieval;inverted indexing technique;dynamic stream data handling;bloom filter technique;indexing engine;run time correlation engine logging framework","","","10","30","","10 Nov 2012","","","IEEE","IEEE Conferences"
"Understanding Log Lines Using Development Knowledge","W. Shang; M. Nagappan; A. E. Hassan; Z. M. Jiang","Sch. of Comput., Queen's Univ., Kingston, ON, Canada; Sch. of Comput., Queen's Univ., Kingston, ON, Canada; Sch. of Comput., Queen's Univ., Kingston, ON, Canada; Dept. of Electr. Eng. & Comput. Sci., York Univ., Toronto, ON, Canada","2014 IEEE International Conference on Software Maintenance and Evolution","6 Dec 2014","2014","","","21","30","Logs are generated by output statements that developers insert into the code. By recording the system behaviour during runtime, logs play an important role in the maintenance of large software systems. The rich nature of logs has introduced a new market of log management applications (e.g., Splunk, XpoLog and log stash) that assist in storing, querying and analyzing logs. Moreover, recent research has demonstrated the importance of logs in operating, understanding and improving software systems. Thus log maintenance is an important task for the developers. However, all too often practitioners (i.e., operators and administrators) are left without any support to help them unravel the meaning and impact of specific log lines. By spending over 100 human hours and manually examining all the email threads in the mailing list for three open source systems (Hadoop, Cassandra and Zookeeper) and performing web search on sampled logging statements, we found 15 email inquiries and 73 inquiries from web search about different log lines. We identified that five types of development knowledge that are often sought from the logs by practitioners: meaning, cause, context, impact and solution. Due to the frequency and nature of log lines about which real customers inquire, documenting all the log lines or identifying which ones to document is not efficient. Hence in this paper we propose an on-demand approach, which associates the development knowledge present in various development repositories (e.g., code commits and issues reports) with the log lines. Our case studies show that the derived development knowledge can be used to resolve real-life inquiries about logs.","1063-6773","978-1-4799-6146-7","10.1109/ICSME.2014.24","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6976068","Software maintenance;Software Logs;Program understanding","Web search;Electronic mail;Context;Google;Software systems;Knowledge engineering;Engines","Internet;public domain software;software maintenance;system monitoring","log lines;development knowledge;log maintenance;open source systems;Web search","","21","","37","","6 Dec 2014","","","IEEE","IEEE Conferences"
"A Stepwise Approach Towards an Interoperable and Flexible Logging Principle for Audit Trails","D. Huemer; A. M. Tjoa","Inst. of Software Technol. & Interactive Syst., Vienna Univ. of Technol., Vienna, Austria; Inst. of Software Technol. & Interactive Syst., Vienna Univ. of Technol., Vienna, Austria","2010 Seventh International Conference on Information Technology: New Generations","1 Jul 2010","2010","","","114","119","Although event recording on a computer system (also known as logging) is of utmost importance for reconstructing and detecting security relevant events, currently no adequate and sophisticated solution for complex environments, such as Grid and Cloud Computing, exist. Current LOG file formats lack of several important factors, hindering automatic evaluation needed for distributed systems to comply with laws and regulations or hindering detection of security breaches. In this paper we present a new concept utilizing XML technology to solve mentioned problems of current LOG file formats and describe the benefits of this new idea resulting in a flexible, interoperable LOG environment enabling automatic evaluation even in locally dispersed computing systems. As a result reliable, robust and consistent LOG files are produced allowing for automatic evaluation.","","978-1-4244-6271-1","10.1109/ITNG.2010.33","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5501447","logging;audit trail;grid computing;cloud computing;XML","Cloud computing;Grid computing;XML;Data security;Power system security;Authorization;Event detection;Information technology;Interactive systems;Computer security","grid computing;Internet;security of data;system monitoring;XML","flexible logging principle;interoperable logging principle;audit trails;computer system;grid computing;cloud computing;distributed systems;security breaches;XML technology","","2","1","17","","1 Jul 2010","","","IEEE","IEEE Conferences"
"Investigation of LabVIEW and Syslog Interface using In-house Developed Generic Logging Software","G. Kaur; R. Sugandhi; S. Trivedi; P. Srivastav; L. M. Awasthi","Institute for Plasma Research,Large Volume Plasma Device,Gandhinagar,India; Institute for Plasma Research,Large Volume Plasma Device,Gandhinagar,India; Institute for Plasma Research,Project Management Cell,Gandhinagar,India; Institute for Plasma Research,Large Volume Plasma Device,Gandhinagar,India; Institute for Plasma Research,Large Volume Plasma Device,Gandhinagar,India","2019 Innovations in Power and Advanced Computing Technologies (i-PACT)","16 Jan 2020","2019","1","","1","5","Status logging using standards such as Syslog is widely recommended and used across various industry verticals. LabVIEW is a widely used data acquisition and control platform for scientific experiments. The integration of LabVIEW and Syslog technologies for status, error and event logging is a topic of considerable interest. This paper discusses the investigation of interface and development of a generic logger software in LabVIEW which provides facility to record messages formatted in RFC 3164 format into a syslog server and benchmark the performance of data transfer. Motivation of this work is driven by machine control system mandate of the large volume plasma device but on larger domain, this can be used in applications concerned to the log management for scientific experiments. The paper discusses motivation, literature survey, software development artifacts and benchmarking results.","","978-1-5386-8190-9","10.1109/i-PACT44901.2019.8959972","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8959972","Log management;Virtual Instrumentation;Process Control;Syslog protocol","","authorisation;data acquisition;data loggers;system monitoring;virtual instrumentation;Web services","LabVIEW;control platform;event logging;data transfer;machine control system;log management;software development artifacts;Syslog interface;generic logging software;industry verticals","","","","13","","16 Jan 2020","","","IEEE","IEEE Conferences"
"Software Logs for Machine Learning in a DevOps Environment","N. Bosch; J. Bosch","Ericsson AB,Development Unit Radio Services & Infra Analytics (DSI Analytics),Gothenburg,Sweden; Chalmers University of Technology,Department of Computer Science and Engineering,Gothenburg,Sweden","2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)","16 Oct 2020","2020","","","29","33","System logs perform a critical function in software-intensive systems as logs record the state of the system and significant events in the system at important points in time. Unfortunately, log entries are typically created in an ad-hoc, unstructured and uncoordinated fashion, limiting their usefulness for analytics and machine learning. In a DevOps environment, especially, unmanaged evolution in log data structure causes frequent disruption of operations in automated data pipelines, dashboards and analytics. In this paper, we present the main challenges of contemporary approaches to generating, storing and managing the evolution of system logs data for large, complex, software-intensive systems based on an in-depth case study at a world-leading telecommunications company. Second, we present an approach for generating and managing the evolution of log data in a DevOps environment that does not suffer from the aforementioned challenges and is optimized for use in machine learning. Third, we provide validation of the approach based on expert interviews that confirm that the approach addresses the identified challenges and problems.","","978-1-7281-9532-2","10.1109/SEAA51224.2020.00016","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9226340","System logs;machine learning;data preprocessing;data generation;DevOps","Machine learning;Companies;Software;Research and development;Standards;Data models;Training","data analysis;data mining;data structures;learning (artificial intelligence);software engineering;system monitoring","automated data pipeline;log data structure;software logs;machine learning;DevOps environment;software-intensive systems;dashboards","","","","18","","16 Oct 2020","","","IEEE","IEEE Conferences"
"File Operations Information Collecting Software Package Used in the Information Security Incidents Investigation","N. Gaidamakin; R. Gibilinda; N. Sinadskiy","Ural Federal University,Yekaterinburg,Russia; Ural Federal University,Yekaterinburg,Russia; Ural Federal University,Yekaterinburg,Russia","2020 Ural Symposium on Biomedical Engineering, Radioelectronics and Information Technology (USBEREIT)","16 Jun 2020","2020","","","559","562","The article describes file operations information collecting software package, that can be implemented in information system structure during information security incidents investigation. The package allows to collect and systematize accomplished file operations on all nodes of the information system structure, using the information obtained from the volume changes log - $UsnJrnl. This log possesses almost comprehensive information about files and actions that performed in relation to them. In addition, this log has several advantages compared to other logs: completeness, reliability and file identification information extraction speed. Software package also works as storage with file operations information backup availability that helps to speed up and simplify the process of information security incidents investigation. Stored information can be processed by SIEM system. It is necessary to say that SIEM system should be able to expand its functionality using e.g. scripts. Main goals of software package and SIEM system interaction are constructing a time line of information security events to accelerate the elimination of the incident consequences, and provide recommendations for applied information protection measures improving.","","978-1-7281-3165-8","10.1109/USBEREIT48449.2020.9117671","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9117671","information security incident;incident investigation;file operation;$UsnJrnl volume change log","","data protection;security of data;software packages;storage management;system monitoring","information security incidents;information system structure;file identification information extraction speed;information storage;SIEM system;information security events;information protection;file operations information collection;software package","","","","5","","16 Jun 2020","","","IEEE","IEEE Conferences"
"A .NET application searching for data in a log file of the KUKA industrial welding robot","I. Košťál","University of Economics in Bratislava, Faculty of Economic Informatics, Dolnozemská cesta 1, 852 35, Slovakia","Proceedings of the 16th International Conference on Mechatronics - Mechatronika 2014","26 Jan 2015","2014","","","656","661","In a robotic workstation designed and created from KUKA industrial robots, manipulators and its control, each of these robots is equipped with some tool, for example, welding pliers. This tool is mounted on the mounting flange of a robot. During tool calibration, the robot operator assigns the working point to this tool. This point is called the TCP (Tool Center Point). All motions of the TCP are controlled by a robot program. By this is addressed through which points the TCP passes during its motion. Each particular KUKA industrial robot of an assembled robotic workstation is tested by the operator in the test operation. The precision of the TCP motion is measured and it is represented by points through which the TCP passes. During testing of a certain activity of a robot it happens many times that the operator finds that the TCP of a robot tool does not perform motions along the prescribed path with a sufficient precision. Therefore he must add other points to this path to refine it. It happens also often that during testing of a certain activity of a robot the operator must change the coordinates of some points, sometimes several times, to refine or to adjust the motion path of the TCP of a robot tool. These additional and modified points identify problematic spots of a motion path of the TCP. The operator performs all additions of new points and changes of the coordinates of existing points of the TCP path through the KCP (KUKA Control Panel) teach pendant in a robot program in the KUKA Robot Language (KRL) and the KUKA System Software (KSS) records them into the appropriate files to the hard drive of the robot control PC. In addition, all the operator actions on the KCP are automatically logged by KSS and he can generate the robot log file Logbuch.txt by KSS. All added and modified path points of the TCP in the robot program during the test operation of the robot are recorded into this log file, too. These points provide important information about the TCP path sections for the operator, with which were problems during testing of a certain activity of the robot. During retesting of this activity of the robot it is therefore necessary to focus specially on these sections of the path, i.e. on the points forming these sections of the path. Thus, the operator quickly needs find out which points he added to the TCP path and also coordinates of points of this path which were repeatedly modified in the robot control program. We have created the .NET search application that is able to connect to the control PC of the KUKA industrial welding robot and find on its disk the log file Logbuch.txt. Then, in this file, according to the user instructions it searches for and summarizes relevant new founded and repeatedly modified the TCP path points in the robot program during test operation of selected activity of a robot. The paper deals with data files of the robot program in which are stored coordinates of the path points of the TCP robot tool, and with outputs of our .NET search application searching this log file of the KUKA industrial welding robot.","","978-80-214-4816-2","10.1109/MECHATRONIKA.2014.7018338","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7018338","robot;Tool Center Point;KRL program;motion programming;log file;testing and retesting robot","Robot kinematics;Service robots;Workstations;Testing;Approximation methods;Robot control","calibration;industrial robots;motion control;motion measurement;robot programming;robotic assembly;robotic welding",".NET search application;KUKA industrial welding robot log file;robotic workstation;KUKA industrial manipulators;welding pliers;mounting flange;tool calibration;robot operator;tool center point;TCP motion control;robot program;assembled robotic workstation;TCP motion measurement;robot tool;robot activity;KCP;KUKA control panel;KUKA robot language;KRL;KUKA system software;KSS;robot control PC;TCP path sections;robot control program","","3","","4","","26 Jan 2015","","","IEEE","IEEE Conferences"
"PADLA: A Dynamic Log Level Adapter Using Online Phase Detection","T. Mizouchi; K. Shimari; T. Ishio; K. Inoue","Graduate School of Information Science and Technology, Osaka University, Osaka, Japan; Graduate School of Information Science and Technology, Osaka University, Osaka, Japan; Graduate School of Science and Technology, Nara Institute of Science and Technology, Nara, Japan; Graduate School of Information Science and Technology, Osaka University, Osaka, Japan","2019 IEEE/ACM 27th International Conference on Program Comprehension (ICPC)","29 Aug 2019","2019","","","135","138","Logging is an important feature for a software system to record its run-time information. Although detailed logs are helpful to identify the cause of a failure in a program execution, constantly recording detailed logs of a long-running system is challenging because of its performance overhead and storage cost. To solve the problem, we propose PADLA (Phase-Aware Dynamic Log Level Adapter) that dynamically adjusts the log level of a running system so that the system can record irregular events such as performance anomalies in detail while recording regular events concisely. PADLA is an extension of Apache Log4j, one of the most popular logging framework for Java. It employs an online phase detection algorithm to recognize irregular events. It monitors run-time performance of a system and learns regular execution phases of a program. If it recognizes a performance anomalies, it automatically changes the log level of a system to record the detailed behavior. In the case study, PADLA successfully recorded a detailed log for performance analysis of a server system under high load while suppressing the amount of log data and performance overhead.","2643-7171","978-1-7281-1519-1","10.1109/ICPC.2019.00029","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8813251","Dynamic Analysis, Log Level, Log4j, Phase Detection, Performance Anomaly","","Java;program diagnostics;public domain software;system recovery","performance overhead;PADLA;software system;run-time information;Phase-Aware Dynamic Log Level Adapter;irregular events;performance anomalies;Apache Log4j;online phase detection algorithm;run-time performance;regular execution phases;server system;log data;logging framework","","","","22","","29 Aug 2019","","","IEEE","IEEE Conferences"
"Towards More Trustable Log Files for Digital Forensics by Means of “Trusted Computing”","B. Böck; D. Huemer; A. M. Tjoa","Secure Bus. Austria, Vienna, Austria; Inst. of Software Technol. & Interactive Syst., Vienna Univ. of Technol., Vienna, Austria; Inst. of Software Technol. & Interactive Syst., Vienna Univ. of Technol., Vienna, Austria","2010 24th IEEE International Conference on Advanced Information Networking and Applications","1 Jun 2010","2010","","","1020","1027","Trustable log data is essential in digital forensic investigations in order to allow reliable reconstruction of events. Existing solutions do not provide adequate protection, exposing the log-producing application to software-based attacks. In this paper we provide a solution based on Trusted Computing using a Trusted Platform Module (TPM) and AMD's Secure Virtual Machine technology (SVM). While current solutions only protect against manipulation of existing logs, we go one step further by establishing hardware-based trust in the log producing application. Our solution ensures confidentiality, integrity and non-repudiation during creation, storage and transmission of log data.","2332-5658","978-1-4244-6696-2","10.1109/AINA.2010.26","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5474823","trust;log;logging;authentication;validation;tpm;trusted;computing;svm","Digital forensics;Application software;Authentication;Protection;Hardware;Virtual machining;Support vector machines;Public key cryptography;Computer networks;Interactive systems","computer forensics;virtual machines","towards more trustable log files;digital forensics;trusted computing;trustable log data;reliable reconstruction;software based attacks;trusted platform module;TPM;secure virtual machine technology;SVM","","10","","35","","1 Jun 2010","","","IEEE","IEEE Conferences"
"DeepLV: Suggesting Log Levels Using Ordinal Based Neural Networks","Z. Li; H. Li; T. -H. P. Chen; W. Shang","Concordia University,Department of Computer Science and Software Engineering,Montreal,Canada; Polytechnique Montréal,Department of Computer Engineering and Software Engineering,Montreal,Canada; Concordia University,Department of Computer Science and Software Engineering,Montreal,Canada; Concordia University,Department of Computer Science and Software Engineering,Montreal,Canada","2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)","7 May 2021","2021","","","1461","1472","Developers write logging statements to generate logs that provide valuable runtime information for debugging and maintenance of software systems. Log level is an important component of a logging statement, which enables developers to control the information to be generated at system runtime. However, due to the complexity of software systems and their runtime behaviors, deciding a proper log level for a logging statement is a challenging task. For example, choosing a higher level (e.g., error) for a trivial event may confuse end users and increase system maintenance overhead, while choosing a lower level (e.g., trace) for a critical event may prevent the important execution information to be conveyed opportunely. In this paper, we tackle the challenge by first conducting a preliminary manual study on the characteristics of log levels. We find that the syntactic context of the logging statement and the message to be logged might be related to the decision of log levels, and log levels that are further apart in order (e.g., trace and error) tend to have more differences in their characteristics. Based on this, we then propose a deep-learning based approach that can leverage the ordinal nature of log levels to make suggestions on choosing log levels, by using the syntactic context and message features of the logging statements extracted from the source code. Through an evaluation on nine large-scale open source projects, we find that: 1) our approach outperforms the state-of-the-art baseline approaches; 2) we can further improve the performance of our approach by enlarging the training data obtained from other systems; 3) our approach also achieves promising results on cross-system suggestions that are even better than the baseline approaches on within-system suggestions. Our study highlights the potentials in suggesting log levels to help developers make informed logging decisions.","1558-1225","978-1-6654-0296-5","10.1109/ICSE43902.2021.00131","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9402068","logs;deep learning;log level;empirical study","Runtime;Manuals;Syntactics;Maintenance engineering;Software systems;Feature extraction;Task analysis","","","","","","66","","7 May 2021","","","IEEE","IEEE Conferences"
"Runtime Verification for Detecting Suspension Bugs in Multicore and Parallel Software","S. A. Asadollah; D. Sundmark; H. Hansson","Malardalen Univ., Västerás, Sweden; Malardalen Univ., Västerás, Sweden; Malardalen Univ., Västerás, Sweden","2017 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","17 Apr 2017","2017","","","77","80","Multicore hardware development increases the popularity of parallel and multicore software, while testing and debugging the software become more difficult, frustrating and costly. Among all types of software bugs, concurrency bugs are both important and troublesome. This type of bugs is increasingly becoming an issue, particularly due to the growing prevalence of multicore hardware. Suspension-based-locking bug is one type of concurrency bugs. This position paper proposes a model based on runtime verification and reflection technique in the context of multicore and parallel software to monitor and detect suspension-based-locking bugs. The model is not only able to detect faults, but also diagnose and even repair them. The model is composed of four layers: Logging, Monitoring, Suspension Bug Diagnosis and Mitigation. The logging layer will observe the events and save them into a file system. The monitoring layer will detect the presents of bugs in the software. The suspension bug diagnosis will identify Suspension bugs by comparing the captured data with the suspension bug properties. Finally, the mitigation layer will reconfigure the software to mitigate the suspension bugs. A functional architecture of a runtime verification tool is also proposed in this paper. This architecture is based on the proposed model and is comprised of different modules.","","978-1-5090-6676-6","10.1109/ICSTW.2017.20","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7899037","Suspension-based-locking bug;parallel application;multicore software;Runtime Verification;monitoring;debugging;concurrency bugs","Computer bugs;Multicore processing;Software;Suspensions;Runtime;Monitoring","concurrency (computers);formal verification;multiprocessing systems;parallel programming;program debugging;program diagnostics","suspension bug detection;parallel software;multicore hardware development;multicore software;software debugging;software bugs;concurrency bugs;suspension-based-locking bug;reflection technique;logging layer;monitoring layer;suspension bug diagnosis;mitigation layer;runtime verification tool","","","","12","","17 Apr 2017","","","IEEE","IEEE Conferences"
"A Blockchain-Based Framework for Secure Log Storage","W. Huang","Tongji University,School of Software Engineering,Shanghai,China","2019 IEEE 2nd International Conference on Computer and Communication Engineering Technology (CCET)","10 Feb 2020","2019","","","96","100","Logs are critical data, which can help us to troubleshoot and identify the person in charge of an unexpected accident. Log systems have been widely used for log storage. However, the traditional log system can't prevent the log from being tampered. Centralized servers are more vulnerable to be attacked. Those who have permission to operate records can easily tamper with logs. Blockchain is a disruptive technology in recent years, which has the advantages of decentralization, tamper-proof and traceability. Given its decentralization and tamper-proof properties, the paper proposed a blockchain-based framework for secure log storage. However, the cost of storing big files in the blockchain is very high. Thus, the paper utilized the InterPlanetary File System(IPFS) to store log files instead of a blockchain. Besides, the paper adopted Ethereum blockchain to store the hash of log files and a smart contract to create an index for log files. The solution is not only applicable to log but also other scenarios requiring secure data storage and efficient retrieval.","","978-1-7281-2871-9","10.1109/CCET48361.2019.8989093","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8989093","Log Storage;Blockchain;IPFS;Ethereum;Smart Contract","","cryptography;data loggers;distributed databases;storage management","blockchain-based framework;secure log storage;log systems;tamper-proof properties;log files;Ethereum blockchain;secure data storage;interplanetary file system;smart contract","","","","16","","10 Feb 2020","","","IEEE","IEEE Conferences"
"Research on Remote Monitoring Technology of Operating State Software of Charging Pile","S. Yang; L. Ding",Nanjing University of Science and Technology; Nanjing University of Science and Technology,"2019 International Conference on Electronic Engineering and Informatics (EEI)","13 Feb 2020","2019","","","299","303","At present, the number of charging piles is increasing and the distribution position is discrete. It is difficult to manage and maintain after being put into use. For the abnormal work caused by the software operation problem in the pile, this paper proposes a remote monitoring operation mode software method. By monitoring information to analyze problems that occur during software operation, it is easy to maintain and reduce operating costs. According to the hardware and software architecture of the charging pile, this article takes the event as the monitoring object, records it in the work log according to the time of occurrence and uploads it to the background. The log content is the basis of software maintenance. For the log does not involve the information, through the background remote view function to obtain, which enhances the flexibility of maintenance. In addition, the fault warning function is added to realize the fault information reporting and fault generation process.","","978-1-7281-4076-6","10.1109/EEI48997.2019.00072","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8991059","component;charging pile;remote monitoring;running state software;software maintenance","","fault diagnosis;software architecture;software maintenance;system monitoring","remote monitoring technology;operating state software;distribution position;software operation problem;remote monitoring operation mode software method;operating costs;software architecture;work log;software maintenance;background remote view function","","","","11","","13 Feb 2020","","","IEEE","IEEE Conferences"
"Evaluating Coding Behavior in Software Development Processes: A Process Mining Approach","P. Ardimento; M. L. Bernardi; M. Cimitile; F. M. Maggi",University of Bari; Giustino Fortunato; Unitelma Sapienza; University of Tartu,"2019 IEEE/ACM International Conference on Software and System Processes (ICSSP)","26 Aug 2019","2019","","","84","93","Process mining is a family of techniques that aim at analyzing business process execution data recorded in event logs. Conformance checking is a branch of this discipline embracing approaches for verifying whether the behavior of a process, as recorded in a log, is in line with some expected behavior provided in the form of a process model. In the literature, process mining techniques have already been used to study software development processes starting from logs derived from version management systems or from document management systems. In this paper, we use conformance checking to test coding behaviors starting from event logs generated from IDE usage. Understanding how developers carry out coding activities and what hurdles they usually face should provide useful tips for improving and supporting software development processes. In particular, through conformance checking, we can compare different process executions, and identify behavioral similarities and differences. In our experimentation, we evaluated the activities performed by 40 novice developers performing coding activities in 5 development sessions. We assessed the developers to distinguish the ones obtaining the best performance. We then compared the behavior extracted from this group of developers with the others. The results show different IDE usage patterns for developers with different skills and performance.","","978-1-7281-3393-5","10.1109/ICSSP.2019.00020","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8812844","Software Development Process, Process Mining, Conformance Checking, IDE Logging","Encoding;Unified modeling language;Software;Data mining;Tools;Task analysis","business data processing;data mining;document handling;program testing;programming environments;software development management;software maintenance","process mining approach;business process execution data;event logs;conformance checking;version management systems;document management systems;coding behavior evaluation;software development processes;coding behaviors testing;IDE usage patterns","","3","","24","","26 Aug 2019","","","IEEE","IEEE Conferences"
"Enhancing HPC System Log Analysis by Identifying Message Origin in Source Code","M. Hickman; D. Fulp; E. Baseman; S. Blanchard; H. Greenberg; W. Jones; N. DeBardeleben","Ultrascale Syst. Res. Center, Los Alamos Nat. Lab. High Performance Comput., Los Alamos, NM, USA; Ultrascale Syst. Res. Center, Los Alamos Nat. Lab. High Performance Comput., Los Alamos, NM, USA; Ultrascale Syst. Res. Center, Los Alamos Nat. Lab. High Performance Comput., Los Alamos, NM, USA; Ultrascale Syst. Res. Center, Los Alamos Nat. Lab. High Performance Comput., Los Alamos, NM, USA; Ultrascale Syst. Res. Center, Los Alamos Nat. Lab. High Performance Comput., Los Alamos, NM, USA; Dept. of Comput. Sci., Coastal Carolina Univ., Conway, SC, USA; Ultrascale Syst. Res. Center, Los Alamos Nat. Lab. High Performance Comput., Los Alamos, NM, USA","2018 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)","18 Nov 2018","2018","","","100","105","Supercomputers, high performance computers, and clusters are composed of very large numbers of independent operating systems that are generating their own system logs. Messages are generated locally on each host and usually are transferred to a central logging infrastructure which keeps a master record of the system as a whole. At Los Alamos National Laboratory (LANL) a collection of open source cloud tools are used which log over a hundred million system log messages per day from over a dozen such systems. Understanding what source code created those messages can be extremely useful to system administrators when they are troubleshooting these complex systems as it can give insight into a subsystem (disk, network, etc.) or even line numbers of source code. Oftentimes, debugging supercomputers is done in environments where open access cannot be provided to all individuals due to security concerns. As such, providing a means for conveying information between system log messages and source code lines allows for communication between system administrators and source developers or supercomputer vendors. In this work, we demonstrate a prototype tool which aims to provide such an expert system. We leverage capabilities from ElasticSearch, one of the open source cloud tools deployed at LANL, and with our own metrics develop a means for correctly matching source code lines as well as files with high confidence. We discuss confidence metrics and show that in our experiments 92% of syslog lines were correctly matched. For any future samples, we predict with 95% confidence that the correct file will be detected between 88.2% and 95.8% of the time. Finally, we discuss enhancements that are underway to improve the tool and study it on a larger dataset.","","978-1-5386-9443-5","10.1109/ISSREW.2018.00-23","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8539171","system logs;source code analysis;ElasticSearch;Logan;expert system","Tools;Kernel;Supercomputers;Measurement;Indexes;Linux","cloud computing;operating systems (computers);parallel machines;parallel processing;program debugging;public domain software;software tools;source code (software);system monitoring","source code lines;operating systems;message origin identification;supercomputer debugging;LANL;ElasticSearch;Los Alamos National Laboratory;central logging infrastructure;system logs;high performance computers;HPC system log analysis;open source cloud tools;expert system;supercomputer vendors","","2","","11","","18 Nov 2018","","","IEEE","IEEE Conferences"
"How Is Logging Practice Implemented in Open Source Software Projects? A Preliminary Exploration","G. Rong; S. Gu; H. Zhang; D. Shao; W. Liu","State Key Lab. of Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. of Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. of Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. of Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. of Novel Software Technol., Nanjing Univ., Nanjing, China","2018 25th Australasian Software Engineering Conference (ASWEC)","27 Dec 2018","2018","","","171","180","Background: Logs are the footprints that software systems produce during runtime, which can be used to understand the dynamic behavior of these software systems. To generate logs, logging practice is accepted by developers to place logging statements in the source code of software systems. Compared to the great number of studies on log analysis, the research on logging practice is relatively scarce, which raises a very critical question, i.e. as the original intention, can current logging practice support capturing the behavior of software systems effectively? Aims: To answer this question, we first need to understand how logging practices are implemented these software projects. Method: In this paper, we carried out an empirical study to explore the logging practice in open source software projects so as to establish a basic understanding on how logging practice is applied in real world software projects. The density, log level (what to log?) and context (where to log?) are measured for our study. Results: Based on the evidence we collected in 28 top open source projects, we find the logging practice is adopted highly inconsistently among different developers both across projects and even within one project in terms of the density and log levels of logging statements. However, the choice of what context the logging statements to place is consistent to a fair degree. Conclusion: Both the inconsistency in density and log level and the convergence of context have forced us to question whether it is a reliable means to understand the runtime behavior of software systems via analyzing the logs produced by the current logging practice.","2377-5408","978-1-7281-1241-1","10.1109/ASWEC.2018.00031","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8587302","log, logging practice, empirical study, Java-based","Tools;Software systems;Measurement;Runtime;Reliability","public domain software;software engineering;system monitoring","logging statements;software systems;log analysis;open source software projects;logging practice support","","1","","30","","27 Dec 2018","","","IEEE","IEEE Conferences"
"Multi-threaded evolution of the data-logging system of the ATLAS experiment at CERN","T. Colombo; W. Vandelli","University of Pavia, Italy; European Organization for Nuclear Research (CERN), Italy","2011 IEEE Nuclear Science Symposium Conference Record","20 Feb 2012","2011","","","2039","2043","The ATLAS experiment observes proton-proton collisions delivered by the LHC accelerator at a centre of mass energy of 7 TeV with a peak luminosity of ~ 1033 cm-2s-1 in 2011. The ATLAS Trigger and Data Acquisition (TDAQ) system selects interesting events on-line in a three-level trigger system in order to store them at a budgeted average rate of ~ 400 Hz for an event size of ~ 1.2 MB. This paper focuses on the TDAQ data-logging system. Its purpose is to receive events from the third level trigger, process them and stream the data into different raw files according to the trigger decision. The system currently in production is based on an essentially single-threaded design that is anticipated not to cope with the increase in event rate and event size foreseen as part of the ATLAS and LHC upgrade programs. This design also severely limits the possibility of performing additional CPU-intensive tasks. Therefore, a novel design able to exploit the full power of multi-core architecture is needed. The main challenge of such a design is the conflict between the largely parallel nature of the data-logging event processing and the constraint of sequential file writing. In this paper, we present a thread-pool based implementation of the TDAQ data-logging software. We report here on the functionality and performance of the new system and on our development experience.","1082-3654","978-1-4673-0120-6","10.1109/NSSMIC.2011.6154415","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6154415","","Random access memory;Data acquisition;Production;Acceleration","high energy physics instrumentation computing;position sensitive particle detectors","TDAQ data-logging system;ATLAS experiment;multithreaded evolution;proton-proton collisions;LHC accelerator;Trigger and Data Acquisition;ATLAS TDAQ system;three-level trigger system;raw files;trigger decision;ATLAS upgrade program;LHC upgrade program;CPU-intensive tasks","","","","9","","20 Feb 2012","","","IEEE","IEEE Conferences"
"Instant Recovery with Write-Ahead Logging: Page Repair, System Restart, Media Restore, and System Failover, Second Edition","G. Graefe; W. Guy; C. Sauer","Goetz Graefe has been a professor, product architect, and industrial researcher since 1987. Like other database vendors, Microsoft SQL Server adopted his designs for query optimization and query execution. He has published tutorial surveys on query execution, sorting, b-tree indexing, concurrency control, logging and recovery, as well as numerous novel techniques and research results in query processing and transactional data storage.; Wey Guy is an independent software engineer. She earned a Master’s degree in computer science from the University of Iowa and spent 15 years with Microsoft’s SQL Server development team before she became an independent software developer in 2011. She has been working with Hewlett Packard Labs since 2012.; Caetano Sauer is a doctoral candidate in computer science at the Technical University of Kaiserslautern, Germany. He earned his M.Sc. degree in 2012 at the same institution. While his interests and experience cover all components of transactional storage and indexing, his research focuses on logging and recovery algorithms. He is advised by Prof. Theo Härder and Dr. Goetz Graefe.","Instant Recovery with Write-Ahead Logging: Page Repair, System Restart, Media Restore, and System Failover, Second Edition","","2016","","","",""," Traditional theory and practice of write-ahead logging and of database recovery focus on three failure classes: transaction failures (typically due to deadlocks) resolved by transaction rollback; system failures (typically power or software faults) resolved by restart with log analysis, ""redo,"" and ""undo"" phases; and media failures (typically hardware faults) resolved by restore operations that combine multiple types of backups and log replay.   The recent addition of single-page failures and single-page recovery has opened new opportunities far beyond the original aim of immediate, lossless repair of single-page wear-out in novel or traditional storage hardware. In the contexts of system and media failures, efficient single-page recovery enables on-demand incremental ""redo"" and ""undo"" as part of system restart or media restore operations. This can give the illusion of practically instantaneous restart and restore: instant restart permits processing new queries and updates seconds after system reboot and instant restore permits resuming queries and updates on empty replacement media as if those were already fully recovered. In the context of node and network failures, instant restart and instant restore combine to enable practically instant failover from a failing database node to one holding merely an out-of-date backup and a log archive, yet without loss of data, updates, or transactional integrity.   In addition to these instant recovery techniques, the discussion introduces self-repairing indexes and much faster offline restore operations, which impose no slowdown in backup operations and hardly any slowdown in log archiving operations. The new restore techniques also render differential and incremental backups obsolete, complete backup commands on a database server practically instantly, and even permit taking full up-to-date backups without imposing any load on the database server.   Compared to the first version of this book, this second edition adds sections on applications of single-page repair, instant restart, single-pass restore, and instant restore. Moreover, it adds sections on instant failover among nodes in a cluster, applications of instant failover, recovery for file systems and data files, and the performance of instant restart and instant restore.","","9781627054201","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/xpl/ebooks/bookPdfWithBanner.jsp?fileName=7464100.pdf&bkn=7464099&pdfType=book","algorithms;databases;transactions;failures;recovery;availability;reliability;write-ahead logging;instant restart;log analysis;redo;undo;rollback;compensation;log replay;instant restore;single-pass restore;virtual backup;big data;file systems;key-value stores;clusters;log shipping;failover;elasticity;failover pool","","","","","","","","","11 May 2016","","","Morgan & Claypool","Morgan and Claypool eBooks"
"HitAnomaly: Hierarchical Transformers for Anomaly Detection in System Log","S. Huang; Y. Liu; C. Fung; R. He; Y. Zhao; H. Yang; Z. Luan","School of Computer Science and Engineering, Sino-German Joint Software Institute, Beihang University, Beijing, China; School of Computer Science and Engineering, Sino-German Joint Software Institute, Beihang University, Beijing, China; Computer Science Department, Virginia Commonwealth University, Richmond, VA, USA; Chinese Academy of Scientific Computer Network Information Center, Beijing, China; Chinese Academy of Scientific Computer Network Information Center, Beijing, China; School of Computer Science and Engineering, Sino-German Joint Software Institute, Beihang University, Beijing, China; School of Computer Science and Engineering, Sino-German Joint Software Institute, Beihang University, Beijing, China","IEEE Transactions on Network and Service Management","9 Dec 2020","2020","17","4","2064","2076","Enterprise systems often produce a large volume of logs to record runtime status and events. Anomaly detection from system logs is crucial for service management and system maintenance. Most existing log-based anomaly detection methods use log event indexes parsed from log data to detect anomalies. Those methods cannot handle unseen log templates and lead to inaccurate anomaly detection. Some recent studies focused on the semantics of log templates but ignored the information of parameter values. Therefore, their approaches failed to address the abnormal logs caused by parameter values. In this article, we propose HitAnomaly, a log-based anomaly detection model utilizing a hierarchical transformer structure to model both log template sequences and parameter values. We designed a log sequence encoder and a parameter value encoder to obtain their representations correspondingly. We then use an attention mechanism as our final classification model. In this way, HitAnomaly is able to capture the semantic information in both log template sequence and parameter values and handle various types of anomalies. We evaluated our proposed method on three log datasets. Our experimental results demonstrate that HitAnomaly has outperformed other existing log-based anomaly detection methods. We also assess the robustness of our proposed model on unstable log data.","1932-4537","","10.1109/TNSM.2020.3034647","National Key Research & Development Program of China; High Performance Computing Key Project; National Natural Science Foundation of China; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9244088","Log data analysis;anomaly detection;hierarchical transformers","Anomaly detection;Semantics;Indexes;Runtime;Task analysis;Transforms;Maintenance engineering","pattern classification;security of data;system monitoring","parameter value;abnormal logs;HitAnomaly;log-based anomaly detection model;log template sequence;log sequence encoder;system log;enterprise systems;service management;system maintenance;classification model;semantic information;attention mechanism","","","","37","IEEE","29 Oct 2020","","","IEEE","IEEE Journals"
"Using Log Files as Knowledge Bases","D. Liu; S. Xu; H. Liu","GradientX, Santa Monica, CA, USA; Changsha Univ., Changsha, China; Changsha Univ., Changsha, China","2013 Second IIAI International Conference on Advanced Applied Informatics","15 Oct 2013","2013","","","80","83","Logging has been mainly used as a debugging aid. We think log files contain a large amount of information which can be used as knowledge bases for different purposes. In this paper, we discuss some key points of holding this view point, list some related Java code, and exhibit some log pieces to demonstrate the usage. We also propose some ways to manage log files. The idea we suggest here might help improving software development process.","","978-0-7695-5071-8","10.1109/IIAI-AAI.2013.38","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6630321","logging;log file;knowledge base;software development","Knowledge based systems;Software;Lakes;Debugging;Java;Reliability;Monitoring","file organisation;knowledge based systems;system monitoring","log file management;knowledge bases;Java code;software development process","","","","13","","15 Oct 2013","","","IEEE","IEEE Conferences"
"Centralized logging of multi tier applications over web Services with NLog custom targets","Ş. S. Dragoş; M. F. Vaida","Helios Laboratory, Technical University of Cluj-Napoca, stefan; Helios Laboratory, Technical University of Cluj-Napoca, stefan","2010 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)","23 Jul 2010","2010","3","","1","5","Multi tier applications are usually comprised of data tiers, logic tiers and presentation tiers. Logging is the technique that allows us to insert tracing statements which will produce persistent results at runtime with no effort. Modern applications use logging solutions like NLog, C# Logger or one of the many others for tracing. Logger solutions offer different ways to persist data, like database storage, file storage or web service calls for storing logs at the server side. This paper describes an enhancement of NLog which allows for server side log storage even if the logs are produced at the clients without exposing sensible resources that require protection. The goal of this enhancement is to provide high code reusability and ease of implementation to developers using it. The reusability has been attained by separating logging components into assemblies that should be referenced within the developed applications, some at the server side and some at the client side, and the ease of implementation by providing logging through NLog classes after a simple target registration at the application start.","","978-1-4244-6725-9","10.1109/AQTR.2010.5520737","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5520737","","Web services;Application software;Databases;Laboratories;Logic;Runtime;File servers;Protection;Assembly;Hardware","software reusability;system monitoring;Web services","centralized logging;multitier applications;Web Services;NLog custom targets;insert tracing statements;data tiers;logic tiers;presentation tiers","","","","7","","23 Jul 2010","","","IEEE","IEEE Conferences"
"Deportment of Logs for Securing the Host System","P. Chauhan; N. Singh; N. Chandra","Dept. of Comput. Sci. & Eng., Amity Univ., Noida, India; Dept. of Comput. Sci. & Eng., Amity Univ., Noida, India; Dept. of Comput. Sci. & Eng., Amity Univ., Noida, India","2013 5th International Conference and Computational Intelligence and Communication Networks","11 Nov 2013","2013","","","355","359","Logs are the files which contain the information about all the events occurring on the system. Logs have been playing a vital role in providing all kinds of information which can be used for several purposes like detecting a suspicious behaviour over the system. The aim of this paper is to study, analyse and generate results by observing host. Log files consist of different header information which can be further used to determine if any kind of malicious activity is discovered then that activity can be traced and blocked.","","978-0-7695-5069-5","10.1109/CICN.2013.80","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6658015","Intrusion Detection System;Host/System logs;Log management","Software;Computers;Organizations;Servers;Intrusion detection;XML","security of data;system monitoring","log deportment;host system security;host observation;log files;malicious activity","","1","","6","","11 Nov 2013","","","IEEE","IEEE Conferences"
"LoRe: Supporting Non-deterministic Events Logging and Replay for KVM Virtual Machines","J. Li; S. Si; B. Li; L. Cui; J. Zheng","State Key Lab. of Software Dev. Environ., Beihang Univ., Beijing, China; Sch. of Comput. Sci. & Eng., Beihang Univ., Beijing, China; State Key Lab. of Software Dev. Environ., Beihang Univ., Beijing, China; Sch. of Comput. Sci. & Eng., Beihang Univ., Beijing, China; Sch. of Comput. Sci. & Eng., Beihang Univ., Beijing, China","2013 IEEE 10th International Conference on High Performance Computing and Communications & 2013 IEEE International Conference on Embedded and Ubiquitous Computing","12 Jun 2014","2013","","","442","449","Cloud computing brings a loose-coupled resources integration paradigm with virtualized, elastic and cost-efficient resource management capabilities. Virtualization-based logging and replay technologies give users the ability to record the executions of the whole virtual machines and recover them at any time in a peer to peer mode, and it has become an important approach to analyze the system vul-nerability, debug the system execution, or recover a failed system. In this paper, we design a logging and replay system named LoRe in KVM (Kernel-based Virtual Machine) which is a widely-used full virtualization solution. In LoRe, the logging of non-deterministic events is achieved based on the Virtual Machine Control Structure (VMCS), and a kernel notification chain is designed to reduce the time consumption of the branches counter matching procedure in the replay process. Moreover, to use less cache and reduce the overhead of log transmission, a reusable circular buffer queue is designed and IOCTL is used for the data transmission. We implemented LoRe in kvm-kmod-2.6.32, and experimental study show that the overhead of LoRe is lower than 8%, and only a small storage space is used.","","978-0-7695-5088-6","10.1109/HPCC.and.EUC.2013.70","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6831952","Cloud Computing;Virtualization;KVM;Logging and Replay;Non-deterministic Event","Registers;Radiation detectors;Kernel;Virtualization;Hardware;Virtual machining;Data communication","cloud computing;queueing theory;resource allocation;virtual machines;virtualisation","nondeterministic events logging;KVM virtual machines;cloud computing;loose-coupled resources integration paradigm;cost-efficient resource management capabilities;virtualization-based logging;replay technologies;LoRe;kernel-based virtual machine;full virtualization solution;virtual machine control structure;VMCS;kernel notification chain;branch counter matching procedure;cache;log transmission overhead;reusable circular buffer queue;IOCTL;kvm-kmod-2.6.32","","1","","20","","12 Jun 2014","","","IEEE","IEEE Conferences"
"Studying Duplicate Logging Statements and Their Relationships with Code Clones","Z. Li; T. -H. P. Chen; J. Yang; W. Shang","Department of Computer Science and Software Engineering, Concordia University, 5618 Montreal, Quebec, Canada, (e-mail: l_zhenha@encs.concordia.ca); Computer Science and Software Engineering, Concordia University, 5618 Montreal, Quebec, Canada, H3G 2W1 (e-mail: peterc@encs.concordia.ca); Department of Computer Science and Software Engineering, Concordia University, 5618 Montreal, Quebec, Canada, (e-mail: jinqiuy@encs.concordia.ca); Department of Computer Science and Software Engineering, Concordia University, Montreal, Quebec, Canada, (e-mail: shang@encs.concordia.ca)","IEEE Transactions on Software Engineering","","2021","PP","99","1","1","Developers rely on software logs for a variety of tasks, such as debugging, testing, program comprehension, verification, and performance analysis. Despite the importance of logs, prior studies show that there is no industrial standard on how to write logging statements. In this paper, we focus on studying duplicate logging statements, which are logging statements that have the same static text message. Such duplications in the text message are potential indications of logging code smells, which may affect developers understanding of the dynamic view of the system. We manually studied over 4K duplicate logging statements and their surrounding code in five large-scale open source systems: Hadoop, CloudStack, Elasticsearch, Cassandra, and Flink. We uncovered five patterns of duplicate logging code smells. For each instance of the duplicate logging code smell, we further manually identify the potentially problematic (i.e., require fixes) and justifiable (i.e., do not require fixes) cases. Then, we contact developers to verify our manual study result. We integrated our manual study result and developers feedback into our automated static analysis tool, DLFinder, which automatically detects problematic duplicate logging code smells. We evaluated DLFinder on the five manually studied systems and three additional systems: Camel, Kafka and Wicket. In total, combining the results of DLFinder and our manual analysis, we reported 91 problematic duplicate logging code smell instances to developers and all of them have been fixed. We further study the relationship between duplicate logging statements, including the problematic instances of duplicate logging code smells, and code clones. We find that 83% of the duplicate logging code smell instances reside in cloned code, but 17% of them reside in micro-clones that are difficult to detect using automated clone detection tools. We also find that more than half of the duplicate logging statements reside in cloned code snippets, and a large portion of them reside in very short code blocks which may not be effectively detected by existing code clone detection tools. Our study shows that, in addition to general source code that implements the business logic, code clones may also result in bad logging practices that could increase maintenance difficulties.","1939-3520","","10.1109/TSE.2021.3060918","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9360483","log;code smell;duplicate log;code clone;static analysis;empirical study","Cloning;Manuals;Tools;Static analysis;Maintenance engineering;Java;Cloud computing","","","","","","","IEEE","22 Feb 2021","","","IEEE","IEEE Early Access Articles"
"Web usage pattern analysis through web logs: A review","D. S. Sisodia; S. Verma","Department of Computer Sc. & Engg., National Institute of Technology Raipur, India; Department of Information Technology & EC, National Institute of Technology Raipur, India","2012 Ninth International Conference on Computer Science and Software Engineering (JCSSE)","9 Aug 2012","2012","","","49","53","Web server log repositories are great source of knowledge, which keeps the record of web usage patterns of different web users. The Web usage pattern analysis is the process of identifying browsing patterns by analyzing the user's navigational behavior. The web server log files which store the information about the visitors of web sites is used as input for the web usage pattern analysis process. First these log files are preprocessed and converted into required formats so web usage mining techniques can apply on these web logs. This paper reviews the process of discovering useful patterns from the web server log file of an academic institute. The obtained results can be used in different applications like web traffic analysis, efficient website administration, site modifications, system improvement and personalization and business intelligence etc.","","978-1-4673-1921-8","10.1109/JCSSE.2012.6261924","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6261924","Log Repositories;Web usage pattern;Web server log;Knowledge extraction;Web usage mining;browsing patterns;Useful patterns","Data mining;Web servers;Knowledge engineering;Browsers;Web pages","data mining;educational institutions;file servers;Internet;Web sites","Web usage pattern analysis process;Web server log repositories;browsing pattern identification;user navigational behavior analysis;Web server log files;Web usage mining techniques;pattern discovery;academic institute;Web traffic analysis;Website administration;site modifications;system improvement;system personalization;business intelligence","","9","","18","","9 Aug 2012","","","IEEE","IEEE Conferences"
"Which Variables Should I Log?","Z. Liu; X. Xia; D. Lo; Z. Xing; A. E. Hassan; S. Li","College of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang China (e-mail: liu_zx@zju.edu.cn); Faculty of Information Technology, Monash University Faculty of Information Technology, 224480 Clayton, Victoria Australia 3800 (e-mail: xxia@zju.edu.cn); School of Information Systems, Singapore Management University, Singapore, Singapore Singapore 17890 (e-mail: davidlo@smu.edu.sg); School of Computer Engineering, Nanyang Technological University, Singapore, Singapore Singapore 639798 (e-mail: zhenchang.xing@anu.edu.au); School of Computing, Queen's University, Kingston, Ontario Canada K7L 3N6 (e-mail: ahmed@cs.queensu.ca); College of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang China (e-mail: shan@zju.edu.cn)","IEEE Transactions on Software Engineering","","2019","PP","99","1","1","Developers usually depend on inserting logging statements into the source code to collect system runtime information. Such logged information is valuable for software maintenance. A logging statement usually prints one or more variables to record vital system status. However, due to the lack of rigorous logging guidance and the requirement of domain-specific knowledge, it is not easy for developers to make proper decisions about which variables to log. To address this need, in this work, we propose an approach to recommend logging variables for developers during development by learning from existing logging statements. Different from other prediction tasks in software engineering, this task has two challenges: 1) Dynamic labels -- different logging statements have different sets of accessible variables, which means in this task, the set of possible labels of each sample is not the same. 2) Out-of-vocabulary words -- identifiers' names are not limited to natural language words and the test set usually contains a number of program tokens which are out of the vocabulary built from the training set and cannot be appropriately mapped to word embeddings. To deal with the first challenge, we convert this task into a representation learning problem instead of a multi-label classification problem. Given a code snippet which lacks a logging statement, our approach first leverages a neural network with an RNN (recurrent neural network) layer and a self-attention layer to learn the proper representation of each program token, and then predicts whether each token should be logged through a unified binary classifier based on the learned representation. To handle the second challenge, we propose a novel method to map program tokens into word embeddings by making use of the pre-trained word embeddings of natural language tokens. We evaluate our approach on 9 large and high-quality Java projects. Our evaluation results show that the average MAP of our approach is over 0.84, outperforming random guess and an information-retrieval-based method by large margins.","1939-3520","","10.1109/TSE.2019.2941943","NSFC Program; National Key Research and Development Program of China; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8840982","Log;Logging Variable;Word Embedding;Representation Learning","Task analysis;Recurrent neural networks;Tools;Compounds;Vocabulary;Software maintenance","","","","2","","","","17 Sep 2019","","","IEEE","IEEE Early Access Articles"
"Event Logs for the Analysis of Software Failures: A Rule-Based Approach","M. Cinque; D. Cotroneo; A. Pecchia","Universitá degli Studi di Napoli Federico II, Naples; Universitá degli Studi di Napoli Federico II, Naples; Universitá degli Studi di Napoli Federico II, Naples","IEEE Transactions on Software Engineering","23 May 2013","2013","39","6","806","821","Event logs have been widely used over the last three decades to analyze the failure behavior of a variety of systems. Nevertheless, the implementation of the logging mechanism lacks a systematic approach and collected logs are often inaccurate at reporting software failures: This is a threat to the validity of log-based failure analysis. This paper analyzes the limitations of current logging mechanisms and proposes a rule-based approach to make logs effective to analyze software failures. The approach leverages artifacts produced at system design time and puts forth a set of rules to formalize the placement of the logging instructions within the source code. The validity of the approach, with respect to traditional logging mechanisms, is shown by means of around 12,500 software fault injection experiments into real-world systems.","1939-3520","","10.1109/TSE.2012.67","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6320555","Event log;logging mechanism;rule-based logging;error detection;software failures","Unified modeling language;Failure analysis;Analytical models;Systematics;Proposals;Software systems","software fault tolerance","event logs;software failures;rule-based approach;logging mechanism;log-based failure analysis;system design time","","37","","52","","3 Oct 2012","","","IEEE","IEEE Journals"
"An automatic reassembly model and algorithm of log file fragments based on graph theory","S. Yi; X. Hu; H. Wu","Information Engineering University, Zhengzhou, 450003, China; Information Engineering University, Zhengzhou, 450003, China; Information Engineering University, Zhengzhou, 450003, China","2015 6th IEEE International Conference on Software Engineering and Service Science (ICSESS)","30 Nov 2015","2015","","","686","689","Automatic reassembly of log file fragments from a collection of mixed fragments is one of the most important process during log file carving. In this paper we propose a general process model and design an algorithm based on graph theory for automatic reassembly of log file fragments. Finally, an experiment is designed and explored and results demonstrated the validity of the proposed model and algorithm.","2327-0594","978-1-4799-8353-7","10.1109/ICSESS.2015.7339150","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7339150","file carve;log file;reassembly;file recovery;graph theory","Algorithm design and analysis;Finite element analysis;Graph theory;File systems;Semantics;Probability;Manganese","graph theory;program assemblers;system monitoring","automatic reassembly model;log file fragments;graph theory;mixed fragments;log file carving","","1","","7","","30 Nov 2015","","","IEEE","IEEE Conferences"
"Analyzing the Impact of System Reliability Events on Applications in the Titan Supercomputer","R. A. Ashraf; C. Engelmann","Comput. Sci. & Math. Div., Oak Ridge Nat. Lab., Oak Ridge, TN, USA; Comput. Sci. & Math. Div., Oak Ridge Nat. Lab., Oak Ridge, TN, USA","2018 IEEE/ACM 8th Workshop on Fault Tolerance for HPC at eXtreme Scale (FTXS)","6 Dec 2018","2018","","","39","48","Extreme-scale computing systems employ Reliability, Availability and Serviceability (RAS) mechanisms and infrastructure to log events from multiple system components. In this paper, we analyze RAS logs in conjunction with the application placement and scheduling database, in order to understand the impact of common RAS events on application performance. This study conducted on the records of about 2 million applications executed on Titan supercomputer provides important insights for system users, operators and computer science researchers. Specifically, we investigate the impact of RAS events on application performance and its variability by comparing cases where events are recorded with corresponding cases where no events are recorded. Such a statistical investigation is possible since we observed that system users tend to execute their applications multiple times. Our analysis reveals that most RAS events do impact application performance, although not always. We also find that different system components affect application performance differently. In particular, our investigation includes the following components: parallel file system processor, memory, graphics processing units, system and user software issues. Our work establishes the importance of providing feedback to system users for increasing operational efficiency of extreme-scale systems.","","978-1-7281-0222-1","10.1109/FTXS.2018.00008","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8564486","HPC-Applications;Performance;Supercomputers;Log-Data-Analytics;Reliability-Availability-Serviceability-(RAS)-log;Application-log;Field-Study","Graphics processing units;Hardware;Supercomputers;Computer architecture;Error correction codes;Resource management","graphics processing units;parallel machines","RAS events;parallel file system processor;extreme-scale systems;Titan supercomputer;extreme-scale computing systems;scheduling database;system components;system reliability events;reliability availability and serviceability;RAS logs;graphics processing units","","3","","15","","6 Dec 2018","","","IEEE","IEEE Conferences"
"Visualization of Server Log Data for Detecting Abnormal Behaviour","R. Suman; B. Far; E. A. Mohammed; A. Nair; S. Janbakhsh","Dept. of Elec. & Comp. Eng., Univ. of Calgary, Calgary, AB, Canada; Dept. of Elec. & Comp. Eng., Univ. of Calgary, Calgary, AB, Canada; Dept. of Software Eng., Lakehead Univ., Thunder Bay, ON, Canada; City of Calgary, Calgary, AB, Canada; City of Calgary, Calgary, AB, Canada","2018 IEEE International Conference on Information Reuse and Integration (IRI)","6 Aug 2018","2018","","","244","247","Modern large-scale IT operations are getting complex every day and generate thousands of log and event files, which contain essential information about the systems and their behavior. System errors are examined manually by human experts, which takes a considerable amount of time and efforts. The system log files, besides other attributes such as time and location, contain messages in textual form, which is essential for analyzing logs behavior. In this paper, we propose a method to extract meaningful information through topic modeling from log messages, combine them into a sequence of events and present them to the practitioners to visually identify abnormal behavioral patterns, which is used for predictive analysis.","","978-1-5386-2659-7","10.1109/IRI.2018.00044","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8424715","Latent Dirichlet Allocation;Topic Modeling;Logs Behavior;Sequence of Events;Abnormal Behavior patterns","Servers;Data visualization;Analytical models;Task analysis;Data mining;Feature extraction;Indexes","data analysis;data visualisation;security of data;system monitoring","event files;system errors;abnormal behavioral patterns;information extraction;server log data visualization;system log files;logs behavior analysis;log messages topic modeling","","","","15","","6 Aug 2018","","","IEEE","IEEE Conferences"
"Characterizing and Detecting Duplicate Logging Code Smells","Z. Li","Concordia University, Canada","2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)","19 Aug 2019","2019","","","147","149","Software logs are widely used by developers to assist in various tasks. Despite the importance of logs, prior studies show that there is no industrial standard on how to write logging statements. Recent research on logs often only considers the appropriateness of a log as an individual item (e.g., one single logging statement); while logs are typically analyzed in tandem. In this paper, we focus on studying duplicate logging statements, which are logging statements that have the same static text message. Such duplications in the text message are potential indications of logging code smells, which may affect developers' understanding of the dynamic view of the system. We manually studied over 3K duplicate logging statements and their surrounding code in four large-scale open source systems and uncovered five patterns of duplicate logging code smells. For each instance of the problematic code smell, we contact developers in order to verify our manual study result. We integrated our manual study result and developers' feedback into our automated static analysis tool, DLFinder, which automatically detects problematic duplicate logging code smells. We evaluated DLFinder on the manually studied systems and two additional systems. In total, combining the results of DLFinder and our manual analysis, DLFinder is able to detect over 85% of the instances which were reported to developers and then fixed.","2574-1934","978-1-7281-1764-5","10.1109/ICSE-Companion.2019.00062","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8802850","log;code smell;duplicate log;static analysis","Software engineering;Manuals;Static analysis;Integrated circuits;Tools;Operating systems","program diagnostics;public domain software;software maintenance;software quality;system monitoring","software logs;single logging statement;duplicate logging statements;problematic code smell;manual study result;problematic duplicate logging code;DLFinder;manual analysis;text message;automated static analysis tool","","","","17","","19 Aug 2019","","","IEEE","IEEE Conferences"
"Identifying Software Process Management Challenges: Survey of Practitioners in a Large Global IT Company","M. Gupta; A. Sureka; S. Padmanabhuni; A. M. Asadullah","Indraprastha Inst. of Inf. Technol., Delhi, India; Indraprastha Inst. of Inf. Technol., Delhi, India; Infosys Ltd., Bangalore, India; Infosys Ltd., Bangalore, India","2015 IEEE/ACM 12th Working Conference on Mining Software Repositories","6 Aug 2015","2015","","","346","356","Process mining consists of mining event logs generated from business process execution supported by Information Systems (IS). Process mining of software repositories has diverse applications because vast data is generated during Software Development Life Cycle (SDLC) and archived in IS such as Version Control System (VCS), Peer Code Review (PCR) System, Issue Tracking System (ITS), and mail archives. There is need to explore its applications on different repositories to aid managers in process management. We conduct two phase surveys and interviews with managers in a large, global, IT company. The first survey and in-person interviews identify the process challenges encountered by them that can be addressed by novel applications of process mining. We filter, group and abstract responses formulating 30 generic problem statements. On the basis of process mining type, we classify identified problems to eight categories such as control analysis, organizational analysis, conformance analysis, and preventive analysis. The second survey asks distinct participants the importance of solving identified problems. We calculate proposed Net Importance Metric (NIM) using 1262 ratings from 43 participants. Combined analysis of NIM and first survey responses reveals that the problems mentioned by few practitioners in first survey are considered important by majority in the second survey. We elaborate on possible solutions and challenges for most frequent and important problems. We believe solving these validated problems will help managers in improving project quality and productivity.","2160-1860","978-0-7695-5594-2","10.1109/MSR.2015.39","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7180093","Process Mining;Qualitative Study;Software Development Life Cycle;Software Repositories","Data mining;Software;Process control;Interviews;Measurement;Companies","business process re-engineering;data mining;information systems;information technology;software process improvement","software process management;global IT company;process mining;event logs mining;business process execution;information systems;IS;software development life cycle;SDLC;net importance metric;NIM;project quality;productivity","","1","","32","","6 Aug 2015","","","IEEE","IEEE Conferences"
"Machine learning to detect anomalies in web log analysis","Q. Cao; Y. Qiao; Z. Lyu","School of Computer Science and Software Engineering, East China Normal University, Shanghai, China; School of Computer Science and Software Engineering, East China Normal University, Shanghai, China; Shanghai International Studies University, Shanghai, China","2017 3rd IEEE International Conference on Computer and Communications (ICCC)","26 Mar 2018","2017","","","519","523","As the information technology develops rapidly, Web servers are easily to be attacked because of their high value. Therefore, Web security has aroused great concern in both academia and industry. Anomaly detection plays a significant role in the field of Web security, and log messages recording detailed system runtime information has become an important data analysis object accordingly. Traditional log anomaly detection relies on programmers to manually inspect by keyword search and regular expression match. Although the programmers can use intrusion detection system to reduce their workload, yet the log system data is huge, attack types are various, and hacking skills are improving, which make the traditional detection not efficient enough. To improve the traditional detection technology, many of anomaly detection mechanisms have been proposed in recent years, especially the machine learning method. In this paper, an anomaly detection system for web log files has been proposed, which adopts a two-level machine learning algorithm. The decision tree model classifies normal and anomalous data sets. The normal data set is manually checked for the establishment of multiple HMMs. The experimental data comes from the real industrial environment where log files have been collected, which contain many true intrusion messages. After comparing with three types of machine learning algorithms used in anomaly detection, the experimental results on this data set suggest that this system achieves higher detection accuracy and can detect unknown anomaly data.","","978-1-5090-6352-9","10.1109/CompComm.2017.8322600","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8322600","machine learning;anomaly detection;log files component","Hidden Markov models;Anomaly detection;Decision trees;Feature extraction;Uniform resource locators;Security","data analysis;Internet;learning (artificial intelligence);security of data","anomaly detection system;web log files;two-level machine learning algorithm;normal data set;higher detection accuracy;web log analysis;information technology;Web servers;Web security;intrusion detection system;log system data;machine learning method;detection technology;system runtime information;data analysis object","","4","","19","","26 Mar 2018","","","IEEE","IEEE Conferences"
"Detection of Software Failures through Event Logs: An Experimental Study","A. Pecchia; S. Russo","Dipt. di Inf. e Sist., Univ. degli Studi di Napoli Federico II, Naples, Italy; Dipt. di Inf. e Sist., Univ. degli Studi di Napoli Federico II, Naples, Italy","2012 IEEE 23rd International Symposium on Software Reliability Engineering","4 Apr 2013","2012","","","31","40","Software faults are recognized to be among the main responsible for system failures in many application domains. Event logs play a key role to support the analysis of failures occurring under real workload conditions. Nevertheless, field experience suggests that event logs may be inaccurate at reporting software failures or they fail to provide accurate support for understanding their causes. This paper analyzes the factors that determine accurate detection of software failures through event logs. The study is based on a data set of 17,387 experiments where failures have been induced by means of software fault injection into three systems. Analysis reveals that the reporting ability of logs collected during the experiments, is not influenced by the type of fault that is activated at runtime. More importantly, analysis demonstrates that, despite the considered systems adopt very similar detection mechanisms, the ability of logs at reporting a given type of failure changes significantly across the systems. A closer inspection of collected logs reveals that characteristics, such as system architecture, placement of the logging instructions and specific supports provided by the execution environment, significantly increase accuracy of logs at runtime.","2332-6549","978-1-4673-4638-2","10.1109/ISSRE.2012.24","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6405402","software fault;ODC;failure;detection mechanism;event log","Radio frequency;Analysis of variance;Runtime;Web servers;Software systems;Detectors","software architecture;software fault tolerance","execution environment;logging instruction placement;system architecture;event log reporting ability;software fault injection;workload conditions;system software failure analysis;software fault recognition;software failure detection","","15","","25","","4 Apr 2013","","","IEEE","IEEE Conferences"
"Performance Analyses on Logging Policies of Log-structured File System","S. Lee; J. Park; Y. I. Eom","College of Software, Sungkyunkwan University; College of Software, Sungkyunkwan University; College of Software, Sungkyunkwan University","2019 IEEE International Conference on Consumer Electronics (ICCE)","7 Mar 2019","2019","","","1","2","Log-structured filesystems have suffered from garbage collection overheads when they have lack of free space. To address the problem, previous research suggested threaded logging that reuses invalidated blocks without a cleaning process. However, threaded logging incurs random writes that can degrade the performance of upcoming read operations, since the invalid blocks can be discontiguous. In this paper, we show the performance of the two logging policies and analyze their performance characteristics.","2158-4001","978-1-5386-7910-4","10.1109/ICCE.2019.8662104","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8662104","","Instruction sets;Performance evaluation;Flash memories;File systems;Degradation;Information and communication technology;Random access memory","file organisation;flash memories;storage management","logging policies;log-structured file system;garbage collection overheads;threaded logging","","","","6","","7 Mar 2019","","","IEEE","IEEE Conferences"
"The monitoring and auditing method of Windows File manipulations","Ma Lin; Chen Houwu; Liu Fuqiang","Naval Academy of Armament, Beijing, China; Naval Academy of Armament, Beijing, China; Naval Academy of Armament, Beijing, China","2012 International Conference on Information Management, Innovation Management and Industrial Engineering","25 Oct 2012","2012","3","","366","368","To completely monitor file manipulations and fully record the audit logs on the Windows operating system, this paper elaborate a policy based approach to monitoring and auditing file manipulating activities, analyze the workflow of the monitoring and auditing process, and proposed some critical technologies on the implementation, which enables monitoring and recording all types of file manipulating activities on the application level, resulting in complete audit log record. Additionally, the technologies we propose are easy to implement.","2155-1472","978-1-4673-1931-7","10.1109/ICIII.2012.6339993","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6339993","dictionary monitoring;file monitoring","Monitoring;Biomedical monitoring;Dictionaries","auditing;file organisation;operating systems (computers);system monitoring;workflow management software","monitoring method;auditing method;Windows file manipulations;file manipulations monitoring;Windows operating system;policy based approach;file manipulating activity;workflow analysis;audit log record","","","","4","","25 Oct 2012","","","IEEE","IEEE Conferences"
"Recursion aware modeling and discovery for hierarchical software event log analysis","M. Leemans; W. M. P. van der Aalst; M. G. J. van den Brand","Eindhoven University of Technology, Eindhoven, The Netherlands; Eindhoven University of Technology, Eindhoven, The Netherlands; Eindhoven University of Technology, Eindhoven, The Netherlands","2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)","5 Apr 2018","2018","","","185","196","This paper presents 1) a novel hierarchy and recursion extension to the process tree model; and 2) the first, recursion aware process model discovery technique that leverages hierarchical information in event logs, typically available for software systems. This technique allows us to analyze the operational processes of software systems under real-life conditions at multiple levels of granularity. The work can be positioned in-between reverse engineering and process mining. An implementation of the proposed approach is available as a ProM plugin. Experimental results based on real-life (software) event logs demonstrate the feasibility and usefulness of the approach and show the huge potential to speed up discovery by exploiting the available hierarchy.","","978-1-5386-4969-5","10.1109/SANER.2018.8330208","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8330208","Reverse Engineering;Process Mining;Recursion Aware Discovery;Event Log;Hierarchical Event Log;Process Discovery;Hierarchical Discovery;Hierarchical Modeling","Analytical models;Unified modeling language;Instruments;Software systems;Data mining;Grammar;Java","data mining;reverse engineering;software engineering;trees (mathematics)","recursion aware modeling;recursion extension;process tree model;recursion aware process model discovery technique;hierarchical information;software systems;operational processes;reverse engineering;process mining;real-life event logs;hierarchical software event log analysis;novel hierarchy extension","","8","","57","","5 Apr 2018","","","IEEE","IEEE Conferences"
"Diagnosing the root-causes of failures from cluster log files","E. Chuah; S. Kuo; P. Hiew; W. Tjhi; G. Lee; J. Hammond; M. T. Michalewicz; T. Hung; J. C. Browne","Institute of High Performance Computing, 1 Fusionopolis Way, #16-16 Connexis, Singapore 138632; Institute of High Performance Computing, 1 Fusionopolis Way, #16-16 Connexis, Singapore 138632; TA∗Star Computational Resource Center, 1 Fusionopolis Way, #16-16 Connexis, Singapore 138632; Institute of High Performance Computing, 1 Fusionopolis Way, #16-16 Connexis, Singapore 138632; Institute of High Performance Computing, 1 Fusionopolis Way, #16-16 Connexis, Singapore 138632; University of Texas at Austin, Texas 78712; TA∗Star Computational Resource Center, 1 Fusionopolis Way, #16-16 Connexis, Singapore 138632; Institute of High Performance Computing, 1 Fusionopolis Way, #16-16 Connexis, Singapore 138632; University of Texas at Austin, Texas 78712","2010 International Conference on High Performance Computing","14 Feb 2011","2010","","","1","10","System event logs are often the primary source of information for diagnosing (and predicting) the causes of failures for cluster systems. Due to interactions among the system hardware and software components, the system event logs for large cluster systems are comprised of streams of interleaved events, and only a small fraction of the events over a small time span are relevant to the diagnosis of a given failure. Furthermore, the process of troubleshooting the causes of failures is largely manual and ad-hoc. In this paper, we present a systematic methodology for reconstructing event order and establishing correlations among events which indicate the root-causes of a given failure from very large syslogs. We developed a diagnostics tool, FDiag, to extract the log entries as structured message templates and uses statistical correlation analysis to establish probable cause and effect relationships for the fault being analyzed. We applied FDiag to analyze failures due to breakdowns in interactions between the Lustre file system and its clients on the Ranger supercomputer at the Texas Advanced Computing Center (TACC). The results are positive. FDiag is able to identify the dates and the time periods that contain the significant events which eventually led to the occurrence of compute node soft lockups.","1094-7256","978-1-4244-8520-8","10.1109/HIPC.2010.5713159","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5713159","Resilient cluster systems;Reliability;Syslog files;Statistical correlation analysis","Correlation;Kernel;Data mining;Manuals;Supercomputers;Correlators;Heating","correlation methods;file organisation;mainframes;parallel machines;pattern clustering;statistical analysis;system recovery","cluster log file;system event log;cluster system failure;software component;system hardware;root cause;statistical correlation analysis;Texas advanced computing center;ranger supercomputer;lustre file system;FDiag","","17","6","34","","14 Feb 2011","","","IEEE","IEEE Conferences"
"Log File Analysis of E-commerce Systems in Rich Internet Web 2.0 Applications","C. J. Aivalis; A. C. Boucouvalas","Dept. of Telecommun. Sci. & Technol., Univ. of Peloponnese, Tripoli, Greece; Dept. of Telecommun. Sci. & Technol., Univ. of Peloponnese, Tripoli, Greece","2011 15th Panhellenic Conference on Informatics","3 Nov 2011","2011","","","222","226","This paper describes the implications of the new trends in web development languages on log file analysis for e-commerce. The new trends in Software Development and the available Web Analytics technologies are explained. The focus is placed on the diversifications introduced to the traces left on the system that by Rich Internet Applications (RIAs). Finally a novel hybrid solution is proposed that is based on the junction of log files with operational data and page tagging, which allows even exacter measurements of customer behavior. It allows a customization of the Analysis Tool and survives the shift of the technologies.","","978-1-61284-962-1","10.1109/PCI.2011.31","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6065092","E-Commerce;Web Analytics;Rich Internet Applications;JavaScript;GWT;Log File Analysis","Browsers;Databases;Google;Web servers;Java","consumer behaviour;electronic commerce;file organisation;Internet;software engineering","log file analysis;e-commerce;software development;Web development languages;Web analytic technologies;rich Internet application;hybrid solution;operational data;page tagging;exacter measurements;customer behavior;Web 2.0 applications","","9","","12","","3 Nov 2011","","","IEEE","IEEE Conferences"
"Event log analysis software design for naval combat system using smart platform","S. Kim; D. Kim; D. Kim","Dept of IT Convergence Engineering, School of Electronic Engineering, Kumoh National Institute of Technology, South Korea; Dept of IT Convergence Engineering, School of Electronic Engineering, Kumoh National Institute of Technology, South Korea; Dept of IT Convergence Engineering, School of Electronic Engineering, Kumoh National Institute of Technology, South Korea","2018 International Conference on Electronics, Information, and Communication (ICEIC)","5 Apr 2018","2018","","","1","2","In this paper, event log analysis software for naval combat systems using smart platform is proposed. To support development of naval combat system, event log analysis software provides gathering node information and log analyzing function. The detail operation of each software is shown in this paper.","","978-1-5386-4754-7","10.23919/ELINFOCOM.2018.8330563","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8330563","Log Analysis;Smart Platform;Naval Combat System;Real-Time","Real-time systems;Filtering;Software algorithms;Software design;Convergence;Data communication","data analysis;military computing;military vehicles;software engineering","naval combat system;smart platform;node information;log analyzing function;event log analysis software design","","","","6","","5 Apr 2018","","","IEEE","IEEE Conferences"
"Can You Capture Information As You Intend To? A Case Study on Logging Practice in Industry","G. Rong; Y. Xu; S. Gu; H. Zhang; D. Shao","Software Institute, Nanjing University,State Key Laboratory of Novel Software Technology,Nanjing,China; Software Institute, Nanjing University,State Key Laboratory of Novel Software Technology,Nanjing,China; Software Institute, Nanjing University,State Key Laboratory of Novel Software Technology,Nanjing,China; Software Institute, Nanjing University,State Key Laboratory of Novel Software Technology,Nanjing,China; Software Institute, Nanjing University,State Key Laboratory of Novel Software Technology,Nanjing,China","2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)","2 Nov 2020","2020","","","12","22","Background: Logs provide crucial information to understand the dynamic behavior of software systems in modern software development and maintenance. Usually, logs are produced by log statements which will be triggered and executed under certain conditions. However, current studies paid very limited attention to developers' Intentions and Concerns (I&C) on logging practice, leading uncertainty that whether the developers' I&C are properly reflected by log statements and questionable capability to capture the expected information of system behaviors in logs. Objective: This study aims to reveal the status of developers' I&C on logging practice and more importantly, how the I&C are properly reflected in software source code in real-world software development. Method: We collected evidence from two sources of a series of interviews and source code analysis which are conducted in a big-data company, followed by consolidation and analysis of the evidence. Results: Major gaps and inconsistencies have been identified between the developers' I&C and real log statements in source code. Many code snippets contained no log statements that the interviewees claimed to have inserted. Conclusion: Developers' original I&C towards logging practice are usually poorly realized, which inevitably impacted the motivation and purpose to conduct this practice.","2576-3148","978-1-7281-5619-4","10.1109/ICSME46990.2020.00012","Research and Development; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9240659","logging practice;developer;intentions and concerns;inconsistencies","Industries;Companies;Tools;Software systems;Software;Interviews;Guidelines","organisational aspects;program diagnostics;software maintenance","real-world software development;software source code;developers;log statements;maintenance;modern software development;software systems;logging practice;capture information","","","","33","","2 Nov 2020","","","IEEE","IEEE Conferences"
"Complete and Interpretable Conformance Checking of Business Processes","L. García-Bañuelos; N. R. T. P. van Beest; M. Dumas; M. L. Rosa; W. Mertens","University of Tartu, Tartu, Estonia; Data61, CSIRO, Brisbane, QLD, Australia; University of Tartu, Tartu, Estonia; Queensland University of Technology, Brisbane, QLD, Australia; Queensland University of Technology, Brisbane, QLD, Australia","IEEE Transactions on Software Engineering","13 Mar 2018","2018","44","3","262","290","This article presents a method for checking the conformance between an event log capturing the actual execution of a business process, and a model capturing its expected or normative execution. Given a process model and an event log, the method returns a set of statements in natural language describing the behavior allowed by the model but not observed in the log and vice versa. The method relies on a unified representation of process models and event logs based on a well-known model of concurrency, namely event structures. Specifically, the problem of conformance checking is approached by converting the event log into an event structure, converting the process model into another event structure, and aligning the two event structures via an error-correcting synchronized product. Each difference detected in the synchronized product is then verbalized as a natural language statement. An empirical evaluation shows that the proposed method can handle real datasets and produces more concise and higher-level difference descriptions than state-of-the-art conformance checking methods. In a survey designed according to the technology acceptance model, practitioners showed a preference towards the proposed method with respect to a state-of-the-art baseline.","1939-3520","","10.1109/TSE.2017.2668418","Australian Research Council Discovery; Estonian Research Council; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7852436","Process mining;conformance checking;process model;event log;event structure","Business;Synchronization;Computational modeling;Data mining;Natural languages;Software systems;Context modeling","business data processing;data mining;natural languages","event log;process model;event structure;conformance checking;technology acceptance model;business process expected execution;business process normative execution;concurrency model;error-correcting synchronized product;natural language statement","","8","","39","","13 Feb 2017","","","IEEE","IEEE Journals"
"Mining and Extraction of Personal Software Process Measures through IDE Interaction Logs","A. Joonbakhsh; A. Sami","CSE & IT Dept., Shiraz Univ., Shiraz, Iran; CSE & IT Dept., Shiraz Univ., Shiraz, Iran","2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)","30 Dec 2018","2018","","","78","81","The Personal Software Process (PSP) is an effective software process improvement method that heavily relies on manual collection of software development data. This paper describes a semi-automated method that reduces the burden of PSP data collection by extracting the required time and size of PSP measurements from IDE interaction logs. The tool mines enriched event data streams so can be easily generalized to other developing environment also. In addition, the proposed method is adaptable to phase definition changes and creates activity visualizations and summarizations that are helpful for software project management. Tools and processed data used for this paper are available on GitHub at: https://github.com/unknowngithubuser1/data.","2574-3864","978-1-4503-5716-6","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8595184","IDE;Personal Software Process","Software;Data mining;Tools;Debugging;Encoding;Manuals;Size measurement","data analysis;data mining;data visualisation;project management;software development management;software maintenance;software process improvement;software quality","software development data;PSP data collection;PSP measurements;IDE interaction logs;event data streams;software project management;Personal Software Process measures;software process improvement method;activity visualizations","","","23","14","","30 Dec 2018","","","IEEE","IEEE Conferences"
"Industry Practices and Event Logging: Assessment of a Critical Software Development Process","A. Pecchia; M. Cinque; G. Carrozza; D. Cotroneo","Dipt. di Ing. Elettr. e Tecnol. dell'Inf., Univ. degli Studi di Napoli Federico II, Naples, Italy; Dipt. di Ing. Elettr. e Tecnol. dell'Inf., Univ. degli Studi di Napoli Federico II, Naples, Italy; Selex ES S.p.A., Finmeccanica Co., Rome, Italy; Dipt. di Ing. Elettr. e Tecnol. dell'Inf., Univ. degli Studi di Napoli Federico II, Naples, Italy","2015 IEEE/ACM 37th IEEE International Conference on Software Engineering","17 Aug 2015","2015","2","","169","178","Practitioners widely recognize the importance of event logging for a variety of tasks, such as accounting, system measurements and troubleshooting. Nevertheless, in spite of the importance of the tasks based on the logs collected under real workload conditions, event logging lacks systematic design and implementation practices. The implementation of the logging mechanism strongly relies on the human expertise. This paper proposes a measurement study of event logging practices in a critical industrial domain. We assess a software development process at Selex ES, a leading Finmeccanica company in electronic and information solutions for critical systems. Our study combines source code analysis, inspection of around 2.3 millions log entries, and direct feedback from the development team to gain process-wide insights ranging from programming practices, logging objectives and issues impacting log analysis. The findings of our study were extremely valuable to prioritize event logging reengineering tasks at Selex ES.","1558-1225","978-1-4799-1934-5","10.1109/ICSE.2015.145","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7202961","Source code analysis;Event logging;Development process;Coding practices;Industry domain","Software;Inspection;Programming;Encoding;Industries;Runtime","program diagnostics;software engineering;system monitoring","critical software development process assessment;event logging mechanism;measurement study;event logging practices;critical industrial domain;Selex ES;electronic and information solutions;source code analysis;log entry inspection;log analysis;event logging reengineering tasks","","31","","31","","17 Aug 2015","","","IEEE","IEEE Conferences"
"Mining constraints for event-based monitoring in systems of systems","T. Krismayer; R. Rabiser; P. GrUnbacher","Christian Doppler Laboratory MEVSS, Institute for Software Systems Engineering, Johannes Kepler University Linz, Austria; Christian Doppler Laboratory MEVSS, Institute for Software Systems Engineering, Johannes Kepler University Linz, Austria; Christian Doppler Laboratory MEVSS, Institute for Software Systems Engineering, Johannes Kepler University Linz, Austria","2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)","23 Nov 2017","2017","","","826","831","The full behavior of software-intensive systems of systems (SoS) emerges during operation only. Runtime monitoring approaches have thus been proposed to detect deviations from the expected behavior. They commonly rely on temporal logic or domain-specific languages to formally define requirements, which are then checked by analyzing the stream of monitored events and event data. Some approaches also allow developers to generate constraints from declarative specifications of the expected behavior. However, independent of the approach, deep domain knowledge is required to specify the desired behavior. This knowledge is often not accessible in SoS environments with multiple development teams independently working on different, heterogeneous systems. In this New Ideas Paper we thus describe an approach that automatically mines constraints for runtime monitoring from event logs recorded in SoS. Our approach builds on ideas from specification mining, process mining, and machine learning to mine different types of constraints on event occurrence, event timing, and event data. The approach further presents the mined constraints to users in an existing constraint language and it ranks the constraints using different criteria. We demonstrate the feasibility of our approach by applying it to event logs from a real-world industrial SoS.","","978-1-5386-2684-9","10.1109/ASE.2017.8115693","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8115693","Constraint mining;event-based monitoring;systems of systems","Monitoring;Data mining;Runtime;Feature extraction;Automation;Heuristic algorithms;System of systems","data mining;formal specification;learning (artificial intelligence);system monitoring","mined constraints;event logs;real-world industrial SoS;deviation detection;constraint language;software-intensive systems of systems;event timing;event occurrence;process mining;specification mining;heterogeneous systems;multiple development teams;SoS environments;deep domain knowledge;declarative specifications;event data;monitored events;temporal logic;expected behavior;runtime monitoring approaches","","1","","29","","23 Nov 2017","","","IEEE","IEEE Conferences"
"Information system log visualization to monitor anomalous user activity based on time","J. J. Hanniel; T. E. Widagdo; Y. D. W. Asnar","Informatics Engineering, Bandung Institute of Technology, Jl. Ganesha No.10 Bandung 40132, Indonesia; Informatics Engineering, Bandung Institute of Technology, Jl. Ganesha No.10 Bandung 40132, Indonesia; Informatics Engineering, Bandung Institute of Technology, Jl. Ganesha No.10 Bandung 40132, Indonesia","2014 International Conference on Data and Software Engineering (ICODSE)","19 Mar 2015","2014","","","1","6","As information systems start to manage the more crucial parts of human lives, their security cannot be neglected. One way to ensure the security is by analyzing their generated log files of anomalous user activity. Data visualization has become a common solution to help get around the problems in log analysis. In this paper, we tried to determine key characteristics of effective data visualization on detecting those anomalous user activity recorded in log files. First we analyzed the log data we have and derived 4 anomalies whose indicators are made into visualization topics. Hence we built 4 data visualizations to detect the 4 anomalies. Next, we transformed our data so that they can be visualized. After that, we analyzed the suitable time-based data visualization method to represent our data and decided on heatmap for its wide application on existing solutions and dot plot for it is able to accommodate all data variables needed on every visualization topic and has the suitable nuance for monitoring purposes. Next we decided on design concept of our data visualizations and implemented them as web-based data visualization. We conducted 2 tests in this paper to determine the key characteristics of effective data visualization. Even though the results are inconclusive, but they hinted that an effective data visualization on this matter should support large amount of perceived information through cognition and support focused exploration.","","978-1-4799-7996-7","10.1109/ICODSE.2014.7062673","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7062673","anomalous user activity;data visualization;log file","Data visualization;Heating;IP networks;Information systems;Geology;Java;Monitoring","cognition;data analysis;data visualisation;information systems;Internet;security of data","focused exploration;cognition;Web-based data visualization;design concept;data variables;dot plot;heatmap;time-based data visualization method;anomaly detection;anomalous user activity detection;log data analysis;log files analysis;security;anomalous user activity monitoring;information system log visualization","","2","","10","","19 Mar 2015","","","IEEE","IEEE Conferences"
"Improving process data reporting system using IT process automation software in sulfuric acid production","S. Sookjadit; T. Thepmanee; A. Julsereewong; P. Julsereewong","Faculty of Engineering, King Mongkut's Institute of Technology Ladkrabang, Bangkok, Thailand; Faculty of Engineering, King Mongkut's Institute of Technology Ladkrabang, Bangkok, Thailand; Faculty of Engineering, King Mongkut's Institute of Technology Ladkrabang, Bangkok, Thailand; Faculty of Engineering, King Mongkut's Institute of Technology Ladkrabang, Bangkok, Thailand","2012 12th International Conference on Control, Automation and Systems","31 Dec 2012","2012","","","588","593","This paper presents a method to improve process data reporting system in a sulfuric acid plant, which faced difficulties in accessing historical process data for effective use in process engineering, production management, quality management, and maintenance system. The proposed method running on existing distributed control system is based on the use of an IT process automation software called OpalisRobot to provide scheduled logging, reporting, and e-mailing criteria and quality control parameter files. Experimental results verify that not only ease of accessing and reporting electronic documents for use of production data and some economic benefits of the proposed work can be obtained, but also errors caused by manual records affecting to final product quality, environmental quality, instruments, safeguarding process equipments, and operator safety can be reduced.","","978-89-93215-04-5","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6393251","data reporting;electronic reporting;data logging;process automation;sulfuric acid production","Production;Process control;Cooling;Poles and towers;Absorption;Automation;Heating","chemical industry;control engineering computing;distributed control;document handling;electronic mail;maintenance engineering;occupational safety;organic compounds;process control;product quality;production control;production engineering computing;quality control","process data reporting system;IT process automation software;sulfuric acid production;sulfuric acid plant;process engineering;production management;quality management;maintenance system;distributed control system;OpalisRobot software;scheduled logging;scheduled reporting;e-mailing criteria;quality control parameter files;electronic document reporting;electronic document access;manual record errors;economic benefits;product quality;environmental quality;process equipment safeguarding;operator safety","","","","5","","31 Dec 2012","","","IEEE","IEEE Conferences"
"Establishing Hypothesis for Recurrent System Failures from Cluster Log Files","E. Chuah; G. Lee; W. Tjhi; S. Kuo; T. Hung; J. Hammond; T. Minyard; J. C. Browne","Inst. of High Performance Comput., Singapore, Singapore; Inst. of High Performance Comput., Singapore, Singapore; Inst. of High Performance Comput., Singapore, Singapore; Inst. of High Performance Comput., Singapore, Singapore; Inst. of High Performance Comput., Singapore, Singapore; Texas Adv. Comput. Center, Austin, TX, USA; Texas Adv. Comput. Center, Austin, TX, USA; Univ. of Texas at Austin, Austin, TX, USA","2011 IEEE Ninth International Conference on Dependable, Autonomic and Secure Computing","2 Jan 2012","2011","","","15","22","A goal for the analysis of supercomputer logs is to establish causal relationships among events which reflect significant state changes in the system. Establishing these relationships is at the heart of failure diagnosis. In principle, a log analysis tool could automate many of the manual steps systems administrators must currently use to diagnose system failures. However, supercomputer logs are unstructured, incomplete and contain considerable ambiguity so that direct discovery of causal relationships is difficult. This paper describes the second generation FDiag log-based failure diagnostics framework that provides automation of the manual failure diagnosis process and determines with high confidence, the likely cause of the failure, the components involved and the event sequences which contain the times of the causal and terminal events. FDiag extracts relevant events from the system logs, performs correlation analysis on these events and from these correlations determines the components involved and the event sequences. The diagnostics capabilities of FDiag are validated by comparing its assessments on known instances of recurrent failures on the Ranger supercomputer at the University of Texas at Austin. We believe FDiag is the first log analyzer to demonstrate this level of diagnostics capability from the system logs of an open source software stack incorporating Linux and the Lustre file system. FDiag will be put into production use for support of failure diagnosis on Ranger in September, 2011.","","978-1-4673-0006-3","10.1109/DASC.2011.27","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6118346","Large cluster systems;Reliability;Failure diagnosis;Hypothesis testing;Syslogs","Servers;Correlation;Laser mode locking;Protocols;Supercomputers;Manuals;Data mining","fault diagnosis;parallel machines;system monitoring","recurrent system failures;cluster log files;supercomputer logs;failure diagnosis;log analysis tool;FDiag log-based failure diagnostics framework;correlation analysis;Ranger supercomputer;open source software;Linux;Lustre file system","","6","","30","","2 Jan 2012","","","IEEE","IEEE Conferences"
"An Approach for Discovering Affiliation Network from XES Formatted Workflow Log","D. Pham; H. Ahn; K. Kim; K. P. Kim","KYONGGI UNIVERSITY,Div. of Comp. Sci. & Eng.,Gyeonggi,South Korea; KYONGGI UNIVERSITY,Div. of Comp. Sci. & Eng.,Gyeonggi,South Korea; KYONGGI UNIVERSITY,Div. of Comp. Sci. & Eng.,Gyeonggi,South Korea; KYONGGI UNIVERSITY,Div. of Comp. Sci. & Eng.,Gyeonggi,South Korea","2019 6th NAFOSTED Conference on Information and Computer Science (NICS)","5 Mar 2020","2019","","","336","341","For more than a decade, process-aware knowledge discovery has received the attention of many scientists and research groups. Many algorithms, techniques, and technologies have been applied to exploit information recorded in the history of business process systems. By elucidating the information that comes from the event log, we better understand what happened in the system. In this paper, we propose an approach to discover affiliation network from the workflow enactment event log recorded in XES format. More precisely, we describe an algorithm to exploit the relationship between activities and performers in the system, represent it under the affiliation network model and conduct it on the Conformance Checking Challenge 2019 data set. By discovering the affiliation network from event log, not only we understand more about the relationship between performers and activities, but also we have an overall view of the role of performers with the operation of the system. The results of this study will contribute to the evaluation and improvement of the performance in information systems.","","978-1-7281-5163-2","10.1109/NICS48868.2019.9023904","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9023904","Discovering affiliation network;process mining;XES log;social network;discovering workflow logs","Xenon;Social networking (online);Information systems;Unified modeling language;Computer architecture;Data mining","business data processing;data mining;information systems;workflow management software","affiliation network model;information systems;XES formatted workflow log;process-aware knowledge discovery;business process systems;workflow enactment event log","","","","23","","5 Mar 2020","","","IEEE","IEEE Conferences"
"LogOptPlus: Learning to Optimize Logging in Catch and If Programming Constructs","S. Lal; N. Sardana; A. Sureka","JIIT, Noida, India; JIIT, Noida, India; ABB Corp. Res., Bangalore, India","2016 IEEE 40th Annual Computer Software and Applications Conference (COMPSAC)","25 Aug 2016","2016","1","","215","220","Software logging is a common software development practice which is used to log program execution points. This execution information is later used by software developers for debugging purpose. Software logging is useful but it has cost and benefit tradeoff. Hence it is important to optimize the number of log statements in the code. However, previous studies on logging show that optimal logging is challenging for software developers. Hence tools and techniques which can help developers in making optimized logging decision can be beneficial. We propose LogOptPlus, a machine learning based tool to help software developers to optimize the number of log statements in the source code, for two focused code construct types. LogOptPlus is a significant extension of our previously published work 'LogOpt'. LogOpt is designed to predict logging for catch-blocks. We extend the functionality of LogOpt to predict logging for both if-blocks and catch-blocks. We identify distinguishing static features from the source code for logging prediction. We present intuition and results of quantitative analysis of all the features. We present results of comprehensive evaluation of LogOptPlus with five different machine learning algorithms on two two large Open Source projects (i.e., Apache Tomcat and CloudStack). Encouraging experimental results on two Open Source projects show that LogOptPlus is effective in logging prediction for two focused code construct types.","0730-3157","978-1-4673-8845-0","10.1109/COMPSAC.2016.149","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7552011","Logging;Machine Learning;Source Code Analysis;Software Debugging and Maintenance;Tracing","Feature extraction;Software;Containers;Debugging;Java;Complexity theory;Statistical analysis","learning (artificial intelligence);program debugging;public domain software;source code (software);system monitoring","LogOptPlus;logging optimization;catch programming constructs;if programming constructs;software logging;software development practice;program execution points logging;software debugging;log statements;optimal logging;software developers;machine learning based tool;source code;catch-blocks logging;if-blocks logging;logging prediction;open source projects;Apache Tomcat;CloudStack;code construct types","","8","","8","","25 Aug 2016","","","IEEE","IEEE Conferences"
"Malware log files for Internet investigation using hadoop: A review","M. S. M. Deli; S. A. Ismail; N. Kama; O. M. Yusop; A. Azmi; Y. Yahya","Advanced Informatics School, Level 5, Menara Razak, Universiti Teknologi Malaysia, Jalan Sultan Yahya Petra, Malaysia; Advanced Informatics School, Level 5, Menara Razak, Universiti Teknologi Malaysia, Jalan Sultan Yahya Petra, Malaysia; Advanced Informatics School, Level 5, Menara Razak, Universiti Teknologi Malaysia, Jalan Sultan Yahya Petra, Malaysia; Advanced Informatics School, Level 5, Menara Razak, Universiti Teknologi Malaysia, Jalan Sultan Yahya Petra, Malaysia; Advanced Informatics School, Level 5, Menara Razak, Universiti Teknologi Malaysia, Jalan Sultan Yahya Petra, Malaysia; Advanced Informatics School, Level 5, Menara Razak, Universiti Teknologi Malaysia, Jalan Sultan Yahya Petra, Malaysia","2017 IEEE Conference on Big Data and Analytics (ICBDA)","8 Feb 2018","2017","","","87","92","On the Internet, malware is one of the most serious threats to system security. Major complex issues and problems on some software systems are oftentimes made by malware. Malware can infect any computer software that causes connection to Internet infrastructure. There are many types of malware and some of the popular malware are Botnet, Trojans, Viruses, Spyware and Adware. Internet users with lesser knowledge of the malware threats are susceptible to this issue. To protect and prevent the computer and internet users from exposing themselves towards malware attacks, identifying the attacks through investigating malware log file is an essential step to curb this threat. The log file exposes crucial information in identifying the malware, such as algorithm and functional characteristic, the network interaction between the source and the destination, and type of malware. By nature, the log file size is humongous and requires the investigation process to be executed on faster and stable platforms such as the big data environment. In this study, the authors had adopted Hadoop, an open source software framework to process and extract the information from the malware log files. The information of data will be used for further prevention and protection from malware threats.","","978-1-5386-0790-9","10.1109/ICBDAA.2017.8284112","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8284112","Internet security;Types of Malware;Malware log files;Big Data environment;Hadoop environment;Log file processing;Log files in Hadoop","Big Data;Spyware;Task analysis;Security;Botnet;Trojan horses","Big Data;data handling;Internet;invasive software;parallel processing;public domain software","malware log file;malware threats;Internet investigation;malware attacks;Internet users;Hadoop;system security;computer software;Internet infrastructure;Botnet;Trojans;Spyware;Viruses;Adware;big data environment;open source software framework","","","","33","","8 Feb 2018","","","IEEE","IEEE Conferences"
"Aspect-oriented modeling in software architecture pattern based on UML","Hui Li; Jingjun Zhang; Yuejuan Chen","School of Information Engineering, Handan College,Hebei, China; Scientific Research Office, Hebei University of Engineering, Handan, China; School of Information & Electronic Engineering, Hebei University of Engineering, Handan, China","2010 The 2nd International Conference on Computer and Automation Engineering (ICCAE)","19 Apr 2010","2010","3","","575","578","Aspect Oriented Programming (AOP) aims at addressing the problem of the crosscutting concerns (e.g. logging, validation, transaction etc.), those functionalities that are scattered among several modules in a given system. Aspects can be defined to modularize such concerns. In this work, we focus on the most popular MVC pattern in software architecture pattern, and introduce the aspect-oriented programming ideas into the MVC model with Unified Modeling Language (UML), also we propose a model of aspect-oriented MVC pattern, which extracts crosscutting concerns of going through the system to form an aspect layer and uses the configuration file to statement the point of weaving. To assess the effects of the idea on MVC pattern, an actual system is carried out. In the end, we report the results of the pattern of the feasibility and superiority.","","978-1-4244-5586-7","10.1109/ICCAE.2010.5451819","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5451819","Aspect-Oriented Programming;UML;MVC;Software Architecture;Object-Oriented Programming","Software architecture;Unified modeling language;Object oriented modeling;Object oriented programming;Logic;Weaving;Application software;Java;Educational institutions;Functional programming","aspect-oriented programming;software architecture;Unified Modeling Language","aspect-oriented modeling;software architecture pattern;UML;aspect oriented programming;Unified Modeling Language","","2","","14","","19 Apr 2010","","","IEEE","IEEE Conferences"
"Novel, highly-parallel software for the online storage system of the ATLAS experiment at CERN: Design and performances","T. Colombo; W. Vandelli","University of Pavia, Italy; European Organization for Nuclear Research (CERN), Switzerland","2012 18th IEEE-NPSS Real Time Conference","24 Jan 2013","2012","","","1","6","The ATLAS experiment observes proton-proton collisions delivered by the LHC accelerator at CERN. The ATLAS Trigger and Data Acquisition (TDAQ) system selects interesting events on-line in a three-level trigger system in order to store them at a budgeted rate of several hundred Hz, for an average event size of ~1.5 MB. This paper focuses on the TDAQ data-logging system and in particular on the implementation and performance of a novel software design, reporting on the effort of exploiting the full power of multi-core hardware. In this respect, the main challenge presented by the data-logging workload is the conflict between the largely parallel nature of the event processing, including the recently introduced on-line event-compression, and the constraint of sequential file writing and checksum evaluation. This is further complicated by the necessity of operating in a fully data-driven mode, to cope with continuously evolving trigger and detector configurations. In this paper we will briefly discuss our development experience using recent concurrency-oriented libraries. We will then concentrate on the results of performance measurements performed on the current data-logging hardware. We will show that, even in the worst workload, the new parallel design is able to compete with the previous single-threaded one, while it is outperforming it in more favourable, realistic workloads. We will as well demonstrate the minimal overhead introduced by the above parallel techniques, considering the whole data-logging software performances with respect to the bare processing speed on the same hardware. Finally, we will discuss the effects of simultaneous multi-threading technologies, as found on recent CPUs. The data-logging operation in fact, mixing data processing and I/O, allows to efficiently exploit the features provided by these technologies.","","978-1-4673-1084-0","10.1109/RTC.2012.6418361","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6418361","","Instruction sets;Writing;Throughput;Hardware;Data acquisition;Large Hadron Collider;Production systems","concurrency control;data acquisition;data loggers;high energy physics instrumentation computing;multiprocessing systems;multi-threading;nuclear electronics;performance evaluation;position sensitive particle detectors;software libraries;trigger circuits","parallel software;online storage system;ATLAS experiment;CERN design;LHC accelerator;TDAQ data logging system;Trigger and Data Acquisition system;three-level trigger system;software design;multicore hardware;data-logging workload;event processing;on-line event-compression;sequential file writing constraint;checksum evaluation;concurrency-oriented libraries;detector performance measurements;data-logging hardware operation;single-threaded design;parallel techniques;multithreading technologies;CPU;I/O operation;proton+proton collision","","","","10","","24 Jan 2013","","","IEEE","IEEE Conferences"
"Conformance Checking: Workflow of Hospitals and Workflow of Open-Source EMRs","E. Asare; L. Wang; X. Fang","Mathematics and Big Data, Anhui University of Science and Technology, Huainan, China; Mathematics and Big Data, Anhui University of Science and Technology, Huainan, China; Mathematics and Big Data, Anhui University of Science and Technology, Huainan, China","IEEE Access","5 Aug 2020","2020","8","","139546","139566","Open Source Electronic Medical Records (EMR) and Electronic Health Records (EHR) are widely used in healthcare institutions because it is mostly free and customizable. Generally, EMRs and EHRs are used in healthcare institutions because their adoption reduces costs and improves patient outcomes through increased efficiency. During the adoption of EMRs/EHRs, whether open-source or closed-source, the number one concern of healthcare institutions is their workflow. When adopting any open-source software, there is a lot to consider, “Free does not mean you have to compromise on utility.” Process mining helps to discover and analyze the actual process executions of an information system (IS). In this paper, we use process mining to check the conformance of the workflow of Open-Source EMRs (workflow from event logs of an EMR) and the workflow of hospitals (workflow of hospitals based on domain knowledge). We modeled the workflow of hospital processes using business process modeling notation (BPMN) and converted it into a Petri net. Event log extracted from an Open-Source EMR (OpenEMR) was preprocessed for process conformance checking in ProM Framework. We check the conformance of log and model using alignment and replay. We display the results based on four metrics (fitness, precision, simplicity, and generalization). Then, we filter logs to check the conformance of Role-based access controls. Our conformance checking results showed that processes in Open-Source EMR align with the processes executed by hospitals.","2169-3536","","10.1109/ACCESS.2020.3012147","National Natural Science Foundation of China; Natural Science Foundation of Educational Government of Anhui Province of China; Anhui Provincial Big Data Foundation; Foundation for top-notch academic disciplines of Anhui Province; Academic and Technology Leader Foundation of Anhui Province; Open Project Program of Key Laboratory of Embedded System and Service Computing of Ministry of Education; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9149860","Conformance checking;electronic health record (EHR);electronic medical record (EMR);open-source software (OSS);process mining;ProM","Data mining;Open source software;Computer bugs;Process control;Hospitals","authorisation;business data processing;data mining;electronic health records;health care;hospitals;Petri nets;public domain software","open-source EMRs;healthcare institutions;closed-source;open-source software;process mining;open source electronic medical records;OpenEMR;role-based access controls;ProM framework;process conformance checking;Petri net;BPMN;business process modeling notation;hospital processes;domain knowledge;event logs;information system;EHR;electronic health records","","1","","31","CCBY","27 Jul 2020","","","IEEE","IEEE Journals"
"Log management comprehensive architecture in Security Operation Center (SOC)","A. Madani; S. Rezayi; H. Gharaee","Network Security Group, ICT Security Faculty, Iran Telecommunication Research Center (ITRC), Tehran, Iran; Network Security Group, ICT Security Faculty, Iran Telecommunication Research Center (ITRC), Tehran, Iran; Network Security Group, ICT Security Faculty, Iran Telecommunication Research Center (ITRC), Tehran, Iran","2011 International Conference on Computational Aspects of Social Networks (CASoN)","1 Dec 2011","2011","","","284","289","With the widespread use of information, variety of security logs have increased greatly, which due need for security log management. Organizations requirements have imposed to collect, store, and analyze tremendous volumes of log data across entire infrastructure for extended durations and at increasingly granular levels. It is the process of generating, transmitting, storing, analyzing, and disposing security log data from network to databases. Due to the wide variety of logs, storing comprises different methods. Recorded events in collection module are processed, normalized and classified. Logs are stored in storage module in order to use in forensic, reviewing, auditing and providing further necessities of correlation module. Routine log correlation analysis is beneficial for identifying security incidents, policy violations, fraudulent activities, troubleshooting and operational network problems. So log management is an important and efficient activity in network monitoring. Finding an effective log management functional architecture for network events analysis is the main goal of this paper. In this paper, we aim to suggest log management architecture with more common functions that are used by vendors. By studying logging architectures the main functions are administration of log collection, normalizing, categorization, queuing prioritizing and storing logged events/alarms by sensors. Log functions are different but the suitable architecture must justify the functions to send a normative, synchronized and prioritized log in an efficient way. The mentioned functions are gathered from SIEM products characteristics. Suggested architecture includes functions and activities in log collection server and storage server.","","978-1-4577-1133-6","10.1109/CASON.2011.6085959","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6085959","log management;SOC;normalizing;storage;event fiields","Security;Servers;Computer architecture;Correlation;Monitoring;Software;System-on-a-chip","security of data","log management comprehensive architecture;security operation center;SOC;security log management;security log data;storage module;correlation module;security incidents;policy violations;fraudulent activities;operational network problems","","8","1","15","","1 Dec 2011","","","IEEE","IEEE Conferences"
"LogSayer: Log Pattern-driven Cloud Component Anomaly Diagnosis with Machine Learning","P. Zhou; Y. Wang; Z. Li; X. Wang; G. Tyson; G. Xie","University of Chinese Academy of Sciences; ICT/CAS, Chinese Academy of Sciences; Purple Mountain Laboratories,China; Stony Brook University; QMUL; CNIC/CAS, Chinese Academy of Sciences","2020 IEEE/ACM 28th International Symposium on Quality of Service (IWQoS)","6 Oct 2020","2020","","","1","10","Anomaly diagnosis is a critical task for building a reliable cloud system and speeding up the system recovery form failures. With the increase of scales and applications of clouds, they are more vulnerable to various anomalies, and it is more challenging for anomaly troubleshooting. System logs that record significant events at critical time points become excellent sources of information to perform anomaly diagnosis. Never-theless, existing log-based anomaly diagnosis approaches fail to achieve high precision in highly concurrent environments due to interleaved unstructured logs. Besides, transient anomalies that have no obvious features are hard to detect by these approaches. To address this gap, this paper proposes LogSayer, a log pattern-driven anomaly detection model. LogSayer represents the system state by identifying suitable statistical features (e.g. frequency, surge), which are not sensitive to the exact log sequence. It then measures changes in the log pattern when a transient anomaly occurs. LogSayer uses Long Short-Term Memory (LSTM) neural networks to learn the historical correlation of log patterns and applies a BP neural network for adaptive anomaly decisions. Our experimental evaluations over the HDFS and OpenStack data sets show that LogSayer outperforms the state-of-the-art log-based approaches with precision over 98%.","1548-615X","978-1-7281-6887-6","10.1109/IWQoS49365.2020.9212954","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9212954","log pattern;anomaly diagnosis;deep learning","Task analysis;Feature extraction;Anomaly detection;Training;Transient analysis;Machine learning;Quality of service","backpropagation;cloud computing;concurrency control;recurrent neural nets;software fault tolerance;statistical analysis;system monitoring;system recovery","OpenStack data sets;HDFS;BP neural network;LSTM;long short-term memory;statistical features;log pattern-driven anomaly detection;system logs;log-based anomaly diagnosis;transient anomalies;interleaved unstructured logs;concurrent environments;anomaly troubleshooting;system recovery;cloud system;machine learning;log pattern-driven cloud component anomaly diagnosis;adaptive anomaly decisions;log sequence;system state;LogSayer","","","","36","","6 Oct 2020","","","IEEE","IEEE Conferences"
"iLSE: An Intelligent Web-Based System for Log Structuring and Extraction","S. Serasinghe; H. Shen; D. Chen","Dept. of Comput., Inf. Inst. of Technol., Colombo, Sri Lanka; Coll. of Sci. & Eng., Flinders Univ., Adelaide, SA, Australia; Sch. of Inf. & Commun. Technol., Griffith Univ., Brisbane, QLD, Australia","2017 24th Asia-Pacific Software Engineering Conference (APSEC)","5 Mar 2018","2017","","","588","593","Analysing software log files has become a challenging task due to the diversity in file structure and the nonstandardisation of log syntax. During the process of extracting log data, it is required to manually decode the log syntax and interpret data semantics, which can become tedious and is often error-prone if not performed carefully. Contemporary log analysis software tools do exist in the market and most of them offer numerous options to analyse log files, however, their sheer focus is on providing log management solutions instead of log analysis capabilities. In particular, none of them offers a generic parsing and extracting solution that can discover hidden data structures, a critical and effortful task in log analysis. We thereby devise such a solution that is able to automatically identify hidden patterns in a given log file and extract useful information by generalising the patterns. The solution is implemented as an intelligent Web-based system known as iLSE (intelligent Log Structuring and Extraction) whose users are not required to possess fluent programming skills. This paper presents a reference architecture for the system as well as a comparison study on how the system performs against contemporary log analysis systems.","","978-1-5386-3681-7","10.1109/APSEC.2017.70","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8305986","software log;structure;extraction;pattern;intelligent;Web-based;reference architecture","Data mining;Engines;Databases;Task analysis;Software;Syntactics","data analysis;data structures;Internet;software tools;system monitoring","file structure;log syntax;data semantics;contemporary log analysis software tools;analyse log files;log management solutions;log analysis capabilities;data structures;critical task;iLSE;contemporary log analysis systems;log data extraction;intelligent Web-based system;software log file analysis","","","","22","","5 Mar 2018","","","IEEE","IEEE Conferences"
"Analysis of execution log files","M. Nagappan","North Carolina State University, Oval Drive, Raleigh NC","2010 ACM/IEEE 32nd International Conference on Software Engineering","27 Oct 2011","2010","2","","409","412","Log analysis can be used to find problems, define operational profiles, and even pro-actively prevent issues. The goal of my dissertation research is to investigate log management and analysis techniques suited for very large and very complex logs, such as those we might expect in a computational cloud system.","1558-1225","978-1-60558-719-6","10.1145/1810295.1810405","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6062226","analysis of textual data;diagnosis;fault isolation;log files","Algorithm design and analysis;Fault diagnosis;Data mining;Heuristic algorithms;Software;Arrays;Security","cloud computing;system monitoring;systems analysis","execution log file analysis;operational profiles;proactively prevent issues;dissertation research;log management;computational cloud system","","2","","12","","27 Oct 2011","","","IEEE","IEEE Conferences"
"Tools and Benchmarks for Automated Log Parsing","J. Zhu; S. He; J. Liu; P. He; Q. Xie; Z. Zheng; M. R. Lyu","Huawei Noah's Ark Lab, China; The Chinese University of Hong Kong, China; Sun Yat-Sen University, China; ETH Zurich, Switzerland; Southwest Minzu University, China; Sun Yat-Sen University, China; The Chinese University of Hong Kong, China","2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)","19 Aug 2019","2019","","","121","130","Logs are imperative in the development and maintenance process of many software systems. They record detailed runtime information that allows developers and support engineers to monitor their systems and dissect anomalous behaviors and errors. The increasing scale and complexity of modern software systems, however, make the volume of logs explodes. In many cases, the traditional way of manual log inspection becomes impractical. Many recent studies, as well as industrial tools, resort to powerful text search and machine learning-based analytics solutions. Due to the unstructured nature of logs, a first crucial step is to parse log messages into structured data for subsequent analysis. In recent years, automated log parsing has been widely studied in both academia and industry, producing a series of log parsers by different techniques. To better understand the characteristics of these log parsers, in this paper, we present a comprehensive evaluation study on automated log parsing and further release the tools and benchmarks for easy reuse. More specifically, we evaluate 13 log parsers on a total of 16 log datasets spanning distributed systems, supercomputers, operating systems, mobile systems, server applications, and standalone software. We report the benchmarking results in terms of accuracy, robustness, and efficiency, which are of practical importance when deploying automated log parsing in production. We also share the success stories and lessons learned in an industrial application at Huawei. We believe that our work could serve as the basis and provide valuable guidance to future research and deployment of automated log parsing.","","978-1-7281-1760-7","10.1109/ICSE-SEIP.2019.00021","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8804456","log management;log parsing;log analysis;anomaly detection;AIOps","Tools;Software as a service;Benchmark testing;Software systems;Task analysis;Anomaly detection;Computer science","data mining;software maintenance;system monitoring","logs explodes;manual log inspection;log messages;automated log parsing;software systems;log datasets;Huawei;software development;software maintenance","","34","","57","","19 Aug 2019","","","IEEE","IEEE Conferences"
"Effect of timer interrupt interval on file system synchronization overhead","H. Son; S. Lee; Y. Won","Department of Computer Software, Hanyang University, Seoul, Korea; Department of Computer Software, Hanyang University, Seoul, Korea; Department of Computer Software, Hanyang University, Seoul, Korea","2016 IEEE International Conference on Network Infrastructure and Digital Content (IC-NIDC)","13 Jul 2017","2016","","","99","102","File system metadata is indispensable in both describing the data and maintaining the file system. Despite the importance of metadata in the file system, the overhead of maintaining the metadata cannot be taken lightly. It is because the metadata also have to be persisted on the storage device and it consumes IO bandwidth as well as creates journaling overhead. In this paper, we find that the random write with synchronous performance of a storage is significantly affected by not only the hardware performance but also timer interrupt interval of the kernel. Extending the timer interrupt interval allows reducing the write volume and increasing the random write followed by fsync() performance of EXT4 file system. We propose intermittent mtime timestamp update on Coarse grain mtime interval instead of Fine grain mtime interval. The experiment results with mtime update interval of 1 second show that the total write volume is decreased by 75% and 28%, respectively compared to total write volume of 1 ms and 10 ms mtime interval, and the throughput increased 3.1× and 1.2× compared to 1 ms and 10 ms mtime interval. Coarse grain mtime update is resolve to the journaling overhead issues while still logging mtime timestamp.","","978-1-5090-1246-6","10.1109/ICNIDC.2016.7974543","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7974543","Journaling overhead;EXT4 file system;Random write;Kernel timer interrupt;mtime","File systems;Metadata;Kernel;Performance evaluation;Throughput;Switches;Linux","file organisation;meta data;operating system kernels;system monitoring","kernel timer interrupt interval;file system synchronization overhead;file system metadata;file system maintainance;random write;write volume;fsync() performance;EXT4 file system;coarse grain mtime interval;fine grain mtime interval;mtime timestamp logging","","","","11","","13 Jul 2017","","","IEEE","IEEE Conferences"
"SMARTLOG: Place error log statement by deep understanding of log intention","Z. Jia; S. Li; X. Liu; X. Liao; Y. Liu","College of Computer Science, National University of Defense Technology, Changsha, China; College of Computer Science, National University of Defense Technology, Changsha, China; College of Computer Science, National University of Defense Technology, Changsha, China; College of Computer Science, National University of Defense Technology, Changsha, China; Big Data Research Center, Peking University, Beijing, China","2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)","5 Apr 2018","2018","","","61","71","Failure-diagnosis logs can dramatically reduce the system recovery time when software systems fail. Log automation tools can assist developers to write high quality log code. In traditional designs of log automation tools, they define log placement rules by extracting syntax features or summarizing code patterns. These approaches are, however, limited since the log placements are far beyond those rules but are according to the intention of software code. To overcome these limitations, we design and implement SmartLog, an intention-aware log automation tool. To describe the intention of log statements, we propose the Intention Description Model (IDM). SmartLog then explores the intention of existing logs and mines log rules from equivalent intentions. We conduct the experiments based on 6 real-world open-source projects. Experimental results show that SmartLog improves the accuracy of log placement by 43% and 16% compared with two state-of-the-art works. For 86 real-world patches aimed to add logs, 57% of them can be covered by SmartLog, while the overhead of all additional logs is less than 1%.","","978-1-5386-4969-5","10.1109/SANER.2018.8330197","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8330197","","Tools;Syntactics;Semantics;Automation;Software;Feature extraction;Training","program diagnostics;software quality;system monitoring;system recovery","log intention;failure-diagnosis logs;software systems;log automation tools;high quality log code;log placement;software code;SmartLog;intention-aware log automation tool;Intention Description Model;system recovery;code patterns;place error log statemen","","5","","39","","5 Apr 2018","","","IEEE","IEEE Conferences"
"Heuristic Recovery of Missing Events in Process Logs","W. Song; X. Xia; H. Jacobsen; P. Zhang; H. Hu","Sch. of Comput. Sci. & Eng., Nanjing Univ. of Sci. & Technol., Nanjing, China; Sch. of Comput. Sci. & Eng., Nanjing Univ. of Sci. & Technol., Nanjing, China; Applic. & Middleware Syst. Res. Group, Tech. Univ. of Munich, Munich, Germany; Coll. of Comput. & Inf., Hohai Univ., Nanjing, China; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China","2015 IEEE International Conference on Web Services","17 Aug 2015","2015","","","105","112","Event logs are of paramount significance for process mining and complex event processing. Yet, the quality of event logs remains a serious problem. Missing events of logs are usually caused by omitting manual recording, system failures, and hybrid storage of executions of different processes. It has been proved that the problem of minimum recovery based on a priori process specification is NP-hard. State-of-the-art approach is still lacking in efficiency because of the large search space. To address this issue, in this paper, we leverage the technique of process decomposition and present heuristics to efficiently prune the unqualified sub-processes that fail to generate the minimum recovery. We employ real-world processes and their incomplete sequences to evaluate our heuristic approach. The experimental results demonstrate that our approach achieves high accuracy as the state-of-the-art approach does, but it is more efficient.","","978-1-4673-7272-5","10.1109/ICWS.2015.24","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7195558","missing events;heuristic recovery;Petri nets;process decomposition;trace replaying","Firing;Petri nets;Time complexity;Business;Runtime;Data mining;Manuals","business data processing;computational complexity","heuristic missing events recovery;process logs;event logs;complex event processing;NP-hard;process specification;process decomposition","","5","","24","","17 Aug 2015","","","IEEE","IEEE Conferences"
"LogAssist: Assisting Log Analysis Through Log Summarization","S. Locke; H. Li; T. -H. P. Chen; W. Shang; W. Liu","Department of Computer Science and Software Engineering, Concordia University, 5618 Montreal, Quebec, Canada, (e-mail: s_loc@encs.concordia.ca); Computer Engineering and Software Engineering, Polytechnique Montreal, 5596 Montreal, Quebec, Canada, H3T 1J4 (e-mail: heng.li@polymtl.ca); Computer Science and Software Engineering, Concordia University, 5618 Montreal, Quebec, Canada, H3G 2W1 (e-mail: peterc@encs.concordia.ca); Department of Computer Science and Software Engineering, Concordia University, Montreal, Quebec, Canada, (e-mail: shang@encs.concordia.ca); Department of Computer Science and Software Engineering, Concordia University, 5618 Montreal, Quebec, Canada, (e-mail: w_liu201@encs.concordia.ca)","IEEE Transactions on Software Engineering","","2021","PP","99","1","1","Logs contain valuable information about the runtime behaviors of software systems. Thus, practitioners rely on logs for various tasks such as debugging, system comprehension, and anomaly detection. However, due to the unstructured nature and large size of logs, there are several challenges that practitioners face with log analysis. In this paper, we propose a novel approach called LogAssist that tackles these challenges and assists practitioners with log analysis. LogAssist provides an organized and concise view of logs by first grouping logs into event sequences (i.e., workflows), which better illustrate the system runtime execution paths. Then, LogAssist compresses the log events in workflows by hiding consecutive events and applying n-gram modeling to identify common event sequences. We evaluated LogAssist on the logs that are generated by two open-source and one enterprise system. We find that LogAssist can reduce the number of log events that practitioners need to investigate by up to 99%. Through a user study with 19 participants, we also find that LogAssist can assist practitioners by reducing the needed time on log analysis tasks by an average of 40%. The participants also rated LogAssist an average of 4.53 out of 5 for improving their experiences of performing log analysis. Finally, we document our experiences and lessons learned from developing and adopting LogAssist in practice. We believe that LogAssist and our reported experiences may lay the basis for future analysis and interactive exploration on logs.","1939-3520","","10.1109/TSE.2021.3083715","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9442364","Log analysis;log compression;n-gram modeling;log abstraction;workflow characterization;log reduction","Task analysis;Runtime;Tools;Testing;Faces;Anomaly detection;Software systems","","","","","","","IEEE","26 May 2021","","","IEEE","IEEE Early Access Articles"
"A survey on link failures in software defined networks","V. Muthumanikandan; C. Valliyammai","Department of Computer Technology, Anna University, Chennai, India; Department of Computer Technology, Anna University, Chennai, India","2015 Seventh International Conference on Advanced Computing (ICoAC)","8 Sep 2016","2015","","","1","5","Link failures are a major threat that occurs within network topology. Probably link failures occur due to low converging time, previously allocated delay and bandwidth, and iterative loops which degrade the performance of the network. Link failures results in invalid routes and unreachable routes within Ethernet based networks. Invalid routes record the duration of the each route containing failed link and summation of invalid route time. Unreachable routes records the total number of routes that are failed during network segregation. In cloud infrastructure, generally link failures have potential loss of too many small packets such as ACK and alive messages. In order to identify failures initially correlate the events log with network traffic which is observed in the links of an event.","","978-1-5090-1933-5","10.1109/ICoAC.2015.7562808","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7562808","Link recovery;Software Defined Networking;Open Flow","Routing;Computer architecture;Software defined networking;Computer network reliability;Reliability;Ports (Computers)","computer network reliability;local area networks;software defined networking;telecommunication network topology;telecommunication traffic","link failure;software defined network;network topology;iterative loop;Ethernet based network;invalid route time summation;network segregation;network traffic","","4","","14","","8 Sep 2016","","","IEEE","IEEE Conferences"
"A novel mind map based approach for log data extraction","P. W. D. C. Jayathilake","University of Moratuwa, Sri Lanka","2011 6th International Conference on Industrial and Information Systems","10 Oct 2011","2011","","","130","135","Software log file analysis helps immensely in software testing and troubleshooting. The first step in automated log file analysis is extracting log data. This requires decoding the log file syntax and interpreting data semantics. The expected output of this phase is an organization of the extracted data for further processing. Log data extractors can be developed using popular programming languages targeting one or few log file formats. Rather than repeating this process for each log file format, it is desirable to have a generic scheme for interpreting elements of a log file and filling a data structure suitable for further processing. The new log data extraction scheme introduced in this paper is an attempt to provide the advanced features demanded by modern log file analysis procedures. It is a generic scheme which is capable of handling both text and binary log files with complex structures and difficult syntax. Its output is a tree filled with the information of interest for the particular case.","2164-7011","978-1-4577-0035-4","10.1109/ICIINFS.2011.6038054","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6038054","log data extraction;log file modeling;log analysis;mind map","Data mining;Software;Semantics;XML;Grammar;Information systems;Monitoring","data structures;program testing","novel mind map;log data extraction;software log file analysis;software testing;software troubleshooting;log file syntax;data semantics;data extraction;log file format;data structure","","3","1","9","","10 Oct 2011","","","IEEE","IEEE Conferences"
"Cleaning structured event logs: A graph repair approach","J. Wang; S. Song; X. Lin; X. Zhu; J. Pei","Key Laboratory for Information System Security, MoE; TNList; School of Software, Tsinghua University, Beijing, China; Key Laboratory for Information System Security, MoE; TNList; School of Software, Tsinghua University, Beijing, China; University of New South Wales, Sydney, Australia; Key Laboratory for Information System Security, MoE; TNList; School of Software, Tsinghua University, Beijing, China; Simon Fraser University, Burnaby, BC, Canada","2015 IEEE 31st International Conference on Data Engineering","1 Jun 2015","2015","","","30","41","Event data are often dirty owing to various recording conventions or simply system errors. These errors may cause many serious damages to real applications, such as inaccurate provenance answers, poor profiling results or concealing interesting patterns from event data. Cleaning dirty event data is strongly demanded. While existing event data cleaning techniques view event logs as sequences, structural information do exist among events. We argue that such structural information enhances not only the accuracy of repairing inconsistent events but also the computation efficiency. It is notable that both the structure and the names (labeling) of events could be inconsistent. In real applications, while unsound structure is not repaired automatically (which needs manual effort from business actors to handle the structure error), it is highly desirable to repair the inconsistent event names introduced by recording mistakes. In this paper, we propose a graph repair approach for 1) detecting unsound structure, and 2) repairing inconsistent event name.","2375-026X","978-1-4799-7964-6","10.1109/ICDE.2015.7113270","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7113270","","Maintenance engineering;Labeling;Insulation;Cleaning;Petri nets;Databases;Approximation algorithms","business data processing;data handling;graph theory","structured event log cleaning;graph repair approach;system errors;dirty event data cleaning techniques;sequences;structural information;computation efficiency;business actors;unsound structure detection;inconsistent event name repairing","","13","","33","","1 Jun 2015","","","IEEE","IEEE Conferences"
"Log-Based Cloud Monitoring System for OpenStack","V. Agrawal; D. Kotia; K. Moshirian; M. Kim","Comput. Eng. Dept., San Jose State Univ., San Jose, CA, USA; Comput. Eng. Dept., San Jose State Univ., San Jose, CA, USA; Comput. Eng. Dept., San Jose State Univ., San Jose, CA, USA; Dept. of Comput. Sci. & Eng., Hankyong Nat. Univ., Anseong, South Korea","2018 IEEE Fourth International Conference on Big Data Computing Service and Applications (BigDataService)","9 Jul 2018","2018","","","276","281","Cloud systems are fast emerging remote computing environment, providing their users with computational resources and services on demand over the Internet. OpenStack is one representative open source platform which helps in building private clouds for the organization. Monitoring the activities in the cloud is a task of utmost importance and log files are often used for it. Logs management plays a very important role in determining various issues in the cloud network or helps in troubleshooting the issues. Often logs are cumbersome to manage and difficult to read. In this paper, we propose a system which monitor the logs of OpenStack components (i.e., Nova, Neutron, Cinder) in real-time and generate an alert for the information, debug, error, warning, and trace messages as soon as an issue recorded in the logs. It helps in easy readability to the administrators. Apart from that our logic also helps to generate warning messages prior to the different resource quota like virtual CPUs, Floating IP's, etc. A feature like that can help the administrator to increase the quota or release some and avoid the inconvenience caused to the users creates virtual machines. We develop our proposed system and demonstrate its feasibility and performances through simulation.","","978-1-5386-5119-3","10.1109/BigDataService.2018.00049","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8405724","Cloud-Monitoring;Log-based;Real-time-Monitoring;OpenStack;Convenient-Management","Monitoring;Cloud computing;Neutrons;Data visualization;Computer architecture;Servers","cloud computing;public domain software","log-based cloud monitoring system;remote computing environment;computational resources;private clouds;log files;logs management;cloud network;OpenStack components;open source platform;Internet","","2","","16","","9 Jul 2018","","","IEEE","IEEE Conferences"
"Open Stack Secure Enterprise File Sync and Share Turnkey Solution","Y. Kuo; T. Yeh; G. Zheng; J. Wu; C. Yang; J. Lin","Inst. for Inf. Ind., Cloud Syst. Software Inst., Taipei, Taiwan; Inst. for Inf. Ind., Cloud Syst. Software Inst., Taipei, Taiwan; Inst. for Inf. Ind., Cloud Syst. Software Inst., Taipei, Taiwan; Inst. for Inf. Ind., Cloud Syst. Software Inst., Taipei, Taiwan; Inst. for Inf. Ind., Cloud Syst. Software Inst., Taipei, Taiwan; Inst. for Inf. Ind., Cloud Syst. Software Inst., Taipei, Taiwan","2014 IEEE 6th International Conference on Cloud Computing Technology and Science","12 Feb 2015","2014","","","1015","1020","The Enterprise File Sync and Share (EFSS) is one of the most important services to provide enterprises' employees with cloud file sync, share, and collaboration services. To take enterprises' concerns into account, such as security, privacy, compliance, and regulation, the existing EFSS solutions are either using private (on-premise) or hybrid cloud service model to provide their services. They usually emphasize that files stored in the solutions are encrypted on transfer and at rest and events occurred in the service are logged as the audit trail. However, support of data encryption and audit trail are not capable of protecting enterprise sensitive data from not well addressed security issues of the EFSS service. The security issues, including employee privacy protection, management of share links and synchronized cloud files, and the secure enterprise directory integration, are pointed out in this article. To address these issues, this work proposes and develops a scalable Secure EFSS service which can be deployed on the on-premise Open Stack cloud infrastructure to securely provide employees with EFSS service. Designs of an integrated security approach are introduced in this article, including data and metadata isolations, Distinct Share Link utility, encryption key management for personal and shared files, sandbox-based cloud file synchronization, and out-of-band authentication method.","","978-1-4799-4093-6","10.1109/CloudCom.2014.17","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7037799","enterprise file sync and share;Open Stack;security","Synchronization;Authentication;Encryption;Databases;File systems","cloud computing;security of data","open stack secure enterprise file sync and share turnkey Solution;data encryption;security issues;employee privacy protection;share links;synchronized cloud files;secure enterprise directory integration;scalable secure EFSS service;integrated security approach;share link utility;encryption key management;sandbox-based cloud file synchronization;out-of-band authentication method","","","1","14","","12 Feb 2015","","","IEEE","IEEE Conferences"
"A technique to search log records using system of linear equations","L. C. Savade; S. Chavan","Department of Information Technology, PES's Modern College of Engineering, Pune, India; Department of Information Technology, Cummins College of Engineering for Women, Pune, India","2012 CSI Sixth International Conference on Software Engineering (CONSEG)","10 Nov 2012","2012","","","1","4","Growth of network technology and easy internet access are the key features for development and use of Database-as-a-Service (DaaS) technique. In DaaS technique it is necessary to ensure safe and correct operation of any industry. In many cases, the types of data records used to conduct security audits are considered sensitive, as they could reveal information about internal network structures, the types of software running, private customer or employee information. In this paper we suggest a technique based on system of linear equations, which enables a trusted party to give the service provider's server the ability to test whether a given keyword appears in log records but the server learns nothing about the keyword and the log content. We compare our scheme with existing encryption schemes and prove that our scheme is efficient and secure.","","978-1-4673-2177-8","10.1109/CONSEG.2012.6349471","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6349471","Database as a service;security;log records","Databases;Encryption;Servers;Protocols;Privacy","authorisation;cloud computing;information retrieval;public key cryptography;trusted computing","log record search;linear equation system;network technology;Internet access;database-as-a-service technique;DaaS technique;security audits;internal network structure information;private customer information;employee information;trusted party;service provider server;keywords;log content","","","18","21","","10 Nov 2012","","","IEEE","IEEE Conferences"
"Unraveling and Learning Workflow Models from Interleaved Event Logs","X. Liu","Dept. of Comput. Sci., Rochester Inst. of Technol., Rochester, NY, USA","2014 IEEE International Conference on Web Services","4 Dec 2014","2014","","","193","200","Business process mining is to extract process knowledge from a system's log in order to reconstruct workflow models. Existing approaches treat a log record as an instance of one workflow model. They do not deal with interleaved logs, where each log record is a mixture of multiple workflow traces. However, such an interleaved log is typical for many systems especially web-based ones where all the user-system interaction traces are recorded and maintained by a web server. Dealing with interleaved logs is challenging due to the lack of prior knowledge of workflow models and noises contained in the log data. In this paper, we propose a two-phase workflow learning process. During the first phase, we use a probabilistic approach to learn the links between operations and the hidden workflow models. We consider a workflow model as a probabilistic distributions over operations and derive it through likelihood maximization. This allows us to identify the membership of an operation to a workflow model, which can be used to unravel a log record and generate a set of workflow instances from it. During the second phase, the sequential patterns between operations within each workflow model are derived from all its instances. We have conducted a comprehensive experimental study, which indicates the effectiveness of the proposed solution.","","978-1-4799-5054-6","10.1109/ICWS.2014.38","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6928898","Process mining;workflow model discovery;interleaved logs;topic modeling;probabilistic models","Hidden Markov models;Probabilistic logic;Mathematical model;Equations;Business;Computational modeling;Data mining","business data processing;data mining;learning (artificial intelligence);statistical distributions;workflow management software","workflow models;interleaved event logs;business process mining;user-system interaction;Web-based systems;two-phase workflow learning process;probabilistic distribution","","5","","15","","4 Dec 2014","","","IEEE","IEEE Conferences"
"Anvaya: An Algorithm and Case-Study on Improving the Goodness of Software Process Models Generated by Mining Event-Log Data in Issue Tracking Systems","P. Juneja; D. Kundra; A. Sureka","Indraprastha Inst. of Inf. Technol., Delhi, India; Indraprastha Inst. of Inf. Technol., Delhi, India; ABB Res., India","2016 IEEE 40th Annual Computer Software and Applications Conference (COMPSAC)","25 Aug 2016","2016","1","","53","62","Issue Tracking Systems (ITS) such as Bugzilla can be viewed as Process Aware Information Systems (PAIS) generating event-logs during the life-cycle of a bug report. Process Mining consists of mining event logs generated from PAIS for process model discovery, conformance and enhancement. We apply process map discovery techniques to mine event trace data generated from ITS of open source Firefox browser project to generate and study process models. Bug life-cycle consists of diversity and variance. Therefore, the process models generated from the event-logs are spaghetti-like with large number of edges, inter-connections and nodes. Such models are complex to analyse and difficult to comprehend by a process analyst. We improve the Goodness (fitness and structural complexity) of the process models by splitting the event-log into homogeneous subsets by clustering structurally similar traces. We adapt the K-Medoid clustering algorithm with two different distance metrics: Longest Common Subsequence (LCS) and Dynamic Time Warping (DTW). We evaluate the goodness of the process models generated from the clusters using complexity and fitness metrics. We study back-forth and self-loops, bug reopening, and bottleneck in the clusters obtained and show that clustering enables better analysis. We also propose an algorithm to automate the clustering process-the algorithm takes as input the event log and returns the best cluster set.","0730-3157","978-1-4673-8845-0","10.1109/COMPSAC.2016.64","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7551993","Bug Tracking System;Clustering;Mining Software Repositories;Process Mining;Process Model Fitness Metric;Process Model Structural Complexity","Computer bugs;Data mining;Complexity theory;Clustering algorithms;Measurement;Data models;Software","data mining;pattern clustering;program debugging","Anvaya;software process models;event-log data mining;issue tracking systems;Bugzilla;process aware information systems;PAIS;event log generation;bug reporting;process mining;event log mining;process model discovery;process map discovery;event trace data mining;ITS;open source Firefox browser project;bug life-cycle;event-logs;homogeneous subsets;structurally similar trace clustering;k-medoid clustering algorithm;distance metrics;longest common subsequence;LCS;dynamic time warping;DTW;bug reopening;clustering process automation","","1","","30","","25 Aug 2016","","","IEEE","IEEE Conferences"
"Discovering signature patterns from event logs","R. P. J. C. Bose; W. M. P. van der Aalst","Eindhoven University of Technology, The Netherlands 5600 MB; Eindhoven University of Technology, The Netherlands 5600 MB","2013 IEEE Symposium on Computational Intelligence and Data Mining (CIDM)","16 Sep 2013","2013","","","111","118","More and more information about processes is recorded in the form of so-called “event logs”. High-tech systems such as X-ray machines and high-end copiers provide their manufacturers and services organizations with detailed event data. Larger organizations record relevant business events for process improvement, auditing, and fraud detection. Traces in such event logs can be classified as desirable or undesirable (e.g., faulty or fraudulent behavior). In this paper, we present a comprehensive framework for discovering signatures that can be used to explain or predict the class of seen or unseen traces. These signatures are characteristic patterns that can be used to discriminate between desirable and undesirable behavior. As shown, these patterns can, for example, be used to predict remotely whether a particular component in an X-ray machine is broken or not. Moreover, the signatures also help to improve systems and organizational processes. Our framework for signature discovery is fully implemented in ProM and supports class labeling, feature extraction and selection, pattern discovery, pattern evaluation and cross-validation, reporting, and visualization. A real-life case study is used to demonstrate the applicability and scalability of the approach.","","978-1-4673-5895-8","10.1109/CIDM.2013.6597225","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6597225","Process Mining;Signature Patterns;Event Log;Discriminatory Patterns","Feature extraction;Labeling;Support vector machines;Association rules;Measurement;Computer aided software engineering","data mining;fault diagnosis;feature extraction;X-ray apparatus","signature patterns;signature discovery;event logs;X-ray machines;high-end copiers;process improvement;fraud detection;characteristic patterns;ProM;class labeling;feature extraction;pattern discovery;pattern evaluation","","17","","34","","16 Sep 2013","","","IEEE","IEEE Conferences"
"Discovering models of parallel workflow processes from incomplete event logs","J. Lekic; D. Milicev","University of Pristina in Kosovska Mitrovica, Faculty of Technical Sciences, Kneza Milosa 7, 38220 Kosovska Mitrovica, Serbia; University of Belgrade, Faculty of Electrical Engineering, Bulevar kralja Aleksandra 73, 11120 Beograd, Serbia","2015 3rd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)","12 Nov 2015","2015","","","477","482","α-algorithm is able to discover a large class of workflow (WF) nets based on the behavior recorded in event logs, with the main limiting assumption that the event log is complete. Our research has been aimed at finding ways of business process models discovering based on examples of traces, i.e., logs of workflow actions that do not meet the requirement of completeness. In this aim, we have modified the existing and introduced a new relation between activities recorded in the event log, which has led to a partial correction of the process models discovering techniques, including the α-algorithm. We have also introduced the notions of causally and weakly complete logs, from which our modified algorithm can produce the same result as the original algorithm from complete logs. The effect of these modifications on the speed of the process model discovering is mostly evident for business processes in which many activities can be performed in parallel. Therefore, this paper presents preliminary results obtained from the investigation of opportunities to discover models of parallel processes based on incomplete event logs.","","978-989-758-136-6","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7323138","Process Mining;Process Model Discovery;Parallel Business Processes;Incomplete Event Log;α-algorithm","Business;Concurrent computing;Petri nets;Limiting;Parallel processing;Electrical engineering;Guidelines","business data processing;data mining;parallel processing","parallel workflow process;process mining;process model discovery;α-algorithm;business process model;models discovering technique","","","","10","","12 Nov 2015","","","IEEE","IEEE Conferences"
"NITRSCT: A Software Security tool for collection and analysis of Kernel Calls","P. Kishore; S. K. Barisal; S. Vaish","NIT Rourkela,Dept. of CSE,Rourkela,India; NIT Rourkela,Dept. of CSE,Rourkela,India; Bharati Vidyapeeth COE,Dept. of IT,Pune,India","TENCON 2019 - 2019 IEEE Region 10 Conference (TENCON)","12 Dec 2019","2019","","","510","515","Software security is the way of developing software such that, it should function well under malicious attack. In this paper, we design and develop an automation tool named NITRSCT (NITR System Call Tracer), using Microsoft's package to listen to the system calls generated by processes or threads during a context switch. It will allow any version of the Windows Operating System to serve as a system call tracer. The said Windows Service can be installed and controlled through Windows Service Manager. This service stores the sequence of system calls in the form of a trace file as well as a text file along with the SQL database for more verbosity. This sequence represents the normal behaviour of the software. A well-defined log file with different level of verbosity along with the Event Manager enables the user to be well informed about the errors that might have incurred while running services on their systems. Therefore, the dataset will help improve the true positive rates detected by anomaly detection systems.","2159-3450","978-1-7281-1895-6","10.1109/TENCON.2019.8929513","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8929513","Dataset;Software Security;System Calls;Event Manager;Context Switch","Kernel;Tools;Instruction sets;Microsoft Windows;Switches;Databases","operating system kernels;security of data;software engineering;SQL;user interfaces","Microsoft package;Windows service manager;text file;trace file;service stores;Windows operating system;context switch;system calls;NITR System;automation tool;malicious attack;kernel calls;software security tool;NITRSCT;anomaly detection systems;running services;well-defined log file","","3","","20","","12 Dec 2019","","","IEEE","IEEE Conferences"
"An interface for coupling optimization algorithms with EPANET in discrete event simulation platforms","L. K. Letting; Y. Hamam; A. M. Abu-Mahfouz","Department of Electrical Engineering, Tshwane University of Technology, Pretoria, South Africa; Department of Electrical Engineering, Tshwane University of Technology, Pretoria, South Africa; Department of Electrical Engineering, Tshwane University of Technology, Pretoria, South Africa","2017 IEEE 15th International Conference on Industrial Informatics (INDIN)","13 Nov 2017","2017","","","1235","1240","The application of simulation optimization in water distribution network analysis and design is a promising method for generating solutions to existing challenges. The absence of a standard interface for coupling the open source EPANET software package to optimization algorithms increases the implementation effort and limits the comparison of results. This work presents a methodology for implementing an interface for coupling optimization algorithms with EPANET. The proposed technique uses the internal simulation clock events in a discrete event simulation platform to co-ordinate optimization loops and data exchange. The utilization of intermediate input/output files is avoided in order to increase the simulation speed. A water distribution network implemented in the EPANET solver is considered as a discrete event to be interfaced with optimization algorithms. The interface module is implemented as a C/C++ mex-file for EPANET in the MATLAB/Simulink platform. The methodology enables the user to evaluate the fitness of the design parameters with easy access to data logging and visualization tools at run-time. The proposed technique is used to implement the particle swarm optimization algorithm (PSO) and applied to design a benchmark water distribution network.","2378-363X","978-1-5386-0837-1","10.1109/INDIN.2017.8104951","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8104951","","Optimization;Algorithm design and analysis;Discrete event simulation;Software;Engines;Sociology;Statistics","discrete event simulation;electronic data interchange;particle swarm optimisation;public domain software;software packages;user interfaces;water supply","optimization algorithms;discrete event simulation platform;simulation optimization;water distribution network analysis;open source EPANET software package;internal simulation clock events;data exchange;interface module;particle swarm optimization algorithm;water distribution network design;PSO;C/C++ mex-file;MATLAB/Simulink platform","","","","21","","13 Nov 2017","","","IEEE","IEEE Conferences"
"Machine Deserves Better Logging: A Log Enhancement Approach for Automatic Fault Diagnosis","T. Jia; Y. Li; C. Zhang; W. Xia; J. Jiang; Y. Liu","Peking Univ., Beijing, China; Peking Univ., Beijing, China; Peking Univ., Beijing, China; Peking Univ., Beijing, China; Tencent Inc., Shenzhen, China; Tencent Inc., Shenzhen, China","2018 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)","18 Nov 2018","2018","","","106","111","When systems fail, log data is often the most important information source for fault diagnosis. However, the performance of automatic fault diagnosis is limited by the ad-hoc nature of logs. The key problem is that existing developer-written logs are designed for humans rather than machines to automatically detect system anomalies. To improve the quality of logs for fault diagnosis, we propose a novel log enhancement approach which automatically identifies logging points that reflect anomalous behavior during system fault. We evaluate our approach on three popular software systems AcmeAir, HDFS and TensorFlow. Results show that it can significantly improve fault diagnosis accuracy by 50% on average compared to the developers' manually placed logging points.","","978-1-5386-9443-5","10.1109/ISSREW.2018.00-22","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8539172","Log enhancement, Automatic fault diagnosis, logging points","Fault diagnosis;Redundancy;Filtering algorithms;Filtering;Software systems;Optimization","data mining;fault diagnosis;program diagnostics;software fault tolerance","logging points;system fault;automatic fault diagnosis;important information source;system anomalies;log enhancement;developer-written logs;log data;software systems;AcmeAir;HDFS;TensorFlow;log file mining","","2","","15","","18 Nov 2018","","","IEEE","IEEE Conferences"
"Automatic Generation of Configuration Manuals for Open-Source Software","Y. Murakami; E. Kagawa; N. Funabiki","Dept. of Electr. & Comput. Eng., Kagawa Nat. Coll. of Technol., Takamatsu, Japan; Fac. of Eng., Kagawa Univ., Takamatsu, Japan; Grad. Sch. of Natural Sci. & Technol., Okayama Univ., Okayama, Japan","2011 International Conference on Complex, Intelligent, and Software Intensive Systems","18 Aug 2011","2011","","","653","658","The installation and initial configuration of an open-source software can be difficult for novice users because of a common lack of adequate manuals. To address this problem, we have proposed a method of automatically generating Web based installation manuals for open-source software by editing the log information recorded during the installation process by a skilled user. A lot of systems using open-source software require changes of configuration files. Actually, middleware systems such as application servers and database management systems, often exhibit the low performance under their default configuration files. Thus, the optimization of such systems by setting the optimal confirmation is essential for significant performance improvements. In this paper, we propose the extension of our previous method to the automatic generation of Web-based configuration manuals. We verify the effectiveness of our proposal through experiments.","","978-1-61284-709-2","10.1109/CISIS.2011.109","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5989055","open source software;configuration manual;automatic generation;Web application","Manuals;Layout;Web pages;Browsers;Open source software;Documentation","database management systems;Internet;middleware;public domain software;user manuals","automatic generation;open source software;novice user;Web-based installation manual;middleware system;database management system;configuration file","","2","","12","","18 Aug 2011","","","IEEE","IEEE Conferences"
"LogTracker: Learning Log Revision Behaviors Proactively from Software Evolution History","S. Li; X. Niu; Z. Jia; J. Wang; H. He; T. Wang",National University of Defense Technology; National University of Defense Technology; National University of Defense Technology; National University of Defense Technology; National University of Defense Technology; National University of Defense Technology,"2018 IEEE/ACM 26th International Conference on Program Comprehension (ICPC)","30 Jan 2020","2018","","","178","17810","Log statements are widely used for postmortem debugging. Despite the importance of log messages, it is difficult for developers to establish good logging practices. There are two main reasons for this. First, there are no rigorous specifications or systematic processes to guide the practices of software logging. Second, logging code co-evolves with bug fixes or feature updates. While previous works on log enhancement have successfully focused on the first problem, they are hard to solve the latter. For taking the first step towards solving the second problem, this paper is inspired by code clones and assumes that logging code with similar context is pervasive in software and deserves similar modifications. To verify our assumptions, we conduct an empirical study on eight open-source projects. Based on the observation, we design and implement LogTracker, an automatic tool that can predict log revisions by mining the correlation between logging context and modifications. With an enhanced modeling of logging context, LogTracker is able to guide more intricate log revisions that cannot be covered by existing tools. We evaluate the effectiveness of LogTracker by applying it to the latest version of subject projects. The results of our experiments show that LogTracker can detect 199 instances of log revisions. So far, we have reported 25 of them, and 6 have been accepted.","2643-7171","978-1-4503-5714-2","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8973078","Software reliability;Software evolution;Maintaining software","","data mining;learning (artificial intelligence);program debugging;public domain software;software maintenance","LogTracker;software evolution history;log statements;log messages;software logging;log enhancement;log revisio learning;correlation mining;postmortem debugging","","","","39","","30 Jan 2020","","","IEEE","IEEE Conferences"
"Improving Log-Based Anomaly Detection with Component-Aware Analysis","K. Yin; M. Yan; L. Xu; Z. Xu; Z. Li; D. Yang; X. Zhang","Ministry of Education,Key Laboratory of Dependable Service Computing in Cyber Physical Society (Chongqing University),China; Ministry of Education,Key Laboratory of Dependable Service Computing in Cyber Physical Society (Chongqing University),China; Ministry of Education,Key Laboratory of Dependable Service Computing in Cyber Physical Society (Chongqing University),China; Ministry of Education,Key Laboratory of Dependable Service Computing in Cyber Physical Society (Chongqing University),China; Ministry of Education,Key Laboratory of Dependable Service Computing in Cyber Physical Society (Chongqing University),China; Chongqing University,School of Big Data & Software Engineering,China; Ministry of Education,Key Laboratory of Dependable Service Computing in Cyber Physical Society (Chongqing University),China","2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)","2 Nov 2020","2020","","","667","671","Logs are universally available in software systems for troubleshooting. They record system run-time states and messages of system activities. Log analysis is an effective way to diagnosis system exceptions, but it will take a long time for engineers to locate anomalies accurately through logs. Many automatic approaches have been proposed for log-based anomaly detection. However, most of the prior approaches did not consider the corresponding system component of a log message. Such component records the log location, which can help detect the location-sequence-related anomalies. In this paper, we propose LogC, a new Log -based anomaly detection approach with Component-aware analysis. LogC contains two phases: (i) turning log messages into log template sequences and component sequences, (ii) feeding such two sequences to train a combined LSTM model for detecting anomalous logs. LogC only needs normal log sequences to train the combined model. We evaluate LogC on two open-source log datasets: HDFS and ThunderBird. Experimental results show that LogC overall outperforms three baselines (i.e., PCA, IM, and DeepLog) in terms of three metrics (precision, recall, and F-measure).","2576-3148","978-1-7281-5619-4","10.1109/ICSME46990.2020.00069","Research and Development; Fundamental Research Funds for the Central Universities; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9240683","Anomaly Detection;Log Analysis;Deep Learning","Measurement;Software maintenance;Turning;Software systems;Anomaly detection;Open source software;Principal component analysis","data analysis;principal component analysis;security of data;system monitoring","log-based anomaly detection;software systems;record system run-time states;system activities;Log analysis;diagnosis system exceptions;system component;log message;component records;log location;location-sequence-related anomalies;LogC;anomaly detection approach;log template sequences;component sequences;anomalous logs;normal log sequences;open-source log datasets;component-aware analysis","","","","22","","2 Nov 2020","","","IEEE","IEEE Conferences"
"Automatic detection of multi-line templates in software log files","P. W. D. C. Jayathilake; N. R. Weeraddana; H. K. E. P. Hettiarachchi","99X Research, 99X Technology, Colombo, Sri Lanka; 99X Research, 99X Technology, Colombo, Sri Lanka; Faculty of Information Technology, University of Moratuwa, Sri Lanka","2017 Seventeenth International Conference on Advances in ICT for Emerging Regions (ICTer)","15 Jan 2018","2017","","","1","8","This research proposes a method for automatically identifying and coding the structure in any text log file. Generated code is expressed in the declarative language LDEL, which was designed and implemented during earlier stages of this research. Once the code is available, the LDEL parser is able to scan a log file with the particular structure and extract log information into a tree-based data structure. Subsequently the extracted information can be processed according to prespecified logic, which is expressed in a procedural language which also was developed as a prior stage in this research. The language scheme used for expressing log file structure was carefully designed after examining log files from numerous domains as a result of the main author's previous research. This scheme is capable of capturing peculiar patterns occurring both within and across lines in log files. Since the development of this scheme and the procedural language, it was noticed that significant manual work was needed for a human expert to examine the log file and write the script that captures its structure. This approach was both time consuming and error prone. With this research we propose and evaluate a method to automatically detect a larger part of the log file structure so that the human specialist can use the generated script with minimal or no modifications.","2472-7598","978-1-5386-2444-9","10.1109/ICTER.2017.8257824","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8257824","Log analysis;Automation;Knowledge Representation;Suffix Tree;Regular Expressions;Data Extraction;Pattern Detection","Software;Data mining;Particle separators;Tools;Arrays;Periodic structures","data structures;formal logic;trees (mathematics)","software log files;text log file;declarative language LDEL;log information;procedural language;log file structure;tree-based data structure;prespecified logic","","1","","28","","15 Jan 2018","","","IEEE","IEEE Conferences"
"LogCluster - A data clustering and pattern mining algorithm for event logs","R. Vaarandi; M. Pihelgas","TUT Centre for Digital Forensics and Cyber Security, Tallinn University of Technology, Tallinn, Estonia; TUT Centre for Digital Forensics and Cyber Security, Tallinn University of Technology, Tallinn, Estonia","2015 11th International Conference on Network and Service Management (CNSM)","4 Jan 2016","2015","","","1","7","Modern IT systems often produce large volumes of event logs, and event pattern discovery is an important log management task. For this purpose, data mining methods have been suggested in many previous works. In this paper, we present the LogCluster algorithm which implements data clustering and line pattern mining for textual event logs. The paper also describes an open source implementation of LogCluster.","","978-3-9018-8277-7","10.1109/CNSM.2015.7367331","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7367331","event log analysis;mining patterns from event logs;event log clustering;data clustering;data mining","Yttrium;Decision support systems;Manganese;Filtering","data mining;pattern clustering;public domain software","data clustering algorithm;IT systems;event pattern discovery;log management task;data mining methods;LogCluster algorithm;line pattern mining algorithm;textual event logs;open source implementation","","35","2","20","","4 Jan 2016","","","IEEE","IEEE Conferences"
"An Approach to Pinpointing Bug-Induced Failure in Logs of Open Cloud Platforms","J. Tong; L. Ying; T. Hongyan; W. Zhonghai","Sch. of Software & Microelectron., Peking Univ., Beijing, China; Nat. Eng. Center of Software Eng., Peking Univ., Beijing, China; Sch. of Software & Microelectron., Peking Univ., Beijing, China; Sch. of Software & Microelectron., Peking Univ., Beijing, China","2016 IEEE 9th International Conference on Cloud Computing (CLOUD)","19 Jan 2017","2016","","","294","302","Software bugs have been one of the dominant causes of system failures, especially in cloud systems based on open source platforms. One big challenge for troubleshooting these cloud systems is to pinpoint the software bug-induced failure in large and complex log files which is a nightmare for administrators. So far, there has been little study on how to identity bug-induced failures based on log analysis. In this paper, we analyze and describe features of bug-induced failure logs from bug repository and Q&A websites, and then propose a general automatic approach to pinpoint logs of bug-induced failure from log files of open cloud platform. In the approach, two algorithms called MPIN and SPIN are presented for log classification. We evaluate our approach by applying logs collected from bug repositories of OpenStack and Hadoop, and five Q&A websites. The experimental result shows that the proposed approach can identify logs of bug-induced failure in OpenStack logs with 83.9% precision, and for Hadoop logs with 82.52% precision.","2159-6190","978-1-5090-2619-7","10.1109/CLOUD.2016.0047","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7820284","open cloud platforms;log analysis;buginduced failures;machine learning","Computer bugs;Cloud computing;Data mining;Classification algorithms;Algorithm design and analysis;Open source software","cloud computing;data handling;parallel processing;pattern classification;program debugging;public domain software;software reliability","software bug-induced failure pinpointing;open cloud platforms;software bugs;system failures;cloud systems;open source platforms;complex log files;log analysis;bug repository;Q&A Web sites;general automatic approach;MPIN;SPIN;log classification;OpenStack;Hadoop","","1","","30","","19 Jan 2017","","","IEEE","IEEE Conferences"
"SDNLog-Foren: Ensuring the Integrity and Tamper Resistance of Log Files for SDN Forensics using Blockchain","P. T. Duy; H. Do Hoang; D. T. Thu Hien; N. Ba Khanh; V. Pham","University of Information Technology, VNU-HCM,Information Security Laboratory; University of Information Technology, VNU-HCM,Information Security Laboratory; University of Information Technology, VNU-HCM,Information Security Laboratory; University of Information Technology, VNU-HCM,Information Security Laboratory; University of Information Technology, VNU-HCM,Information Security Laboratory","2019 6th NAFOSTED Conference on Information and Computer Science (NICS)","5 Mar 2020","2019","","","416","421","Despite bringing many benefits of global network configuration and control, Software Defined Networking (SDN) also presents potential challenges for both digital forensics and cybersecurity. In fact, there are various attacks targeting a range of vulnerabilities on vital elements of this paradigm such as controller, Northbound and Southbound interfaces. In addition to solutions of security enhancement, it is important to build mechanisms for digital forensics in SDN which provide the ability to investigate and evaluate the security of the whole network system. It should provide features of identifying, collecting and analyzing log files and detailed information about network devices and their traffic. However, upon penetrating a machine or device, hackers can edit, even delete log files to remove the evidences about their presence and actions in the system. In this case, securing log files with fine-grained access control in proper storage without any modification plays a crucial role in digital forensics and cybersecurity. This work proposes a blockchain-based approach to improve the security of log management in SDN for network forensics, called SDNLog-Foren. This model is also evaluated with different experiments to prove that it can help organizations keep sensitive log data of their network system in a secure way regardless of being compromised at some different components of SDN.","","978-1-7281-5163-2","10.1109/NICS48868.2019.9023852","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9023852","SDN security;SDN forensics;Secure log files;Blockchain-based security;Integrity and Tamper Resistance.","Digital forensics;Computer hacking;Control systems","authorisation;computer crime;computer network security;digital forensics;software defined networking;system monitoring","SDN forensics;global network configuration;Software Defined Networking;digital forensics;cybersecurity;Southbound interfaces;security enhancement;log file analysis;fine-grained access control;blockchain;log management;network forensics;sensitive log data;SDNLog-Foren;log file identification;log file collection","","1","","22","","5 Mar 2020","","","IEEE","IEEE Conferences"
"DLFinder: Characterizing and Detecting Duplicate Logging Code Smells","Z. Li; T. Chen; J. Yang; W. Shang","Concordia University, Canada; Concordia University, Canada; Concordia University, Canada; Concordia University, Canada","2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)","26 Aug 2019","2019","","","152","163","Developers rely on software logs for a wide variety of tasks, such as debugging, testing, program comprehension, verification, and performance analysis. Despite the importance of logs, prior studies show that there is no industrial standard on how to write logging statements. Recent research on logs often only considers the appropriateness of a log as an individual item (e.g., one single logging statement); while logs are typically analyzed in tandem. In this paper, we focus on studying duplicate logging statements, which are logging statements that have the same static text message. Such duplications in the text message are potential indications of logging code smells, which may affect developers' understanding of the dynamic view of the system. We manually studied over 3K duplicate logging statements and their surrounding code in four large-scale open source systems: Hadoop, CloudStack, ElasticSearch, and Cassandra. We uncovered five patterns of duplicate logging code smells. For each instance of the code smell, we further manually identify the problematic (i.e., require fixes) and justifiable (i.e., do not require fixes) cases. Then, we contact developers in order to verify our manual study result. We integrated our manual study result and developers' feedback into our automated static analysis tool, DLFinder, which automatically detects problematic duplicate logging code smells. We evaluated DLFinder on the four manually studied systems and two additional systems: Camel and Wicket. In total, combining the results of DLFinder and our manual analysis, we reported 82 problematic code smell instances to developers and all of them have been fixed.","1558-1225","978-1-7281-0869-8","10.1109/ICSE.2019.00032","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8811945","log;code smell;duplicate log;static analysis;empirical study","Manuals;Static analysis;Cloud computing;Debugging;Tools;Semantics;Java","cloud computing;data handling;parallel processing;program debugging;program diagnostics;public domain software","DLFinder;duplicate logging code smells;software logs;single logging statement;duplicate logging statements;problematic duplicate logging code;static text message;dynamic view;open source systems;Hadoop;CloudStack;ElasticSearch;Cassandra;developers feedback","","5","","54","","26 Aug 2019","","","IEEE","IEEE Conferences"
"On Comparing Software Quality Metrics of Traditional vs Blockchain-Oriented Software: An Empirical Study","M. Ortu; M. Orrú; G. Destefanis","University of Cagliari, Cagliari, Italy; Universitá degli Studi di Milano-Bicocca, Milano, Italy; Brunel University, London, United Kingdom","2019 IEEE International Workshop on Blockchain Oriented Software Engineering (IWBOSE)","14 Mar 2019","2019","","","32","37","Driven by the surge of interest generated around blockchain technologies over the last years, a new category of systems, called Blockchain-Oriented Software (BOS), which are strictly tied to Blockchain distributed environment, has become increasingly popular. Yet, there is not a thorough understanding of their structure and behaviour and if and to which extent they differ from traditional software systems. The present work provide a first statistical characterisation of BOS. We analysed and compared 5 C++ open source Blockchain-Oriented and 5 Traditional Java software systems, aiming at detecting potential differences between the two categories of projects, and specifically in the statistical distribution of 10 software metrics. Although, in general, the statistical distributions for Traditional software and Blockchain software show similarities, the distribution of Average Cyclomatic and Ration Comment To Code metrics reveal significant differences in their queue, whereas the Number of Statements metric shows meaningful differences on the double Pareto distribution.","","978-1-7281-1807-9","10.1109/IWBOSE.2019.8666575","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8666575","Mining software repositories;metrics;blockchain oriented software;software engineering","Blockchain;Software systems;Log-normal distribution;Software metrics;Statistical distributions","Java;Pareto distribution;software metrics;software quality","ration comment to code metrics;average cyclomatic metrics;C++ open source blockchain-oriented software systems;blockchain distributed environment;number of statements metric;software metrics;blockchain technologies;double Pareto distribution;statistical distribution;BOS;statistical characterisation","","4","","29","","14 Mar 2019","","","IEEE","IEEE Conferences"
"Deciphering event logs in SharePoint Server: A methodology based on process mining","M. A. Chaves; E. R. Córdoba","Universidad de Costa Rica, Pontificia Universidad Católica de Chile; Universidad de Costa Rica, Pontificia Universidad Católica de Chile","2014 XL Latin American Computing Conference (CLEI)","24 Nov 2014","2014","","","1","12","Nowadays, the information systems are an indispensable resource for the organizations. The processes that are managed through these systems most of the time are hard to understand, maintain and improve. The data associated to the process becomes the main source and input to do all types of analysis. Process Mining allows the extraction of useful knowledge from the generated information of the corporate systems. This work suggests a methodology based on process mining to execute process support analysis of a corporate intranet implemented in SharePoint Server. With the extracted information it is possible to do analysis from several perspectives. The obtained results allow administrators of this type of technology platforms to evaluate the techniques used and generate benefits. The methodology was applied in a case study for a Retail client, besides doing an exploratory analysis of the data for two additional clients in the Industrial Safety industry in Chile.","","978-1-4799-6130-6","10.1109/CLEI.2014.6965174","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6965174","Business Processes;Process Mining;BPMN;SharePoint Server;Intranet;PAIS;Methodology;Process Analysis;Event Logs","Servers;Data mining;PROM;Monitoring;Business;Software;Computational modeling","cryptography;data mining;file servers;information systems;intranets;organisational aspects","event logs deciphering;SharePoint server;process mining;information systems;corporate intranet;process support analysis;technology platforms;retail client;industrial safety industry;organizations;knowledge extraction","","2","","27","","24 Nov 2014","","","IEEE","IEEE Conferences"
"Event Logging in an Industrial Development Process: Practices and Reengineering Challenges","F. Baccanico; G. Carrozza; M. Cinque; D. Cotroneo; A. Pecchia; A. Savignano","Critiware S.r.l., Naples, Italy; Selex ES S.p.A - A Finmeccanica Co., Rome, Italy; Critiware S.r.l., Naples, Italy; Critiware S.r.l., Naples, Italy; Critiware S.r.l., Naples, Italy; Critiware S.r.l., Naples, Italy","2014 IEEE International Symposium on Software Reliability Engineering Workshops","15 Dec 2014","2014","","","10","13","This paper discusses our preliminary analysis of event logging practices adopted in a large-scale industrial development process at Selex ES, a top-leading Finmeccanica company in electronic and information technologies for defense systems, aerospace, and land security. The analysis aims to support log reengineering activities that are currently conducted at SELEX ES. At time being, some of the issues described in the paper have been fixed by system developers. Analysis encompasses total around 50+ millions lines of log produced by an Air Traffic Control (ATC) system. Analysis reveals that event logging is not strictly regulated by company-wide practices, which results into heterogeneous logs across different development teams. We introduce our ongoing effort at developing an automatic support to browse collected logs along with a uniform logging policy supplementing the reengineering process.","","978-1-4799-7377-4","10.1109/ISSREW.2014.69","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6983789","Event logging;Development Process;Air Traffic Control;Logging Practices","Software;Semantics;Software reliability;Production;Data mining;Syntactics;Data analysis","air traffic control;program diagnostics","event logging;large-scale industrial development process;Selex ES;top-leading Finmeccanica company;electronic and information technology;defense system;aerospace;land security;log reengineering activity;system developer;air traffic control system;ATC system;company-wide practice;heterogeneous logs;logging policy;reengineering process","","4","","14","","15 Dec 2014","","","IEEE","IEEE Conferences"
"Using Message Logs and Resource Use Data for Cluster Failure Diagnosis","E. Chuah; A. Jhumka; J. C. Browne; N. Gurumdimma; S. Narasimhamurthy; B. Barth","Alan Turing Inst., London, UK; Univ. of Warwick, Coventry, UK; Univ. of Texas at Austin, Austin, TX, USA; Univ. of Jos, Jos, Nigeria; Seagate Technol., Havant, UK; Texas Adv. Comput. Center, Univ. of Texas at Austin, Austin, TX, USA","2016 IEEE 23rd International Conference on High Performance Computing (HiPC)","2 Feb 2017","2016","","","232","241","Failure diagnosis for large compute clusters using only message logs is known to be incomplete. Recent availability of resource use data provides another potentially useful source of data for failure detection and diagnosis. Early work combining message logs and resource use data for failure diagnosis has shown promising results. This paper describes the CRUMEL framework which implements a new approach to combining rationalized message logs and resource use data for failure diagnosis. CRUMEL identifies patterns of errors and resource use and correlates these patterns by time with system failures. Application of CRUMEL to data from the Ranger supercomputer has yielded improved diagnoses over previous research. CRUMEL has: (i) showed that more events correlated with system failures can only be identified by applying different correlation algorithms, (ii) confirmed six groups of errors, (iii) identified Lustre I/O resource use counters which are correlated with occurrence of Lustre faults which are potential flags for online detection of failures, (iv) matched the dates of correlated error events and correlated resource use with the dates of compute node hang-ups and (v) identified two more error groups associated with compute node hang-ups. The pre-processed data will be put on the public domain in September, 2016.","","978-1-5090-5411-4","10.1109/HiPC.2016.035","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7839688","Large cluster system;Cluster log data;Correlation analysis;Failure diagnosis;Lustre file-system","Radiation detectors;Correlation;Data mining;Electronic mail;Monitoring;Software;Error correction codes","data analysis;pattern clustering;resource allocation;software fault tolerance","message log;resource use data;cluster failure diagnosis;CRUMEL framework;Lustre I/O resource use","","5","","30","","2 Feb 2017","","","IEEE","IEEE Conferences"
"Network Security Systems Log Analysis for Trends and Insights: A Case Study","A. K. Meena; N. Hubballi; Y. Singh; V. Bhatia; K. Franke","Indian Institute of Technology Indore,India; Indian Institute of Technology Indore,India; Indian Institute of Technology Indore,India; Indian Institute of Technology Indore,India; Norwegian University of Science and Technology,Norway","2020 IEEE International Conference on Advanced Networks and Telecommunications Systems (ANTS)","8 Feb 2021","2020","","","1","6","Network perimeter security appliances like firewalls, intrusion detection systems mediate communications and log details pertaining to various events. Logs generated by these systems are used to identify security compromises, vulnerable systems, mis-configurations, etc and serve as a valuable asset for a network administrator. In this paper, we report on a study conducted using logs generated by production level security appliances deployed in our university network. In particular, we process the logs generated by firewall, intrusion detection/prevention system and domain name system service to identify trends and gain insights. We process 71 million network connection records which includes 95.7 thousand alerts generated by an open source intrusion detection system collected over a period of 31 days and derive statistics to understand end host level behavioral trends. In our analysis we compare hosts which are known to be infected with malware or running Peer-to-Peer applications and remaining using a set of relevant parameters and identify clearly differentiated behavioral trends.","2153-1684","978-1-7281-9290-1","10.1109/ANTS50601.2020.9342776","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9342776","Log analysis;perimeter security devices;behavioral trends;firewall;intrusion detection system","Firewalls (computing);Intrusion detection;Production;Market research;Telecommunications;Peer-to-peer computing;Security","computer network security;invasive software","network perimeter security appliances;security compromises;vulnerable systems;network administrator;production level security appliances;university network;domain name system service;network connection records;open source intrusion detection system;end host level behavioral trends;firewall system;differentiated behavioral trends;network security system log analysis;log details;intrusion prevention system;peer-to-peer applications","","","","16","","8 Feb 2021","","","IEEE","IEEE Conferences"
"Design and implementation of real-time visualization tool for network security monitoring","A. Safdar; H. Durad; M. Alam","Department of Computer and Information Science, PIEAS, Islamabad, Pakistan; Department of Computer and Information Science, PIEAS, Islamabad, Pakistan; Department of Computer Science, COMSATS, Islamabad, Pakistan","2018 15th International Bhurban Conference on Applied Sciences and Technology (IBCAST)","12 Mar 2018","2018","","","477","483","A picture is worth a thousand words. Everyone knows that images are used to efficiently communicate information. For instance in network monitoring, instead of handing someone a log file that records how an attack happened, one can use a picture, a visual representation of the log records. Visualization is becoming more and more important as time to focus on every little detail is running out. For this very reason most of the Complex Event Processor's manufacturers consider integrating dashboards in their product very critical and of high priority. This project deals with pictorial monitoring of network traffic and its associated anomalies by employing open-source tools only. It also focuses on near-miss events which could lead to disastrous results via constant real time network monitoring, so that every critical information is provided promptly. Overall implementation can be considered as aggregation of the processes like Collection of Data, Detection of atypical and abnormal activities through their Analysis, Extraction of core information and last but not the least its display in a comprehensible and intelligible manner so that for the viewer it would take a few seconds to see whats going on in the network.","2151-1411","978-1-5386-3564-3","10.1109/IBCAST.2018.8312267","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8312267","Network Monitoring;Network Security Visualization;Near-Miss events;complex event processing","Monitoring;Security;Tools;Data visualization;Communication networks;Open source software;Real-time systems","data analysis;data visualisation;public domain software;security of data;system monitoring","open-source tools;disastrous results;complex event processor;near-miss events;network traffic;pictorial monitoring;log records;visual representation;network security monitoring;real-time visualization tool;critical information","","","","26","","12 Mar 2018","","","IEEE","IEEE Conferences"
"Domain Independent Event Analysis for Log Data Reduction","T. Kalamatianos; K. Kontogiannis; P. Matthews","Dept. of Electr. & Comput. Eng., Nat. Tech. Univ. of Athens, Athens, Greece; Dept. of Electr. & Comput. Eng., Nat. Tech. Univ. of Athens, Athens, Greece; Comput. Assoc. Technol., CA Labs., Ditton Park, UK","2012 IEEE 36th Annual Computer Software and Applications Conference","10 Nov 2012","2012","","","225","232","Analyzing the run time behavior of large software systems is a difficult and challenging task. Log analysis has been proposed as a possible solution. However, such an analysis poses unique challenges, mostly due to the volume and diversity of the logged data that is collected, thus making this analysis often intractable for practical purposes. In this paper, we present a log analysis technique that aims to compute a smaller, compared to the original, collection of events that relate to a given analysis objective. The technique is based on computing a similarity score between the logged events and a collection of significant events that we refer to as beacons. The major novelties of the proposed technique are that it is domain independent and that it does not require the use of a pre-existing training data set. The technique has been evaluated against the DARPA Intrusion Detection Evaluation 1999 and the KDD 1999 data sets with promising results.","0730-3157","978-0-7695-4736-7","10.1109/COMPSAC.2012.33","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6340147","Software engineering;dynamic analysis;software maintenance;system understanding;log analysis;log reduction","Standards;Intrusion detection;Algorithm design and analysis;Weight measurement;Software;Analytical models","data analysis;data reduction;program diagnostics;security of data","domain independent event analysis;log data reduction;run time behavior analysis;large software systems;log analysis technique;similarity score;DARPA Intrusion Detection Evaluation 1999 data sets;KDD 1999 data sets","","2","","23","","10 Nov 2012","","","IEEE","IEEE Conferences"
"Paddy: An Event Log Parsing Approach using Dynamic Dictionary","S. Huang; Y. Liu; C. Fung; R. He; Y. Zhao; H. Yang; Z. Luan","Beihang University,Sino-German Joint Software Institute,Beijing,China; Beihang University,Sino-German Joint Software Institute,Beijing,China; Virginia Commonwealth University,Computer Science Department,Richmond,Virginia,USA; Chinese Academy of Scientific Computer Network Information Center,Beijing,China; Chinese Academy of Scientific Computer Network Information Center,Beijing,China; Beihang University,Sino-German Joint Software Institute,Beijing,China; Beihang University,Sino-German Joint Software Institute,Beijing,China","NOMS 2020 - 2020 IEEE/IFIP Network Operations and Management Symposium","8 Jun 2020","2020","","","1","8","Large enterprise systems often produce a large volume of event logs, and event log parsing is an important log management task. The goal of log parsing is to construct log templates from log messages and convert raw log messages into structured log messages. A log parser can help engineers monitor their systems and detect anomalous behaviors and errors. Most existing log parsing methods focus on offline methods, which require all log data to be available before parsing. In addition, the massive volume of log messages makes the process complex and time-consuming. In this paper, we propose Paddy, an online event log parsing method. Paddy uses a dynamic dictionary structure to build an inverted index, which can search the template candidates efficiently with a high rate of recall. The use of Jaccard similarity and length feature to rank candidates can improve parsing precision. We evaluated our proposed method on 16 real log datasets from various sources including distributed systems, supercomputers, operating systems, mobile systems, and standalone software. Our experimental results demonstrate that Paddy achieves the highest accuracy on eight data sets out of sixteen datasets compared to other baseline methods. We also evaluated the robustness and runtime efficiency of the methods and the experimental results show that our method Paddy achieves superior stableness and is scalable with a large volume of log messages.","2374-9709","978-1-7281-4973-8","10.1109/NOMS47738.2020.9110435","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9110435","Log Parsing;Dynamic Dictionary;Log analysis","","data structures;feature extraction;system monitoring","log management;structured log messages;Paddy;event log parsing;dynamic dictionary structure;Jaccard similarity;length feature;log analysis;diagnostic information extraction;enterprise systems","","","","22","","8 Jun 2020","","","IEEE","IEEE Conferences"
"Detecting Plagiarism Based on the Creation Process","J. Schneider; A. Bernstein; J. v. Brocke; K. Damevski; D. C. Shepherd","Institute of Information Systems, University of Liechtenstein, Vaduz, Liechtenstein; Department of Informatics, University of Zurich, Zrich, Switzerland; Institute of Information Systems, University of Liechtenstein, Vaduz, Liechtenstein; Department of Computer Science, Virginia Commonwealth University, Richmond, VA; ABB Corporate Research, Raleigh, NC","IEEE Transactions on Learning Technologies","26 Sep 2018","2018","11","3","348","361","All methodologies for detecting plagiarism to date have focused on the final digital “outcome”, such as a document or source code. Our novel approach takes the creation process into account using logged events collected by special software or by the macro recorders found in most office applications. We look at an author's interaction logs with the software used to create the work. Detection relies on comparing the histograms of multiple logs' command use. A work is classified as plagiarism if its log deviates too much from logs of “honestly created” works or if its log is too similar to another log. The technique supports the detection of plagiarism for digital outcomes that stem from unique tasks, such as theses and equal tasks such as assignments for which the same problem sets are solved by multiple students. Focusing on the latter case, we evaluate this approach using logs collected by an interactive development environment (IDE) from more than 60 students who completed three programming assignments.","1939-1382","","10.1109/TLT.2017.2720171","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7959638","Plagiarism detection;log analysis;distance metrics;histogram based detection;outlier detection of logs","Plagiarism;Software;Human computer interaction;Tools;Programming;Electronic mail;Manuals","document handling;recording","creation process;final digital outcome;logged events;special software;macro recorders;office applications;honestly created works;digital outcomes;plagiarism detection","","4","","38","","27 Jun 2017","","","IEEE","IEEE Journals"
"Software and Infrastructure Log-Based Framework for Identifying the Causes of System Faults","N. Hanakawa; M. Obana","Inf. Manage. Dept., Hannan Univ., Japan; Dept. of Inf. Syst., Osaka Inst. of Technol., Osaka, Japan","2018 25th Asia-Pacific Software Engineering Conference (APSEC)","23 May 2019","2018","","","608","617","Recently, computer systems have become increasingly complex, involving a wide range of infrastructure and software technologies in the same system. Because these complex technologies affect each other, it is difficult to identify the causes of system faults while the system is running. Faults caused by a mix of software and infrastructure issues are particularly difficult for software engineers to resolve. We therefore propose a framework for identifying the causes of system faults that targets both software and infrastructure problems. The aim of this research is to identify faults that are related to software and/or infrastructure. The framework is based on system logs, including both software and infrastructure logs. When a fault occurs while a system is running, software and infrastructure logs are first collected and then compared with a large dataset of past logs to rapidly identify the cause of the fault. After that, debug mode level logs are collected in order to identify proper bugs of source code level and/or setting values of infrastructure configuration file level. The framework is also applied to two examples of real system faults. We find that the first fault was caused by software bugs, but the second was caused by both software bugs and infrastructure problems. We have also implemented a tracing and replay tool based on the framework. We tried to detect the true causes of 17 system faults that were randomly picked up past log dataset. 96% system logs in the 17 faults occurrence operation patterns were detected the differences between fault and normal cases. Then the two concrete session error faults occurred by two causes, infrastructure problems and software bugs. In this way, the framework will help software engineers to identify the causes of faults even when they find it difficult to separate software and infrastructure issues.","2640-0715","978-1-7281-1970-0","10.1109/APSEC.2018.00076","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8719576","Web application, infrastructure, maintenance phase, seamless debugging, detecting faults","Software;Fault diagnosis;Tools;IP networks;Servers;Computer bugs;Knowledge engineering","fault diagnosis;program debugging;program testing;software engineering;software fault tolerance;software maintenance","infrastructure issues;computer systems;software technologies;software engineers;infrastructure problems;infrastructure logs;infrastructure configuration file level;software bugs;concrete session error faults;system faults;system logs","","","","20","","23 May 2019","","","IEEE","IEEE Conferences"
"The Development of Logging Large-scale Management Information System","N. Gaifang","Coll. of Comput. & Inf. Technol., Henan Normal Univ., Xinxiang, China","2010 International Conference on Challenges in Environmental Science and Computer Engineering","24 Jun 2010","2010","2","","425","428","After analyzing the requirements of large-scale management system on genetic log service modules, a flexible, reliable and convenient solution, Log Service, is presented based on struts frame in order to solve the drawbacks of existing log management middleware in using or migration. The JDBC Appender is used to store the log records, parsed by the record parser, into the database. So it is convenient for audit module to view and maintain the log records. It's shown that Log Service provides an all-sided mechanism to satisfy the logging requirement from development stage to implement stage in a large-scale system. And it also provides an effective and flexible manipulating method for the audit management information in running stage.","","978-1-4244-5924-7","10.1109/CESCE.2010.248","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5493318","Log Management;Log Analysis;Specialization;Practical;Convenience","Large-scale systems;Management information systems;Power system management;Application software;Information management;Software development management;Conference management;Environmental management;Technology management;Engineering management","large-scale systems;management information systems;middleware;recording;records management","management information system;genetic log service module;struts frame;log management middleware;JDBC appender;record parser;large scale system","","1","4","5","","24 Jun 2010","","","IEEE","IEEE Conferences"
"Automatic Reverse Engineering of Interaction Models from System Logs","S. Wolny; A. Mazak; M. Wimmer","Christian Doppler Labratory for Model-Integrated Smart Production (CDL-MINT), Johannes Kepler University Linz, 4020 Linz, Austria; Christian Doppler Labratory for Model-Integrated Smart Production (CDL-MINT), Johannes Kepler University Linz, 4020 Linz, Austria; Christian Doppler Labratory for Model-Integrated Smart Production (CDL-MINT), Johannes Kepler University Linz, 4020 Linz, Austria","2019 24th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)","17 Oct 2019","2019","","","57","64","Nowadays, software-as well as hardware systems produce log files that enable a continuous monitoring of the system during its execution. Unfortunately, such text-based log traces are very long and difficult to read, and therefore, reasoning and analyzing runtime behavior is not straightforward. However, dealing with log traces is especially needed in cases, where (i) the execution of the system did not perform as intended, (ii) the process flow is unknown because there are no records, and/or (iii) the design models do not correspond to its real-world counterpart. These facts cause that log data has to be prepared in a more user-friendly way (e.g., in form of graphical representations) and algorithms are needed for automatically monitoring the system's operation, and for tracking the system components interaction patterns. For this purpose we present an approach for transforming raw sensor data logs to a UML or SysML sequence diagram in order to provide a graphical representation for tracking log traces in a time-ordered manner. Based on this sequence diagram, we automatically identify interaction models in order to analyze the runtime behavior of system components. We implement this approach as prototypical plug-in in the modeling tool Enterprise Architect and evaluate it by an example of a self-driving car.","1946-0759","978-1-7281-0303-7","10.1109/ETFA.2019.8869502","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8869502","Log traces;model transformation;sequence diagram;interaction model","Object oriented modeling;Unified modeling language;Runtime;Reverse engineering;Automobiles;Generators;Transforms","object-oriented programming;reverse engineering;system monitoring;Unified Modeling Language","log files;continuous monitoring;reasoning;runtime behavior;log traces;design models;graphical representation;system components interaction patterns;raw sensor data;interaction models;modeling tool Enterprise Architect;automatic reverse engineering;hardware systems;system logs;UML;SysML sequence diagram","","1","","17","","17 Oct 2019","","","IEEE","IEEE Conferences"
"Distributed Event Monitoring for Software Defined Networks","Q. Vuong; H. M. Tran; S. T. Le","Sch. of Comput. Sci. & Eng., HCMC Vietnam Nat. Univ., Vietnam; Sch. of Comput. Sci. & Eng., HCMC Vietnam Nat. Univ., Vietnam; Sch. of Comput. Sci. & Eng., HCMC Vietnam Nat. Univ., Vietnam","2015 International Conference on Advanced Computing and Applications (ACOMP)","3 Mar 2016","2015","","","90","97","Software defined network separates data and control planes that facilitate network management functions, especially enabling programmable network control functions. Event monitoring is a fault management function involved in collecting and filtering event notification messages from network devices. This study presents an approach of distributed event monitoring for software defined network. Monitoring events usually deals with a large amount of event log data, log collecting and filtering processes thus require a high degree of automation and efficiency. This approach takes advantage of the OpenFlow and syslog protocols to collect and store log events obtained from network devices on a syslog server. It also uses the adaptive semantic filtering method to filter and present non-trivial events for system administrators to take further actions. We have evaluated this approach on a network simulation platform and provided some log collection and filtering results with analysis.","","978-1-4673-8234-2","10.1109/ACOMP.2015.29","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7422379","Event Monitoring;Event Filtering;Syslog;Adaptive Semantic Filtering;Software Defined Network","Monitoring;Switches;Protocols;Computer architecture;Software;Network topology","adaptive filters;protocols;software defined networking","fault management function;adaptive semantic filtering method;syslog server;syslog protocol;OpenFlow;filtering process;log collecting;event log data;event notification message;programmable network control function;network management function;software defined network;distributed event monitoring","","1","","22","","3 Mar 2016","","","IEEE","IEEE Conferences"
"Keep it moving: Proactive workload management for reducing SLA violations in large scale SaaS clouds","A. Roy; R. Ganesan; S. Sarkar","Next Gen Computing Lab, Infosys Labs, Electronics City, Bangalore 560100, India; Next Gen Computing Lab, Infosys Labs, Electronics City, Bangalore 560100, India; Next Gen Computing Lab, Infosys Labs, Electronics City, Bangalore 560100, India","2013 IEEE 24th International Symposium on Software Reliability Engineering (ISSRE)","2 Jan 2014","2013","","","421","430","Software failures, workload-related failures and job overload conditions bring about SLA violations in software-as-a-service (SaaS) systems. Existing work does not address mitigation of SLA violations completely as (i) none of them address mitigation of SLA violations in business specific scenarios (SaaS, in our case), (ii) while some do not address software and workload-related failures, other approaches do not address the problem of target PM selection for workload migration comprehensively (leaving out vital considerations like workload compatibility checks between migrating VM and VMs at the target PM) and (iii) a clear mathematical mapping between workload, resource demand and SLA is lacking. In this paper, we present the Keep It Moving (KIM) software framework for the cloud controller that helps minimize service failures due to SLA violation of availability, utilization and response time in SaaS cloud data centers. Though we consider migration to be the primary mitigation technique, we also try to mitigate SLA violations without migration. We achieve this by performing a capacity check on the host physical machine (PM) before the migration to identify if enough capacity is available on the current PM to address the upcoming SLA violations by restart/reboot or VM resizing. In certain cases such as workload-related failures due to corrupt files, we prefer workload rerouting to a replica VM over migration. We formulate the selection of a target PM as a multi-objective optimization problem. We validate our proposed approach by using a trace-based discrete event simulation of a virtualized data center where failure and workload characteristics are simulated from data extracted from a real SaaS business server logs. We found that a 60% reduction in SLA violation is possible using our approach as well as reducing VM downtime by approximately 10%.","2332-6549","978-1-4799-2366-3","10.1109/ISSRE.2013.6698895","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6698895","SLA violation;business SaaS data center;application logs;failures;multi-objective optimization","Software as a service;Time factors;Business;Servers;Availability;Databases","cloud computing;optimisation;software reliability","proactive workload management;large scale SaaS clouds;SLA violations reduction;software failures;workload-related failures;job overload conditions;software-as-a-service systems;mathematical mapping;Keep It Moving software framework;cloud controller;primary mitigation technique;multiobjective optimization problem;trace-based discrete event simulation;virtualized data center","","8","","25","","2 Jan 2014","","","IEEE","IEEE Conferences"
"Towards structured log analysis","D. Jayathilake","99X Research, 99X Technology, Colombo, Sri Lanka","2012 Ninth International Conference on Computer Science and Software Engineering (JCSSE)","9 Aug 2012","2012","","","259","264","Value of software log file analysis has been constantly increasing with the value of information to organizations. Log management tools still have a lot to deliver in order to empower their customers with the true strength of log information. In addition to the traditional uses such as testing software functional conformance, troubleshooting and performance benchmarking, log analysis has proven its capabilities in fields like intrusion detection and compliance evaluation. This is verified by the emphasis on log analysis in regulations like PCI DSS, FISMA, HIPAA and frameworks such as ISO 27001 and COBIT. In this paper we present an in depth analysis into current log analysis domains and common problems. A practical guide to the use of few popular log analysis tools is also included. Lack of proper support for structured analysis is identified as one major flaw in existing tools. After that, we describe a framework we developed for structured log analysis with the view of providing a solution to open problems in the domain. The core strength of the framework is its ability to handle many log file formats that are not well served by existing tools and providing sophisticated infrastructure for automating recurring log analysis procedures. We prove the usefulness of the framework with a simple experiment.","","978-1-4673-1921-8","10.1109/JCSSE.2012.6261962","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6261962","log analysis;structured logs;log data extraction;log management;mind map","Data mining;Software;Organizations;Databases;Standards;Engines;Monitoring","system monitoring","structured log analysis;software log file analysis;log management tools;log information;intrusion detection;compliance evaluation;PCI DSS;FISMA;HIPAA;ISO 27001;COBIT;recurring log analysis procedure automation","","8","6","20","","9 Aug 2012","","","IEEE","IEEE Conferences"
"The Statechart Workbench: Enabling scalable software event log analysis using process mining","M. Leemans; W. M. P. van der Aalst; M. G. J. van den Brand","Eindhoven University of Technology, Eindhoven, The Netherlands; Eindhoven University of Technology, Eindhoven, The Netherlands; Eindhoven University of Technology, Eindhoven, The Netherlands","2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)","5 Apr 2018","2018","","","502","506","To understand and maintain the behavior of a (legacy) software system, one can observe and study the system's behavior by analyzing event data. For model-driven reverse engineering and analysis of system behavior, operation and usage based on software event data, we need a combination of advanced algorithms and techniques. In this paper, we present the Statechart Workbench: a novel software behavior exploration tool. Our tool provides a rich and mature integration of advanced (academic) techniques for the analysis of behavior, performance (timings), frequency (usage), conformance and reliability in the context of various formal models. The accompanied Eclipse plugin allows the user to interactively link all the results from the Statechart Workbench back to the source code of the system and enables users to get started right away with their own software. The work can be positioned in-between reverse engineering and process mining. Implementations, documentation, and a screen-cast (https://youtu.be/xR4XfU3E5mk) of the proposed approach are available, and a user study demonstrates the novelty and usefulness of the tool.","","978-1-5386-4969-5","10.1109/SANER.2018.8330248","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8330248","Reverse Engineering;Process Mining;Behavior Exploration;Performance Analysis;Usage Analysis;Deviation Analysis;Program Analysis;Model-driven Analysis","Tools;Software;Software algorithms;Visualization;Task analysis;Analytical models;Data mining","data analysis;data mining;reverse engineering;software engineering","scalable software event log analysis;software behavior exploration tool;formal models;software event data;system behavior;model-driven reverse engineering;software system;process mining;statechart workbench","","3","","18","","5 Apr 2018","","","IEEE","IEEE Conferences"
"Experience Report: Log Mining Using Natural Language Processing and Application to Anomaly Detection","C. Bertero; M. Roy; C. Sauvanaud; G. Tredan","LAAS, Univ. de Toulouse, Toulouse, France; LAAS, Univ. de Toulouse, Toulouse, France; LAAS, Univ. de Toulouse, Toulouse, France; LAAS, Univ. de Toulouse, Toulouse, France","2017 IEEE 28th International Symposium on Software Reliability Engineering (ISSRE)","16 Nov 2017","2017","","","351","360","Event logging is a key source of information on a system state. Reading logs provides insights on its activity, assess its correct state and allows to diagnose problems. However, reading does not scale: with the number of machines increasingly rising, and the complexification of systems, the task of auditing systems' health based on logfiles is becoming overwhelming for system administrators. This observation led to many proposals automating the processing of logs. However, most of these proposal still require some human intervention, for instance by tagging logs, parsing the source files generating the logs, etc. In this work, we target minimal human intervention for logfile processing and propose a new approach that considers logs as regular text (as opposed to related works that seek to exploit at best the little structure imposed by log formatting). This approach allows to leverage modern techniques from natural language processing. More specifically, we first apply a word embedding technique based on Google's word2vec algorithm: logfiles' words are mapped to a high dimensional metric space, that we then exploit as a feature space using standard classifiers. The resulting pipeline is very generic, computationally efficient, and requires very little intervention. We validate our approach by seeking stress patterns on an experimental platform. Results show a strong predictive performance (≈ 90% accuracy) using three out-of-the-box classifiers.","2332-6549","978-1-5386-0941-5","10.1109/ISSRE.2017.43","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8109100","Anomaly detection;logfile;NLP;word2vec;machine learning;VNF","Natural language processing;Tools;Standards;Training;Servers;Stress;Memory management","data mining;natural language processing;pattern classification;system monitoring;text analysis;word processing","feature space;standard classifiers;word embedding technique;log formatting;logfile processing;minimal human intervention;system state;event logging;anomaly detection;natural language processing;log mining","","22","","26","","16 Nov 2017","","","IEEE","IEEE Conferences"
"Context-Aware Learning for Anomaly Detection with Imbalanced Log Data","P. Sun; E. Yuepeng; T. Li; Y. Wu; J. Ge; J. You; B. Wu","Institute of Information Engineering,Chinese Academy of Sciences,Beijing,China,100093; Institute of Information Engineering,Chinese Academy of Sciences,Beijing,China,100093; Institute of Information Engineering,Chinese Academy of Sciences,Beijing,China,100093; University of Exeter,College of Engineering, Mathematics and Physical Sciences,Exeter,UK,EX4 4QF; Institute of Information Engineering,Chinese Academy of Sciences,Beijing,China,100093; Institute of Information Engineering,Chinese Academy of Sciences,Beijing,China,100093; Institute of Information Engineering,Chinese Academy of Sciences,Beijing,China,100093","2020 IEEE 22nd International Conference on High Performance Computing and Communications; IEEE 18th International Conference on Smart City; IEEE 6th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)","26 Apr 2021","2020","","","449","456","Logs are used to record runtime states and significant events for a software system. They are widely used for anomaly detection. Logs produced by most of the real-world systems show clear characteristics of imbalanced data because the number of samples in different classes varies sharply. The distribution of imbalanced data makes the anomaly classifier bias toward the majority class, so it is difficult for a classifier to learn to detect anomalies correctly. Most existing methods for log-based anomaly detection ignore this important problem, so they perform poorly on real-world systems. In this paper, we propose a context-aware method named AllContext for anomaly detection with imbalanced log data. AllContext transforms each log event into a vector, which contains not only the semantic information of each word, but also the semantics of the region where each word is located. Such rich semantic information enables our method to understand the imbalanced log data better and deeper. We conduct extensive experiments on multi-class and binary imbalanced log datasets. The accuracy of the proposed AllContext is more than twofold of that by a baseline state-of-the-art. To evaluate the robustness of the proposed method, we assess AllContext on the imbalanced unseen log data, where all samples in the test dataset do not exist in the training dataset, and the accuracy achieved by AllContext reaches 0.98. The experiments show that the proposed solution achieves accurate results on both imbalanced and unseen log data.","","978-1-7281-7649-9","10.1109/HPCC-SmartCity-DSS50907.2020.00055","Chinese Academy of Sciences; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9408016","Context aware;Imbalanced log data;Unseen log data;Anomaly detection","Training;Context-aware services;Runtime;High performance computing;Semantics;Transforms;Software systems","","","","","","30","","26 Apr 2021","","","IEEE","IEEE Conferences"
"An optimized approach for live VM migration using log records","A. Mohan; S. Shine","Department of Computer Science & Engineering, College of Engineering, Trivandrum; Department of Computer Science & Engineering, College of Engineering, Trivandrum","2013 Fourth International Conference on Computing, Communications and Networking Technologies (ICCCNT)","30 Jan 2014","2013","","","1","4","Virtual Machines(VMs) refer to the software implementation of a computer that runs its own operating system and applications as if it is a physical machine. Live migration of VMs allows a server administrator to move a running virtual machine among different physical machines without disconnecting the client or application. Total migration time and downtime are two key performance metrics that the clients of a VM service care about the most, because they are concerned about service degradation and the duration for which the service is completely unavailable. Among the already existing approaches for live VM migration, pre-copy approach transfers VM in a manner that balances the requirements of minimizing both the downtime and the total migration time. But this approach is inefficient in the case when the page-dirtying rate is very high because the total migration time will also increase with it. We propose a method in which the migration time can be reduced by transferring the pages that are not recently used and by sending the log records of modifications instead of resending the dirty pages.","","978-1-4799-3926-8","10.1109/ICCCNT.2013.6726826","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6726826","Cloud computing;Live VM migration;Pre-copy;Migration time;Down time;Log records","Virtual machining;Servers;Operating systems;Cloud computing;Computational modeling;Virtualization;Prediction algorithms","operating systems (computers);optimisation;system monitoring;virtual machines","optimized approach;virtual machines;software implementation;operating system;physical machine;server administrator;total migration time;downtime;performance metrics;VM service;service degradation;live VM migration;precopy approach;page-dirtying rate;log records;dirty pages","","3","2","10","","30 Jan 2014","","","IEEE","IEEE Conferences"
"Linking Resource Usage Anomalies with System Failures from Cluster Log Data","E. Chuah; A. Jhumka; S. Narasimhamurthy; J. Hammond; J. C. Browne; B. Barth","Univ. of Texas at Austin, Austin, TX, USA; Univ. of Warwick, Coventry, UK; Xyratex, Havant, UK; Texas Adv. Comput. Center, Austin, TX, USA; Univ. of Texas at Austin, Austin, TX, USA; Texas Adv. Comput. Center, Austin, TX, USA","2013 IEEE 32nd International Symposium on Reliable Distributed Systems","7 Nov 2013","2013","","","111","120","Bursts of abnormally high use of resources are thought to be an indirect cause of failures in large cluster systems, but little work has systematically investigated the role of high resource usage on system failures, largely due to the lack of a comprehensive resource monitoring tool which resolves resource use by job and node. The recently developed TACC_Stats resource use monitor provides the required resource use data. This paper presents the ANCOR diagnostics system that applies TACC_Stats data to identify resource use anomalies and applies log analysis to link resource use anomalies with system failures. Application of ANCOR to first identify multiple sources of resource anomalies on the Ranger supercomputer, then correlate them with failures recorded in the message logs and diagnosing the cause of the failures, has identified four new causes of compute node soft lockups. ANCOR can be adapted to any system that uses a resource use monitor which resolves resource use by job.","1060-9857","978-0-7695-5115-9","10.1109/SRDS.2013.20","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6656267","Large clusters;Resource Anomalies and Failures;Linux O/S;Lustre file-system;Cluster log data","Correlation;Supercomputers;Data mining;Resource management;Monitoring;Software;Algorithm design and analysis","fault tolerant computing;parallel machines;resource allocation","resource usage anomalies;system failures;cluster log data;cluster systems;comprehensive resource monitoring tool;TACC_Stats resource;ANCOR diagnostics system;TACC_Stats data;ranger supercomputer;message logs","","31","","41","","7 Nov 2013","","","IEEE","IEEE Conferences"
"Efficient Event Log Mining with LogClusterC","C. Zhuge; R. Vaarandi","Centre for Digital Forensics & Cyber Security, Tallinn Univ. of Technol., Tallinn, Estonia; Centre for Digital Forensics & Cyber Security, Tallinn Univ. of Technol., Tallinn, Estonia","2017 ieee 3rd international conference on big data security on cloud (bigdatasecurity), ieee international conference on high performance and smart computing (hpsc), and ieee international conference on intelligent data and security (ids)","17 Jul 2017","2017","","","261","266","Nowadays, many organizations collect large volumes of event log data on a daily basis, and the analysis of collected data is a challenging task. For this purpose, data mining methods have been suggested in past research papers, and several data clustering algorithms have been developed formining line patterns from event logs. In this paper, we introduce an open-source tool called LogClusterC which implements the LogCluster algorithm for discovering line patterns and outliers from event logs. According to our performance measurements, LogClusterC compares favorablyto other publicly available log clustering tools.","","978-1-5090-6296-6","10.1109/BigDataSecurity.2017.26","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7980352","event log clustering;mining line patterns from event logs;LogCluster algorithm;data clustering;data mining","Clustering algorithms;Algorithm design and analysis;Data mining;Switches;Partitioning algorithms;Correlation;Security","data mining;pattern clustering;public domain software","event log data mining;LogClusterC;data collection;data clustering algorithms;line pattern mining;event logs;open-source tool;performance measurements","","","","19","","17 Jul 2017","","","IEEE","IEEE Conferences"
"Detecting attacks leveraging vulnerabilities fixed in MS17-010 from Event Log","M. Fujimoto; W. Matsuda; T. Mitsunaga","The University of Tokyo,Tokyo,Japan; The University of Tokyo,Tokyo,Japan; The University of Tokyo,Tokyo,Japan","2019 IEEE Conference on Application, Information and Network Security (AINS)","27 Jan 2020","2019","","","42","47","Many organizations have experienced the damages of cyberattacks leveraging Windows vulnerabilities. Unpatched Windows have been used still now, especially in Industrial Control System (ICS) for operational reasons. In that case, attackers likely abuse them to expand infection. Especially vulnerabilities fixed in MS17-010 has been leveraged for spreading infection of malware such as the WannaCry ransomware and other malware for targeted attacks. Many systems (e.g., electric noticeboard, payment terminal, car production line) around the world were exploited by leveraging Windows vulnerabilities, leading to system failures of a variety of critical infrastructure. Attackers can easily exploit the vulnerabilities since convenient tools for attacking such as ”EternalBlue” or ”Eternal Romance” are published on the Internet. This tool abuses legitimate processes running on Windows systems. Thus operators may hardly notice the trace of attacks. Attacks leveraging vulnerabilities can be mitigated by applying security updates; however, sometimes applying security updates is difficult because of its long-term life cycle and a severe requirement for availability. There are several methods for detecting attacks leveraging vulnerabilities such as the Intrusion Detection System (IDS), but sometimes it is difficult to implement since it needs to alter the existing system structure. In this research, we propose a method for detecting attacks leveraging the vulnerabilities fixed in MS17-010 by analyzing Window's built-in Event Logs. The proposed method can detect attacks against almost all supported versions of Windows. Furthermore, it can be easily integrated into a production environment since it only uses Windows standard functions.","","978-1-7281-3306-5","10.1109/AINS47559.2019.8968703","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8968703","Windows;vulnerability;Event Log;detection;EternalBlue;Detection","Integrated circuits;Standards organizations;Production;Tools;Security;Ransomware;Task analysis","Internet;invasive software","Windows vulnerabilities;unpatched Windows;Window's built-in event logs;Internet;industrial control system;Windows standard functions;MS17-010;intrusion detection system;attack detection;attacks leveraging vulnerabilities;Windows systems;malware detection","","","","19","","27 Jan 2020","","","IEEE","IEEE Conferences"
"Software Architecture Design of the Real- Time Processes Monitoring Platform","A. Batyuk; V. Voityshyn; V. Verhun","Dept. of Automated Control Systems, Institute of Computer Science and Information Technologies, Lviv Polytechnic National University, Lviv, Ukraine; Dept. of Automated Control Systems, Institute of Computer Science and Information Technologies, Lviv Polytechnic National University, Lviv, Ukraine; Dept. of Automated Control Systems, Institute of Computer Science and Information Technologies, Lviv Polytechnic National University, Lviv, Ukraine","2018 IEEE Second International Conference on Data Stream Mining & Processing (DSMP)","4 Oct 2018","2018","","","98","101","Understanding of how business processes are executed in real-life is vitally important for a company. Any process leaves a digital footprint that can be transformed into so-called event logs and analyzed with process mining techniques. A software platform with the purpose of near realtime processes monitoring is implemented. Design of the represented platform is based on the lambda architecture combining online and offline process mining algorithms with advanced analytics based on machine learning.","","978-1-5386-2874-4","10.1109/DSMP.2018.8478589","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8478589","process mining;event data;event logs;business process management;BPM;XES;lambda architecture","Data mining;Software;Computer architecture;Real-time systems;Task analysis;Monitoring;Software algorithms","business data processing;data mining;process monitoring;software architecture","online process mining algorithms;offline process mining algorithms;software architecture design;business processes;digital footprint;software platform;lambda architecture;realtime processes monitoring platform;event logs;machine learning;advanced analytics","","4","","23","","4 Oct 2018","","","IEEE","IEEE Conferences"
"Learning to Log: Helping Developers Make Informed Logging Decisions","J. Zhu; P. He; Q. Fu; H. Zhang; M. R. Lyu; D. Zhang","Shenzhen Res. Inst., Chinese Univ. of Hong Kong, Shenzhen, China; Shenzhen Res. Inst., Chinese Univ. of Hong Kong, Shenzhen, China; Microsoft, Washington, DC, USA; Microsoft Res., Beijing, China; Shenzhen Res. Inst., Chinese Univ. of Hong Kong, Shenzhen, China; Microsoft Res., Beijing, China","2015 IEEE/ACM 37th IEEE International Conference on Software Engineering","17 Aug 2015","2015","1","","415","425","Logging is a common programming practice of practical importance to collect system runtime information for postmortem analysis. Strategic logging placement is desired to cover necessary runtime information without incurring unintended consequences (e.g., Performance overhead, trivial logs). However, in current practice, there is a lack of rigorous specifications for developers to govern their logging behaviours. Logging has become an important yet tough decision which mostly depends on the domain knowledge of developers. To reduce the effort on making logging decisions, in this paper, we propose a ""learning to log"" framework, which aims to provide informative guidance on logging during development. As a proof of concept, we provide the design and implementation of a logging suggestion tool, Log Advisor, which automatically learns the common logging practices on where to log from existing logging instances and further leverages them for actionable suggestions to developers. Specifically, we identify the important factors for determining where to log and extract them as structural features, textual features, and syntactic features. Then, by applying machine learning techniques (e.g., Feature selection and classifier learning) and noise handling techniques, we achieve high accuracy of logging suggestions. We evaluate Log Advisor on two industrial software systems from Microsoft and two open-source software systems from Git Hub (totally 19.1M LOC and 100.6K logging statements). The encouraging experimental results, as well as a user study, demonstrate the feasibility and effectiveness of our logging suggestion tool. We believe our work can serve as an important first step towards the goal of ""learning to log"".","1558-1225","978-1-4799-1934-5","10.1109/ICSE.2015.60","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7194593","","Feature extraction;Software systems;Context;Data mining;Runtime;Syntactics","learning (artificial intelligence);programming;public domain software","logging programming practice;logging decisions;system runtime information;postmortem analysis;strategic logging placement;learning-to-log framework;Log Advisor logging suggestion tool;machine learning techniques;noise handling techniques;logging suggestions;industrial software systems;Microsoft;open-source software systems;Git Hub","","48","","48","","17 Aug 2015","","","IEEE","IEEE Conferences"
"An End-To-End Log Management Framework for Distributed Systems","P. He","Comput. Sci. & Eng. Dept., Chinese Univ. of Hong Kong, Hong Kong, China","2017 IEEE 36th Symposium on Reliable Distributed Systems (SRDS)","19 Oct 2017","2017","","","266","267","Logs have been widely employed to ensure the reliability of distributed systems, because logs are often the only data available that records system runtime information. Compared with logs generated by traditional standalone systems, distributed system logs are often large-scale and of great complexity, invalidating many existing log management methods. To address this problem, the paper describes and envisions an end-to-end log management framework for distributed systems. Specifically, this framework includes strategic logging placement, log collection, log parsing, interleaved logs mining, anomaly detection, and problem identification.","","978-1-5386-1679-6","10.1109/SRDS.2017.41","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8069095","","Anomaly detection;Runtime;Complexity theory;Software reliability","data mining;learning (artificial intelligence);public domain software;recording;system monitoring","End-To-End Log Management Framework;records system runtime information;traditional standalone systems;distributed system logs;strategic logging placement;interleaved log mining;distributed system reliability","","1","","15","","19 Oct 2017","","","IEEE","IEEE Conferences"
"Bridging the divide between software developers and operators using logs","W. Shang","Software Analysis and Intelligence Lab (SAIL), Queen's University, Canada","2012 34th International Conference on Software Engineering (ICSE)","28 Jun 2012","2012","","","1583","1586","There is a growing gap between the software development and operation worlds. Software developers rarely divulge development knowledge about the software to operators, while operators rarely communicate field knowledge to developers. To improve the quality and reduce the operational cost of large-scale software systems, bridging the gap between these two worlds is essential. This thesis proposes the use of logs as mechanism to bridge the gap between these two worlds. Logs are messages generated from statements inserted by developers in the source code and are often used by operators for monitoring the field operation of a system. However, the rich knowledge in logs has not yet been fully used because of their non-structured nature, their large scale, and the use of the ad hoc log analysis techniques. Through case studies on large commercial and open source systems, we plan to demonstrate the value of logs as a tool to support developers and operators.","1558-1225","978-1-4673-1067-3","10.1109/ICSE.2012.6227031","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6227031","","History;Software systems;Biological system modeling;Testing;Computer bugs;Bridges","knowledge management;public domain software;software quality","software developers;software development;operation worlds;knowledge development;field knowledge;operational cost;large-scale software systems quality;logs;source code;messages generation;nonstructured nature;ad hoc log analysis techniques;open source systems","","10","1","19","","28 Jun 2012","","","IEEE","IEEE Conferences"
"Analysis of a short on-line course through logged data recording by a self-developed logging module","P. Esztelecki; G. Korosi","University of Szeged, Faculty of Science and Informatics, Szeged; University of Szeged, Faculty of Science and Informatics, Szeged","2018 International Conference on Computer, Information and Telecommunication Systems (CITS)","19 Aug 2018","2018","","","1","5","Online education has gained a wide popularity in today's global information boom. Prominent universities offer more and more online courses with modern audio-visual content, which have become available for almost everyone. The courses can be completed self-paced allowing for much more flexibility. Such a learning approach has already reformed higher education and seeks ways to penetrate into the realm of the secondary and primary education. Our team conducted a research in the higher classes of different primary schools in the Province of Vojvodina, Serbia. Participants of the Hungarian minority took part in a course named “Conscious and Safe Internet Use” and obtained a valuable knowledge with the help of videos and optional course materials. Student activities were all recorded with the help of a special self-developed logging software for later processing. Data were processed by statistical and data mining methods. According to the values of correlation coefficients and R Square Statistical value it can be stated, that those participants who scored well at the pre-test stage expectedly had better results during the final testing stage. The number of video views, the age of students and the size of their hometown had also influenced the outcome of the tests. The detailed findings are presented in this paper under the Results chapter.","","978-1-5386-4599-4","10.1109/CITS.2018.8440183","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8440183","E-learning;Educational Data Logging;Log Data Analysis","Videos;Data mining;Mice;Education;Software;Internet;Big Data","computer aided instruction;data mining;distance learning;educational courses;educational institutions;further education;statistical analysis","logged data recording;online education;global information boom;prominent universities;audio-visual content;learning approach;higher education;secondary education;primary education;Hungarian minority;valuable knowledge;optional course materials;student activities;special self-developed logging software;self-developed logging module;primary schools;short on-line course;data mining methods;statistical methods;R square statistical value;conscious and safe internet use","","","","9","","19 Aug 2018","","","IEEE","IEEE Conferences"
"Log-based testing","A. Elyasov","Dept. of Information and Computing Sciences, Utrecht University, Utrecht, The Netherlands","2012 34th International Conference on Software Engineering (ICSE)","28 Jun 2012","2012","","","1591","1594","This thesis presents an ongoing research on using logs for software testing. We propose a complex and generic logging and diagnosis framework, that can be efficiently used for continuous testing of future Internet applications. To simplify the diagnosis of logs we suggest to reduce its size by means of rewriting.","1558-1225","978-1-4673-1067-3","10.1109/ICSE.2012.6227029","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6227029","log file analysis;instrumentation;rewriting","Instruments;Internet;Libraries;Graphical user interfaces;Automation;Software testing","Internet;program testing","log-based testing;software testing;logging framework;diagnosis framework;future Internet application continuous testing;rewriting","","3","3","15","","28 Jun 2012","","","IEEE","IEEE Conferences"
"An integrated visualization on network events VAST 2011 Mini Challenge #2 Award: “Outstanding integrated overview display”","W. M. Lamagna","Universidad de Buenos Aires, Master on Datamining and Knowledge Discovery, Spain","2011 IEEE Conference on Visual Analytics Science and Technology (VAST)","15 Dec 2011","2011","","","319","321","To visualize security trends for the data set provided by the VAST 2011 Mini Challenge #2 a custom tool has been developed. Open source tools [1,2], web programming languages [4,7] and an open source database [3] has been used to work with the data and create a visualization for security log files containing network security trends. In this paper, the tools and methods used for the analysis are described. The methods include the log synchronization with different timezone and the development of heat maps and parallel coordinates charts. To develop the visualization, Processing and Canvas [4,7] was used.","","978-1-4673-0014-8","10.1109/VAST.2011.6102493","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6102493","visual analysis;security trends;heat map;logs;vast challenge","Heating;Security;Image color analysis;Synchronization;Data visualization;Fires;Arrays","data visualisation;Internet;programming languages;public domain software;security of data","integrated visualization;VAST 2011 mini challenge #2 award;outstanding integrated overview display;data set security trend visualization;open source tools;Web programming languages;open source database;network security trends;log synchronization;heat map development;parallel coordinates charts;processing;canvas","","1","","8","","15 Dec 2011","","","IEEE","IEEE Conferences"
"Parallel and pipelined processing of large-scale mobile comminucation data using hadoop open-source framework","M. Koca; I. Ari; U. Koçak; O. Çalıkuş; C. Sezgin","Bilgisayar Mühendisliği Bölümü, Özyegin Üniversitesi, Turkey; Bilgisayar Mühendisliği Bölümü, Özyegin Üniversitesi, Turkey; Bilgisayar Mühendisliği Bölümü, Özyegin Üniversitesi, Turkey; Avea Ar-Ge Laboratuarları, Turkey; Avea Ar-Ge Laboratuarları, Turkey","2012 20th Signal Processing and Communications Applications Conference (SIU)","28 May 2012","2012","","","1","4","The fast increase in mobile device and bandwidth usage is generating big workloads on the IT infrastructures of mobile service providers and increasing management costs. These providers collect log files continuously and use these logs for billing, operational and marketing purposes. In this paper, we describe the design, implementation and efficient parallel processing of large-scale mobile logs using the open-source Hadoop-based low-cost private cloud system for near real-time analytics. We find that batching of small files, parallel loading and pipelining of different workloads by overlapping their disk-and-CPU intensive phases can have significant performance benefits. Optimizations were performed in the light of these findings. Our web-based interface helps users explore progress and performance of their workloads.","2165-0608","978-1-4673-0056-8","10.1109/SIU.2012.6204832","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6204832","","Erbium;Abstracts;Loading;Privacy;Receivers","batch processing (computers);cloud computing;file organisation;invoicing;marketing;mobile communication;mobile computing;parallel processing;pipeline processing;public domain software;records management;user interfaces","parallel processing;pipelined processing;large-scale mobile communication data;mobile device;bandwidth usage;IT infrastructures;mobile service providers;log files;billing;marketing;open source Hadoop-based low-cost private cloud system;parallel loading;file batching;disk-and-CPU intensive phase overlapping;optimization;Web-based interface","","","","1","","28 May 2012","","","IEEE","IEEE Conferences"
"Database management strategy and recovery methods of Android","Q. Li; X. Hu; H. Wu","Department of Computer Science, Information Engineering University, Zhengzhou, 450003, China; Department of Computer Science, Information Engineering University, Zhengzhou, 450003, China; Department of Computer Science, Information Engineering University, Zhengzhou, 450003, China","2014 IEEE 5th International Conference on Software Engineering and Service Science","23 Oct 2014","2014","","","727","730","Database analysis and the recovery of deleted record are two of the most important parts in digital forensics. This paper focuses on the management mechanism of Android S MS database, involving the analyses of write-in rules of database and log files during the data updating process. Based on the research results, a recovery method for database operating records from WAL file is proposed. To solve the problem that the log file is emptied, this paper presents a recovery method for the deleted log file for ext4 file system, thus constructing a database operating record timeline. Finally, an experiment based on images generated in different time and conditions is carried out to validate the effectiveness of the proposed method, discussion on its limitations is conducted.","2327-0594","978-1-4799-3279-5","10.1109/ICSESS.2014.6933670","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6933670","SQLite;Android;WAL;database recovery;ext4","Databases;Smart phones;Image restoration;Androids;Humanoid robots;File systems","Android (operating system);database management systems;digital forensics;system recovery","database management strategy;recovery method;database analysis;digital forensics;Android SMS database;write-in rules;log files;data updating process;database operating record timeline","","6","3","7","","23 Oct 2014","","","IEEE","IEEE Conferences"
"Event Logs Generated by an Operating System Running on a COTS Computer During IEMI Exposure","C. Kasmi; J. Lopes-Esteves; N. Picard; M. Renard; B. Beillard; E. Martinod; J. Andrieu; M. Lalande","Wireless, Software and Hardware Security Labs, French Network and Information Security Agency, Paris, France; Wireless, Software and Hardware Security Labs, French Network and Information Security Agency, Paris, France; XLIM OSA-UMR CNRS n°7252, Limoges, France; Wireless, Software and Hardware Security Labs, French Network and Information Security Agency, Paris, France; XLIM OSA-UMR CNRS n°7252, Limoges, France; XLIM OSA-UMR CNRS n°7252, Limoges, France; XLIM OSA-UMR CNRS n°7252, Limoges, France; XLIM OSA-UMR CNRS n°7252, Limoges, France","IEEE Transactions on Electromagnetic Compatibility","11 Dec 2014","2014","56","6","1723","1726","Many studies were devoted to the analysis and the detection of electromagnetic attacks against critical electronic systems at the system or the component levels. Some attempts have been made to correlate effects scenarios with events logged by the kernel of the operating system (OS) of commercial-off-the-shelf computer running Windows. Due to the closed principle of the last OS, we decided to perform such an analysis on a computer running a Linux distribution in which a complete access to logs is available. It will be demonstrated that a computer running such an open OS allows detecting the perturbations induced by intentional electromagnetic interferences at different levels of the targeted computer.","1558-187X","","10.1109/TEMC.2014.2357060","French Network and Information Security Agency—ANSSI; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6906268","Electromagnetic compatibility (EMC);electromagnetic interference;software engineering;system analysis and design;Electromagnetic compatibility (EMC);electromagnetic interference;software engineering;system analysis and design","Universal Serial Bus;Hardware;Sensors;Electromagnetic interference;Monitoring","computational electromagnetics;electromagnetic interference;Linux;operating system kernels;security","intentional electromagnetic interferences;Linux distribution;Windows;commercial-off-the-shelf computer;OS kernel;operating system kernel;critical electronic systems;electromagnetic attack detection;electromagnetic attack analysis;IEMI exposure;COTS computer;event log generation","","4","","10","","22 Sep 2014","","","IEEE","IEEE Journals"
"Mining multi-attribute sequential pattern in onboard failure logging","M. Zhu; Y. Li; S. Chen","Failure Analysis Department, Technology and Quality, CISCO, Shanghai, China; Failure Analysis Department, Technology and Quality, CISCO, Shanghai, China; Failure Analysis Department, Technology and Quality, CISCO, Shanghai, China","2014 11th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)","11 Dec 2014","2014","","","644","649","Onboard Failure Logging (OBFL) is an advanced feature of hardware system. It records failure-related data which is useful for failure analysis process and system reliability improvement. OBFL records are event sequences with multi-attributes. There are lots of algorithms proposed for sequential pattern mining, whereas not much effort has been made to use attribute held by events. However such attributes are critical for failure pattern detecting in failure analysis process. In this paper, we point out the problem of mining multi-attribute sequential pattern in OBFL dataset and propose a new algorithm, called MA-PrefixSpan, to solve it. Finally, we design the OBFL Analysis System to generate the real world OBFL datasets and apply MA-PrefixSpan to mine the failure pattern. The results show that the algorithm can effectively locate the multi-attribute failure patterns which are correlated with failure trends.","","978-1-4799-5148-2","10.1109/FSKD.2014.6980910","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6980910","Sequential Pattern Mining;Multi-attribute;OBFL;Onboard Failure Logging;Failure Analysis","Databases;Data mining;Algorithm design and analysis;Failure analysis;Hardware;Partitioning algorithms;Market research","data mining;software reliability;system monitoring;system recovery","multiattribute sequential pattern mining;onboard failure logging;hardware system;failure-related data;failure analysis process;system reliability improvement;OBFL records;event sequences;failure pattern detection;OBFL dataset;MA-PrefixSpan;OBFL analysis system;failure trends","","","","11","","11 Dec 2014","","","IEEE","IEEE Conferences"
"Data analysis of file forensic investigation","P. Salunkhe; S. Bharne; P. Padiya","Department of Computer Engineering, Ramrao Adik Institute of Technology Nerul, Navi Mumbai; Department of Computer Engineering, Ramrao Adik Institute of Technology, Nerul, Navi Mumbai; Department of Computer Engineering, Ramrao Adik Institute of Technology, Nerul, Navi Mumbai","2016 International Conference on Signal Processing, Communication, Power and Embedded System (SCOPES)","26 Jun 2017","2016","","","372","375","Rapidly growing Internet Technology may cause cybercrimes committed by attackers. Different type of digital devices is being used to commit an attack. To detect such a criminal activity forensic investigator has to use various data recovery methods and practical framework. There are various type of forensic tool kit (FTK), freeware software's, techniques and tools are available for file forensic investigation. Decision Tree (DT) is also one of the technique which can help for file forensic investigation purpose. So, system can adopt a way by using Decision Tree for generating, storing and analyzing data retrieved from log files which pose as evidence in file forensic analysis. This paper focuses on how Decision Tree can allow system to quickly, easily and inexpensively analysis of log data available in various file formats for file forensic analysis.","","978-1-5090-4620-1","10.1109/SCOPES.2016.7955854","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7955854","Digital forensic lifecycle;log file collection;File forensic analysis;k-means clustering technique;Classifier as Decision tree","Data mining;Tools;Digital forensics;Databases;Decision trees;Computer crime","computer crime;data analysis;decision trees;digital forensics","data analysis;file forensic investigation;Internet technology;cybercrimes;attackers;digital devices;criminal activity;data recovery;forensic tool kit;FTK;freeware software;decision tree;data generation;data storing;log files","","","","13","","26 Jun 2017","","","IEEE","IEEE Conferences"
"Collaborative-Design Conflicts: Costs and Solutions","J. y. Bang; Y. Brun; N. Medvidović",Kakao Corporation; University of Massachusetts Amherst; University of Southern California,"IEEE Software","29 Nov 2018","2018","35","6","25","31","Collaborative design exposes software architects to the risk of making conflicting modeling changes that either can't be merged or, when merged, violate consistency rules, nonfunctional requirements, or other system constraints. Such design conflicts are common and incur a high cost, including having to redo and abandon work. Proactive conflict detection can alleviate this risk. This article motivates the need for design conflict detection, describes the benefits of such detection to practitioners, and identifies requirements for building detection tools. In particular, FLAME is a collaborative-design framework that efficiently and continuously detects design conflicts. This article is part of a theme issue on collaborative modeling.","1937-4194","","10.1109/MS.2018.290110057","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8409920","proactive conflict detection;design;FLAME;Framework for Logging and Analyzing Modeling Events;software architecture;collaborative modeling;collaborative design;software development;software engineering","Collaboration;Analytical models;Software architecture;Collaborative software;Software development;Computational modeling;Software engineering","configuration management;groupware;product design;software architecture;source code (software)","software architects;consistency rules;nonfunctional requirements;proactive conflict detection;design conflict detection;collaborative modeling;collaborative-design conflicts;FLAME","","","","13","","11 Jul 2018","","","IEEE","IEEE Magazines"
"Component behavior discovery from software execution data","Cong Liu; B. van Dongen; N. Assy; W. M. P. van der Aalst","Department of Mathematics and Computer Science, Eindhoven University of Technology, The Netherlands; Department of Mathematics and Computer Science, Eindhoven University of Technology, The Netherlands; Department of Mathematics and Computer Science, Eindhoven University of Technology, The Netherlands; Department of Mathematics and Computer Science, Eindhoven University of Technology, The Netherlands","2016 IEEE Symposium Series on Computational Intelligence (SSCI)","13 Feb 2017","2016","","","1","8","Tremendous amounts of data can be recorded during software execution. This provides valuable information on software runtime analysis. Many crashes and exceptions may occur, and it is a real challenge to understand how software is behaving. Software is usually composed of various components. A component is a nearly independent part of software that full-fills a clear function. Process mining aims to discover, monitor and improve real processes by extracting knowledge from event logs. This paper presents an approach to utilize process mining as a tool to discover the real behavior of software and analyze it. The unstructured software execution data may be too complex, involving multiple interleaved components, etc. Applying existing process mining techniques results in spaghetti-like models with no clear structure and no valuable information that can be easily understood by end. In this paper, we start with the observation that software is composed of components and we use this information to decompose the problem into smaller independent ones by discovering a behavioral model per component. Through experimental analysis, we illustrate that the proposed approach facilitates the discovery of more understandable software models. All proposed approaches have been implemented in the open-source process mining toolkit ProM.","","978-1-5090-4240-1","10.1109/SSCI.2016.7849947","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7849947","","Data mining;Data models;Software systems;Monitoring;Concurrent computing;Petri nets","data mining;object-oriented programming","component behavior discovery;software execution data;software runtime analysis;software crashes;knowledge extraction;event logs;spaghetti-like models;software models;open-source process mining toolkit ProM","","1","","20","","13 Feb 2017","","","IEEE","IEEE Conferences"
"Immutable Log Storage as a Service","W. Pourmajidi; L. Zhang; J. Steinbacher; T. Erwin; A. Miranskyy","Department of Computer Science, Ryerson University, Toronto, Canada; Department of Computer Science, Ryerson University, Toronto, Canada; IBM Canada Lab, Toronto, Canada; IBM Watson and Cloud Platform, Austin, USA; Department of Computer Science, Ryerson University, Toronto, Canada","2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)","19 Aug 2019","2019","","","280","281","Logs contain critical information about the quality of the rendered services on the Cloud and can be used as digital evidence. Hence, we argue that the critical nature of logs calls for immutability and verification mechanism without a presence of a single trusted party. In this paper, we propose a blockchain-based log system, called Logchain, which can be integrated with existing private and public blockchains. To validate the mechanism, we create Logchain as a Service (LCaaS) by integrating it with Ethereum public blockchain network. We show that the solution is scalable (being able to process 100 log files per second) and fast (being able to ""seal"" a log file in 23 seconds, on average).","2574-1934","978-1-7281-1764-5","10.1109/ICSE-Companion.2019.00114","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8802749","Blockchain, Hierarchical Ledger, bitcoin, privacy, log systems, Cloud log systems Log tampering, Log storage, Ethereum","","cloud computing;cryptography;data mining;storage management;system monitoring","immutable log storage;critical information;rendered services;digital evidence;critical nature;immutability;verification mechanism;single trusted party;Ethereum public blockchain network;log file;private blockchains;logchain","","","","8","","19 Aug 2019","","","IEEE","IEEE Conferences"
"An event-log analysis and simulation-based approach for quantifying sustainability metrics in production facilities","S. Rai; M. Daniels","PARC-A Xerox Company, 800 Phillips Road, Webster, NY 14450, USA; Xerox Corporation, 800 Phillips Road, Webster, NY 14450, USA","2015 Winter Simulation Conference (WSC)","18 Feb 2016","2015","","","1033","1043","This paper describes a discrete-event simulation and event-log analysis based approach for computing sustainability metrics in production environments to perform various types of comparative analysis and assessments. Event logs collected from the production environment are analyzed to compute current state sustainability metrics such as energy usage, carbon footprint and heating/cooling requirements. Bootstrapping based forecasting leveraging expert input is utilized to estimate future demand. The forecasted demand is then simulated to predict sustainability metrics. The discrete-event simulation results from the forecasted data and computation of heat produced is combined with thermodynamic models of heat transfer through the thermal envelope of the facility to provide more accurate estimates of true carbon footprint associated with the production operations while also enabling cross-comparative studies of setting operations in different geographical locations. The framework and software tool enables the integration of productivity metrics and sustainability metrics in decision-making process for designing and operating production environments.","1558-4305","978-1-4673-9743-8","10.1109/WSC.2015.7408231","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7408231","","Measurement;Production;Heating;Green products;Carbon;Computational modeling;Heat transfer","cooling;decision making;discrete event simulation;heating;production engineering computing;production facilities;sustainable development","event-log analysis;simulation-based approach;sustainability metrics;production facilities;state sustainability metrics;energy usage;carbon footprint;cooling requirements;heating requirements;discrete-event simulation;thermodynamic models;heat transfer;thermal envelope;production operations;software tool;productivity metrics;decision-making process","","","","7","","18 Feb 2016","","","IEEE","IEEE Conferences"
"Normalization of Unstructured Log Data into Streams of Structured Event Objects","D. Tovarňák; T. Pitner","Institute of Computer Science, Masaryk University, Brno, Czech Republic; Faculty of Informatics, Masaryk University, Brno, Czech Republic","2019 IFIP/IEEE Symposium on Integrated Network and Service Management (IM)","20 May 2019","2019","","","671","676","Monitoring plays a crucial role in the operation of any sizeable distributed IT infrastructure. Whether it is a university network or cloud datacenter, monitoring information is continuously used in a wide spectrum of ways ranging from mission-critical jobs, e.g. accounting or incident handling, to equally important development-related tasks, e.g. debugging or fault-detection. Whilst pursuing a novel vision of new-generation event-driven monitoring systems, we have identified that a particularly rich source of monitoring information, computer logs, is also one of the most problematic in terms of automated processing. Log data are predominantly generated in an ad-hoc manner using a variety of incompatible formats with the most important pieces of information, i.e. log messages, in the form of unstructured strings. This clashes with our long-term goal of designing a system enabling its users to transparently define real-time continuous queries over homogeneous streams of properly defined monitoring event objects with explicitly described structure. Our goal is to bridge this gap by normalizing the poorly structured log data into streams of structured event objects. The combined challenge of this goal is structuring the log data, whilst considering the high velocity with which they are generated in modern IT infrastructures. This paper summarizes the contributions of a dissertation thesis ""Normalization of Unstructured Log Data into Streams of Structured Event Objects"" dealing with the matter at hand in detail.","1573-0077","978-3-903176-15-7","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8717898","log management;logging;data integration;normalization;stream processing;monitoring","Monitoring;Clustering algorithms;Task analysis;Data mining;Software;Computer languages;Pattern matching","cloud computing;computer centres;query processing;system monitoring","new-generation event-driven monitoring systems;particularly rich source;monitoring information;computer logs;log messages;unstructured strings;long-term goal;real-time continuous queries;homogeneous streams;properly defined monitoring event objects;explicitly described structure;poorly structured log data;structured event objects;university network;accounting;equally important development-related tasks;unstructured log data normalization;incident handling;fault-detection","","","","15","","20 May 2019","","","IEEE","IEEE Conferences"
"Design and Implementation of a Journaling File System for Phase-Change Memory","E. Lee; S. Hoon Yoo; H. Bahn","Department of Software, Chungbuk National University, Cheongju, Korea; Korea Air Force Academy, Cheongju, Korea; Department of Computer Science and Engineering, Ewha University, Seoul, Korea","IEEE Transactions on Computers","3 Apr 2015","2015","64","5","1349","1360","Journaling file systems are widely used in modern computer systems as they provide high reliability at reasonable cost. However, existing journaling file systems are not efficient for emerging PCM (phase-change memory) storage because they are optimized for hard disks. Specifically, the large amount of data that they write during journaling degrades the performance of PCM storage seriously as it has a long write latency. In this paper, we present a new journaling file system for PCM, called Shortcut-JFS, that reduces write traffic to PCM by more than half of existing journaling file systems running on block I/O interfaces. To do this, we devise two novel schemes that can be used under byte-addressable I/O interfaces: 1) differential logging that journals only the modified part of a block and 2) in-place checkpointing that eliminates the overhead of block copying. We implement Shortcut-JFS on Linux 2.6.32 and measure the performance of Shortcut-JFS compared to those of existing journaling and log-structured file systems. The results show that the performance improvement of Shortcut-JFS against Ext4 and LFS is 54 and 96 percent, respectively, on average.","1557-9956","","10.1109/TC.2014.2329674","National Research Foundation of Korea; MEST; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6827914","File system;phase-change memory;reliability;journaling;logging","File systems;Checkpointing;Phase change materials;Computer crashes;Phase change memory;Reliability;Hard disks","checkpointing;file organisation;phase change memories","journaling file systems;modern computer systems;PCM storage;phase-change memory storage;hard disks;Shortcut-JFS;block I/O interfaces;byte-addressable I/O interfaces;differential logging;in-place checkpointing;block copying;Linux 2.6.32;log-structured file systems","","26","","37","","9 Jun 2014","","","IEEE","IEEE Journals"
"Scaling data-plane logging in large scale networks","A. Arefin; A. Khurshid; M. Caesar; K. Nahrstedt","Department of Computer Science, University of Illinois at Urbana-Champaign, USA; Department of Computer Science, University of Illinois at Urbana-Champaign, USA; Department of Computer Science, University of Illinois at Urbana-Champaign, USA; Department of Computer Science, University of Illinois at Urbana-Champaign, USA","2011 - MILCOM 2011 Military Communications Conference","12 Jan 2012","2011","","","1308","1314","Understanding and troubleshooting wide area networks (such as military backbone networks and ISP networks) are challenging tasks due to their large, distributed, and highly dynamic nature. Building a system that can record and replay fine-grained behaviors of such networks would simplify this problem by allowing operators to recreate the sequence and precise ordering of events (e.g., packet-level forwarding decisions, route changes, failures) taking place in their networks. However, doing this at large scales seems intractable due to the vast amount of information that would need to be logged. In this paper, we propose a scalable and reliable framework to monitor fine-grained data-plane behavior within a large network. We give a feasible architecture for a distributed logging facility, a tree-based data structure for log compression and show how this logged information helps network operators to detect and debug anomalous behavior of the network. Experimental results obtained through trace-driven simulations and Click software router experiments show that our design is lightweight in terms of processing time, memory requirement and control overhead, yet still achieves over 99% precision in capturing network events.","2155-7586","978-1-4673-0081-0","10.1109/MILCOM.2011.6127483","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6127483","","Monitoring;Random access memory;Reliability;Gold;Timing;Data structures;Nonvolatile memory","program debugging;security of data;tree data structures;wide area networks","data-plane logging;large scale networks;wide area networks;military backbone networks;ISP networks;fine-grained behaviors;packet-level forwarding decisions;route changes;failures;fine-grained data-plane behavior;distributed logging facility;tree-based data structure;log compression;logged information;anomalous network behavior detection;anomalous network behavior debugging;trace-driven simulations;Click software router;memory requirement;control overhead","","3","3","19","","12 Jan 2012","","","IEEE","IEEE Conferences"
"Anomaly Detection via Mining Numerical Workflow Relations from Logs","B. Zhang; H. Zhang; P. Moscato; A. Zhang","The University of Newcastle,School of Electrical Engineering and Computing,NSW,Australia; The University of Newcastle,School of Electrical Engineering and Computing,NSW,Australia; The University of Newcastle,School of Electrical Engineering and Computing,NSW,Australia; Sun Yat-sen University,School of Mathematics,Zhuhai,China","2020 International Symposium on Reliable Distributed Systems (SRDS)","12 Nov 2020","2020","","","195","204","Complex software-intensive systems, especially distributed systems, generate logs for troubleshooting. The logs are text messages recording system events, which can help engineers determine the system's runtime status. This paper proposes a novel approach named ADR (stands for Anomaly Detection by workflow Relations), which employs matrix nullspace to mine numerical relations from log data. The mined relations can be used for both offline and online anomaly detection and facilitate fault diagnosis. We have evaluated ADR on log data collected from two distributed systems. ADR successfully mined 87 and 669 numerical relations from the logs and used them to detect anomalies with high precision and recall. For online anomaly detection, ADR employs PSO (Particle Swarm Optimization) to find the optimal sliding windows' size and achieves fast anomaly detection. The experimental results confirm that ADR is effective for both offline and online anomaly detection.","2575-8462","978-1-7281-7626-0","10.1109/SRDS51746.2020.00027","Australian Research Council; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9252062","anomaly detection, workflow relations, invariants, logs, fault diagnosis","Training;Runtime;Distributed databases;Tools;Particle swarm optimization;Anomaly detection;Optimization","data mining;fault diagnosis;particle swarm optimisation","complex software-intensive systems;distributed systems;system events;ADR;log data;mined relations;numerical relations;fast anomaly detection;online anomaly detection;mining numerical workflow relations;PSO;particle swarm optimization;matrix nullspace","","","","40","","12 Nov 2020","","","IEEE","IEEE Conferences"
"LibSearchNet: Library log file initiatives - As a part of semantic library interface development for the VirCA 3D virtual collaboration arena","G. Bujdosó; M. Csernoch; M. Borbély; E. Dani; M. Némethi-Takács; K. Koltay; L. Balázs","Dept. of Library and Information Science, Faculty of Informatics, University of Debrecen, Hungary; Dept. of Library and Information Science, Faculty of Informatics, University of Debrecen, Hungary; Dept. of Library and Information Science, Faculty of Informatics, University of Debrecen, Hungary; Dept. of Library and Information Science, Faculty of Informatics, University of Debrecen, Hungary; Dept. of Library and Information Science, Faculty of Informatics, University of Debrecen, Hungary; University and National Library, University of Debrecen, Hungary; University and National Library, University of Debrecen, Hungary","2013 IEEE 4th International Conference on Cognitive Infocommunications (CogInfoCom)","23 Jan 2014","2013","","","567","572","In this paper we examine the possibilities for analyzing the behavior of library users. We point out that the softwares that we can use cannot fulfill library requirements. There are many data given for on-line searches in library systems hidden from the analyzers. After examining the possibilities and some log files produced by the library systems, we propose how log files could give more usable information for the semantic design of the intelligent library in the VirCA virtual collaboration system and of other on-line systems, too. These data can be applied as input information for intelligent systems in learning user searching behavior and help them predict user needs more precisely.","","978-1-4799-1546-0","10.1109/CogInfoCom.2013.6719312","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6719312","log file initiatives;intelligent library systems;log file analysis;VirCA","Libraries;Semantics;Collaboration;Software;Conferences;Artificial intelligence;Three-dimensional displays","data analysis;digital libraries;information needs;information retrieval;Internet;virtual reality","LibSearchNet;library log file initiatives;semantic library interface development;VirCA 3D virtual collaboration arena;library user behavior analysis;library requirements;online search;library systems;semantic design;intelligent library;user searching behavior;user needs prediction","","3","","29","","23 Jan 2014","","","IEEE","IEEE Conferences"
"The research and implementation of metadata cache backup technology based on CEPH file system","Ling Zhan; Xieyun Fang; Daping Li","Division of Information Science and Technology, Wenhua University, Wuhan, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, China","2016 IEEE International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)","4 Aug 2016","2016","","","72","77","Based on research and analysis of the Ceph file system, the log cache backup scheme is proposed. The log cache backup scheme reduces metadata access delays by caching logs and reducing the time of storing logs to server clusters. In order to prevent the loss of cached data in metadata servers, a cached data backup scheme is proposed. Compared to the metadata management subsystem of the Ceph, the log cache backup scheme can effectively improve the access performance of metadata. Finally, based on open source codes of the Ceph file system, the log cache backup scheme is implemented. Compared to the performance of the Ceph metadata management subsystem, experiment results show that performance improvements of the log cache backup scheme are up to 11.5%.","","978-1-5090-2594-7","10.1109/ICCCBDA.2016.7529537","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7529537","log;cache;metadata;backup;CEPH","Metadata;Delays;File systems;Bandwidth","cache storage;meta data;public domain software","metadata cache backup technology;CEPH file system;log cache backup;metadata access delays;caching logs;server clusters;metadata servers;cached data backup;metadata access performance;open source codes","","","","30","","4 Aug 2016","","","IEEE","IEEE Conferences"
"Logger4u: Predicting debugging statements in the source code","S. Saini; N. Sardana; S. Lal","Department of Computer Science, Jaypee Institute of Information Technology, Noida; Department of Computer Science, Jaypee Institute of Information Technology, Noida; Department of Computer Science, Jaypee Institute of Information Technology, Noida","2016 Ninth International Conference on Contemporary Computing (IC3)","20 Mar 2017","2016","","","1","7","Software logging is an essential programming practice that saves important runtime information that can be used later by software developers for troubleshooting, debugging and monitoring the software. Even though software logging has numerous benefits this practice is underutilized because of lack of any formal guiding principles to developers for making strategic and efficient logging decisions. Logging should be optimized because too much logging can cause performance overheads; sparse logging can leave out vital information that might give clues to developers about the real issues. In absence of any formal guidelines developers rely solely on their domain knowledge and experience while making logging decisions. In order to lessen this effort of making decisions we have proposed a machine learning based framework, Logger4u for if-block logging prediction. We extract and use 28 distinctive static features from the source code helpful in making well informed logging decisions. We use Support Vector Machine (two variants, 1 linear and 1 RBF kernel based) models, Multilayer Perceptron with back propagation model and Random forest model in our work. Our approach gives encouraging results for if-block logging task. The accuracy achieved by the Linear SVM, MLP, Random Forest and kernel SVM are 73.05%, 74.62%, 79.84% and 81.22% respectively","","978-1-5090-3251-8","10.1109/IC3.2016.7880255","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7880255","Logging;Machine Learning;Multilayer Perceptron Principal Component Analysis;Random Forest;Support Vector Machine;Software Debugging","Support vector machines;Feature extraction;Kernel;Multilayer perceptrons;Containers;Neurons","backpropagation;feature extraction;multilayer perceptrons;program debugging;random processes;source code (software);support vector machines;system monitoring","Logger4u;software debugging statements;source code;software logging;software troubleshooting;software monitoring;decision making;sparse logging;logging decisions;machine learning based framework;if-block logging prediction;feature extraction;support vector machine;multilayer perceptron;backpropagation model;random forest model","","3","","14","","20 Mar 2017","","","IEEE","IEEE Conferences"
"PADM: Page Rank-Based Anomaly Detection Method of Log Sequences by Graph Computing","X. Yan; W. Zhou; Y. Gao; Z. Zhang; J. Han; G. Fu","Univ. of Chinese Acad. of Sci., Beijing, China; Univ. of Chinese Acad. of Sci., Beijing, China; Univ. of Chinese Acad. of Sci., Beijing, China; Univ. of Chinese Acad. of Sci., Beijing, China; Univ. of Chinese Acad. of Sci., Beijing, China; Nat. Comput. Network Emergency Response Tech. Team, Coordination Center of China, China","2014 IEEE 6th International Conference on Cloud Computing Technology and Science","12 Feb 2015","2014","","","700","703","With the popularity of various software applications in cloud computing, software exception becomes an important issue. How to detect the exceptions more quickly seems to be crucial for the software service company. To solve the above problem, this paper presents an efficient log anomaly detection method named PADM (Page Rank-based Anomaly Detection Method) based on the graph computing algorithm. In this method, the logs are transformed into a graph to represent the complex relationship between the log records, then we design an extended Page Rank algorithm based on the graph to get the importance score for each log. After that, we compare the scores to that of the training logs to determine whether they are abnormal or not. Finally, we compare PADM with other anomaly detection methods on the real logs, and the results show that it outperforms the currently widely used mechanisms with higher accuracy, lower time complexity and better scalability.","","978-1-4799-4093-6","10.1109/CloudCom.2014.70","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7037742","graph representation;anomaly detection;pagerank value;graph computing;log sequences","Time complexity;Algorithm design and analysis;Markov processes;Testing;Scalability;Software;Training","graph theory;security of data;Web sites","PADM;Page Rank-based anomaly detection method;log sequences;graph computing;software exception;software service company;log anomaly detection method;log records;training logs","","4","","8","","12 Feb 2015","","","IEEE","IEEE Conferences"
"Mining Periodic Patterns from Nested Event Logs","J. R. Getta; M. Zimniak; W. Benn","Sch. of Comput. Sci. & Software Eng., Univ. of Wollongong, Wollongong, NSW, Australia; Fac. of Comput. Sci., Tech. Univ. Chemnitz, Chemnitz, Germany; Fac. of Comput. Sci., Tech. Univ. Chemnitz, Chemnitz, Germany","2014 IEEE International Conference on Computer and Information Technology","15 Dec 2014","2014","","","160","167","Information about periodic computations of processes, events, and software components can be used to improve performance of software systems. This work investigates mining periodic patterns of events from historical information related to processes, events, and software components. We introduce a concept of a nested event log that generalizes historical information stored in the application traces, event logs and dynamic profiles. We show how a nested event log can be compressed into a reduced event table and later on converted into a workload histogram suitable for mining periodic patterns of events. The paper defines a concept of periodic pattern and its validation in a workload histogram. We propose two algorithms for mining periodic patterns and we define the quality indicators for the patterns found. We show, that a system of operations on periodic patterns introduced in this work can be used to derive new periodic patterns with some of the quality indicators better from the original ones. The paper is concluded with an algorithm for deriving periodic patterns with the given quality constraints.","","978-1-4799-6239-6","10.1109/CIT.2014.27","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6984648","periodic patterns;process mining;nested events;nested logs","Histograms;Data mining;Heuristic algorithms;Software systems;Transforms;Database systems","data mining","periodic pattern mining;nested event logs;software component;event table;workload histogram;quality indicator;quality constraint","","4","","31","","15 Dec 2014","","","IEEE","IEEE Conferences"
"Process Mining and Automatic Process Discovery","M. R. Peña; S. Bayona-Oré","Universidad Nacional Mayor de San Marcos, Lima, Perú; Universidad Nacional Mayor de San Marcos, Lima, Perú","2018 7th International Conference On Software Process Improvement (CIMPS)","27 Jan 2019","2018","","","41","46","Business Processes Modeling is essential for the management and execution of processes. However, when the execution of the processes differs from the pre-established models is necessary to review the traces and records of events to know these differences. One goal of process mining is to discover the real processes through the extraction of knowledge from the records of events available in the information systems. This paper describes a systematic literature review to identify the algorithms developed for automatic discovery of business processes. 20 articles that included algorithm proposals were identified and the results show that the algorithms provide similar models to the records of events when these events are clean without noise. In addition, it is observed that the most used technique to model the flows is Petri net.","","978-1-7281-0158-3","10.1109/CIMPS.2018.8625621","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8625621","process mining;event log;process model;discovery;techniques;algorithms","Unified modeling language;Data mining;Petri nets;Software;Silicon compounds;Monitoring;Machine learning","business data processing;business process re-engineering;data mining;information systems;Petri nets","process mining;automatic process discovery;Business Processes Modeling;systematic literature review;automatic discovery","","","","0","","27 Jan 2019","","","IEEE","IEEE Conferences"
"Impact of Team Size, Project Scale and Level of Education on Software Build Event Status in Enriched Event Streams","A. Sami; R. K. Kakolaki; A. Taghados","CSE and IT Department, Shiraz University, Shiraz, Iran; CSE and IT Department, Shiraz University, Shiraz, Iran; CSE and IT Department, Shiraz University, Shiraz, Iran","2019 27th Iranian Conference on Electrical Engineering (ICEE)","5 Aug 2019","2019","","","2045","2049","Creating a successful team based on number of developers, previous experience of programmer, and their level of education has been a topic of research for decades. Some studies suggest small size teams are better, while others find larger teams more successful. In addition, large projects are more complicated and more error prone. Discovery of the optimal values for these factors in software engineering is always considered as a major challenge. This paper examined the size of the team and scale of project and the level of education of developers on the status of Build events of software based on 11 million actual interactions performed by 83 programmers in a dataset called Enriched Event Streams dataset. The programmers had various programming skills and used variety of programming languages. The results based on 15,000 hours of development activity log indicate that the optimal size of the team is medium size, with developers engaged in large scaled projects without graduate degrees from universities. In other words, experience in large scale projects provides more advantages in contrast to graduate education that has negative impact on programming skills.","2642-9527","978-1-7281-1508-5","10.1109/IranianCEE.2019.8786442","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8786442","Team Size;Project Scale;Level of Education;Build Event;Enriched Event Streams dataset","Education;Software engineering;Programming profession;Computer science education;Project management","computer science education;software engineering","team size;project scale;software engineering;programming skills;graduate education;educational level;enriched event streams dataset;software build event status","","","","19","","5 Aug 2019","","","IEEE","IEEE Conferences"
"Process mining for project management","J. Joe; T. Emmatty; Y. Ballal; S. Kulkarni","Computer Engineering Department, St. Francis Institute of Technology, Mumbai, India; Computer Engineering Department, St. Francis Institute of Technology, Mumbai, India; Computer Engineering Department, St. Francis Institute of Technology, Mumbai, India; Computer Engineering Department, St. Francis Institute of Technology, Mumbai, India","2016 International Conference on Data Mining and Advanced Computing (SAPIENCE)","12 Dec 2016","2016","","","41","46","Business process mining or process mining is the intersection between data mining and business process modelling that extracts business patterns from event logs. Event logs are freely available in any organization. Business logs are a potential source of useful information. By the various patterns that are present in the logs, a lot can be estimated about the type of procedures that should be incorporated into the organization for better performance. Event logs store information about time and event data of business processes. Process mining algorithms are used to mine business process models using event logs. Generating automated business models out of this could provide valuable insight to a firm eventually leading to customer satisfaction. Process Mining works by three phases: discovery, conformation and alteration. By using process mining, many kinds of information can be collected about the process, such as control-flow, performance, organizational information and decision patterns. A process model could be represented as Petri nets which is a formal graphical representation of the workflow diagram or it can be represented as Business Process Modelling Notation. This project aims to develop a user friendly platform which is capable of generating petri net like models by process mining. By using various process mining algorithms we will develop software which would mine the event logs of a particular firm. It would provide a data or workflow analysis scheme. This would optimize business process intelligence and thus provide alternative and superior work strategies. In this project, we are mainly targeting project management using process mining. There are many projects that are undertaken by an IT company that all follow the same procedure. The concept of business process mining can be used in order to improve the performance of a company by optimizing its Software Development Life Cycle. By feeding the previous logs of a similar project of the company, the software would give a flowgraph. This flowgraph can help to identify the sequence of the activities, roles in the organization as well as various efficiency parameters. The algorithm being used is the Heuristic Miner Algorithm for process mining.","","978-1-4673-8594-7","10.1109/SAPIENCE.2016.7684142","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7684142","process mining;event logs","Data mining;Project management;Heuristic algorithms;Software algorithms;Software;Organizations","business process re-engineering;competitive intelligence;customer satisfaction;data analysis;data mining;flow graphs;Petri nets;software engineering;workflow management software","project management;business process mining;data mining;business process modelling notation;business pattern extraction;event logs;automated business model generation;customer satisfaction;discovery phase;conformation phase;alteration phase;Petri nets;workflow diagram formal graphical representation;data analysis scheme;workflow analysis scheme;business process intelligence optimization;IT company;software development life cycle optimization;flowgraph;heuristic miner algorithm","","","","6","","12 Dec 2016","","","IEEE","IEEE Conferences"
"RALD: Reliable Transactional Software Controller for Redundancy Array of Log Structured Disks","Z. Sun; M. Guo; H. Dong; Y. Liu; Z. Liu; L. Xu","Inst. of Comput. Technol., Beijing, China; Inst. of Comput. Technol., Beijing, China; Inst. of Comput. Technol., Beijing, China; Lenovo Group, Ltd., Beijing, China; Inst. of Comput. Technol., Beijing, China; Lenovo Group, Ltd., Beijing, China","2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems","30 Nov 2015","2015","","","110","121","An inconsistent update in a parity-based RAID can cause data loss when a disk failure occurs. There are two ways to restore the consistency: 1, Re-calculating parities, which does not work with disk failures. 2, Using transactions to record and replay updated data-contents in the transaction log, which is a single point of failure. It needs much bandwidth to replicate a transaction log of data-contents over network. We proposed the Above-Logging Transaction (ALT), and designed the Redundancy Array of Log-structured Disks (RALD). All updates histories of data-contents were on log-structured disks. ALTs recorded updated data's addresses on those log-structured disks into the transaction log by which ALTs mapped the consistent blocks into the read-only snapshot's space after they finished. To avoid the single point of failure, the RALD copied the ALTs log into those log-structured disks. By using flushes, the RALD utilized write-buffers safely. We had evaluated the RALD on SATA3 HDDs. The RALD can restore consistency from disk failures plus system crashes. The ALT's logs consumes little bandwidth, less than 0.5MB/s per replica. Compared to the Linux MD RAID: on write-dominant traces, the RALD has 20% to 190% more IOPS, on read-dominant traces, the RALD has 30% to 160% more IOPS if its internal caches have effective read-ahead, otherwise it has 10% to 50% less IOPS.","","978-1-4799-8937-9","10.1109/HPCC-CSS-ICESS.2015.13","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7336152","RAID;consistency;reliability;transaction;logging;address mapping;snapshot;flush","Random access memory;Metadata;Bandwidth;Media;Software;Nonvolatile memory;Software reliability","disc drives;hard discs;RAID;redundancy;transaction processing","RALD;reliable transactional software controller;parity-based RAID;disk failure;data loss;consistency restoration;parity recalculation;updated data-content replay;updated data-content record;transaction log replication;above-logging transaction;ALT;redundancy array-of-log-structured disks;data addresses;consistent block mapping;read-only snapshot space;flushes;write-buffers;SATA3 HDD;system crashes;write-dominant traces;IOPS;read-dominant traces;internal caches","","","1","33","","30 Nov 2015","","","IEEE","IEEE Conferences"
"Processing of the results from educational forum of the virtual courses in order to analyze them: Making automatic reports from the log files","L. M. Romero-Moreno; F. E. de Salamanca Ros","Departamento de Lenguajes y Sistemas Informáticos, Escuela Técnica Superior de Ingeniería Informática, Universidad de Sevilla Sevilla, España; Departamento de Lenguajes y Sistemas Informáticos, Escuela Técnica Superior de Ingeniería Informática Universidad de Sevilla, Sevilla, España","2016 11th Iberian Conference on Information Systems and Technologies (CISTI)","28 Jul 2016","2016","","","1","6","Methodology eLearning improves educational and promotes to share and collaborate. This work presents a way to analyze and automate log files from the educational forums in a course of virtual learning systems. We have developed a piece of software to process the logs files. Text reports and database files are produced. Then it is possible to study them and to obtain conclusions about the work of our students in this context.","","978-9-8998-4346-2","10.1109/CISTI.2016.7521497","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7521497","Learning analytics;collavorative learning;collavorative interactions;eLearning platform","Electronic learning;Silicon;Software;Internet;IP networks;Learning systems;Databases","computer aided instruction;educational courses","educational forum;virtual courses;automatic reports;e-learning;electronic learning;log file analysis;log file automation;virtual learning systems;text reports;database files","","","","","","28 Jul 2016","","","IEEE","IEEE Conferences"
"Simple event correlator - Best practices for creating scalable configurations","R. Vaarandi; B. Blumbergs; E. Çalışkan","Department of Computer Science, Tallinn University of Technology, Tallinn, Estonia; Department of Computer Science, Tallinn University of Technology, Tallinn, Estonia; Cyber Security Institute, TÜBİTAK, Kocaeli, Turkey","2015 IEEE International Multi-Disciplinary Conference on Cognitive Methods in Situation Awareness and Decision","18 May 2015","2015","","","96","100","During the past two decades, event correlation has emerged as a prominent monitoring technique, and is essential for achieving better situational awareness. Since its introduction in 2001 by one of the authors of this paper, Simple Event Correlator (SEC) has become a widely used open source event correlation tool. During the last decade, a number of papers have been published that describe the use of SEC in various environments. However, recent SEC versions have introduced a number of novel features not discussed in existing works. This paper fills this gap and provides an up-to-date coverage of best practices for creating scalable SEC configurations.","2379-1675","978-1-4799-8015-4","10.1109/COGSIMA.2015.7108181","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7108181","Simple Event Correlator;event correlation;event processing;log file analysis","Correlation;Context;IP networks;Pattern matching;Reactive power;Monitoring;Conferences","public domain software;system monitoring","event correlation;prominent monitoring technique;situational awareness;simple event correlator;open source event correlation tool;SEC","","6","","21","","18 May 2015","","","IEEE","IEEE Conferences"
"Requirement Checking: Generating Uses Cases out of Navigational Logs in Web Applications","Z. Ding; M. Jiang; G. Pu; J. Liu","Center of Math Comput. & Software Eng., Zhejiang Sci-Tech Univ., Hangzhou, China; Center of Math Comput. & Software Eng., Zhejiang Sci-Tech Univ., Hangzhou, China; Software Eng. Inst., East China Normal Univ., Shanghai, China; Software Eng. Inst., East China Normal Univ., Shanghai, China","2010 10th International Conference on Quality Software","7 Sep 2010","2010","","","341","344","For a web application, we rebuild the use cases from the log file by applying Natural Language Processing technique; then based on these use cases we construct component model and extract the component behavior; finally, we compare this implementation behavior with the design behavior to check if these two behaviors are matched.","2332-662X","978-0-7695-4131-0","10.1109/QSIC.2010.17","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5562981","Web application;use cases;natural language processing;component model;behavior checking","Navigation;Object oriented modeling;Wire;Testing;Component architectures;Software;Adaptation model","Internet;natural language processing;program verification","requirement checking;navigational logs;Web applications;natural language processing technique;log file;component model construction;component behavior extraction","","","","7","","7 Sep 2010","","","IEEE","IEEE Conferences"
"Dynamic logging with Dylog for networked embedded systems","W. Dong; C. Huang; J. Wang; C. Chen; J. Bu","College of Computer Science, Zhejiang University; College of Computer Science, Zhejiang University; School of Software and TNLIST, Tsinghua University; College of Computer Science, Zhejiang University; College of Computer Science, Zhejiang University","2014 Eleventh Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)","18 Dec 2014","2014","","","381","389","We present Dylog, a dynamic logging facility for networked embedded systems. Dylog employs several techniques to enable lightweight and interactive logging. First, Dylog uses binary instrumentation for dynamically inserting or removing logging statements, enabling interactive debugging at the runtime. Second, Dylog incorporates an efficient storage system and log collection protocol for recording and transferring the logging messages. In particular, Dylog significantly reduces the communication cost by storing string identifiers and restoring them back to corresponding strings at the PC. Third, Dylog employs MAC layer timestamping and a linear clock model for reconstructing the synchronized time of the logging messages with a very high precision. We implement and evaluate Dylog on TinyOS 2.1.1/TelosB. Results show that Dylog incurs a reasonable overhead. Dylog can help gain great visibility into the system behaviors, and diagnose performance issues at the source code level.","2155-5494","978-1-4799-4657-0","10.1109/SAHCN.2014.6990375","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6990375","","Wireless sensor networks;Synchronization;Ash;Instruments;Runtime;Protocols;Embedded systems","embedded systems;operating systems (computers);program debugging;system monitoring;telecommunication computing;wireless sensor networks","dynamic logging;Dylog;networked embedded systems;lightweight logging;interactive logging;binary instrumentation;interactive debugging;MAC layer;linear clock model;TinyOS 2.1.1/TelosB","","3","2","28","","18 Dec 2014","","","IEEE","IEEE Conferences"
"Automated Localization for Unreproducible Builds","Z. Ren; H. Jiang; J. Xuan; Z. Yang","Key Lab. for Ubiquitous Network & Service Software of Liaoning Province, Dalian Univ. of Technol., Dalian, China; Key Lab. for Ubiquitous Network & Service Software of Liaoning Province, Dalian Univ. of Technol., Dalian, China; Sch. of Comput. Sci., Wuhan Univ., Wuhan, China; Dept. of Comput. Sci., Western Michigan Univ., Kalamazoo, MI, USA","2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)","2 Sep 2018","2018","","","71","81","Reproducibility is the ability of recreating identical binaries under pre-defined build environments. Due to the need of quality assurance and the benefit of better detecting attacks against build environments, the practice of reproducible builds has gained popularity in many open-source software repositories such as Debian and Bitcoin. However, identifying the unreproducible issues remains a labour intensive and time consuming challenge, because of the lacking of information to guide the search and the diversity of the causes that may lead to the unreproducible binaries. In this paper we propose an automated framework called RepLoc to localize the problematic files for unreproducible builds. RepLoc features a query augmentation component that utilizes the information extracted from the build logs, and a heuristic rule-based filtering component that narrows the search scope. By integrating the two components with a weighted file ranking module, RepLoc is able to automatically produce a ranked list of files that are helpful in locating the problematic files for the unreproducible builds. We have implemented a prototype and conducted extensive experiments over 671 real-world unreproducible Debian packages in four different categories. By considering the topmost ranked file only, RepLoc achieves an accuracy rate of 47.09%. If we expand our examination to the top ten ranked files in the list produced by RepLoc, the accuracy rate becomes 79.28%. Considering that there are hundreds of source code, scripts, Makefiles, etc., in a package, RepLoc significantly reduces the scope of localizing problematic files. Moreover, with the help of RepLoc, we successfully identified and fixed six new unreproducible packages from Debian and Guix.","1558-1225","978-1-4503-5638-1","10.1145/3180155.3180224","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8453064","Unreproducible Build;Localization;Software Maintenance","Software;Task analysis;Feature extraction;Filtering;Hafnium;Software engineering;Computer science","program diagnostics;public domain software;query processing;security of data;software maintenance","detecting attacks;open-source software repositories;unreproducible binaries;problematic files;query augmentation component;build logs;heuristic rule-based filtering component;search scope;weighted file ranking module;ranked list;topmost ranked file;automated localization;unreproducible builds;pre-defined build environments;quality assurance;Bitcoin;Guix;RepLoc features;unreproducible Debian packages","","4","","","","2 Sep 2018","","","IEEE","IEEE Conferences"
"Process Discovery from Dependence-Complete Event Logs","W. Song; H. Jacobsen; C. Ye; X. Ma","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Middleware Systems Research Group; College of Information Science and Technology, Hainan University, Haikou, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China","IEEE Transactions on Services Computing","20 May 2017","2016","9","5","714","727","Process mining, especially process discovery, has been utilized to extract process models from event logs. One challenge faced by process discovery is to identify concurrency effectively. State-of-the-art approaches employ activity orders in traces to undertake process discovery and they require stringent completeness notions of event logs. Thus, they may fail to extract appropriate processes when event logs cannot meet the completeness criteria. To address this problem, we propose in this paper a novel technique which leverages activity dependences in traces. Based on the observation that activities with no dependencies can be executed in parallel, our technique is in a position to discover processes with concurrencies even if the logs fail to meet the completeness criteria. That is, our technique calls for a weaker notion of completeness. We evaluate our technique through experiments on both real-world and synthetic event logs, and the conformance checking results demonstrate the effectiveness of our technique and its relative advantages compared with state-of-the-art approaches.","1939-1374","","10.1109/TSC.2015.2426181","National Basic Research Program of China; National Natural Science Foundation of China; Specialized Research Fund for the Doctoral Program of Higher Education; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7095577","Process discovery;event logs;completeness notion;dynamic dependence graphs;transformation rules","Process control;Routing;Computational modeling;Data mining;Petri nets;Noise;Jacobian matrices","concurrency (computers);data mining;information systems","process discovery;dependence-complete event logs;process mining;process models;concurrency identification;activity orders;completeness criteria;real-world event logs;synthetic event logs;conformance checking;information systems","","28","","43","","27 Apr 2015","","","IEEE","IEEE Journals"
"Analyzing web application log files to find hit count through the utilization of Hadoop MapReduce in cloud computing environment","S. Narkhede; T. Baraskar; D. Mukhopadhyay","Department of Information Technology, Maharashtra Institute of Technology, Pune, India; Department of Information Technology, Maharashtra Institute of Technology, Pune, India; Department of Information Technology, Maharashtra Institute of Technology, Pune, India","2014 Conference on IT in Business, Industry and Government (CSIBIG)","12 Mar 2015","2014","","","1","7","MapReduce has been widely applied in various fields of data and compute intensive applications and also it is important programming model for cloud computing. Hadoop is an open-source implementation of MapReduce which operates on terabytes of data using commodity hardware. We have applied this Hadoop MapReduce programming model for analyzing web log files so that we could get hit count of specific web application. This system uses Hadoop file system to store log file and results are evaluated using Map and Reduce function. Experimental results show hit count for each field in log file. Also due to MapReduce runtime parallelization response time is reduced.","","978-1-4799-3064-7","10.1109/CSIBIG.2014.7056950","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7056950","Hadoop;mapreduce;cloud computing;file system;data processing","Computational modeling;Analytical models;Data models;Abstracts;Load modeling;User interfaces;Three-dimensional displays","cloud computing;parallel processing;public domain software","Web application log files;hit count;Hadoop MapReduce utilization;cloud computing environment;open-source implementation;commodity hardware;Web log files;Hadoop file system;log file;MapReduce runtime parallelization response time","","4","","23","","12 Mar 2015","","","IEEE","IEEE Conferences"
"Predicting the Next Process Event Using Convolutional Neural Networks","A. Al-Jebrni; H. Cai; L. Jiang","School of Software, Shanghai Jiao Tong University, Shanghai, China; School of Software, Shanghai Jiao Tong University, Shanghai, China; School of Software, Shanghai Jiao Tong University, Shanghai, China","2018 IEEE International Conference on Progress in Informatics and Computing (PIC)","6 May 2019","2018","","","332","338","Adding the feature of business process event prediction to information systems increases its productivity in the long run, enhances the quality of the taken decisions, and eliminates inconsistencies. Inspired by the previous works of employing deep learning approaches to predicting the next process event based on files of logged events, we propose the use of one-dimensional convolutional neural networks (1D CNN) to address the same problem. In the proposed approach we used a five-layer 1D CNN method to predict the next process event based on the previous instances. This paper compared the proposed approach with other approaches that used recurrent neural networks (RNN) with Long Short-Term Memory (LSTM) neural networks, and others on eight datasets. The proposed approach outperformed all the previous studies of the state-of-the-art in this domain on all the provided datasets.","","978-1-5386-7672-1","10.1109/PIC.2018.8706282","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8706282","one-dimensional;convolutional neural networks;prediction;process events;deep learning","Deep learning;Hidden Markov models;Business;Predictive models;Recurrent neural networks;Information systems;Encoding","business data processing;convolutional neural nets;learning (artificial intelligence);recurrent neural nets","recurrent neural networks;next process event;business process event prediction;deep learning approaches;logged events;one-dimensional convolutional neural networks;five-layer 1D CNN method;long short-term memory neural networks;RNN;LSTM","","","","24","","6 May 2019","","","IEEE","IEEE Conferences"
"On the Use of Automated Log Clustering to Support Effort Reduction in Continuous Engineering","C. M. Rosenberg; L. Moonen","Simula Res. Lab., Oslo, Norway; Simula Res. Lab., Oslo, Norway","2018 25th Asia-Pacific Software Engineering Conference (APSEC)","23 May 2019","2018","","","179","188","Continuous engineering (CE) practices, such as continuous integration and continuous deployment, have become key to modern software development. They are characterized by short automated build and test cycles that give developers early feedback on potential issues. CE practices help to release software more frequently, and reduces risk by increasing incrementality. However, effective use of CE practices in industrial projects requires making sense of the vast amounts of data that results from the repeated build and test cycles. The goal of this paper is to investigate to what extent these data can be treated more effectively by automatically grouping logs of runs that failed for the same underlying reasons, and what effort reduction can be achieved. To this end, we replicate and extend earlier work on system log clustering to evaluate its efficacy in the CE context, and to investigate the impact of five alternative log vectorization techniques. We built a prototype tool that is used to conduct an empirical case study on continuous deployment logs provided by our industrial collaborator. Questions to be answered include: (1) Can we reduce the effort needed to discover all latent issues in a set of failing runs? (2) How to best leverage the contrast between passing and failing runs to increase accuracy? (3) What trade-offs are there between effort reduction and accuracy? We present a quantitative and qualitative analysis of the results of our study. We conclude by evaluating the trade-offs, and give recommendations for applying this approach in practice.","2640-0715","978-1-7281-1970-0","10.1109/APSEC.2018.00032","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8719470","continuous deployment;problem identification;diagnosis;event log mining;event log analysis","Software;Pipelines;Testing;Production;Tools;Buildings;Hardware","data mining;pattern clustering;software engineering","automated log clustering;effort reduction;continuous engineering practices;continuous integration;continuous deployment;modern software development;CE practices;CE context","","","","32","","23 May 2019","","","IEEE","IEEE Conferences"
"Mercury Lab Management Software","T. Merport; O. Proskurowski","Marvell NanoLab, Univ. of California, Berkeley, CA, USA; Marvell NanoLab, Univ. of California, Berkeley, CA, USA","2012 19th Biennial University/Government/Industry, Micro/Nano Symposium (UGIM)","23 Jul 2012","2012","","","1","2","Mercury is a system that helps management, staff, and members efficiently use laboratory resources. Mercury emphasizes accounting by utilizing a double entry accounting system: lab activities are recorded and debited/credited to the appropriate accounts in real-time. It is a Management Information System or more appropriately a Member Information System. The software is named Mercury after the Roman god that acted as a messenger (often depicted holding a purse). The components of Mercury are a relational database management system (Ingres), daemons, and clients. It is a dual, three tier application (see the diagram below). The client program that runs in the laboratory is called MercuryClient. It connects to a session management daemon, Mercury Server. There is also a client system that runs in a browser, MercuryWeb. Most of the logic or business rules for the system are implemented in the database as stored procedures. This helps insure data integrity and improves speed. It also minimizes duplication of procedures in the middle tier and clients. MercuryClient is a Java application that members use in the laboratory. When the application is run, a sign-in window appears. Members enter their login name, password, and select a project associated with their account. If members are qualified to use the lab, the full MercuryClient screen appears (lab charges commence). At this point they are connected to the Mercury database through Mercury Server and have access to equipment status, qualifications, materials, viewing who is in the lab, and more. The main task for members once logged in, will be to select an equipment row and enable the equipment. Several rules are checked at this point including presence, equipment and facility qualifications, and problem reports. MercuryClient maintains a continual session with the server and holds session information such as location, lab time, and idle time. MercuryWeb is a web application that provides lab members and staff access to the Mercury database system through any web browser. MercuryWeb is written in Java and uses SQL queries and stored procedures to access and update data in the Ingres RDMS. MercuryWeb also allows creating various reports in PDF, Word, Excel, and PowerPoint formats. MercuryWeb includes the following major modules: Accounting, Inventory, Member Management, Online Tests, Facilities, Reservations, Calendar, and Tasks. The Accounting module is used for day to day tasks as well as to create end of month financial statements and reports. The Inventory module helps to maintain inventory of supplies and parts used in the lab. Member Management provides member and staff account setup and administration. Online Tests allow creating, taking, and grading tests online, completely replacing paper based tests. Facilities are used to define resources (equipment, utilities, and locations) and create associations between them. The Reservation modules allow lab members to reserve frequently used equipment. The inspiration for Mercury was The Berkeley Computer Integrated Manufacturing System (BCIMS) used in the Microlab (known more commonly as the Wand). Twenty years of use by members and staff and two million activities captured by the Wand provided a solid foundation for the design of Mercury.","2375-5350","978-1-4577-1752-9","10.1109/UGIM.2012.6247079","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6247079","","Laboratories;Servers;Software;Browsers;Databases;Java;Qualifications","management information systems;relational databases;virtual instrumentation","Mercury lab management software;laboratory resources;double entry accounting system;management information system;member information system;relational database management system;Ingres;MercuryClient;Mercury Server;MercuryWeb;data integrity;Java application;SQL queries;SQL stored procedures;financial statements;financial reports;înventory module;reservation modules","","1","","","","23 Jul 2012","","","IEEE","IEEE Conferences"
"Log-Based Reliability Analysis of Software as a Service (SaaS)","S. Banerjee; H. Srikanth; B. Cukic","Lane Dept. of Comput. Sci. & Electr. Eng., West Virginia Univ., Morgantown, WV, USA; Lotus Div., IBM, Littleton, MA, USA; Lane Dept. of Comput. Sci. & Electr. Eng., West Virginia Univ., Morgantown, WV, USA","2010 IEEE 21st International Symposium on Software Reliability Engineering","11 Nov 2010","2010","","","239","248","Software as a Service (SaaS) has gained momentum in the past few years and businesses have been increasingly moving to SaaS model for their IT solutions. SaaS is a newer and transformed model where software is delivered to customers as a service over the web. With the SaaS model, there is a need for service providers to ensure that the services are available and reliable for end users at all times, which introduces significant pressure on the service provider to ensure right test processes and methodologies to minimize any impact to the provisions in Service Level Agreements (SLA). There is lack of research on the unique approaches to reliability analysis of SaaS suites. In this paper, we expand traditional approaches to reliability analysis of traditional web servers and propose methods tailored towards assessing the workload and reliability of SaaS applications. In addition we show the importance of data filtration when assessing SaaS reliability from log files. Finally, we discuss the suitability of reliability measures with respect to their relevance in the context of SLAs.","2332-6549","978-1-4244-9056-1","10.1109/ISSRE.2010.46","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5635046","Software as a Service;Software Reliability;Software Analytics","Software reliability;Web server;Navigation;Usability;Organizations","Internet;software architecture;software reliability","log based reliability analysis;software as a service;Web;service level agreements;data filtration","","27","","27","","11 Nov 2010","","","IEEE","IEEE Conferences"
"Logzip: Extracting Hidden Structures via Iterative Clustering for Log Compression","J. Liu; J. Zhu; S. He; P. He; Z. Zheng; M. R. Lyu","Sun Yat-Sen University & The Chinese University of Hong Kong; Huawei Noah’s Ark Lab, China; The Chinese University of Hong Kong; ETH Zurich; Sun Yat-Sen University; The Chinese University of Hong Kong","2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)","9 Jan 2020","2019","","","863","873","System logs record detailed runtime information of software systems and are used as the main data source for many tasks around software engineering. As modern software systems are evolving into large scale and complex structures, logs have become one type of fast-growing big data in industry. In particular, such logs often need to be stored for a long time in practice (e.g., a year), in order to analyze recurrent problems or track security issues. However, archiving logs consumes a large amount of storage space and computing resources, which in turn incurs high operational cost. Data compression is essential to reduce the cost of log storage. Traditional compression tools (e.g., gzip) work well for general texts, but are not tailed for system logs. In this paper, we propose a novel and effective log compression method, namely logzip. Logzip is capable of extracting hidden structures from raw logs via fast iterative clustering and further generating coherent intermediate representations that allow for more effective compression. We evaluate logzip on five large log datasets of different system types, with a total of 63.6 GB in size. The results show that logzip can save about half of the storage space on average over traditional compression tools. Meanwhile, the design of logzip is highly parallel and only incurs negligible overhead. In addition, we share our industrial experience of applying logzip to Huawei's real products.","2643-1572","978-1-7281-2508-4","10.1109/ASE.2019.00085","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8952406","logs;structure extraction;log compression;log management;iterative clustering","Clustering algorithms;Iterative algorithms;Sparks;Software engineering;Software systems","Big Data;data compression;iterative methods;pattern clustering;software engineering;storage management;system monitoring","structures extraction;runtime information;data source;software engineering;Big Data;data compression;log compression method;iterative clustering;Logzip;memory size 63.6 GByte","","1","","49","","9 Jan 2020","","","IEEE","IEEE Conferences"
"Task Analysis-Based User Event Logging for Mobile Applications","R. Durán-Sáez; X. Ferré; H. Zhu; Q. Liu","DLSIIS, Univ. Politec. de Madrid, Madrid, Spain; DLSIIS, Univ. Politec. de Madrid, Madrid, Spain; Sch. of Software Eng., Tongji Univ., Shanghai, China; Sch. of Software Eng., Tongji Univ., Shanghai, China","2017 IEEE 25th International Requirements Engineering Conference Workshops (REW)","2 Oct 2017","2017","","","152","155","Task analysis defines the user-centered tasks that users will carry out to use a system, and it serves as the basis for the definition of usability requirements related to efficiency and effectiveness. Usability and UX (User eXperience) are especially important for mobile application development. We aim to provide support to task modelling for Android application development, and to specify user events to be logged for further analysis. Selection of the task modelling technique has been carried out in a two steps approach, allowing, for the next step of developing, a tool to support task modelling and annotation.","","978-1-5386-3488-2","10.1109/REW.2017.45","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8054844","task analysis;task annotation;automated usability evaluation;usability;Android","Usability;Mobile communication;Tools;Analytical models;Complexity theory;Androids","Android (operating system);human factors;mobile computing;software engineering;task analysis","User eXperience;mobile application development;Android application development;task modelling technique;usability requirements;task analysis-based user event logging;user-centered tasks","","2","","25","","2 Oct 2017","","","IEEE","IEEE Conferences"
"LogProv: Logging events as provenance of big data analytics pipelines with trustworthiness","R. Wang; D. Sun; G. Li; M. Atif; S. Nepal","Data61, Commonwealth Scientific and Industrial Research Ogranisation, Australia; Data61, Commonwealth Scientific and Industrial Research Ogranisation, Australia; Shanghai Jiaotong University, China; National Computational Infrastructure, Australia; Data61, Commonwealth Scientific and Industrial Research Ogranisation, Australia","2016 IEEE International Conference on Big Data (Big Data)","6 Feb 2017","2016","","","1402","1411","Provenance is information about the origin and creation of data. In data science and engineering, such information is useful and sometimes even critical. In spite of that, provenance for big data is under-explored due to the challenges from the `Vs' of big data. In data analytics, users need to query history, reproduce intermediate or final results, tune models, and adjust parameters in runtime for making data-driven decisions. In addition, users need to evaluate data and pipeline trustworthiness. Towards realising these functionalities for big data provenance, we propose a solution, called LogProv, which needs to renovate data pipelines or even some of big data software infrastructure to generate structured logs for pipeline events, and then stores data and logs separately. The data are explicitly linked to the logs, which implicitly record pipeline semantics. Semantic information can be retrieved from the logs easily since the logs are well defined and structured beforehand. We implemented LogProv in Apache Pig, and adopted ElasticSearch to provide query service. In this paper LogProv is evaluated in a Hadoop ecosystem hosted by a cloud and empirically case-studied. The results show that LogProv is efficient since the performance overhead is no more than 10%, the query can be responded within 1 second, the trustworthiness is marked clearly, and there is no impact on the data processing logic of original pipelines.","","978-1-4673-9005-7","10.1109/BigData.2016.7840748","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7840748","","Pipelines;Big data;Semantics;Pipeline processing;Data analysis;History;Australia","Big Data;data analysis;data integrity;decision making;query processing;storage management;system monitoring;trusted computing","LogProv;logging events;Big Data analytics pipeline provenance;trustworthiness;query history;data-driven decision making;Big Data software infrastructure;data storage;Apache Pig;ElasticSearch;query service","","6","","31","","6 Feb 2017","","","IEEE","IEEE Conferences"
"Learning Latent Events From Network Message Logs","S. Satpathi; S. Deb; R. Srikant; H. Yan","University of Illinois at Urbana-Champaign (UIUC), Champaign, IL, USA; AT&T Labs, Florham Park, NJ, USA; University of Illinois at Urbana-Champaign (UIUC), Champaign, IL, USA; AT&T Labs, Florham Park, NJ, USA","IEEE/ACM Transactions on Networking","16 Aug 2019","2019","27","4","1728","1741","We consider the problem of separating error messages generated in large distributed data center networks into error events. In such networks, each error event leads to a stream of messages generated by hardware and software components affected by the event. These messages are stored in a giant message log. We consider the unsupervised learning problem of identifying the signatures of events that generated these messages; here, the signature of an error event refers to the mixture of messages generated by the event. One of the main contributions of the paper is a novel mapping of our problem which transforms it into a problem of topic discovery in documents. Events in our problem correspond to topics and messages in our problem correspond to words in the topic discovery problem. However, there is no direct analog of documents. Therefore, we use a non-parametric change-point detection algorithm, which has linear computational complexity in the number of messages, to divide the message log into smaller subsets called episodes, which serve as the equivalents of documents. After this mapping has been done, we use a well-known algorithm for topic discovery, called LDA, to solve our problem. We theoretically analyze the change-point detection algorithm, and show that it is consistent and has low sample complexity. We also demonstrate the scalability of our algorithm on a real data set consisting of 97 million messages collected over a period of 15 days, from a distributed data center network which supports the operations of a large wireless service provider.","1558-2566","","10.1109/TNET.2019.2930040","AT and T; National Science Foundation; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8782613","Unsupervised learning;data mining;event message log;change point detection;Bayesian inference;data center networks;time series mixture","Data centers;Data mining;Classification algorithms;Detection algorithms;IEEE transactions;Hardware;Software","computational complexity;computer centres;data mining;text analysis;unsupervised learning","latent events;network message logs;error messages;distributed data center network;error event;giant message log;unsupervised learning problem;topic discovery problem;change-point detection algorithm;large wireless service provider;hardware components;software components","","1","","32","","31 Jul 2019","","","IEEE","IEEE Journals"
"Mining Android App Usages for Generating Actionable GUI-Based Execution Scenarios","M. Linares-Vásquez; M. White; C. Bernal-Cárdenas; K. Moran; D. Poshyvanyk","Coll. of William & Mary, Williamsburg, VA, USA; Coll. of William & Mary, Williamsburg, VA, USA; Coll. of William & Mary, Williamsburg, VA, USA; Coll. of William & Mary, Williamsburg, VA, USA; Coll. of William & Mary, Williamsburg, VA, USA","2015 IEEE/ACM 12th Working Conference on Mining Software Repositories","6 Aug 2015","2015","","","111","122","GUI-based models extracted from Android app execution traces, events, or source code can be extremely useful for challenging tasks such as the generation of scenarios or test cases. However, extracting effective models can be an expensive process. Moreover, existing approaches for automatically deriving GUI-based models are not able to generate scenarios that include events which were not observed in execution (nor event) traces. In this paper, we address these and other major challenges in our novel hybrid approach, coined as MONKEYLAB. Our approach is based on the Record→Mine→Generate→Validate framework, which relies on recording app usages that yield execution (event) traces, mining those event traces and generating execution scenarios using statistical language modeling, static and dynamic analyses, and validating the resulting scenarios using an interactive execution of the app on a real device. The framework aims at mining models capable of generating feasible and fully replayable (i.e., actionable) scenarios reflecting either natural user behavior or uncommon usages (e.g., corner cases) for a given app. We evaluated MONKEYLAB in a case study involving several medium-to-large open-source Android apps. Our results demonstrate that MONKEYLAB is able to mine GUI-based models that can be used to generate actionable execution scenarios for both natural and unnatural sequences of events on Google Nexus 7 tablets.","2160-1860","978-0-7695-5594-2","10.1109/MSR.2015.18","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7180072","GUI models;mobile apps;mining execution traces and event logs;language models","Graphical user interfaces;Testing;Androids;Humanoid robots;Vocabulary;Analytical models;History","Android (operating system);data mining;graphical user interfaces;program diagnostics;public domain software;source code (software);system monitoring","Android App usage mining;actionable GUI-based execution scenarios;GUI-based model extraction;Android app execution traces;source code;MONKEYLAB;Record→Mine→Generate→Validate framework;statistical language modeling;static analyses;dynamic analyses;natural user behavior;medium-to-large open-source Android apps;Google Nexus 7 tablets","","32","","54","","6 Aug 2015","","","IEEE","IEEE Conferences"
"A Statistical Comparison of Java and Python Software Metric Properties","G. Destefanis; M. Ortu; S. Porru; S. Swift; M. Marchesi","Brunel Univ., Uxbridge, UK; Brunel Univ., Uxbridge, UK; Brunel Univ., Uxbridge, UK; Brunel Univ., Uxbridge, UK; Brunel Univ., Uxbridge, UK","2016 IEEE/ACM 7th International Workshop on Emerging Trends in Software Metrics (WETSoM)","9 Jan 2017","2016","","","22","28","This paper presents a statistical analysis of 20 opensource object-oriented systems with the purpose of detecting differences in metrics distribution between Java and Python projects. We selected ten Java projects from the Java Qualitas Corpus and ten projects written in Python. For each system, we considered 10 class-level software metrics.We performed a best fit procedure on the empirical distributions through the log-normal distribution and the double Pareto distribution to identify differences between the two languages. Even though the statistical distributions for projects written in Java and Python may appear the same for lower values of the metric, performing the procedure with the double Pareto distribution for the Number of Local Methods metric reveals that major differences can be noticed along the queue of the distributions. On the contrary, the same analysis performed with the Number of Statements metric reveals that only the initial portion of the double Pareto distribution shows differences between the two languages. In addition, the dispersion parameter associated to the log-normal distribution fit for the total Number Of Methods can be used for distinguishing Java projects from Python projects.","2327-0969","978-1-4503-4177-6","10.1109/WETSoM.2016.012","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7809284","software metrics;java;python","Java;Software metrics;Log-normal distribution;Statistical distributions;Software quality","Java;object-oriented programming;Pareto distribution;public domain software;software metrics;statistical analysis","Java projects;Python projects;software metric properties;statistical analysis;opensource object-oriented systems;metrics distribution;Java Qualitas Corpus;log-normal distribution;double Pareto distribution;number of local methods metrics;distribution queues;number of statements metric;dispersion parameter","","","","20","","9 Jan 2017","","","IEEE","IEEE Conferences"
"Analyzing Developer Sentiment in Commit Logs","V. Sinha; A. Lazar; B. Sharif","Dept. of Comput. Sci. & Inf. Syst., Youngstown State Univ., Youngstown, OH, USA; Dept. of Comput. Sci. & Inf. Syst., Youngstown State Univ., Youngstown, OH, USA; Dept. of Comput. Sci. & Inf. Syst., Youngstown State Univ., Youngstown, OH, USA","2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR)","26 Jan 2017","2016","","","520","523","The paper presents an analysis of developer commit logs for GitHub projects. In particular, developer sentiment in commits is analyzed across 28,466 projects within a seven year time frame. We use the Boa infrastructure's online query system to generate commit logs as well as files that were changed during the commit. We analyze the commits in three categories: large, medium, and small based on the number of commits using a sentiment analysis tool. In addition, we also group the data based on the day of week the commit was made and map the sentiment to the file change history to determine if there was any correlation. Although a majority of the sentiment was neutral, the negative sentiment was about 10% more than the positive sentiment overall. Tuesdays seem to have the most negative sentiment overall. In addition, we do find a strong correlation between the number of files changed and the sentiment expressed by the commits the files were part of. Future work and implications of these results are discussed.","","978-1-4503-4186-8","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7832940","sentiment analysis;commit logs;Java projects","Sentiment analysis;Software;Software engineering;Data mining;Correlation;Information systems;Java","data mining;Java;query processing;sentiment analysis","developer sentiment analysis;commit logs;GitHub projects;Boa infrastructure online query system;large-commits;medium-commits;small-commits;file change history;neutral sentiments;negative sentiments;positive sentiments","","2","1","7","","26 Jan 2017","","","IEEE","IEEE Conferences"
"Similarity Calculation of Executable Using Intel Pin Instrumentation Framework","M. Bhowmik; M. Nara; B. R. Mohan","National Institute of Technology,Department of Information Technology,Karnataka,India; National Institute of Technology,Department of Information Technology,Karnataka,India; National Institute of Technology,Department of Information Technology,Karnataka,India","2020 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)","4 Jan 2021","2020","","","169","170","With the increase in the number of open-source and commercial code in the market, copyright and license infringement cases are on the rise. The lack of availability of source code makes identification a difficult task as existing techniques heavily rely on the source code. We propose two similarity measurement methods using the instruction log and the call-trace of each executable using Intel Pin tool. A Software Plagiarism Detector(SPD) was developed using the Intel Pin instrumentation tool and we have tested this approach on different small executable single-threaded and multi-threaded files. The results portray the validity of this method. We also talk about the possibility to expand this method for bigger software.","","978-1-7281-7735-9","10.1109/ISSREW51248.2020.00066","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9307639","software similarity;software plagiarism;software piracy;executable;instrumentation","Tools;Software;Instruments;Plagiarism;Pins;Software measurement;Instruction sets","computer crime;copyright;multi-threading;program diagnostics;program verification;public domain software;source code (software)","Intel Pin instrumentation framework;open source code;license infringement cases;source code;similarity measurement;Intel Pin instrumentation tool;executable single threaded files;executable multithreaded files;copyright cases;call trace;software plagiarism detector;similarity calculation;software piracy detection;similarity checkers","","","","7","","4 Jan 2021","","","IEEE","IEEE Conferences"
"The Study on an Intelligent General-Purpose Automated Software Testing Suite","X. Wu; J. Sun","Sch. of Inf. Eng., Chang'an Univ., Xi'an, China; State Key Lab. of Integrated Service Networks, Xidian Univ., Xi'an, China","2010 International Conference on Intelligent Computation Technology and Automation","26 Jul 2010","2010","3","","993","996","To make the labor intensive manual software testing automated, we present the design and implementation of an intelligent general-purpose automated software testing suite. With the two main tools in the suite: an automated software testing scheduler, Harness, and a log file error analyzer, Logscanner, the summary report can be sent to the testing engineer immediately after the test is over via email, even when the testing engineer is away from the testing site. The functionality and features of the general purpose automated software testing suite help to increase the testing efficiency, reduce the labor intensity of software testing, which in turn, leads to the decrease in testing cost.","","978-1-4244-7280-2","10.1109/ICICTA.2010.115","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5523591","software testing;general-purpose;automated testing;testing suite","Software testing;Automatic testing;System testing;Workstations;Error analysis;Sun;Computer architecture;Design automation;Design engineering;Manuals","program testing;software quality","labor intensive manual software testing;intelligent general purpose automated software testing suite;automated software testing scheduler;log file error analyzer;log scanner;engineer testing;labor intensity","","1","9","10","","26 Jul 2010","","","IEEE","IEEE Conferences"
"An approach for mining web service composition patterns from execution logs","Ran Tang; Ying Zou","Department of Electrical and Computer Engineering, Queen's University, Kingston, Canada; Department of Electrical and Computer Engineering, Queen's University, Kingston, Canada","2010 12th IEEE International Symposium on Web Systems Evolution (WSE)","9 Nov 2010","2010","","","53","62","A service-oriented application is composed of multiple web services to fulfill complex functionality that cannot be provided by individual web service. The combination of services is not random. In many cases, a set of services are repetitively used together in various applications. We treat such a set of services as a service composition pattern. The quality of the patterns is desirable due to the extensive uses and testing in the large number of applications. Therefore, the service composition patterns record the best practices in designing and developing reliable service-oriented applications. The execution log tracks the execution of services in a service-oriented application. To document the service composition patterns, we propose an approach that automatically identifies service composition patterns from various applications using execution logs. We locate a set of associated services using Apriori algorithm and recover the control flows among the services by analyzing the order of service invocation events in the execution log. A case study shows that our approach can effectively detect service composition patterns.","1550-4441","978-1-4244-8637-3","10.1109/WSE.2010.5623568","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5623568","component;web service;composition;pattern mining;log","Web services;Runtime;Parallel processing;Merging;Organizations;Process control","data mining;software architecture;Web services","Web service composition pattern mining;execution log;service oriented application","","4","1","26","","9 Nov 2010","","","IEEE","IEEE Conferences"
"MobiLogLeak: A Preliminary Study on Data Leakage Caused by Poor Logging Practices","R. Zhou; M. Hamdaqa; H. Cai; A. Hamou-Lhadj","Concordia University,Dept. Electrical and Computer Engineering,Montreal,Canada; School of Computer Science, Reykjavik University,Iceland; School of Electrical Engineering and Computer Science, Washington State University,Pullman,USA; Concordia University,Dept. Electrical and Computer Engineering,Montreal,Canada","2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER)","2 Apr 2020","2020","","","577","581","Logging is an essential software practice that is used by developers to debug, diagnose and audit software systems. Despite the advantages of logging, poor logging practices can potentially leak sensitive data. The problem of data leakage is more severe in applications that run on mobile devices, since these devices carry sensitive identification information ranging from physical device identifiers (e.g., IMEI MAC address) to communications network identifiers (e.g., SIM, IP, Bluetooth ID), and application-specific identifiers related to the location and the users' accounts. This preliminary study explores the impact of logging practices on data leakage of such sensitive information. Particularly, we want to investigate whether log-related statements inserted into an application code could lead to data leakage. While studying logging practices in mobile applications is an active research area, to our knowledge, this is the first study that explores the interplay between logging and security in the context of mobile applications for Android. We propose an approach called MobiLogLeak, an approach that identifies log statements in deployed apps that leak sensitive data. MobiLogLeak relies on taint flow analysis. Among 5,000 Android apps that we studied, we found that 200 apps leak sensitive data through logging.","1534-5351","978-1-7281-5143-4","10.1109/SANER48275.2020.9054831","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9054831","Taint Flow Analysis;Mobile Applications;Data Leakage;Logging Practices","","mobile computing;program diagnostics;security of data","MobiLogLeak;data leakage;software practice;software systems;sensitive data;mobile devices;sensitive identification information;physical device identifiers;communications network identifiers;application-specific identifiers;log-related statements;application code;mobile applications;log statements;logging practices;security;taint flow analysis;Android apps","","1","","15","","2 Apr 2020","","","IEEE","IEEE Conferences"
"Framework for Preprocessing and Feature Extraction from Weblogs for Identification of HTTP Flood Request Attacks","D. S. Sisodia; N. Verma","National Institute of Technology Raipur,Department of Computer Science and Engineering,Raipur,India; National Institute of Technology Raipur,Project Fellow, CCOST Sponsored MRP,Raipur,India","2018 International Conference on Advanced Computation and Telecommunication (ICACAT)","19 Dec 2019","2018","","","1","4","The HTTP flood attacks are carried out through enormous HTTP requests generated by automated software agents within a short period. The application layer is more vulnerable to HTTP flood attacks and exhausted computing and communication resources of the web server to disrupt the different web services. All HTTP requests are stored at the server as a web log file. However, malicious automated software agents camouflage their behavior on the web server logs and pose a great challenge to detect their HTTP requests. It is assumed that navigational behavior of actual visitors and automated software agents are fundamentally different. In this paper, a framework for weblog preprocessing and extracting various predefined features from raw web server logs is implemented. The most effective features are identified which are potentially useful in differentiating legitimate users and automated software agents. The sessionized HTTP feature vectors are also labeled as an actual visitor or possible web robots. The experiments are performed on raw weblogs of a commercial web portal.","","978-1-5386-5367-8","10.1109/ICACAT.2018.8933587","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8933587","DDoS;HTTP flood Attacks;Web robots;Web server logs;Preprocessing;Feature extraction.","Feature extraction;Bot (Internet);Web servers;Robots;Hidden Markov models;Software agents;Crawlers","computer network security;feature extraction;file servers;hypermedia;Internet;software agents;transport protocols","HTTP flood request attacks;Web services;Web log file;malicious automated software agents;weblog preprocessing;Web server logs;HTTP feature vectors","","","","26","","19 Dec 2019","","","IEEE","IEEE Conferences"
"Reliability feedback through system log analysis","K. Vinod; P. Pandit; M. Ramachandra","Philips Electronics India Ltd., Bengaluru - 560045, India; Philips Electronics India Ltd., Bengaluru - 560045, India; Philips Electronics India Ltd., Bengaluru - 560045, India","2013 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)","19 Dec 2013","2013","","","82","83","The healthcare industry has taken a significant pie in the information technology where a great progress is being shown in analyzing the research and development data collected over the years. Yet, most of the potential for value creation is still unclaimed. [1] In a typical healthcare system, the actual usage of the system can be determined when one transitions from the monitoring or a limited release phase of the project to the volume deployment mode. An early, if not a continuous feedback, can be ensured when the systems are usually beta tested at the selected sites. This is where we bring in the system log file analysis to play a major role in determining the reliability of the deployed system to receive an incessant and established feedback.","","978-1-4799-2552-0","10.1109/ISSREW.2013.6688877","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6688877","","Reliability;Computer crashes;Measurement;Medical services;Industries;Monitoring;Magnetic resonance imaging","data analysis;feedback;file organisation;health care;research and development;software reliability;system monitoring;systems analysis","reliability feedback;system log file analysis;healthcare industry;information technology;research and development data analysis;volume deployment mode","","","","3","","19 Dec 2013","","","IEEE","IEEE Conferences"
"Context-based analytics - establishing explicit links between runtime traces and source code","J. Cito; F. Oliveira; P. Leitner; P. Nagpurkar; H. C. Gall","Dept. of Inf., Univ. of Zurich, Zurich, Switzerland; IBM T.J. Watson Res. Center, Yorktown Heights, NY, USA; Dept. of Inf., Univ. of Zurich, Zurich, Switzerland; IBM T.J. Watson Res. Center, Yorktown Heights, NY, USA; Dept. of Inf., Univ. of Zurich, Zurich, Switzerland","2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track (ICSE-SEIP)","24 Aug 2017","2017","","","193","202","Diagnosing problems in large-scale, distributed applications running in cloud environments requires investigating different sources of information to reason about application state at any given time. Typical sources of information available to developers and operators include log statements and other runtime information collected by monitors, such as application and system metrics. Just as importantly, developers rely on information related to changes to the source code and configuration files (program code) when troubleshooting. This information is generally scattered, and it is up to the troubleshooter to inspect multiple implicitly-connected fragments thereof. Currently, different tools need to be used in conjunction, e.g., log aggregation tools, source-code management tools, and runtime-metric dash boards, each requiring different data sources and workflows. Not surprisingly, diagnosing problems is a difficult proposition. In this paper, we propose Context-Based Analytics, an approach that makes the links between runtime information and program-code fragments explicit by constructing a graph based on an application-context model. Implicit connections between information fragments are explicitly represented as edges in the graph. We designed a framework for expressing application-context models and implemented a prototype. Further, we instantiated our prototype framework with an application-context model for two real cloud applications, one from IBM and another from a major telecommunications provider. We applied context-based analytics to diagnose two issues taken from the issue tracker of the IBM application and found that our approach reduced the effort of diagnosing these issues. In particular, context-based analytics decreased the number of required analysis steps by 48% and the number of needed inspected traces by 40% on average as compared to a standard diagnosis approach.","","978-1-5386-2717-4","10.1109/ICSE-SEIP.2017.1","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7965443","DevOps;Software Analytics;Runtime Information","Runtime;Context;Measurement;Context modeling;Tools;Software;Time series analysis","cloud computing;software engineering","context-based analytics;runtime traces;source code;cloud environments;program-code fragments;application-context models","","1","","32","","24 Aug 2017","","","IEEE","IEEE Conferences"
"Automatic server role identification for cloud infrastructure construction","S. Kitajima; T. Uchiumi; S. Kikuchi; Y. Matsumoto","System Management Lab., System Software Laboratories, FUJITSU LABORATORIES LTD., 4-1-1 Kamikodanaka, Nakahara, Kawasaki, Kanagawa 211-8588, Japan; System Management Lab., System Software Laboratories, FUJITSU LABORATORIES LTD., 4-1-1 Kamikodanaka, Nakahara, Kawasaki, Kanagawa 211-8588, Japan; System Management Lab., System Software Laboratories, FUJITSU LABORATORIES LTD., 4-1-1 Kamikodanaka, Nakahara, Kawasaki, Kanagawa 211-8588, Japan; System Management Lab., System Software Laboratories, FUJITSU LABORATORIES LTD., 4-1-1 Kamikodanaka, Nakahara, Kawasaki, Kanagawa 211-8588, Japan","2013 IEEE 2nd International Conference on Cloud Networking (CloudNet)","16 Jan 2014","2013","","","147","155","The recent progress in computer performance and the development of virtualization technologies has led to the prevalence of cloud computing. Data center providers providing public cloud services have to install additional resources and infrastructures continuously to keep up with the increasing demands from cloud users. Since the newly installed infrastructure (e.g. servers) usually have similar structure as the existing infrastructure, the configuration settings for the existing ones can be copied and used for the new one. One of the exceptions is network setting (e.g. IP address) which must be customized for each infrastructure. However, the customization requires manual configuration, which can cause misconfigurations, resulting in communication failures in the new infrastructure. One of the promising approaches to identify the misconfigurations is to detect the differences between the communication logs recorded in the existing infrastructure and the new infrastructure being developed. In order to execute this approach, we need to identify a pair of servers that play the same role in the existing and new infrastructure so that we can verify whether or not the same functions are working properly in both of these infrastructures. In this paper, we propose a method that automatically identifies the pair of servers playing the same role by detecting the common communication patterns observed in both infrastructures. We evaluated our method in actual cloud infrastructure and confirmed that it identified 94.1% of corresponding pairs of servers correctly.","","978-1-4799-0568-3","10.1109/CloudNet.2013.6710569","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6710569","Cloud computing;data center;misconfiguration detection;communication logs","Servers;Ports (Computers);Cloud computing;IP networks;Conferences;Laboratories;Electronic mail","cloud computing;computer centres;file servers;virtualisation","automatic server role identification;cloud infrastructure construction;computer performance;virtualization technologies;cloud computing;data center;public cloud services;cloud users;configuration settings;network setting exceptions;manual configuration;communication logs;communication patterns","","","1","11","","16 Jan 2014","","","IEEE","IEEE Conferences"
"A Black-Box Approach to Latency and Throughput Analysis","D. Brahneborg; W. Afzal; A. Čaušević","Infoflex Connect AB, Stockholm, Sweden; Software Testing Lab., Malardalen Univ., Vasteras, Sweden; Software Testing Lab., Malardalen Univ., Vasteras, Sweden","2017 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)","10 Aug 2017","2017","","","603","604","To enable fast and reliable delivery of mobile text messages (SMS), special bidirectional protocols are often used. Measuring the achieved throughput and involved latency is however non-trivial, due to the complexity of these protocols. Modifying an existing system would incur too much of a risk, so instead a new tool was created to analyse the log files containing information about this traffic in a black-box fashion. When the produced raw data was converted into graphs, they gave new insights into the behaviour of both the protocols and the remote systems involved.","","978-1-5386-2072-4","10.1109/QRS-C.2017.113","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8004393","log file analysis;latency;black-box","Throughput;Electromyography;Protocols;Tools;Mobile communication;Software reliability","data analysis;electronic messaging;mobile computing;telecommunication traffic","mobile text messages;black-box Approach;graphs;remote systems;log file analysis;latency analysis;throughput analysis;SMS","","","","1","","10 Aug 2017","","","IEEE","IEEE Conferences"
"An Efficient Forensics Architecture in Software-Defined Networking-IoT Using Blockchain Technology","M. Pourvahab; G. Ekbatanifard","Department of Computer Engineering, Rasht Branch, Islamic Azad University, Rasht, Iran; Department of Computer Engineering, Lahijan Branch, Islamic Azad University, Lahijan, Iran","IEEE Access","5 Aug 2019","2019","7","","99573","99588","A Potential solution for solving forensic is the use of blockchain in software-defined networking (SDN). The blockchain is a distributed peer-to-peer network that can be utilized on SDN-based Internet of Things (IoT) environments for security provisioning. Hence, to meet some challenges in digital forensics such as data integrity, evidence deletion or alteration, blockchain is used. However, some problems such as poor attack detection and slow processing existed in previous works. To address these issues, an efficient forensics architecture is proposed in SDN-IoT that establishes the Chain of Custody (CoC) in blockchain technology. The proposed SDN-based IoT architecture is initiated with flow table rules on switches for the three different traffics Voice over Internet Protocol (VoIP), File Transfer Protocol (FTP), and Hyper Text Transfer Protocol (HTTP). In this work, overloaded switches migrate the packets to nearby switches to balance the packet flow. The packets disobeying flow rules will be discarded by switches. The blockchain-based distributed controller in this forensic architecture is designed to use the Linear Homomorphic Signature (LHS) algorithm for validating users. Each controller is fed with a classifier that uses the Neuro Multi-fuzzy to classify malicious packets based on packet features. The logs of events are used and stored on the blockchain in the proposed SDN-IoT architecture. We evaluated the performance of our forensic architecture and compared it to the existing model using various performance measures. Our evaluation results demonstrate performance improvement by reducing delay, response time and processing time, increasing throughput, accuracy, and security parameters.","2169-3536","","10.1109/ACCESS.2019.2930345","MehranNet ISP (Internet Service Provider), Langarud, Guilan, Iran; Islamic Azad University; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8768376","Software-defined networking;the Internet of Things;forensics;security;blockchain","Blockchain;Security;Computer architecture;Forensics;Internet of Things;Peer-to-peer computing;Software defined networking","computer network security;digital signatures;Internet of Things;Internet telephony;peer-to-peer computing;software defined networking;telecommunication traffic;transport protocols","software-defined networking-IoT;blockchain technology;peer-to-peer network;digital forensics;evidence deletion;SDN-based IoT architecture;flow table rules;Internet Protocol;File Transfer Protocol;Hyper Text Transfer Protocol;nearby switches;packet flow;flow rules;blockchain-based distributed controller;forensic architecture;malicious packets;packet features;SDN-based Internet of Things environments","","10","","52","CCBY","22 Jul 2019","","","IEEE","IEEE Journals"
"LogTransfer: Cross-System Log Anomaly Detection for Software Systems with Transfer Learning","R. Chen; S. Zhang; D. Li; Y. Zhang; F. Guo; W. Meng; D. Pei; Y. Zhang; X. Chen; Y. Liu",Nankai University; Nankai University; Nankai University; Nankai University; Nankai University; Tsinghua University; Tsinghua University; Nankai University; Nankai University; Nankai University,"2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE)","11 Nov 2020","2020","","","37","47","System logs, which describe a variety of events of software systems, are becoming increasingly popular for anomaly detection. However, for a large software system, current unsupervised learning-based methods are suffering from low accuracy due to the high diversity of logs, while the supervised learning methods are nearly infeasible to be used in practice because it is time-consuming and labor-intensive to obtain sufficient labels for different types of software systems. In this paper, we propose a novel framework, LogTransfer, which applies transfer learning to transfer the anomalous knowledge of one type of software system (source system) to another (target system). We represent every template using Glove, which considers both global word co-occurrence and local context information, to address the challenge that different types of software systems are different in log syntax while the semantics of logs should be reserved. We apply an LSTM network to extract the sequential patterns of logs, and propose a novel transfer learning method sharing fully connected networks between source and target systems, to minimize the impact of noises in anomalous log sequences. Extensive experiments have been performed on switch logs of different vendors collected from a top global cloud service provider. LogTransfer achieves an averaged 0.84 F1-score and outperforms the state-of-the-art supervised and unsupervised log-based anomaly detection methods, which are consistent with the experiments conducted on the public HDFS and Hadoop application datasets.","2332-6549","978-1-7281-9870-5","10.1109/ISSRE5003.2020.00013","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9251092","Transfer learning;system log;anomaly detection;word embedding;LSTM","Supervised learning;Semantics;Switches;Syntactics;Software systems;Software reliability;Anomaly detection","cloud computing;data handling;security of data;unsupervised learning","cross-system;software system;system logs;unsupervised learning-based methods;supervised learning methods;source system;target system;transfer learning method;unsupervised log-based anomaly detection methods;temperature 0.84 F","","","","36","","11 Nov 2020","","","IEEE","IEEE Conferences"
"HireBuild: An Automatic Approach to History-Driven Repair of Build Scripts","F. Hassan; X. Wang",NA; NA,"2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)","2 Sep 2018","2018","","","1078","1089","Advancements in software build tools such as Maven reduce build management effort, but developers still need specialized knowledge and long time to maintain build scripts and resolve build failures. More recent build tools such as Gradle give developers greater extent of customization flexibility, but can be even more difficult to maintain. According to the TravisTorrent dataset of open-source software continuous integration, 22% of code commits include changes in build script files to maintain build scripts or to resolve build failures. Automated program repair techniques have great potential to reduce cost of resolving software failures, but the existing techniques mostly focus on repairing source code so that they cannot directly help resolving software build failures. To address this limitation, we propose HireBuild: History-Driven Repair of Build Scripts, the first approach to automatic patch generation for build scripts, using fix patterns automatically generated from existing build script fixes and recommending fix patterns based on build log similarity. From TravisTorrent dataset, we extracted 175 build failures and their corresponding fixes which revise Gradle build scripts. Among these 175 build failures, we used the 135 earlier build fixes for automatic fix-pattern generation and the more recent 40 build failures (fixes) for evaluation of our approach. Our experiment shows that our approach can fix 11 of 24 reproducible build failures, or 45% of the reproducible build failures, within comparable time of manual fixes.","1558-1225","978-1-4503-5638-1","10.1145/3180155.3180181","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8453189","Patch Generation;Software Build Scripts;Build Logs","Maintenance engineering;Software;Tools;Task analysis;Computer bugs;Data mining;Software engineering","software fault tolerance;software maintenance;software tools;source code (software)","software build failures;History-Driven Repair;build log similarity;Gradle build scripts;software build tools;build script files;fix patterns;reproducible build failures;HireBuild;Automated program repair techniques;source code;automatic patch generation;build script fixes","","9","","51","","2 Sep 2018","","","IEEE","IEEE Conferences"
"Detecting APT Attacks Against Active Directory Using Machine Leaning","W. Matsuda; M. Fujimoto; T. Mitsunaga","The University of Tokyo, Tokyo, Japan; The University of Tokyo, Tokyo, Japan; The University of Tokyo, Tokyo, Japan","2018 IEEE Conference on Application, Information and Network Security (AINS)","3 Feb 2019","2018","","","60","65","In Advanced Persistent Threat (APT) attacks, attackers who can intrude into an organization network tend to stay inside the network or repeat intrusion multiple times until they are able to accomplish their goals. When Active Directory(AD), a centralization management system for Windows computers, is in place, attackers try to disguise themselves as users of legitimate Domain Administrator accounts, which is the highest privileged account of the AD environment. Activities on the Windows system are recorded in the built-in Windows activity logging system called the Event logs and is commonly used for investigation of attacks. However, if attackers leverage legitimate accounts or built-in Windows tools in order to avoid detection, it is quite difficult to detect attacks from Event logs since attackers' activities are recorded as activities of legitimate administrator accounts. Although there are various antivirus software, detecting such a sophisticated attack is often very difficult. In this research, we focus on processing attack activity data recorded in the Event logs, and propose a new method based on outlier detection and machine learning for detecting attacks that utilize legitimate accounts. We achieved a high precision rate even if legitimate Domain Administrator accounts are leveraged in attacks.","","978-1-5386-6925-9","10.1109/AINS.2018.8631486","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8631486","machine learning;outlier detection;unsupervised learning;Active Directory;Event log;APT;Golden Ticket","Authentication;Microsoft Windows;Computers;Machine learning;Tools;Anomaly detection","computer viruses;learning (artificial intelligence);Microsoft Windows (operating systems);security of data","legitimate administrator accounts;APT attacks;advanced persistent threat;active directory;windows activity logging system;domain administrator;machine leaning;organization network;event logs;antivirus software","","1","","10","","3 Feb 2019","","","IEEE","IEEE Conferences"
"Performance Model Derivation of Operational Systems through Log Analysis","M. Awad; D. A. Menascé","Comput. Sci. Dept., George Mason Univ., Fairfax, VA, USA; Comput. Sci. Dept., George Mason Univ., Fairfax, VA, USA","2016 IEEE 24th International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS)","8 Dec 2016","2016","","","159","168","Manually developing analytic performance models of operational systems can be challenging, time consuming, and costly. This paper describes a method that uses system logs and configuration files to automatically derive analytic performance models of operational systems. The method described here automatically determines: (1) the system software servers, (2) the system devices, (3) the deployment of software servers to devices, (4) the communication patterns between software servers for each external use-case, and (5) the probability at which interactions between servers occur. The method was implemented and validated on a multi-tier system. The results showed that the method is capable of deriving the workload model and system model by parsing the system configuration files and log files and inferring user-system interaction patterns and client-server interaction diagrams.","2375-0227","978-1-5090-3432-1","10.1109/MASCOTS.2016.41","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7774577","automatic performance model derivation;queuing networks;client-server interaction diagrams;software queuing networks;log and configuration files","Analytical models;Databases;Computational modeling;Web servers;Load modeling","client-server systems;file servers;system monitoring","performance model derivation;operational systems;log analysis;analytic performance models;system logs;system configuration files;system software servers;multitier system;workload model;system model;user-system interaction patterns;client-server interaction diagrams","","2","","25","","8 Dec 2016","","","IEEE","IEEE Conferences"
"Detection and confirmation of web robot requests for cleaning the voluminous web log data","T. H. Sardar; Z. Ansari","Department of Information Science and Engineering P.A. College of Engineering, Mangalore Karnataka, India; Department of Computer Science and Engineering P.A. College of Engineering, Mangalore Karnataka, India","2014 International Conference on the IMpact of E-Technology on US (IMPETUS)","20 Mar 2014","2014","","","13","19","Web robots are software applications that run automated tasks over the internet. They traverse the hyperlink structure of the World Wide Web so that they can retrieve information. There are many reasons to distinguish web robot requests and user requests. Some tasks of web robots can be harmful to the web. Firstly, Web robots are employed for assemble business intelligence at e-commerce sites. In such a state of affairs, the e-commerce site may need to detect robots. Secondly, many e-commerce sites carry out Web traffic scrutiny to deduce the way their customers have accessed the site. Unfortunately, such scrutiny can be erroneous by the presence of Web robots. Thirdly, Web robots often consume considerable network bandwidth and server resources at the expense of other users. A web log file is a web server file automatically created and maintained by a web server to check the activity performed by it. It maintains a history of page requests on its site. In this paper we have used four methods together to detect and finally confirm requests as a robot request. Experiments have been performed on the log file generated from the server of an operational web site named vtulife.com which contains data of march-20l3. In our research results o.f web robot detection using various techniques have been compared and an integrated approach is proposed for the confirmation of the robot request.","","978-93-329-0264-0","10.1109/IMPETUS.2014.6775871","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6775871","web usage mining;web robot detection;web log file","Robots;IP networks;Web servers;Web sites;Browsers;Search engines","competitive intelligence;electronic commerce;file servers;information retrieval;Internet;software agents;telecommunication traffic","Web robot request detection;Web robot request confirmation;voluminous Web log data cleaning;Internet;hyperlink structure;World Wide Web;information retrieval;business intelligence;e-commerce sites;Web traffic scrutiny;Web log file;Web server file;vtulife.com","","6","1","21","","20 Mar 2014","","","IEEE","IEEE Conferences"
"A New Process Mining Algorithm Based on Event Type","D. Wang; J. Ge; H. Hu; B. Luo","State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China","2011 IEEE Ninth International Conference on Dependable, Autonomic and Secure Computing","2 Jan 2012","2011","","","1144","1151","The aim of process mining is to rediscover the process model from the event log which is recorded by the information system. Although the omnipresence of the event logs in information system, rarely part of them are considered to analyze the processes. In this paper, we present a new mining algorithm based on the event type we defined. This algorithm not only can detect all of the SWF-nets and short-loops, but also can directly detect the implicit dependency. Because we can obtain more task information from the event log, we can deal with a wider subclass of WF-nets with the algorithm we have presented.","","978-1-4673-0006-3","10.1109/DASC.2011.186","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6118862","Process mining;Workflow;Post-tasks;Petri net","Parallel processing;Educational institutions;Computational modeling;Software;Information systems;Synchronization;Business","data mining;information systems","process mining algorithm;event type;information system","","4","","17","","2 Jan 2012","","","IEEE","IEEE Conferences"
"System anomaly detection in distributed systems through MapReduce-Based log analysis","Yan Liu; Ning Cao; Wei Pan; Guangwei Qiao","Ideal Institute of Information and Technology, Changchun, China; College of Information Science and Engineering, Northeastern University, Shenyang, China; Ideal Institute of Information and Technology, Changchun, China; Ideal Institute of Information and Technology, Changchun, China","2010 3rd International Conference on Advanced Computer Theory and Engineering(ICACTE)","20 Sep 2010","2010","6","","V6-410","V6-413","System anomaly detection is very important for development, maintenance and performance refinement in large scale distributed systems. It's a good way to obtain the troubleshooting and problem diagnosis by analyzing system logs produced by distributed systems. However, due to the increasing scale and complexity of distributed systems, the size of logs must be very large. Thus, it's inefficient for common methods to analyze system logs on single node. Therefore, there is a great demand to adopt a distributed method for anomaly detection techniques based on log analysis. In this paper, a MapReduce-Based Framework is implemented to analyze the distributed log for detecting anomaly. The framework is built on top of Hadoop, an open source distributed file system and MapReduce implementation. We first make use of Random Access File to realize an incremental way for aggregating system logs from each node of the monitored cluster, and collect them to the analysis cluster. Then, we apply the K-means clustering algorithm to integrate the collected logs. After that, we implement a MapReduce-Based algorithm to parser these clustered log files. Furthermore, in order to make the best use of this collected data, a flexible and powerful way is utilized to display monitoring and analysis results. Thus, we can monitor system status of large distributed cluster and detect its anomalies.","2154-7505","978-1-4244-6542-2","10.1109/ICACTE.2010.5579173","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5579173","MapReduce;log analysis;anomaly detection;distributed system;K-means","Monitoring;Data mining;Distributed databases;Clustering algorithms;Programming;Graphical user interfaces;Algorithm design and analysis","distributed processing;pattern clustering;public domain software;security of data;software maintenance;system monitoring","system anomaly detection;open source distributed file system;MapReduce-based log analysis;problem diagnosis;Hadoop;random access file;K-means clustering algorithm","","","","21","","20 Sep 2010","","","IEEE","IEEE Conferences"
"Framework for mining event correlations and time lags in large event sequences","M. Zöller; M. Baum; M. F. Huber","Institute of Computer Science, University of Goettingen, Germany; Institute of Computer Science, University of Goettingen, Germany; USU Software AG, Rüppurrer Str. 1, Karlsruhe, Germany","2017 IEEE 15th International Conference on Industrial Informatics (INDIN)","13 Nov 2017","2017","","","805","810","Event correlation is the task of detecting dependencies between events in event sequences, e.g., for predictive maintenance based on log-files. In this work, a new data-driven, generic framework for event correlation is presented. First, we use a fast preliminary test statistic to determine candidate event type pairs. Next, the precise distribution of the time lag between those pairs is calculated. For this purpose, a new efficient iterative method is developed that aligns two event sequences and finds the optimal event assignments. In our experiments, the proposed method is orders of magnitude faster than state-of-the-art methods but always yields similar (or even better) results.","2378-363X","978-1-5386-0837-1","10.1109/INDIN.2017.8104876","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8104876","","Correlation;Ice;Iterative closest point algorithm;Monitoring;Optimization","data mining;iterative methods;maintenance engineering;statistical analysis","event correlation mining;iterative method;predictive maintenance;log-files;optimal event assignments;event sequences;time lag;candidate event type pairs","","2","","17","","13 Nov 2017","","","IEEE","IEEE Conferences"
"Software Networks of Java Class and Application in Fault Localization","C. Li; L. Liu; X. Li",NA; NA; NA,"2012 Second International Conference on Intelligent System Design and Engineering Application","3 Apr 2012","2012","","","1117","1120","Complex networks are backbones of complex systems. A lot of empirical analysis demonstrates that software is kind of artificial complex systems that expose the small-world effects and follow scale-free degree distribution. Here analyzed is Java class complex networks construction in binary file with BCEL (Byte Code Engineering Library) of Apache Jakarta Project. This method needn't source code to generate networks, so it can extract all java software's inner structure to assist programmer to understand software macroscopic features. Existing lectures almost verified variety of software networks are complex networks, and some investigators have concluded software networks cannot yet produce factual instruction in software engineering. Here we utilize log information in Java networks to diagnose software fault and exception. Experiments show that software networks can not only visualize the software structure, but also really instruct software fault localization.","","978-1-4577-2120-5","10.1109/ISdea.2012.403","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6173401","Complex Networks;Software Networks;Fault Localization;Java","Java;Complex networks;Layout;Software algorithms;Software engineering;Software systems","complex networks;Java;large-scale systems;software engineering;software fault tolerance","fault localization;Java class;empirical analysis;artificial complex systems;scale free degree distribution;complex networks construction;BCEL;byte code engineering library;Apache Jakarta Project;Java software;software macroscopic features;software networks;software engineering;software structure;software fault localization","","2","","8","","3 Apr 2012","","","IEEE","IEEE Conferences"
"Runtime Prediction of Failure Modes from System Error Logs","A. Shalan; M. Zulkernine","Sch. of Comput., Queen's Univ., Kingston, ON, Canada; Sch. of Comput., Queen's Univ., Kingston, ON, Canada","2013 18th International Conference on Engineering of Complex Computer Systems","19 Sep 2013","2013","","","232","241","Predicting potential failure occurrences during runtime is important to achieve system resilience and avoid hazardous consequences of failures. Existing failure prediction techniques in software systems involve forecasting failure counts, effects, and occurrences. Most of these techniques predict failures that may occur in future runtime intervals and only few techniques predict them at runtime. However, they do not estimate the failure modes and they require extensive instrumentation of source code. In this paper, we provide an approach for predicting failure occurrences and modes during system runtime. Our methodology utilizes system error log records to craft runtime error-spread signature. Using system error log history, we determine a predictive function (estimator) for each failure mode based on these signatures. This estimator can be used to predict a failure mode eventuality measure (a probability of failure mode occurrence) from system error log during system runtime. An experimental evaluation using PostgreSQL opensource database is provided. Our results show high accuracy of failure occurrence and mode predictions.","","978-0-7695-5007-7","10.1109/ICECCS.2013.41","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6601828","software reliability;failure prediction;failure mode;regression analysis;runtime error log","Runtime;History;Equations;Software systems;Accuracy;Radiation detectors;Mathematical model","public domain software;software reliability;SQL","runtime prediction;failure modes;system error logs;failure occurrences;failure prediction techniques;software systems;runtime intervals;system error log records;runtime error-spread signature;PostgreSQL opensource database;software reliability","","2","","33","","19 Sep 2013","","","IEEE","IEEE Conferences"
"The software development process methodology of resource-based access control","J. Zhang","Information Engineering Academe, Qingdao University, Shandong Province, China","2010 The 2nd International Conference on Computer and Automation Engineering (ICCAE)","19 Apr 2010","2010","4","","111","117","This paper presents the software development process methodology of resource-based access control in which artifacts and milestones in RUP software development process are abstracted as the abstract documents which can be associated with each other as the same things to be managed with public attributes. Productions and modifications as operations on artifacts and milestones are abstracted as editing operations. The process can equip the abstract document with the abstract role, and equip the role with personnel and man-hour, and then record actual man-hour for tracking and controlling the cost and quality. Personnel associated with the abstract document can easily find own tasks after logging in. Such abstract processes, making the entire software development process to manage the abstract document as the core, we can greatly simplify the entire software development process. The costs and quality is tracked and controlled, it is easy to find and locate defects as a basis of the plan, thus to adjust and improve the overall software development process in real time, and continuously improve the standards and standardization level of software technology.","","978-1-4244-5586-7","10.1109/ICCAE.2010.5451762","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5451762","Rational Unified Process;software development process;access control;Agile Software Development;Extreme Programming;UML;use cases;iterative;phases;milestones, software quality;software cost;quality factor;defect tracking;traceability;abstract classes;abstract documentation;the role of abstraction","Programming;Access control;Software development management;Personnel;Costs;Resource management;Production;Software standards;Standards development;Standardization","authorisation;software cost estimation;software prototyping;software quality","software development process methodology;resource-based access control;RUP software development process;rational unified process;agile software development;software quality;software cost","","","","20","","19 Apr 2010","","","IEEE","IEEE Conferences"
"Examining Case Management Demand Using Event Log Complexity Metrics","M. Benner-Wickner; M. Book; T. Brückmann; V. Gruhn","paluno - The Ruhr Inst. for Software Technol., Univ. of Duisburg-Essen, Essen, Germany; paluno - The Ruhr Inst. for Software Technol., Univ. of Duisburg-Essen, Essen, Germany; paluno - The Ruhr Inst. for Software Technol., Univ. of Duisburg-Essen, Essen, Germany; paluno - The Ruhr Inst. for Software Technol., Univ. of Duisburg-Essen, Essen, Germany","2014 IEEE 18th International Enterprise Distributed Object Computing Conference Workshops and Demonstrations","6 Dec 2014","2014","","","108","115","One of the main goals of process mining is to automatically discover meaningful process models from event logs. Since these logs are the essential source of information for discovery algorithms, their quality is of high importance. In recent years, many studies on the quality of resulting process models have been conducted. However, the analysis of event log quality prior to the generation of models has been neglected. For example, yet there are no metrics which can measure the degree of event log quality that is needed so that discovery algorithms can be applied. Especially in the context of case management (CM) where processes are less structured, complex event logs reduce the effectiveness of the process discovery. In order to avoid mining impractical ""spaghetti processes"", it would be convenient to measure the event log complexity prior to discovery steps. In this paper, we provide our research results concerning the design and applicability of such metrics. First of all, they shall help to indicate whether the event log quality is sufficient for traditional process discovery. In case of very poor quality, they indicate the demand for more agile techniques (e.g. adaptive CM or agenda-driven CM).","2325-6605","978-1-4799-5467-4","10.1109/EDOCW.2014.25","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6975349","metrics;process mining;case management","Complexity theory;Data mining;Process control;Context;Noise;Length measurement","computational complexity;data mining","process mining;event log quality;discovery algorithms;case management;event log complexity metrics","","","","25","","6 Dec 2014","","","IEEE","IEEE Conferences"
"Process Cube for Software Defect Resolution","M. Gupta; A. Sureka","Indraprastha Inst. of Inf. Technol. - Delhi (IIITD), New Delhi, India; Indraprastha Inst. of Inf. Technol. - Delhi (IIITD), New Delhi, India","2014 21st Asia-Pacific Software Engineering Conference","23 Apr 2015","2014","1","","239","246","Online Analytical Processing (OLAP) cube is a multi-dimensional dataset used for analyzing data in a Data Warehouse (DW) for the purpose of extracting actionable intelligence. Process mining consists of analyzing event log data produced from Process Aware Information Systems (PAIS) for the purpose of discovering and improving business processes. Process cube is a concept which falls at the intersection of OLAP cube and process mining. Process cube facilitates process mining from multiple-dimensions and enables comparison of process mining results across various dimensions. We present an application of process cube to software defect resolution process to analyze and compare process data from a multi-dimensional perspective. We present a framework, a novel perspective to mine software repositories using process cube. Each cell of process cube is defined by metrics from multiple process mining perspectives like control flow, time, conformance and organizational perspective. We conduct a case-study on Google Chromium project data in which the software defect resolution process spans three software repositories: Issue Tracking System (ITS), Peer Code Review System (PCR) and Version Control System (VCS). We define process cube with 9 dimensions as issue report timestamp, priority, state, closed status, OS, component, bug type, reporter and owner. We define hierarchies along various dimensions and cluster members to handle sparsity. We apply OLAP cube operations such as slice, dice, roll-up and drill-down, and create materialized sub log for each cell. We demonstrate the solution approach by discovering process map and compare process mining results from Control Flow and Time perspective for Performance and Security issues.","1530-1362","978-1-4799-7426-9","10.1109/APSEC.2014.45","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7091316","Empirical Software Engineering;Issue Tracking System;Mining Software Repositories;OLAP;Peer Code Review System;Process Cube;Process Mining;Version Control System","Data mining;Security;Software;Process control;Google;Measurement;Chromium","configuration management;data mining;data warehouses;program debugging;software engineering;software maintenance","online analytical processing cube;multidimensional dataset;data warehouse;DW;actionable intelligence extraction;process mining;event log data analysis;process aware information systems;PAIS;software defect resolution process;software repositories;control flow;organizational perspective;conformance perspective;Google Chromium project data;issue tracking system;ITS;peer code review system;PCR;version control system;VCS;issue report timestamp;issue report priority;issue report state;issue report closed status;issue report OS;issue report component;bug type;reporter;owner;OLAP cube operations;security issues;performance issues","","4","","16","","23 Apr 2015","","","IEEE","IEEE Conferences"
"A Survey on Log Anomaly Detection using Deep Learning","R. B. Yadav; P. S. Kumar; S. V. Dhavale","Defence Institute of Advanced Technology,Department of Computer Science and Engineering,Pune,India; Defence Institute of Advanced Technology,Department of Computer Science and Engineering,Pune,India; Defence Institute of Advanced Technology,Department of Computer Science and Engineering,Pune,India","2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)","15 Sep 2020","2020","","","1215","1220","Logs generated from the security systems, network devices, servers, and various software applications are one of the ways to record the operational happening of the equipment or software. These logs are assets for extracting meaningful information related to system behavior. Increasing usage of computer devices and the evolution of software systems can be considered as one of triggering acts for the concentration on the analysis of logs. Also, considering the massive volume of unstructured data, it raises the requirement for automatic analysis of these logs. The log analysis is helpful for understanding system behavior, malfunctioning detection, security scanning, and failure prediction. Machine learning(ML) and Deep Learning (DL) methods have been proved potent tools for data classification problems and have been applied to various fields of research. The purpose of this survey is to review recent research on log anomaly detection using Deep Neural Networks. Survey also presents the brief of log parsing approaches, types of datasets used for log analysis, and various concepts proposed for Log Anomaly detection.","","978-1-7281-7016-9","10.1109/ICRITO48877.2020.9197818","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9197818","Log Anomaly;Deep Learning;LSTM;CNN;Autoencoder;Log Parsing","Anomaly detection;Machine learning;Feature extraction;Software;Support vector machines;Security;Principal component analysis","learning (artificial intelligence);neural nets;pattern classification;security of data;software fault tolerance;system monitoring","log anomaly detection;security systems;network devices;software applications;system behavior;computer devices;software systems;log analysis;deep learning;deep neural networks;malfunction detection;security scanning;failure prediction;machine learning;data classification;log parsing","","","","37","","15 Sep 2020","","","IEEE","IEEE Conferences"
"GenLog: Accurate Log Template Discovery for Stripped X86 Binaries","M. Zhang; Y. Zhao; Z. He","Dept. of Comput. Sci. & Technol., Tsinghua Univ., Beijing, China; Dept. of Comput. Sci. & Technol., Tsinghua Univ., Beijing, China; Res. Inst. of Tsinghua Univ. in Shenzhen, Shenzhen, China","2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC)","11 Sep 2017","2017","1","","337","346","Log analysis plays an important role for computer failure diagnosis. With the ever increasing size and complexity of logs, the task of analyzing logs has become cumbersome to carry out manually. For this reason, recent research has focused on automatic analysis techniques for large log files. However, log messages are texts with certain formats and it is very challenging for automatic analysis to understand the semantic meanings of log messages. The current state-of-the-art approaches depend on the quality of observed log messages or source code producing these log messages. In this paper, we propose a method GenLog that can extract log templates from stripped executables (neither source code nor debugging information need to be available). GenLog finds all log related functions in a binary through a combined bottom-up and top down slicing method, reconstructs the memory buffers where log messages were constructeStripped X86 Binaries d, and identifies components of log messages using data flow analysis and taint propagation analysis. GenLog can be used to analyze large binary code, and is suitable for commercial off-the-shelf (COTS) software or dynamic libraries. We evaluated GenLog on four X86 executables and one of them is Nginx. The experiments show that GenLog can identify the template for log messages in testing log files with a precision of 99.9%.","0730-3157","978-1-5386-0367-3","10.1109/COMPSAC.2017.137","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8029626","","Binary codes;Software;IP networks;Data mining;Reactive power;Semantics;Production","data flow analysis;program debugging;program slicing;program testing;software fault tolerance;source code (software)","GenLog;log analysis;log template discovery;stripped X86 binaries;computer failure diagnosis;source code;log templates extraction;bottom-up slicing method;top down slicing method;memory buffers reconstruction;binary code analysis;log files testing;data flow analysis;taint propagation analysis","","1","","40","","11 Sep 2017","","","IEEE","IEEE Conferences"
"ICAS: An inter-VM IDS Log Cloud Analysis System","S. Yang; W. Chen; Y. Wang","Free Software Laboratory of National Center for High-Performance Computing, Taichung, Taiwan, China; Free Software Laboratory of National Center for High-Performance Computing, Taichung, Taiwan, China; Free Software Laboratory of National Center for High-Performance Computing, Taichung, Taiwan, China","2011 IEEE International Conference on Cloud Computing and Intelligence Systems","13 Oct 2011","2011","","","285","289","Cloud computing can reduce mainframe management costs, so more and more users choose to build their own cloud hosting environment. In cloud computing, all the commands through the network connection, therefore, information security is particularly important. In this paper, we will explore the types of intrusion detection systems, and integration of these types, provided an effective and output reports, so system administrators can understand the attacks and damage quickly. With the popularity of cloud computing, intrusion detection system log files are also increasing rapidly, the effect is limited and inefficient by using the conventional analysis system. In this paper, we use Hadoop's MapReduce algorithm analysis of intrusion detection System log files, the experimental results also confirmed that the calculation speed can be increased by about 89%. For the system administrator, IDS Log Cloud Analysis System (called ICAS) can provide fast and high reliability of the system.","2376-595X","978-1-61284-204-2","10.1109/CCIS.2011.6045076","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6045076","Hadoop;IDS;MapReduce;Cloud Computing","Cloud computing;Intrusion detection;Computer architecture;File systems;Correlation;Monitoring","cloud computing;security of data;virtual machines","ICAS;interVM IDS log cloud analysis system;cloud computing;mainframe management cost reduction;cloud hosting environment;information security;intrusion detection system log files;conventional analysis system;Hadoop MapReduce algorithm analysis","","12","","23","","13 Oct 2011","","","IEEE","IEEE Conferences"
"DRAT: An Unobtrusive, Scalable Approach to Large Scale Software License Analysis","C. A. Mattmann; J. Oh; T. Palsulich; L. J. McGibbney; Y. Gil; V. Ratnakar","Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Comput. Sci. Dept., Univ. of Southern California, Los Angeles, CA, USA; USC Inf. Sci. Inst., Univ. of Southern California, Marina Del Rey, CA, USA","2015 30th IEEE/ACM International Conference on Automated Software Engineering Workshop (ASEW)","7 Mar 2016","2015","","","97","101","The Apache Release Audit Tool (RAT) performs software open source license auditing and checking, however RAT fails to successfully audit today's large code bases. Being a natural language processing (NLP) tool and a crawler, RAT marches through a code base, but uses rudimentary black lists and white lists to navigate source code repositories, and often does a poor job of identifying source code versus binary files. In addition RAT produces no incremental output and thus on code bases that themselves are ""Big Data"", RAT could run for e.g., a month and still not provide any status report. We introduce Distributed ""RAT"" or the Distributed Release Audit Tool (DRAT). DRAT overcomes RAT's limitations by leveraging: (1) Apache Tika to automatically detect and classify files in source code repositories and determine what is a binary file, what is source code, what are notes that need skipping, etc. (2) Apache Solr to interactively perform analytics on a code repository and to extract metadata using Apache Tika, and finally (3) Apache OODT to run RAT on per-MIME type (e.g., C/C++, Java, Javascript, etc.) and per configurable K-file sized chunks in a MapReduce workflow. Each Mapper task is an instance of RAT running on a K-file sized per Multipurpose Internet Mail Extensions (MIME) type chunk (split using Tika) and each mapper produces and incremental and intermediate log file, and where the Reducer aggregates the individual log files.","","978-1-4673-9775-9","10.1109/ASEW.2015.14","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7426645","DRAT;Open source;Software license auditing","Licenses;Metadata;Open source software;Java;Open systems;Government","meta data;pattern classification;software engineering;software reviews;source code (software)","Multipurpose Internet Mail Extensions;MapReduce workflow;per-MIME type;Apache OODT;metadata extraction;code repository analytics;Apache Solr;binary file;source code repositories;file detection;file classification;Apache Tika;DRAT;Distributed Release Audit Tool;large scale software license analysis","","","","12","","7 Mar 2016","","","IEEE","IEEE Conferences"
"Characterizing Direct Monitoring Techniques in Software Systems","M. Cinque; D. Cotroneo; R. D. Corte; A. Pecchia","Dipartimento di Ingegneria Elettrica e delle Tecnologie dell’Informazione, Università degli Studi di Napoli Federico II, Napoli, Italy; Dipartimento di Ingegneria Elettrica e delle Tecnologie dell’Informazione, Università degli Studi di Napoli Federico II, Napoli, Italy; Dipartimento di Ingegneria Elettrica e delle Tecnologie dell’Informazione, Università degli Studi di Napoli Federico II, Napoli, Italy; Dipartimento di Ingegneria Elettrica e delle Tecnologie dell’Informazione, Università degli Studi di Napoli Federico II, Napoli, Italy","IEEE Transactions on Reliability","29 Nov 2016","2016","65","4","1665","1681","Monitoring is a consolidated practice to characterize the dependability behavior of a software system. A variety of techniques, such as event logging and operating system probes, are currently used to generate monitoring data for troubleshooting and failure analysis. In spite of the importance of monitoring, whose role can be essential in critical software systems, there is a lack of studies addressing the assessment and the comparison of the techniques aiming to monitor the occurrence of failures during operations. This paper proposes a method to characterize the monitoring techniques implemented in a software system. The method is based on a fault injection approach and allows measuring 1) precision and recall of a monitoring technique and 2) the dissimilarity of the data it generates upon failures. The method has been used in two critical software systems implementing event logging, assertion checking, and source code instrumentation techniques. We analyzed a total of 3 844 failures. With respect to our data, we observed that the effectiveness of a technique is strongly affected by the system and type of failure, and that the combination of different techniques is potentially beneficial to increase the overall failure reporting ability. More important, our analysis revealed a number of practical implications to be taken into account when developing a monitoring technique.","1558-1721","","10.1109/TR.2016.2570564","NAPOLI FUTURA Start-up Project; Italian Ministry of Education; University and Research; MINIMINDS PON; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7493640","Assertion checking;code instrumentation;critical systems;event logging;monitoring;software faults","Monitoring;Software systems;Runtime;Measurement;Probes","software reliability;system recovery","characterizing direct monitoring techniques;software systems;dependability behavior;data monitoring;failure analysis;troubleshooting analysis;critical software systems;monitoring techniques;fault injection approach;two critical software systems;assertion checking;source code instrumentation techniques;event logging;failure reporting ability;software reliability","","10","","49","","16 Jun 2016","","","IEEE","IEEE Journals"
"A Qualitative Study of the Benefits and Costs of Logging from Developers' Perspectives","H. Li; W. Shang; B. Adams; M. Sayagh; A. E. Hassan","School of Computing, Queen's University, Kingston, Ontario Canada (e-mail: hengli@cs.queensu.ca); Department of Computer Science and Software Engineering, Concordia University, Montreal, Quebec Canada (e-mail: shang@encs.concordia.ca); Genie Informatique et Genie Logiciel, Ecole Polytechnique de Montreal, Montreal, Quebec Canada H3T 1J4 (e-mail: bram.adams@polymtl.ca); School of Computing, Queen's University, 4257 Kingston, Ontario Canada (e-mail: msayagh@cs.queensu.ca); School of Computing, Queen's University, Kingston, Ontario Canada (e-mail: ahmed@cs.queensu.ca)","IEEE Transactions on Software Engineering","","2020","PP","99","1","1","Software developers insert logging statements in their source code to collect important runtime information of software systems. In practice, logging appropriately is a challenge for developers. Prior studies aimed to improve logging by proactively inserting logging statements in certain code snippets or by learning where to log from existing logging code. However, there exists no work that systematically studies developers' logging considerations, i.e., the benefits and costs of logging from developers' perspectives. Without understanding developers' logging considerations, automated approaches for logging decisions are based primarily on researchers' intuition which may not be convincing to developers. In order to fill the gap between developers' logging considerations and researchers' intuition, we performed a qualitative study that combines a survey of 66 developers and a case study of 223 logging-related issue reports. The findings of our qualitative study draw a comprehensive picture of the benefits and costs of logging from developers' perspectives. We observe that developers consider a wide range of logging benefits and costs, while most of the uncovered benefits and costs have never been observed nor discussed in prior work. We also observe that developers use ad hoc strategies to balance the benefits and costs of logging. Developers need to be fully aware of the benefits and costs of logging, in order to better benefit from logging (e.g., leveraging logging to enable users to solve problems by themselves) and avoid unnecessary negative impact (e.g., exposing users' sensitive information). Future research needs to consider such a wide range of logging benefits and costs when developing automated logging strategies. Our findings also inspire opportunities for researchers and logging library providers to help developers balance the benefits and costs of logging, for example, to support different log levels for different parts of a logging statement, or to help developers estimate and reduce the negative impact of logging statements.","1939-3520","","10.1109/TSE.2020.2970422","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8976297","software logging;issue reports;developer survey;qualitative analysis","Runtime;Libraries;Tools;Software systems;Standards;Computational modeling","","","","2","","","","30 Jan 2020","","","IEEE","IEEE Early Access Articles"
"Quantifying the Effects of Contention on Parallel File Systems","S. A. Wright; S. A. Jarvis","Dept. of Comput. Sci., Univ. of Warwick, Coventry, UK; Dept. of Comput. Sci., Univ. of Warwick, Coventry, UK","2015 IEEE International Parallel and Distributed Processing Symposium Workshop","1 Oct 2015","2015","","","932","940","As we move towards the Exactable era of supercomputing, node-level failures are becoming more common-place, frequent check pointing is currently used to recover from such failures in long-running science applications. While compute performance has steadily improved year-on-year, parallel I/O performance has stalled, meaning check pointing is fast becoming a bottleneck to performance. Using current file systems in the most efficient way possible will alleviate some of these issues and will help prepare developers and system designers for Exactable, unfortunately, many domain-scientists simply submit their jobs with the default file system configuration. In this paper, we analyse previous work on finding optimality on Lustre file systems, demonstrating that by exposing parallelism in the parallel file system, performance can be improved by up to 49x. However, we demonstrate that on systems where many applications are competing for a finite number of object storage targets (OSTs), competing tasks may reduce optimal performance considerably. We show that reducing each job's request for OSTs by 40% decreases performance by only 13%, while increasing the availability and quality of service of the file system. Further, we present a series of metrics designed to analyse and explain the effects of contention on parallel file systems. Finally, we re-evaluate our previous work with the Parallel Log-structured File System (PLFS), comparing it to Lustre at various scales. We show that PLFS may perform better than Lustre in particular configurations, but that at large scale PLFS becomes a bottleneck to performance. We extend the metrics proposed in this paper to explain these performance deficiencies that exist in PLFS, demonstrating that the software creates high levels of self-contention at scale.","","978-1-4673-7684-6","10.1109/IPDPSW.2015.8","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7284412","Data storage systems;file servers;file systems;high performance computing;optimization;performance analysis;supercomputers","Bandwidth;File systems;Optimization;Quality of service;Measurement;Supercomputers","checkpointing;file organisation;parallel machines","parallel I-O performance;Lustre file system;object storage targets;OST;parallel log-structured file system;PLFS;checkpointing","","","","24","","1 Oct 2015","","","IEEE","IEEE Conferences"
"Characterizing the Natural Language Descriptions in Software Logging Statements","P. He; Z. Chen; S. He; M. R. Lyu","Shenzhen Research Institute, The Chinese University of Hong Kong,Shenzhen,China; Shenzhen Research Institute, The Chinese University of Hong Kong,Shenzhen,China; Shenzhen Research Institute, The Chinese University of Hong Kong,Shenzhen,China; Shenzhen Research Institute, The Chinese University of Hong Kong,Shenzhen,China","2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE)","17 Feb 2020","2018","","","178","189","The following topics are dealt with: program testing; program diagnostics; learning (artificial intelligence); program debugging; software maintenance; public domain software; formal verification; Java; Android (operating system); mobile computing.","2643-1572","978-1-4503-5937-5","10.1145/3238147.3238193","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9000026","Logging;natural language processing;empirical study","","learning (artificial intelligence);program debugging;program diagnostics;program testing;software maintenance","program testing;program diagnostics;learning (artificial intelligence);program debugging;software maintenance;public domain software;formal verification;Java;Android (operating system);mobile computing","","2","","63","","17 Feb 2020","","","IEEE","IEEE Conferences"
"Towards a Knowledge Warehouse and Expert System for the Automation of SDLC Tasks","R. Kapur; B. Sodhi","Indian Institute of Technology Ropar, India.; Indian Institute of Technology Ropar, India.","2019 IEEE/ACM International Conference on Software and System Processes (ICSSP)","26 Aug 2019","2019","","","5","8","Cost of a skilled and competent software developer is high, and it is desirable to minimize dependency on such costly human resources. One of the ways to minimize such costs is via automation of various software development tasks. Recent advances in Artificial Intelligence (AI) and the availability of a large volume of knowledge bearing data at various software development related venues present a ripe opportunity for building tools that can automate software development tasks. For instance, there is significant latent knowledge present in raw or unstructured data associated with items such as source files, code commit logs, defect reports, comments, and so on, available in the Open Source Software (OSS) repositories. We aim to leverage such knowledge-bearing data, the latest advances in AI and hardware to create knowledge warehouses and expert systems for the software development domain. Such tools can help in building applications for performing various software development tasks such as defect prediction, effort estimation, code review, etc.","","978-1-7281-3393-5","10.1109/ICSSP.2019.00011","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8812829","Automated Software Engineering, Software Maintenance, Data Mining, Supervised Learning","Software;Feature extraction;Data mining;Task analysis;Tools;Estimation;Measurement","artificial intelligence;data mining;expert systems;public domain software;software development management;software quality","knowledge warehouse;expert system;SDLC tasks;skilled software developer;competent software developer;software development tasks;knowledge bearing data;software development related venues;Open Source Software repositories;knowledge-bearing data;software development domain;latent knowledge","","","","27","","26 Aug 2019","","","IEEE","IEEE Conferences"
"Non-Local Correction of Process Models Using Event Logs","A. A. Mitsyuk","PAIS Lab., Nat. Res. Univ., Moscow, Russia","2017 Ivannikov ISPRAS Open Conference (ISPRAS)","1 Feb 2018","2017","","","6","11","Information and software systems almost never stabilize after the release. They change, among other reasons, due to difficulties of implementation, bug repairs, and environmental shifts. Thus, their behavior differs from models, which were used at the design stage. Process owner usually wants relevant, up-to-date models. Fortunately, most modern information systems write detailed event logs of their functioning. In this paper, we describe an algorithm of non-local process correction, that employs the two fundamental algorithmic paradigms: “divide and conquer” and “greedy processing”. It decomposes the process model and repairs sub-models in a greedy way using event logs with actual behavior. Using this procedure, it is possible to correct both local and non-local inconsistencies. The paper considers the algorithm and the results of its experimental evaluation using several artificial examples.","","978-1-5386-1132-6","10.1109/ISPRAS.2017.00008","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8273290","process model repair;process model correction;event logs;WF-nets;process mining","Maintenance engineering;Biological system modeling;Petri nets;Information systems;Computational modeling;Economics;Software systems","data mining;divide and conquer methods;greedy algorithms;information systems;software maintenance","nonlocal inconsistencies;process models;event logs;software systems;bug repairs;environmental shifts;behavior differs;design stage;process owner;up-to-date models;modern information systems;nonlocal process correction;fundamental algorithmic paradigms;greedy processing;process model;actual behavior;divide and conquer algorithmic paradigm;submodel repair","","","","24","","1 Feb 2018","","","IEEE","IEEE Conferences"
"Lightweight searchable screen video recording","M. Marder; A. Geva; Y. Ruan",IBM Research-Haifa; IBM Research-Haifa; IBM Research-Thomas J Watson Research Center,"2012 Visual Communications and Image Processing","17 Jan 2013","2012","","","1","6","Command logging of maintenance and operation activities of modern computer systems has become an integral component of customer and audit requirements. In recent years, this logging has usually been achieved via desktop video recording. However, the conventional approach of video recording requires high computation overhead, high network bandwidth, and a large storage size. Searching through video files is also a challenge. In this paper, we present a lossy, but text text-preserving, compression scheme that meets these challenges by creating a sparse bitonal image suitable for optical character recognition (OCR). Using our system for auditing, the bitonal image gets stored on a server. Due to the mechanism's text-preserving compression, we can apply OCR off-line to create annotations of each video frame, making the output searchable. Compared to state-of-the-art compression of raw video, our approach can reduce file size by 50-80%, while using CPU and memory resources similar to other methods.","","978-1-4673-4407-4","10.1109/VCIP.2012.6410783","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6410783","Text segmentation;screen image;binarization;audit;screen recording;monitoring","Image coding;Image color analysis;Image edge detection;Servers;Switches;Optical character recognition software;Image segmentation","data compression;video coding;video recording","lightweight searchable screen video recording;command logging;computer systems;desktop video recording;compression scheme;sparse bitonal image;optical character recognition;auditing;text-preserving compression;state-of-the-art compression;raw video;CPU;memory resources","","2","1","18","","17 Jan 2013","","","IEEE","IEEE Conferences"
"Assessing and improving the effectiveness of logs for the analysis of software faults","M. Cinque; D. Cotroneo; R. Natella; A. Pecchia","Dipartimento di Informatica e Sistemistica, Università degli Studi di Napoli Federico II, Via Claudio 21, 80125, Naples, Italy; Dipartimento di Informatica e Sistemistica, Università degli Studi di Napoli Federico II, Via Claudio 21, 80125, Naples, Italy; Dipartimento di Informatica e Sistemistica, Università degli Studi di Napoli Federico II, Via Claudio 21, 80125, Naples, Italy; Dipartimento di Informatica e Sistemistica, Università degli Studi di Napoli Federico II, Via Claudio 21, 80125, Naples, Italy","2010 IEEE/IFIP International Conference on Dependable Systems & Networks (DSN)","9 Aug 2010","2010","","","457","466","Event logs are the primary source of data to characterize the dependability behavior of a computing system during the operational phase. However, they are inadequate to provide evidence of software faults, which are nowadays among the main causes of system outages. This paper proposes an approach based on software fault injection to assess the effectiveness of logs to keep track of software faults triggered in the field. Injection results are used to provide guidelines to improve the ability of logging mechanisms to report the effects of software faults. The benefits of the approach are shown by means of experimental results on three widely used software systems.","2158-3927","978-1-4244-7501-8","10.1109/DSN.2010.5544279","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5544279","","Open source software;Software systems;Computer networks;Guidelines;Supercomputers;Runtime;Computer crashes;Computer errors;Fault diagnosis;Web server","program verification;software fault tolerance","software faults analysis;event logs;computing system dependability;system outages;software fault injection;logging mechanisms","","16","1","24","","9 Aug 2010","","","IEEE","IEEE Conferences"
"Big data: Mining of log file through hadoop","B. Kotiyal; A. Kumar; B. Pant; R. H. Goudar","Computer Science Department, GEU University Bell Road; Information Technology Department, GEU University, Bell Road; Information Technology Department, GEU University, Bell Road; Computer Science Department, GEU University Bell Road","2013 International Conference on Human Computer Interactions (ICHCI)","1 Sep 2014","2013","","","1","7","The unremitting increase of computational strength has produced tremendous flow of data in the past two decades. This tremendous flow of data is known as “big data”. Big data is the data which cannot be processed with the aid of existing tools or techniques and if processed can result in interesting information's such as analysing the behaviour of the user, business intelligence etc. This paper discusses the difference between the traditional relational database and big data; it also shows the characteristics of big data. The paper also focuses on the distinct big data channels processes along with the various challenges and as well as on how big data is a solution to the organizations. Big data does not only focus to store and handle the large volume of data but also to analysed and extract the correct information from the data in lesser time span. At last it discusses about hadoop an open source framework that allows the distributed processing for massive datasets on cluster of computers which is shown with using the log file for extraction of information based on user query.","","978-1-4673-5703-6","10.1109/ICHCI-IEEE.2013.6887797","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6887797","Big Data;Web Log File;Hadoop;Hadoop Distributed File System;Distributed Processing;MapReduce","Big data;Organizations;Distributed databases;Data mining;Computers;Distributed processing","competitive intelligence;data handling;data mining;organisational aspects;public domain software;query processing;relational databases","log file mining;Hadoop;computational strength;business intelligence;relational database;big data channels;organizations;open source framework;information extraction;user query","","6","","13","","1 Sep 2014","","","IEEE","IEEE Conferences"
"Performance study on logging to another main memory database","X. Qin; X. Zhou","Key Laboratory of Data Engineering and Knowledge Engineering, School of Information, Renmin University of China, Beijing 100872, China; Department of Computer Science and Technology, Xuzhou Normal University, Jiangsu 221008, China","4th International Conference on New Trends in Information Science and Service Science","17 Jun 2010","2010","","","36","40","In update intensive main memory database applications, huge volume of log records is generated, to maintain the ACID properties of the database system, the log records should be persistent efficiently. Delegating logging of one main memory database to another main memory database is proposed. The scheme is elaborated in detail in terms of architecture, logging & safeness levels, checkpointing, and recovery. Strict durability and relax durability are provided. When some form of non-volatile memory is used to temporarily holding log records, not only logging efficiency is improved, but also the scheme could guarantee full ACID of the system. We also propose using parallel logging to speedup log persistence by writing logs to multiple disks in parallel. Since interconnection network techniques progress by leaps and bounds, the scheme eliminates the concern about whether the system's overall performance may be slowed down by bandwidth and latency limitations. Experiment results demonstrate the feasibility of the proposal.","","978-89-88678-17-6","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5488503","main memory database;remote logging;parallel logging","Checkpointing;Database systems;Transaction databases;Network servers;Availability;Nonvolatile memory;Writing;Communication industry;Defense industry;Wood industry","checkpointing;data recording;database management systems;memory architecture;multiprocessor interconnection networks;random-access storage;software reliability;system monitoring","main memory database;log records;ACID properties;architecture;safeness levels;checkpointing;recovery;durability;nonvolatile memory;parallel logging;multiple disks;interconnection network techniques;system overall performance;bandwidth limitations;latency limitations","","","2","16","","17 Jun 2010","","","IEEE","IEEE Conferences"
"A system of job log analyzing for Hadoop","Zhao Xiaogang; Ding Ling; Ma Zhiqiang; Liu Xu","Department of software engineering, International School of Software, Wuhan University, China; Department of Computer Science and Technology, Xianning Colleage, China; Department of Electronics Information and Engineering, Henan Vocational College of Agriculture, Zhengzhou, China; Baidu Company, China","2012 International Conference on Information Management, Innovation Management and Industrial Engineering","25 Oct 2012","2012","3","","238","243","Handling the huge amount of history logs produced by Hadoop distributed computing platform is really a troublesome task and always these history files looks useless. But if we want find out the health degree of cluster platform we must analyze the huge history logs produced by the running jobs. It seems that a single-machine analyzing program cannot satisfy you because of its low speed, high demand of memory and CPU. In this thesis we tried to solve this problem in a distributed way with the Map/Reduce calculation model. We also built a data platform(hive and MySQL) to store these data. From the experiment we can see the distributed way to process log files get good performance when data log files are huge.","2155-1472","978-1-4673-1931-7","10.1109/ICIII.2012.6339963","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6339963","Hadoop;history log;Map/Reduce;distribute computing","History;Databases;Blogs","distributed processing;program diagnostics;SQL;storage management","job log analyzing system;history logs;Hadoop distributed computing platform;history files;health degree;cluster platform;running jobs;single-machine analyzing program;memory;CPU;distributed problem solving;Map/Reduce calculation model;a data platform;hive;MySQL;data storage;process log files;data log files","","1","","11","","25 Oct 2012","","","IEEE","IEEE Conferences"
"Discovering Hidden Errors from Application Log Traces with Process Mining","M. Cinque; R. Della Corte; A. Pecchia",Università degli Studi di Napoli Federico II; Università degli Studi di Napoli Federico II; Università degli Studi di Napoli Federico II,"2019 15th European Dependable Computing Conference (EDCC)","7 Nov 2019","2019","","","137","140","Over the past decades logs have been widely used for detecting and analyzing failures of computer applications. Nevertheless, it is widely accepted by the scientific community that failures might go undetected in the logs. This paper proposes a measurement study with a dataset of 3,794 log traces obtained from normative and failure runs of the Apache web server. We use process mining (i) to infer a model of the normative log behavior, e.g., presence and ordering of messages in the traces, and (ii) to detect failures within arbitrary traces by looking for deviations from the model (conformance checking). Analysis is done with the Integer Linear Programming (ILP) Miner, Inductive Miner and Alpha++ Miner algorithms. Our measurements indicate that, although only around 18% failure traces contain explicit error keywords and phrases, conformance checking allows detecting up to 87% failures at high precision, which means that most of the errors are hidden across the traces.","2641-810X","978-1-7281-3929-6","10.1109/EDCC.2019.00034","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8893303","process mining;application log;trace;software errors;testing","Web servers;Expert systems;Training;Integer linear programming;Measurement","data mining;file servers;integer programming;Internet;linear programming","normative log behavior;arbitrary traces;conformance checking;Integer Linear Programming Miner;Inductive Miner;Alpha++ Miner algorithms;explicit error keywords;hidden errors;application log traces;process mining;computer applications;scientific community;measurement study;Apache web server","","","","12","","7 Nov 2019","","","IEEE","IEEE Conferences"
"Knowledge extraction of the behaviour of software developers by the analysis of time recording logs","A. Peralta; F. P. Romero; J. A. Olivas; M. Polo","Department of Information Systems and Technologies, University of Castilla La Mancha, Paseo de la Universidad s/n, 13071 Ciudad Real, Spain; Department of Information Systems and Technologies, University of Castilla La Mancha, Paseo de la Universidad s/n, 13071 Ciudad Real, Spain; Department of Information Systems and Technologies, University of Castilla La Mancha, Paseo de la Universidad s/n, 13071 Ciudad Real, Spain; Department of Information Systems and Technologies, University of Castilla La Mancha, Paseo de la Universidad s/n, 13071 Ciudad Real, Spain","International Conference on Fuzzy Systems","23 Sep 2010","2010","","","1","8","Software development project management has a poor reputation in terms of avoiding cost and schedule overruns. The cause of this situation is based on the feature of the software development process that is characterized by quickly growing complexity and change. Therefore, there are many uncertainties to define exactly the necessary time to complete a tasks according to the person's performance. In this scenario Soft-Computing techniques may offer new approaches with the aim of helping the participants of the project to manage their time, give priority to their activities and readjust the work to complete satisfactorily the project tasks. This work presents an automatic features extraction process with the aim of defining the elements involved in a software project. This knowledge is represented by means fuzzy sets and fuzzy prototypes. The source of data is the Personal Software Project time recording logs. A preliminary experiment illustrates the feasibility of this approach.","1098-7584","978-1-4244-6921-5","10.1109/FUZZY.2010.5584364","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5584364","","Software;Prototypes;Time measurement;Productivity;Pragmatics;Programming;Feature extraction","knowledge acquisition;project management;software management","knowledge extraction;project management;software development process;time recording logs analysis;soft computing techniques;personal software project","","4","","28","","23 Sep 2010","","","IEEE","IEEE Conferences"
"S-WAL: Fast and Efficient Write-Ahead Logging for Mobile Devices","D. H. Kang; W. Kang; Y. I. Eom","College of Software, Sungkyunkwan University, Suwon, South Korea; Core Platform and Infrastructure Architecture, eBay, San Jose, CA, USA; College of Software, Sungkyunkwan University, Suwon, South Korea","IEEE Transactions on Consumer Electronics","23 Sep 2018","2018","64","3","319","327","A crash-consistency mechanism of database application (hereinafter DBMS for short) imposes an enormous burden on the journaling process of the file system (e.g., JBD2) in that it employs regular files on the file system to persistently preserve the application's journal data with synchronous system calls. Unfortunately, much of the previous research is impractical in real-world scenarios because they require changing a lot of source lines of code in multiple software layers of the software stack or using special hardware (e.g., the transactional flash storage or NVM technologies). In this paper, we propose a new journal mode of DBMS, called S-WAL, which compresses the raw data journaled by the database to alleviate both the redundant journaling operations and harmful write amplification. We believe that S-WAL is a practical way to support application-level crash consistency on the existing mobile devices because only a few lines of DBMS code need to be changed, without the need to employ special hardware. We demonstrate the effectiveness of S-WAL by running four popular mobile applications on the latest smartphone. Our evaluation results show that S-WAL considerably outperforms existing journal modes of DBMS in all cases. In the best case, S-WAL reduces the elapsed time by up to 7.5× more than the PERSIST mode and by up to 51% more than the traditional WAL mode.","1558-4127","","10.1109/TCE.2018.2859630","National Research Foundation of Korea; Ministry of Science, ICT; Ministry of Science and ICT, Korea through the SW Starlab Support Program; Institute for Information and Communications Technology Promotion; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8419334","Database systems;mobile applications;write-ahead logging","Databases;Mobile handsets;Performance evaluation;File systems;Computer crashes;Hardware;Mobile applications","database management systems;mobile computing;storage management;system recovery","crash-consistency mechanism;database application;journaling process;file system;regular files;synchronous system calls;source lines;multiple software layers;software stack;transactional flash storage;journal mode;raw data;redundant journaling operations;application-level crash consistency;PERSIST mode;traditional WAL mode;mobile devices;mobile applications;efficient write-ahead logging;S-WAL;DBMS code;application journal data;S;S","","1","","26","","25 Jul 2018","","","IEEE","IEEE Journals"
"Efficient Real-Time Traffic Generation for 5G RAN","D. Corcoran; P. Kreuger; C. Schulte","KTH Royal Institute of Technology,Ericsson AB and Software and Computer Systems,Stockholm,Sweden; Research Institutes of Sweden,RISE AI,Kista,Sweden; KTH Royal Institute of Technology,Software and Computer Systems,Stockholm,Sweden","NOMS 2020 - 2020 IEEE/IFIP Network Operations and Management Symposium","8 Jun 2020","2020","","","1","9","Modern telecommunication and mobile networks are increasingly complex from a resource management perspective, with diverse combinations of software and infrastructure elements that need to be configured and tuned for efficient operation with high quality of service. Increased real-time automation at all levels and time-frames is a critical tool in controlling this complexity. A key component in automation is practical and accurate simulation methods that can be used in live traffic scenarios. This paper introduces a new method with supporting algorithms for sampling key parameters from live or recorded traffic which can be used to generate large volumes of synthetic traffic with very similar rate distributions and temporal characteristics. Multiple spatial renewal processes are used to generate fractional Gaussian noise, which is scaled and transformed into a log-normal rate distribution with discrete arrival events, fitted to the properties observed in given recorded traces. This approach works well for modelling large user aggregates but is especially useful for medium sized and relatively small aggregates, where existing methods struggle to reproduce the most important properties of recorded traces. The technique is demonstrated through experimental comparisons with data collected from an operational LTE network to be highly useful in supporting self-learning and automation algorithms which can ultimately reduce complexity, increase energy efficiency, and reduce total network operation costs.","2374-9709","978-1-7281-4973-8","10.1109/NOMS47738.2020.9110314","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9110314","Communication system traffic;Fractals;Machine learning;Parametric statistics;System simulation","","5G mobile communication;Gaussian noise;log normal distribution;Long Term Evolution;quality of service;radio access networks;resource allocation;telecommunication network management;telecommunication traffic","5G RAN;mobile networks;resource management perspective;live traffic scenarios;temporal characteristics;fractional Gaussian noise;log-normal rate distribution;discrete arrival events;operational LTE network;energy efficiency;network operation costs;real-time traffic generation;spatial renewal processes;real-time automation;quality of service","","","","28","","8 Jun 2020","","","IEEE","IEEE Conferences"
"Virtual Machine Replay Update: Improved Implementation for Modern Hardware Architecture","J. Yu; P. Zhou; Y. Wu; C. Zhao","Inst. of Software, Beijing, China; Inst. of Software, Beijing, China; Inst. of Software, Beijing, China; Inst. of Software, Beijing, China","2012 IEEE Sixth International Conference on Software Security and Reliability Companion","6 Aug 2012","2012","","","1","6","This paper describes a successive and updated work of Revirt project which presents a virtual machine replay framework on Xen hyper visor. As both the commodity hardware and Xen hyper visor have been changed significantly since the first publication of Revirt, the initial implementation does not meet the needs of modern architecture any more. This paper presents an improved implementation of virtual machine execution replay system called CAS Motion. CAS Motion has three contributions. First, CAS Motion uses the performance monitor of Intel Core2 processor to construct time point of recorded events, which makes the event record more complete and precise. Second, CAS Motion can fully support multi-core hardware platform which is prevalent today. Third, CAS Motion is developed with more general architecture design, which makes it deployable on upstream Xen hyper visor and Dom0. Our experiments under a varity of workloads shows CAS Motion has low performance impact on monitored DomU. The growth of record log is also in acceptable range.","","978-1-4673-2670-4","10.1109/SERE-C.2012.26","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6258437","Revirt;execution replay;determinism;virtual machines;Xen","Hardware;Virtual machine monitors;Monitoring;Radiation detectors;Virtual machining;Registers;Operating systems","multiprocessing systems;virtual machines","virtual machine replay update;modern hardware architecture;Revirt project;virtual machine replay framework;Xen hyper visor;Revirt publication;virtual machine execution replay system;CAS Motion;Intel Core2 processor;recorded events;multicore hardware platform;architecture design;DomU","","1","","11","","6 Aug 2012","","","IEEE","IEEE Conferences"
"An e-shop log file analysis toolbox","A. C. Boucouvalas; C. J. Aivalis","Department of Telecommunications Science and Technology, Faculty of Science and Technology, University of Peloponnese, Tripoli, 22100, Greece; Department of Telecommunications Science and Technology, Faculty of Science and Technology, University of Peloponnese, Tripoli, 22100, Greece","2010 7th International Symposium on Communication Systems, Networks & Digital Signal Processing (CSNDSP 2010)","20 Sep 2010","2010","","","289","294","Most e-commerce applications are implemented without any significant means of built-in performance measuring mechanisms, although responsiveness is a major factor for high revenue. Often, overall response times increase without the administrators even noticing. It is therefore crucial to have a precise measuring system that also reports customer behavioral patterns. We created a customizable e-commerce log file analyzer that measures performance mainly through combining a mix of log file data with e-shop information. It is capable of supporting multiple e-shops and can perform cross comparisons between them. It is easy to use and the software is extendable in order to support different web server architectures. It displays patterns taken by the user interaction with the e-shop and allows the administrators to locate less visited pages and make improvements or promote them better. This way an e-shop may benefit since it may lead to higher user satisfaction and profitability. The administrator can regularly study the readings and take the appropriate actions, if overall performance decay is observed.","","978-1-86135-369-6","10.1109/CSNDSP16145.2010.5580415","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5580415","","Databases;Web server;Robots;Valves;IP networks;Service oriented architecture","consumer behaviour;customer satisfaction;electronic commerce;file servers;Internet;performance evaluation;profitability;retail data processing","e-shop log file analysis toolbox;e-commerce applications;built-in performance measuring mechanisms;administrators;customer behavioral patterns;e-commerce log file analyzer;log file data;web server architectures;user interaction;less visited pages;profitability;user satisfaction;performance decay","","","","12","","20 Sep 2010","","","IEEE","IEEE Conferences"
"GME: A Contemporary Approach Workflow Process Improvement of Software by Uncovering Hidden Transactions of a Healthcare Legacy Application","S. Sharma; S. Srivastava",MU Jaipur; MU Jaipur,"2018 8th International Conference on Cloud Computing, Data Science & Engineering (Confluence)","23 Aug 2018","2018","","","436","441","In organization numbers are increasing day by day with a drastic pace which prefers the extraction of the workflow of processes to interpret the operational processes. For a viably and sorted out approach to drive the development in the realm of digitization is utilized by the approach of work process extraction. The work process extraction/mining are otherwise called process mining. The goal of Workflow mining is to get the extraction of data of an association's method of business by changing over the logs of occasion information recorded in association's frameworks. This impact to the enhance conformation of processes to organization regulation where workflow mining approach for analysis is actualized. Work process Mining strategies are absolutely rely upon the nearness of framework occasion log information. We accept to involve setting various endeavors on building our strategies or frameworks to record the greater part of the old information. The urge to comprehend and expand their procedures of businesses entails the process exploration practices. This paper displays a philosophy how programming occasion log information is analyzed to grasp and advance the product work process by utilizing arrangement which best in class utilized as a part of the product code clone streamlining for the human services area application.","","978-1-5386-1719-9","10.1109/CONFLUENCE.2018.8442779","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8442779","Workflow net;Genetic Mining for Events(GME);Genetic Algorithm(GA);Process Mining;Process workflow;Petri net;IoE (Internet of Events);Clone Optimization;workflow;petrinet;BPM(Business Process Model);MXML(Mining XML)","Data mining;Petri nets;Computational modeling;Standards;Semantics;Organizations","data mining;health care;software maintenance;workflow management software","process exploration practices;product work process;hidden transactions;healthcare legacy application;organization numbers;drastic pace;operational processes;work process extraction/mining;occasion information;organization regulation;workflow mining approach;framework occasion;work process mining strategies","","","","19","","23 Aug 2018","","","IEEE","IEEE Conferences"
"Evidence-Based Behavioral Model for Calendar Schedules of Individual Mobile Phone Users","I. H. Sarker; M. A. Kabir; A. Colman; J. Han","Sch. of Software & Electr. Eng., Swinburne Univ. of Technol., Melbourne, VIC, Australia; Sch. of Comput. & Math., Charles Sturt Univ., Bathurst, NSW, Australia; Sch. of Software & Electr. Eng., Swinburne Univ. of Technol., Melbourne, VIC, Australia; Sch. of Software & Electr. Eng., Swinburne Univ. of Technol., Melbourne, VIC, Australia","2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA)","26 Dec 2016","2016","","","584","593","The electronic calendar usually serves as a personal organizer and is a valuable resource for managing daily activities or schedules of the users. Naturally, a calendar provides various contextual information about individual's scheduled events/appointments, e.g., meeting. A number of researchers have utilized such information to predict human behavior for mobile communication, by assuming a predefined event-behavior mapping which is static and non-personalized. However, in the real world, people differ from each other in how they respond to incoming calls during their scheduled events, even a particular individual may respond differently subject to what type of event is scheduled in the calendar. Thus a static behavioral model does not necessarily map to calendar schedules and corresponding phone call response behavior of individuals. Therefore, we propose an evidencebased behavioral model (EBM) that dynamically identifies the actual call response behavior of individuals for various calendar events based on their mobile phone log that records the data related to a user's phone call activities. Experiments on real datasets show that our proposed technique better captures the user's call response behavior for various calendar events, thereby enabling more appropriate rules to be created for the purpose of automated handling of incoming calls in an intelligent call interruption management system.","","978-1-5090-5206-6","10.1109/DSAA.2016.86","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7796945","mobile phone;call log;calendar;user behavior modeling;mobile data mining;rule discovery;personalization;call interruptions;intelligent systems","Mobile communication;Mobile handsets;Context;Schedules;Interrupters;Australia;Electronic mail","mobile computing;scheduling","evidence based behavioral model;calendar schedules;individual mobile phone users;electronic calendar;personal organizer;contextual information;individual scheduled events-appointments;human behavior;mobile communication;EBM;mobile phone log;intelligent call interruption management system","","4","","24","","26 Dec 2016","","","IEEE","IEEE Conferences"
"LogLiDAR: An Internet of Things Solution for Counting and Scaling Logs","F. Martí; A. R. M. Forkan; P. P. Jayaraman; C. McCarthy; H. Ghaderi","Swinburne University of Technology,Melbourne,Australia; Swinburne University of Technology,Melbourne,Australia; Swinburne University of Technology,Melbourne,Australia; Swinburne University of Technology,Melbourne,Australia; Swinburne University of Technology,Melbourne,Australia","2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)","24 May 2021","2021","","","413","415","Accurate counting and measurement of logs (known as log scaling) stacked in piles, is an integral part of the wood log supply chain. Currently, these tasks are manual, and hence labour intensive and prone to human errors. In this paper, we present LogLiDAR an IoT sensing-based solution for counting and scaling logs using LiDAR (Light Detection and Ranging) images. LogLiDAR incorporates an interactive user interface to explore log counting and scaling. The underlying system for processing LiDAR images is developed using a pipeline of point cloud library (PCL) algorithms. This work is the first attempt to develop an IoT-based (LiDAR) solution for counting and scaling logs for the log industry.","","978-1-6654-0424-2","10.1109/PerComWorkshops51409.2021.9431022","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9431022","Wood Log;Internet of Things;Log Scaling;Point Cloud;LiDAR;Visualisation","Laser radar;Three-dimensional displays;Conferences;Software algorithms;Supply chains;Pipelines;User interfaces","","","","","","7","","24 May 2021","","","IEEE","IEEE Conferences"
"How Much Undocumented Knowledge is there in Agile Software Development?: Case Study on Industrial Project Using Issue Tracking System and Version Control System","S. Saito; Y. Iimura; A. K. Massey; A. I. Antón","Software Innovation Center, NTT Corp., Tokyo, Japan; Software Innovation Center, NTT Corp., Tokyo, Japan; Dept. of Inf. Syst., Univ. of Maryland, Baltimore, MD, USA; Sch. of Interactive Comput., Georgia Inst. of Technol., Atlanta, GA, USA","2017 IEEE 25th International Requirements Engineering Conference (RE)","25 Sep 2017","2017","","","194","203","In agile software development projects, software engineers prioritize implementation over documentation to eliminate needless documentation. Is the cost of missing documentation greater than the cost of producing unnecessary or unused documentation? Even without these documents, software engineers maintain other software artifacts, such as tickets in an issue tracking system (ITS) or source code committed to a version control system (VCS). Do these artifacts contain the necessary knowledge? In this paper, we examine undocumented knowledge in an agile software development project at NTT. For our study, we collected 159 commit logs in a VCS and 102 tickets in the ITS from the three-month period of the project. We propose a ticket-commit network chart (TCC) that visually represents time-series commit activities along with filed issue tickets. We also implement a tool to generate the TCC using both commit log and ticket data. Our study revealed that in 16% of all commits, software engineers committed source code to the VCS without a corresponding issue ticket in the ITS. Had these commits been based on individual issue tickets, these ""unissued"" tickets would have accounted for 20% of all tickets. Software users and requirements engineers also evaluated the contents of these commits and found that 42% of the ""unissued"" tickets were required for software operation and 23% of those were required for requirements modification.","2332-6441","978-1-5386-3191-1","10.1109/RE.2017.33","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8048905","Agile Agile Development;Issue Tracking System;Version Control System;Requirements Knowledge","Software;Agile software development;Documentation;Tools;Data visualization;Knowledge management","configuration management;formal specification;formal verification;project management;software development management;software prototyping;source code (software);system documentation","unissued tickets;requirements engineers;undocumented knowledge;industrial project;issue tracking system;version control system;agile software development project;software artifacts;source code;ticket-commit network chart;time-series commit activities;commit log;requirements modification","","1","","23","","25 Sep 2017","","","IEEE","IEEE Conferences"
"Distributed Logging System for ROS-Based Systems","Y. Koo; S. Kim","Autonomous Driving System Research Group, ETRI, Daejeon, Republic of Korea; Autonomous Driving System Research Group, ETRI, Daejeon, Republic of Korea","2019 IEEE International Conference on Big Data and Smart Computing (BigComp)","4 Apr 2019","2019","","","1","3","The Robot Operating System (ROS) is a widely-used development and service platform for robot clusters and autonomous driving systems. ROS provides a data logging tool named ROSBAG to record and play back messages between processes into permanent storage devices. Despite of its excellent functionality for distributed communication, only a single logging node is allowed to gather data streams. Although such a single node logging is only appropriate for small systems, it does not have the capability to deal with big data distributed systems. In this paper, we present a distributed logging system for ROS-based systems. We wrapped ROSBAG tools with a Python-based parallel ssh tool - pssh to send commands to start and stop logging. We also supported a synchronous replay method to play back the data streams separately stored in several storage devices. Our mechanism can evenly distribute bandwidth consumption of storage and networks for collecting and storing data. It also disperses the logging data into the storage devices in several computers and improves the available logging duration previously restricted by storage capacity.","2375-9356","978-1-5386-7789-6","10.1109/BIGCOMP.2019.8679372","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8679372","logging;distributed system;ROS;Python","Bandwidth;Tools;Autonomous vehicles;Robots;Distributed databases;Computers;Operating systems","Big Data;data loggers;mobile robots;operating systems (computers);parallel processing;Python;robot programming;software tools","distributed logging system;ROS-based systems;ROSBAG tools;data streams;logging data;service platform;robot clusters;autonomous driving systems;data logging tool;permanent storage devices;distributed communication;single logging node;big data distributed systems;data collection;Python-based parallel ssh tool;synchronous replay method;data storage;logging duration;robot operating system","","2","","3","","4 Apr 2019","","","IEEE","IEEE Conferences"
"Differential Debugging","D. Spinellis",Athens University of Economics and Business,"IEEE Software","3 Sep 2013","2013","30","5","19","21","Finding yourself in a situation with a working and a buggy system is quite common. Differential debugging methodically can help by comparing a known good system with a buggy one, working toward the problem source. Some simple steps include applying differential debugging by looking at log files and increasing a system's log verbosity when needed. If the system doesn't offer a sufficiently detailed logging mechanism, you can tease out its runtime behavior with tools that trace calls to the operating system or that trace network packets. You can also compare carefully the two environments where the systems operate. The Web extra at http://youtu.be/qnXS6b4hakg is an audio podcast of author Diomidis Spinellis reading his Tools of the Trade column, in which he discusses how comparing a good system with a buggy one can help locate the source of the problem.","1937-4194","","10.1109/MS.2013.103","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6588528","debug;Unix tools;trace;log file","Software testing;Computer bugs;Debugging;Runtime","program debugging","differential debugging;buggy system;log files;system log;runtime behavior;operating system;network packets","","","","","","3 Sep 2013","","","IEEE","IEEE Magazines"
"Evolvable and scalable system of content servers for a large digital preservation archives","Q. L. Nguyen; A. Lake; M. Huber","US National Archives and Records Administration, College Park, MD, USA; US National Archives and Records Administration, College Park, MD, USA; US National Archives and Records Administration, College Park, MD, USA","2010 IEEE International Systems Conference","7 Jun 2010","2010","","","306","310","Building a large-scale digital preservation archives system (LDPAS) for long-term access is a challenging endeavor. The three most significant system requirements contributing to that challenge are the scalability of the system, accommodation of heterogeneous digital holdings, and the evolvability of both the constituting technologies and the data formats in storage. The amount of digital-born materials requiring long-term preservation and access is overwhelming due to the fast pace of information technology and its widespread utilization in government, business corporations, academic institutions, and the general public. Compounding to the data volume challenge is the heterogeneity of the data, which range from the products of office automation and geospatial images, to multimedia artifacts. Storing and preserving digital objects must be accomplished in a way that allows access to the objects independently of the platform technology and software applications used to create these objects. As the technologies change rapidly, the LDPAS must accommodate the changes to its benefit, while also increasing in scale. A successful LDPAS must be able to adapt to the two-fold evolutionary demands. The evolution of the system software with time is the norm in any large-scale long-term system. But, even more important is the system's capability to cope with the evolution of the very software applications that generated the digital objects archived within it. In this paper, we propose an architecture of a system of Content Servers, which can scale with the data volume growth, continually sustain evolution in the face of technological change, and provide different levels of service to the different kinds of users accessing the LDPAS. We describe the concept, components and services of a Content Server in the context of digital archiving, service-oriented architecture (SOA) and the Open Archive Information System (OAIS) reference model. Several possible deployment configurations are shown to illustrate the efficient utilization of low level services and the flexibility of the system to scale up or down.","","978-1-4244-5884-4","10.1109/SYSTEMS.2010.5482447","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5482447","Electronic Records;Digital Archives;digital preservation;SOA;OAIS;evolvability;Content Server","Log-periodic dipole antennas;Large-scale systems;Application software;Service oriented architecture;Scalability;Information technology;Government;Office automation;Multimedia systems;System software","information retrieval systems;records management;software architecture","large-scale digital preservation archives system;heterogeneous digital holdings;content servers;digital-born materials;long-term preservation;LDPAS system;two-fold evolutionary demands;digital archiving;service-oriented architecture;open archive information system;OAIS reference model","","5","","20","","7 Jun 2010","","","IEEE","IEEE Conferences"
"Estimating Global Completeness of Event Logs: A Comparative Study","J. Pei; L. Wen; H. Yang; J. Wang; X. Ye","School of Software, Tsinghua University, Beijing, China; School of Software, Tsinghua University, Beijing, China; School of Software, Tsinghua University, Beijing, China; School of Software, Tsinghua University, Beijing, China; School of Software, Tsinghua University, Beijing, China","IEEE Transactions on Services Computing","9 Apr 2021","2021","14","2","441","457","Event logs are the basis of process mining techniques and tools that extract process behavior information for better understanding and optimization of business processes. While it has been widely realized that the degree of completeness of event logs may largely determine the effectiveness of these techniques, how to estimate the completeness of event logs has not yet been fully addressed. This is mainly because ground-truth process models are usually unknown. To attack this problem, we pay a closer look to several concepts and implicit assumptions in the log completeness estimation problem and characterize it as a special case of the species estimation problem in the field of statistics. Although species estimation is still an open problem, a number of statistic models and techniques with approximate solutions have been available. To investigate the relevance of these methods for event log completeness estimation, we have designed and conducted a wide scope of empirical study and quantitative experiments on both real-world and synthesized event logs to compare the performance of these methods. In addition, the completeness estimation of several important and widely used real-world events logs are reported for the first time together with some best practice experience learned through this research.","1939-1374","","10.1109/TSC.2018.2805912","National Key Research and Development Program of China; National Natural Science Foundation of China; Tsinghua TNList Lab Key Projects; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8291718","Quality and metrics;business process management;process mining;event logs;completeness","Estimation;Task analysis;Measurement;Computational modeling;Algorithm design and analysis;Prediction algorithms","","","","1","","56","IEEE","14 Feb 2018","","","IEEE","IEEE Journals"
"Modeling Web Logs to Enhance the Analysis of Web Usage Data","P. Hernandez; I. Garrigos; J. Mazon","Dept. Software & Comput. Syst., Univ. of Alicante, Alicante, Spain; Dept. Software & Comput. Syst., Univ. of Alicante, Alicante, Spain; Dept. Software & Comput. Syst., Univ. of Alicante, Alicante, Spain","2010 Workshops on Database and Expert Systems Applications","30 Sep 2010","2010","","","297","301","Web log files store data related to the use of a website. Analyzing these data in detail is therefore crucial for improving the user browsing experience. However, usually Web log data are stored in flat files in different formats which hinders their analysis, thus obliging to use specific Web log analysis tools. In this context, approaches for structuring Web log data to better analyze them are highly required. To this aim, in this paper we propose a metamodel for Web log data in order to unify features of every available format at the same time that unnecessary technical details are omitted. This metamodel supports the design of Web log models regardless the format of the Web log files. A set of guidelines have been also proposed to define a multidimensional schema of a data warehouse from our Web log model to be used in advanced analysis tools such as OLAP (OnLine Analytical Processing) or data mining tools in order to enhance the analysis of Web usage by means of log data.","2378-3915","978-1-4244-8049-4","10.1109/DEXA.2010.65","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5591192","log;data analysis;metamodel","Data models;Guidelines;Data warehouses;Analytical models;Data mining;Context;Servers","data analysis;data mining;data warehouses;Internet;Web sites","Web log modelling;Web usage data analysis;Website;user browsing experience;Web log analysis tools;metamodel;data warehouse;online analytical processing;data mining tools","","","","14","","30 Sep 2010","","","IEEE","IEEE Conferences"
"Mining GitHub: Why Commit Stops -- Exploring the Relationship between Developer's Commit Pattern and File Version Evolution","Y. Weicheng; S. Beijun; X. Ben","Sch. of Software Eng., Shanghai Jiao Tong Univ., Shanghai, China; Sch. of Software Eng., Shanghai Jiao Tong Univ., Shanghai, China; Sch. of Software Eng., Shanghai Jiao Tong Univ., Shanghai, China","2013 20th Asia-Pacific Software Engineering Conference (APSEC)","6 Mar 2014","2013","2","","165","169","Using the freeware in GitHub, we are often confused by a phenomenon: the new version of GitHub freeware usually was released in an indefinite frequency, and developers often committed nothing for a long time. This evolution phenomenon interferes with our own development plan and architecture design. Why do these updates happen at that time? Can we predict GitHub software version evolution by developers' activities? This paper aims to explore the developer commit patterns in GitHub, and try to mine the relationship between these patterns (if exists) and code evolution. First, we define four metrics to measure commit activity and code evolution: the changes in each commit, the time between two commits, the author of each changes, and the source code dependency. Then, we adopt visualization techniques to explore developers' commit activity and code evolution. Visual techniques are used to describe the progress of the given project and the authors' contributions. To analyze the commit logs in GitHub software repository automatically, Commits Analysis Tool (CAT) is designed and implemented. Finally, eight open source projects in GitHub are analyzed using CAT, and we find that: 1) the file changes in the previous versions may affect the file depend on it in the next version, 2) the average days around ""huge commit"" is 3 times of that around normal commit. Using these two patterns and developer's commit model, we can predict when his next commit comes and which file may be changed in that commit. Such information is valuable for project planning of both GitHub projects and other projects which use GitHub freeware to develop software.","1530-1362","978-1-4799-2144-7","10.1109/APSEC.2013.133","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6754372","repository mining;GitHub;commit pattern;version evolution;visualization technology","Software;Data mining;Measurement;Java;Visualization;History;Data visualization","data mining;data visualisation;software maintenance","GitHub freeware;commit pattern;file version evolution;GitHub software version evolution;pattern mining;code evolution;visualization technique;GitHub software repository mining;commits analysis tool;CAT;open source project","","7","","8","","6 Mar 2014","","","IEEE","IEEE Conferences"
"Function profiling for embedded software by utilizing QEMU and analyzer tool","T. Van Dung; I. Taniguchi; T. Hieda; H. Tomiyama","Graduate School of Science and Engineering, Ritsumeikan University, 1-1-1 Noji-higashi, Kusatsu, Shiga 525-8577 Japan; College of Science and Engineering, Ritsumeikan University, 1-1-1 Noji-higashi, Kusatsu, Shiga 525-8577 Japan; Research Organization of Science and Engineering, Ritsumeikan University, 1-1-1 Noji-higashi, Kusatsu, Shiga 525-8577 Japan; College of Science and Engineering, Ritsumeikan University, 1-1-1 Noji-higashi, Kusatsu, Shiga 525-8577 Japan","2013 IEEE 56th International Midwest Symposium on Circuits and Systems (MWSCAS)","2 Dec 2013","2013","","","1251","1254","Function profiling is crucial for optimized embedded software which needs to have resource constraint, low level power consumption and real-time ability. In this work, we provide a fast and reliable solution by utilizing an instruction set simulator named QEMU and creating an analyzer tool. We developed a tracing module inside the simulator to trace execution information of software in run-time and record it in a log file. Our implementation takes advantages of the dynamic binary translation of QEMU to keep its speed fast and use an analyzer tool to analyze the log file and creates a function profile. We implemented this methodology for ARM architecture, and evaluated many kinds of embedded applications.","1558-3899","978-1-4799-0066-4","10.1109/MWSCAS.2013.6674881","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6674881","Function profile;QEMU;dynamic binary translation;ARM architecture","Computer architecture;Optimization;Operating systems;Linux;Educational institutions;Algorithms","embedded systems;file organisation;instruction sets;program diagnostics","function profiling;QEMU;analyzer tool;optimized embedded software;resource constraint;low level power consumption;instruction set simulator;tracing module;execution information tracing;dynamic binary translation;log file;ARM architecture;run-time","","1","","8","","2 Dec 2013","","","IEEE","IEEE Conferences"
"Efficient Recovery of Missing Events","J. Wang; S. Song; X. Zhu; X. Lin; J. Sun","Tsinghua National Laboratory for Information Science and Technology, KLiss, MoE, School of Software, Tsinghua University, Beijing, China; Tsinghua National Laboratory for Information Science and Technology, KLiss, MoE, School of Software, Tsinghua University, Beijing, China; Tsinghua National Laboratory for Information Science and Technology, KLiss, MoE, School of Software, Tsinghua University, Beijing, China; University of New South Wales, Sydney, NSW, Australia; Tsinghua National Laboratory for Information Science and Technology, KLiss, MoE, School of Software, Tsinghua University, Beijing, China","IEEE Transactions on Knowledge and Data Engineering","5 Oct 2016","2016","28","11","2943","2957","For various entering and transmission issues raised by human or system, missing events often occur in event data, which record execution logs of business processes. Without recovering the missing events, applications such as provenance analysis or complex event processing built upon event data are not reliable. Following the minimum change discipline in improving data quality, it is also rational to find a recovery that minimally differs from the original data. Existing recovery approaches fall short of efficiency owing to enumerating and searching over all of the possible sequences of events. In this paper, we study the efficient techniques for recovering missing events. According to our theoretical results, the recovery problem appears to be NP-hard. Nevertheless, advanced indexing, pruning techniques are developed to further improve the recovery efficiency. The experimental results demonstrate that our minimum recovery approach achieves high accuracy, and significantly outperforms the state-of-the-art technique for up to five orders of magnitudes improvement in time performance.","1558-2191","","10.1109/TKDE.2016.2594785","Tsinghua University Initiative Scientific Research Program; Tsinghua National Laboratory Special Fund for Big Data Science and Technology; National Key Technology Support Program; NSFC; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7523405","Data repairing;event data processing;petri net","Business;Petri nets;Engineering drawings;Indexes;Routing;Sun;Data mining","business data processing;computational complexity;data integrity;data mining;indexing;optimisation;query processing","missing event data recovery;business process;data quality;NP-hard problem;indexing technique;pruning technique;data querying;data mining","","12","","28","","27 Jul 2016","","","IEEE","IEEE Journals"
"Do Code Smells Impact the Effort of Different Maintenance Programming Activities?","Z. Soh; A. Yamashita; F. Khomh; Y. Guéhéneuc","NA; Dept. of Inf. Technol., Oslo & Akershus Univ. Coll. of Appl. Sci., Oslo, Norway; NA; NA","2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)","23 May 2016","2016","1","","393","402","Empirical studies have shown so far that code smells have relatively low impact over maintenance effort at file level. We surmise that previous studies have found low effects of code smells because the effort considered is a ""sheer-effort"" that does not distinguish between the kinds of developers' activities. In our study, we investigate the effects of code smells at the activity level. Examples of activities are: reading, editing, searching, and navigating, which are performed independently over different files during maintenance. We conjecture that structural attributes represented in the form of different code smells do indeed have an effect on the effort for performing certain kinds of activities. To verify this conjecture, we revisit a previous study about the impact of code smell on maintenance effort, using the same dataset, but considering activity effort. Six professional developers were hired to perform three maintenance tasks on four functionally equivalent Java Systems. Each developer performs two maintenance tasks. During maintenance task, we monitor developers' logs. Then, we define an annotation schema to identify developers' activities and assess whether code smells affect different maintenance activities. Results show that different code smells affect differently activity effort. Yet, the size of the changes performed to solve the task impacts the effort of all activities more than code smells and file size. While code smells impact the editing and navigating effort more than file size, the file size impacts the reading and searching activities more than code smells. One major implication of these results is that if code smells indeed affect the effort of certain kinds of activities, it means that their effects are contingent on the type of maintenance task at hand, where some kinds of activities will become more predominant than others.","","978-1-5090-1855-0","10.1109/SANER.2016.103","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7476660","code smells;programming activity;maintenance effort;program comprehension;software quality","Maintenance engineering;Navigation;Java;Programming;Software;Companies;Software engineering","file organisation;Java;software maintenance;software quality","maintenance programming activities;code smells;file level;structural attributes;maintenance effort;functionally equivalent Java systems;developer log monitoring;annotation schema;file size","","14","","40","","23 May 2016","","","IEEE","IEEE Conferences"
"Using Multi-Level Information in Hierarchical Process Mining: Balancing Behavioural Quality and Model Complexity","S. J. J. Leemans; K. Goel; S. J. van Zelst","Queensland University of Technology,Australia; Queensland University of Technology,Australia; Fraunhofer FIT & RWTH University,Germany","2020 2nd International Conference on Process Mining (ICPM)","22 Oct 2020","2020","","","137","144","Process mining techniques aim to derive knowledge of the execution of processes, by means of automated analysis of behaviour recorded in event logs. A well-known challenge in process mining is to strike an adequate balance between the behavioural quality of a discovered model compared to the event log and the model's complexity as perceived by stakeholders. At the same time, events typically contain multiple attributes related to parts of the process at different levels of abstraction, which are often ignored by existing process mining techniques, resulting in either highly complex and/or incomprehensible process mining results. This paper addresses this problem by extending process mining to use event-level attributes readily available in event logs. We introduce (1) the concept of multi-level logs and generalise existing hierarchical process models, which support multiple modelling formalisms and notions of activities in a single model, (2) a framework, instantiation and implementation for process discovery of hierarchical models, and (3) a corresponding conformance checking technique. The resulting framework has been implemented as a plug-in of the open-source process mining framework ProM, and has been evaluated qualitatively and quantitatively using multiple real-life event logs.","","978-1-7281-9832-3","10.1109/ICPM49681.2020.00029","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9230000","Process mining;process model complexity;process discovery;hierarchical process models;multi-level event logs","Complexity theory;Petri nets;Stakeholders;Semantics;Business;Standards;Integrated circuit modeling","business data processing;data mining;information systems;public domain software","multilevel information;hierarchical process mining;behavioural quality;model complexity;event log;process mining techniques;hierarchical process models;process discovery;open-source process mining framework;ProM;conformance checking technique","","","","39","","22 Oct 2020","","","IEEE","IEEE Conferences"
"An Automated Approach to Estimating Code Coverage Measures via Execution Logs","B. Chen; J. Song; P. Xu; X. Hu; Z. M. J. Jiang","York University,Toronto,Canada; Baidu Inc.,Beijing,China; Baidu Inc.,Beijing,China; Baidu Inc.,Beijing,China; York University,Toronto,Canada","2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE)","17 Feb 2020","2018","","","305","316","Software testing is a widely used technique to ensure the quality of software systems. Code coverage measures are commonly used to evaluate and improve the existing test suites. Based on our industrial and open source studies, existing state-of-the-art code coverage tools are only used during unit and integration testing due to issues like engineering challenges, performance overhead, and incomplete results. To resolve these issues, in this paper we have proposed an automated approach, called LogCoCo, to estimating code coverage measures using the readily available execution logs. Using program analysis techniques, LogCoCo matches the execution logs with their corresponding code paths and estimates three different code coverage criteria: method coverage, statement coverage, and branch coverage. Case studies on one open source system (HBase) and five commercial systems from Baidu and systems show that: (1) the results of LogCoCo are highly accurate (> 96% in seven out of nine experiments) under a variety of testing activities (unit testing, integration testing, and benchmarking); and (2) the results of LogCoCo can be used to evaluate and improve the existing test suites. Our collaborators at Baidu are currently considering adopting LogCoCo and use it on a daily basis.","2643-1572","978-1-4503-5937-5","10.1145/3238147.3238214","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9000084","software testing;logging code;test coverage;empirical studies;software maintenance","","program diagnostics;program testing","software testing;software systems;open source studies;integration testing;automated approach;readily available execution logs;LogCoCo;code paths;code coverage criteria;method coverage;statement coverage;branch coverage;open source system;testing activities;unit testing","","2","","78","","17 Feb 2020","","","IEEE","IEEE Conferences"
"An Extensible Parsing Pipeline for Unstructured Data Processing","S. Jain; A. de Buitléir; E. Fallon","Software Research Institute, Athlone Institute of Technology,Athlone,Ireland; Network Management Lab,Ericsson,Athlone,Ireland; Software Research Institute, Athlone Institute of Technology,Athlone,Ireland","2021 23rd International Conference on Advanced Communication Technology (ICACT)","10 Mar 2021","2021","","","312","318","Network monitoring and diagnostics systems depict the running system's state and generate enormous amounts of unstructured data through log files, print statements, and other reports. It is not feasible to manually analyze all these files due to limited resources and the need to develop custom parsers to convert unstructured data into desirable file formats. Prior research focuses on rule-based and relationship-based parsing methods to parse unstructured data into structured file formats; these methods are labor-intensive and need large annotated datasets. This paper presents an unsupervised text processing pipeline that analyses such text files, removes extraneous information, identifies tabular components, and parses them into a structured file format. The proposed approach is resilient to changes in the data structure, does not require training data, and is domain-independent. We experiment and compare topic modeling and clustering approaches to verify the accuracy of the proposed technique. Our findings indicate that combining similarity and clustering algorithms to identify data components had better accuracy than topic modeling.","1738-9445","979-11-88428-06-9","10.23919/ICACT51234.2021.9370654","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9370654","Unsupervised Data Mining;Information Extraction;Clustering;Topic Modeling","Pipelines;Training data;Machine learning;Metadata;Data mining;Monitoring;Text processing","data mining;data structures;grammars;learning (artificial intelligence);natural language processing;pattern clustering;text analysis","unsupervised text processing pipeline;parse unstructured data;relationship-based;desirable file formats;custom parsers;print statements;log files;running system;diagnostics systems;unstructured data processing;extensible parsing pipeline;data components;training data;data structure;structured file format;analyses such text files","","","","22","","10 Mar 2021","","","IEEE","IEEE Conferences"
"From scripts to specifications: the evolution of a flight software testing effort","A. Groce; K. Havelund; M. Smith","Oregon State University, Corvallis, OR; California Institute of Technology, Pasadena, CA; California Institute of Technology, Pasadena, CA","2010 ACM/IEEE 32nd International Conference on Software Engineering","27 Oct 2011","2010","2","","129","138","This paper describes the evolution of a software testing effort during a critical period for the flagship Mars Science Laboratory rover project at the Jet Propulsion Laboratory. Formal specification for post-run analysis of log files, using a domain-specific language, LogScope, replaced scripted real-time analysis. Log analysis addresses the key problems of on-the-fly approaches and cleanly separates specification and execution. Mining the test repository suggested the inadequacy of the scripted approach, and encouraged a partly engineer-driven development. LogScope development should hold insights for others facing the tight deadlines and reactionary nature of testing for critical projects. LogScope received a JPL Mariner Award for ""improving productivity and quality of the MSL Flight Software"" and has been discussed as an approach for other flight missions. We note LogScope features that most contributed to ease of adoption and effectiveness. LogScope is general and can be applied to any software producing logs.","1558-1225","978-1-60558-719-6","10.1145/1810295.1810314","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6062146","Python;development practices;logs;runtime verification;space flight software;temporal logic;test infrastructure;testing","Software;Telemetry;Space vehicles;Laboratories;Libraries;Semantics","aerospace computing;data mining;formal specification;program diagnostics;program testing","software testing effort;flagship Mars Science Laboratory rover project;Jet Propulsion Laboratory;post-run analysis;log files;domain specific language;LogScope;scripted real-time analysis;log analysis;on-the-fly approach;test repository;engineer driven development;LogScope development;JPL Mariner Award;MSL flight software;flight missions;formal specification","","7","","13","","27 Oct 2011","","","IEEE","IEEE Conferences"
"A malicious software evaluation system based on behavior association","Yunlong Wu; Dong Cui; Qiang Zhang","School of Computer, Wuhan University, China; School of Information & Electronic Engineering, Hebei University of Engineering, Handan, China; School of Information & Electronic Engineering, Hebei University of Engineering, Handan, China","2010 International Conference on Optics, Photonics and Energy Engineering (OPEE)","12 Jul 2010","2010","1","","258","260","The malicious software detection based on characteristics matching cannot find unknown malicious software and the origin of harms. To solve this problem, a method is proposed to detect malicious software according to the subject-object association. It uses SSDT HOOK technology to monitor the software behaviors and records those into logs. To improve the accuracy of detection, it proposes a risk assessment algorithm. First it does the subject-object behavior association in logs, and then makes the risk assessment for every subject to find the origin of harms.","2158-7442","978-1-4244-5234-7","10.1109/OPEE.2010.5508137","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5508137","behavior association;malicious software detection;SSDT HOOK;risk assessment","Software systems;Kernel;Power engineering and energy;Risk management;Monitoring;Software safety;Photonics;Optical computing;Statistics;Software design","security of data;software performance evaluation","malicious software evaluation system;behavior association;subject object association;SSDT HOOK technology;software behaviors;risk assessment;malicious software detection","","","","6","","12 Jul 2010","","","IEEE","IEEE Conferences"
"Secure Knowledge and Cluster-Based Intrusion Detection Mechanism for Smart Wireless Sensor Networks","A. Mehmood; A. Khanan; M. M. Umar; S. Abdullah; K. A. Z. Ariffin; H. Song","Institute of Information Technology, Kohat University of Science and Technology, Kohat, Pakistan; Centre for Artificial Intelligence Technology, Research Centre for Software and Management, Faculty of Information Science and Technology, Universiti Kebangsaan Malaysia, Bangi, Malaysia; Department of Management and Information Technology, Jubail Industrial College, Jubail, Saudi Arabia; Centre for Artificial Intelligence Technology, Research Centre for Software and Management, Faculty of Information Science and Technology, Universiti Kebangsaan Malaysia, Bangi, Malaysia; Centre for Artificial Intelligence Technology, Research Centre for Software and Management, Faculty of Information Science and Technology, Universiti Kebangsaan Malaysia, Bangi, Malaysia; Department of Electrical, Computer, Software, and Systems Engineering, Embry-Riddle Aeronautical University, Daytona Beach, FL, USA","IEEE Access","2 Mar 2018","2018","6","","5688","5694","Wireless sensor networks, due to their nature, are more prone to security threats than other networks. Developments in WSNs have led to the introduction of many protocols specially developed for security purposes. Most of these protocols are not efficient in terms of putting an excessive computational and energy consumption burden on small nodes in WSNs. This paper proposes a knowledge-based context-aware approach for handling the intrusions generated by malicious nodes. The system operates on a knowledge base, located at the base station, which is used to store the events generated by the nodes inside the network. The events are categorized and the cluster heads (CHs) are acknowledged to block maliciously repeated activities generated. The CHs can also get informational records about the maliciousness of intruder nodes by using their inference engines. The mechanism of events logging and analysis by the base station greatly affects the performance of nodes in the network by reducing the extra security-related load on them.","2169-3536","","10.1109/ACCESS.2017.2770020","Ministry of Higher Education, Malaysia; Universiti Kebangsaan, Malaysia; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8094860","Intrusion detection system;knowledge base;cluster based WSN;security","Base stations;Wireless sensor networks;Knowledge based systems;Security;Monitoring;Engines;Protocols","power consumption;security of data;wireless sensor networks","cluster heads;intruder nodes;smart wireless sensor networks;security threats;malicious nodes;WSN;computational energy consumption;cluster-based intrusion detection;knowledge-based context-aware approach;maliciously repeated activities blocking;informational records;inference engines;events logging;events analysis","","5","","27","","3 Nov 2017","","","IEEE","IEEE Journals"
"Log file's centralization to improve cloud security","M. Amar; M. Lemoudden; B. El Ouahidi","Laboratoire de Recherche Informatique, Mohammed-V University, Faculty of Sciences, B.O. 1014, Rabat, Morocco; Laboratoire de Recherche Informatique, Mohammed-V University, Faculty of Sciences, B.O. 1014, Rabat, Morocco; Laboratoire de Recherche Informatique, Mohammed-V University, Faculty of Sciences, B.O. 1014, Rabat, Morocco","2016 2nd International Conference on Cloud Computing Technologies and Applications (CloudTech)","9 Feb 2017","2016","","","178","183","The usage of cloud-computing architectures and characteristics has been enhanced in recent years. This approach brings the availability of storage and user services as needed. But it also brings many drawbacks that put the privacy and the security of the system at stake. Log file generation has Big Data characteristics that should be considered for upgrade from a manual method to an automatic one based on Big Data solutions. Therefore, this paper proposes a log file centralization and a diagnostic approach based on the misuse and the anomaly detection techniques, which will improve the detection of attacks. The FP-growth algorithm presented will ensure the prevention of consecutive violations.","","978-1-4673-8894-8","10.1109/CloudTech.2016.7847696","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7847696","Attack;Cloud-computing;FP-Growth;Log;Security","Manuals;Security;Cloud computing;Media;Protocols;Servers;Computers","Big Data;cloud computing;file organisation;security of data;software architecture;user interfaces","log file centralization;cloud security;cloud computing architectures;user services;Big Data","","5","","24","","9 Feb 2017","","","IEEE","IEEE Conferences"
"FLogFS: A lightweight flash log file system","B. Nahill; Z. Zilic","Lincoln Laboratory, Massachusetts Institute of Technology, Lexington, MA; Integrated Microsystems Laboratory, McGill University, Montreal, QC, Canada","2015 IEEE 12th International Conference on Wearable and Implantable Body Sensor Networks (BSN)","19 Oct 2015","2015","","","1","6","Non-volatile storage is an important element for many low-power wearable sensor platforms for data aggregation, audit logs, and to enable offline analytics and debugging. NAND flash is an increasingly appealing choice due to its low cost, low power consumption, and small footprint; but it requires high software complexity and overhead to use effectively in such resource-constrained environments. Many wearable processing systems have limited program memory and RAM, on the order of kilobytes, however current NAND flash file systems require 10s of kilobytes of code and RAM to provide rudimentary logging facilities to SD cards or flash memories. By constraining access patterns to practical cases for logging and optimizing operations around the timing needs of real-time systems, we can do better. This paper presents the design and evaluation of a NAND flash logging file system, available freely under a permissive license, in only a few kilobytes of ROM and a few hundred bytes of RAM.","2376-8894","978-1-4673-7201-5","10.1109/BSN.2015.7299353","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7299353","","Ash;Resource management;Random access memory;Timing;Real-time systems;Logic gates;Error correction codes","body sensor networks;medical computing;random-access storage;read-only storage;telemedicine","FLogFS;lightweight flash log file system;low-power wearable sensor platform;data aggregation;audit log;offline analytics;debugging;low power consumption;footprint;wearable processing system;program memory;RAM;logging facility;SD card;flash memory;NAND flash logging file system design;NAND flash logging file system evaluation;ROM","","1","","10","","19 Oct 2015","","","IEEE","IEEE Conferences"
"Interconnected Personal Health Record Ecosystem Using IoT Cloud Platform and HL7 FHIR","J. Hong; P. Morris; J. Seo","Software R&D Center Samsung Electron. Co., Ltd., Yongin, South Korea; Software R&D Center Samsung Electron. Co., Ltd., Yongin, South Korea; Software R&D Center Samsung Electron. Co., Ltd., Yongin, South Korea","2017 IEEE International Conference on Healthcare Informatics (ICHI)","14 Sep 2017","2017","","","362","367","Personal health record (PHR) systems have yet to reach mass adoption despite years of varied implementations. In this paper, we examine the limitations of current PHRs and propose an interconnected PHR system built in adherence with healthcare data communication standards and an IoT Cloud platform. The new ecosystem includes: an interoperable hospital information system to store electronic medical records (EMRs) and transfer them to a PHR system via email; a public cloud that supports encrypted data sharing for big data analysis services; a PHR gateway repository based on an IoT module which communicates with hospitals and public clouds; and a mobile application to manage and view PHR. This system can store and share raw EMR and life log data based on the healthcare communication standard ""HL7 FHIR"". To validate the usability of the interconnected PHR system, a clinical trial is underway to develop an obesity management model from individuals' hospital-provided genomic data, on the one hand, and diet and exercise logs gathered by the mobile application, on the other. We demonstrate how the proposed IoT Cloud based PHR ecosystem is interoperable and practical for promoting healthcare big data analysis services.","","978-1-5090-4881-6","10.1109/ICHI.2017.82","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8031175","PHR;healthcare;IoT;cloud;ARTIK;HL7;FHIR","Cloud computing;Hospitals;Mobile applications;Biomedical imaging;Ecosystems;Information systems","Big Data;biomedical communication;cloud computing;cryptography;data analysis;electronic health records;health care;Internet of Things;internetworking;medical computing;mobile computing","interconnected personal health record ecosystem;IoT Cloud platform;HL7 FHIR;personal health record systems;interconnected PHR system;healthcare data communication standards;interoperable hospital information system;electronic medical records;public cloud;PHR gateway repository;IoT module;mobile application;healthcare communication standard;PHR ecosystem;healthcare big data analysis services;obesity management model;hospital-provided genomic data;diet logs;exercise logs;IoT Cloud based PHR ecosystem;encrypted data sharing;big data analysis services;life log data","","5","","23","","14 Sep 2017","","","IEEE","IEEE Conferences"
"A Method to Automatically Filter Log Evidences for Intrusion Forensics","J. Zhang; X. Fu; X. Du; B. Luo; Z. Zhao","Software Inst., Nanjing Univ., Nanjing, China; Software Inst., Nanjing Univ., Nanjing, China; Dept. of Comput. & Inf. Sci., Temple Univ., Philadelphia, PA, USA; Software Inst., Nanjing Univ., Nanjing, China; Software Inst., Nanjing Univ., Nanjing, China","2013 IEEE 33rd International Conference on Distributed Computing Systems Workshops","12 Dec 2013","2013","","","39","44","An important data source for intrusion forensics is various types of logs from the systems and networks being investigated. However, there are still many problems when using these logs for forensic analysis. Firstly, with the development of computers and Internet, intrusion behaviors involve more types and more quantities of logs, and these massive and complex log evidences make forensics analyst overwhelmed. Secondly, among the large number of logs that investigators need to analyze, the data related to criminal behaviors only accounts for a very small proportion and most of the rest data are useless records resulted from normal behaviors. Large amount of forensic data and high proportion of useless records make it very difficult to investigate and collect evidences. In addition, this makes criminal behaviors that submerged in a large amount of useless records easily overlooked. This paper introduces a new method for the reduction of candidate log evidences for intrusion forensics. Its main idea is to extract the key attribute fields as features of log records and assign a score to each log record. This score is used to indicate the degree of redundancy of the record. The greater the score is, the more likely the records are redundant. Our experiments based on Darpa2000 and Snort real-world data show that this method can significantly reduce the interference caused by useless data for forensic analysis: it removes 57% and 82% useless data in Darpa2000 and the Snort real-world data, respectively.","2332-5666","978-1-4799-3248-1","10.1109/ICDCSW.2013.7","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6679860","intrusion forensics;log evidences;Darpa2000;Snort real-world data","Forensics;Algorithm design and analysis;Itemsets;Educational institutions;Software;Feature extraction","digital forensics","log evidences filtering;intrusion forensics;Internet;criminal behaviors;candidate log evidence reduction;Darpa2000;Snort real-world data;interference reduction","","","","10","","12 Dec 2013","","","IEEE","IEEE Conferences"
"AVA: Supporting Debugging with Failure Interpretations","F. Pastore; L. Mariani","Univ. of Milano Bicocca, Milan, Italy; Univ. of Milano Bicocca, Milan, Italy","2013 IEEE Sixth International Conference on Software Testing, Verification and Validation","29 Jul 2013","2013","","","416","421","Several debugging techniques can be used to automatically identify the code fragments or the runtime events likely responsible of a failure. These techniques are useful, but can help reducing the debugging effort only to a given extent. In fact, even when these techniques are successful, software developers still have to invest a lot of effort in understanding if and why something detected as suspicious is really wrong. In this paper we present the tool implementing the AVA technique. AVA, compared to other approaches dedicated to automatic debugging, in addition to automatically identifying the events likely responsible of a failure, generates an explanation about why these events have been considered suspicious. This explanation can be used by developers to quickly discard imprecise outputs and more effectively work on the relevant anomalies.","2159-4848","978-0-7695-4968-2","10.1109/ICST.2013.58","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6569755","debugging;failure interpretation;anomalies;log file","Debugging;Production facilities;Software;Data models;Automata;Context;Classification algorithms","program debugging;system recovery","failure interpretation;debugging technique;code fragment;runtime event;software development;AVA technique;automatic debugging;anomalies","","5","","23","","29 Jul 2013","","","IEEE","IEEE Conferences"
"Quality Measurement of Software Based on Characteristics of Functionality, Reliability, and Maintainability","Sholiq; D. A. Dewangga; A. P. Subriadi; F. A. Muqtadiroh","department of Information Systems, Kampus ITS Sukolilo-Surabaya, Surabaya, 60111, Indonesia; department of Information Systems, Kampus ITS Sukolilo-Surabaya, Surabaya, 60111, Indonesia; department of Information Systems, Kampus ITS Sukolilo-Surabaya, Surabaya, 60111, Indonesia; department of Information Systems, Kampus ITS Sukolilo-Surabaya, Surabaya, 60111, Indonesia","2018 International Seminar on Research of Information Technology and Intelligent Systems (ISRITI)","14 Oct 2019","2018","","","192","197","The purpose of this study is to determine the level of quality of software “application of software cost estimation ” which is a product of previous research that will be opened to the public. Testing uses the ISO 9126-1 standard that applies internationally. The ISO standard is chosen because it has complete quality characteristics. The characteristics tested in this study are functionality, reliability, and maintainability. These three characteristics are the factors that most influence the quality of the application. The results of this study are quality measurement values based on ISO 9126-1 and recommendations for improving application quality. After measuring the application/software quality, we know that the characteristics of functionality and reliability are very good, while the maintainability characteristics are still in the bad category. Required to add some features such as to record the activity log, to diagnose the application, and to test features which are installed in the application. Finally, the procedures, methods, and metrics used in this study can be adopted for other software quality testing. Adjustments, as needed, are needed with the condition of the software object being tested.","","978-1-5386-7422-2","10.1109/ISRITI.2018.8864449","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8864449","Software Quality;Functionality;Reliability;Maintainability;ISO 9126","Software;ISO Standards;Software reliability;Testing;Software measurement","ISO standards;program testing;software cost estimation;software quality;software reliability;software standards","quality measurement values;application quality;maintainability characteristics;software quality testing;software application;software cost estimation;ISO 9126-1 standard;software reliability;software functionality","","","","22","","14 Oct 2019","","","IEEE","IEEE Conferences"
"Improving Continuous Software Development in Academic Scenarios using GitHubTracker","L. Silvestre; J. M. Vera","Universidad de Talca,Facultad de Ingeniería,Depto. de Ciencias de la Computación,Curicó,Chile; Universidad de Talca,Facultad de Ingeniería,Depto. de Ciencias de la Computación,Curicó,Chile","2019 38th International Conference of the Chilean Computer Science Society (SCCC)","23 Jan 2020","2019","","","1","8","Continuous software development enables the early identification of potential problems in terms of software code, system integration, and team communication. In the field of software engineering education, usually, software development depends on the particular methodology for evaluating the technical performance of students in terms of software code and system integration of software development. Actually, GitHub is used for mining the information stored in GitHub's event logs, trying to understand how its students to performance on software development. However, there are few studies and experiences describing how GitHub can support continuous software development. In this paper, we present a case study in two software engineering courses at the University of Talca using GitHubTracker. GitHubTracker is a tool for improving continuous software development using the main GitHub's features. The results indicate that student teams were more effective in terms of performance and productivity than the other teams.","1522-4902","978-1-7281-5613-2","10.1109/SCCC49216.2019.8966391","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8966391","GitHub;software engineering;continuous software development;software repository","Software;Measurement;Productivity;Tools;Software engineering;Complexity theory;Testing","computer science education;educational courses;educational institutions;software engineering;source code (software)","continuous software development;GitHubTracker;software code;system integration;software engineering education;software engineering courses;University of Talca","","","","24","","23 Jan 2020","","","IEEE","IEEE Conferences"
"Yaiao Application Collecting Log File RF 4G Android-Based Replacing Data Cables to Laptop on Traditional RF Drive Test Models","A. Wijaya; Y. Saragih; I. Lammada; R. Hidayat; S. A. Elisabet; N. Thi Nhu Van","Universitas Singaperbangsa Karawang,Electrical of Engineering,Karawang,Indonesia; Universitas Singaperbangsa Karawang,Electrical of Engineering,Karawang,Indonesia; Universitas Singaperbangsa Karawang,Electrical of Engineering,Karawang,Indonesia; Universitas Singaperbangsa Karawang,Electrical of Engineering,Karawang,Indonesia; Universitas Singaperbangsa Karawang,Electrical of Engineering,Karawang,Indonesia; Electric Power University,Faculty of Engineering,Hanoi,Vietnam","2020 3rd International Conference on Mechanical, Electronics, Computer, and Industrial Technology (MECnIT)","14 Aug 2020","2020","","","271","275","The number of network users is expected to continue to grow an average of 3% year-on-year, mainly because the market is developing. Drive test is the process of monitoring a 4G frequency radio network to monitor any changes that occur in the operation of network devices. RF drive test equipment itself is quite a lot and is quite expensive. In this research, is to make the function of the Yaiao application to accommodate 4G RF files that replace the data cable connected to the laptop. The drive test tool is simpler because it only uses an Android smartphone with G-Net Report Pro Software and uses stages by running the Yaiao application and measuring the signal using G-Net Report Pro Software based on the specified location and then the Yaiao software will detect log files of the results of a successful RF collected on the smartphone's internal storage and sends RF log files directly to the server. If sending fails, there is also an upload menu that works to send files manually. This application serves to replace the function of the cable by using the internet network and replace the measurement data storage using internal storage on smartphones and a success rate of 70%.","","978-1-7281-7403-7","10.1109/MECnIT48290.2020.9166601","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9166601","Radio Frequency (RF);Yaiao application;Smartphone;Server.","","4G mobile communication;Internet;mobile computing;smart phones;telecontrol;test equipment","Yaiao application collecting log file RF 4G Android-based;data cable;traditional RF drive test models;network users;year-on-year;4G frequency radio network;network devices;RF drive test equipment;drive test tool;Android smartphone;G-Net Report Pro Software;Yaiao software;log files;successful RF;internet network;measurement data storage","","","","11","","14 Aug 2020","","","IEEE","IEEE Conferences"
"A Framework for Event Log Generation and Knowledge Representation for Process Mining in Healthcare","R. Gatta; M. Vallati; J. Lenkowicz; C. Casà; F. Cellini; A. Damiani; V. Valentini","Ist. di Radiologia, Univ. Cattolica del Sacro Cuore, Rome, Italy; Sch. of Comput. & Eng., Univ. of Huddersfield, Huddersfield, UK; Ist. di Radiologia, Univ. Cattolica del Sacro Cuore, Rome, Italy; Ist. di Radiologia, Univ. Cattolica del Sacro Cuore, Rome, Italy; Polo Sci. Oncologiche, Policlinico Univ. A. Gemelli, Rome, Italy; Ist. di Radiologia, Univ. Cattolica del Sacro Cuore, Rome, Italy; Ist. di Radiologia, Univ. Cattolica del Sacro Cuore, Rome, Italy","2018 IEEE 30th International Conference on Tools with Artificial Intelligence (ICTAI)","16 Dec 2018","2018","","","647","654","Process Mining is of growing importance in the healthcare domain, where the quality of delivered services depends on the suitable and efficient execution of processes encoding the vast amount of clinical knowledge gained via the evidence-based medicine paradigm. In particular, to assess and measure the quality of delivered treatments, there is a strong interest in tools able to perform conformance checking. In process mining for the healthcare domain, a number of major challenges are posed by: (i) the complexity of involved data, that refers to patients' aspects such as disease, behaviour, clinical history, psychology, etc; (ii) the availability of data, that come from the heterogeneous, fragmented and scant connected healthcare system; and (iii) the wide range of available standards for communication (DICOM, IHE, etc.) or data representation (ICD9, SNOMED, etc.) purposes. To effectively perform process mining in the healthcare domain, it is crucial to build event logs capturing all the steps of running processes, which have to be derived by the knowledge stored in the Electronic Health Records. It is therefore crucial to cope with aforementioned data-related challenges. In this paper, we aim at supporting the exploitation of process mining in the healthcare domain, particularly with regards to conformance checking. We therefore introduce a set of specifically-designed techniques, provided as a suite of software packages written in R. In particular, the suite provides a flexible and agile way to automatically and reliably build Event Log from clinical data sources, and to effectively perform conformance checking.","2375-0197","978-1-5386-7449-9","10.1109/ICTAI.2018.00103","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8576101","Process Mining;Healthcare;Conformance Checking","Data mining;Guidelines;Tools;Task analysis;Diseases;Pipelines","data mining;diseases;health care;knowledge representation;medical computing;medical information systems","event log generation;running processes;scant connected healthcare system;fragmented healthcare system;heterogeneous healthcare system;conformance checking;healthcare domain;process mining","","","","27","","16 Dec 2018","","","IEEE","IEEE Conferences"
"LibSearchNet: Analyses of library log files to identify search flows","M. Csernoch; G. Bujdosó; M. Borbély; E. Dani; M. Némethi-Takács; K. Koltay; L. Balázs","Dept. of Library and Information Science, Faculty of Informatics, University of Debrecen, Hungary; Dept. of Library and Information Science, Faculty of Informatics, University of Debrecen, Hungary; Dept. of Library and Information Science, Faculty of Informatics, University of Debrecen, Hungary; Dept. of Library and Information Science, Faculty of Informatics, University of Debrecen, Hungary; Dept. of Library and Information Science, Faculty of Informatics, University of Debrecen, Hungary; University and National Library, University of Debrecen, Hungary; University and National Library, University of Debrecen, Hungary","2013 IEEE 4th International Conference on Cognitive Infocommunications (CogInfoCom)","23 Jan 2014","2013","","","543","548","The online libraries of the next generation library users would mean creating 3D virtual spaces where they would navigate as they do in traditional libraries. To achieve this goal the surface has to be built and this new environment has offer options with which non-library educated users would navigate effectively. Reaching this state the activities of present day online library users and library systems should be thoroughly analyzed. The sources of the analyses are the library log files. Well-designed log files would reveal, on one hand, the patterns of the users' activities which are crucial in building algorithms for the mental representation of searches. On the other hand, the analyses of these log files would shed light on the operation of the system.","","978-1-4799-1546-0","10.1109/CogInfoCom.2013.6719307","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6719307","log file analysis;user- and system-launched searches;VirCA;search net;users' mental search lexikon","Libraries;Three-dimensional displays;Conferences;Surface treatment;Educational institutions;Software;Search problems","data analysis;digital libraries;information retrieval;Internet;virtual reality","LibSearchNet;library log file analysis;online libraries;next generation library users;3D virtual spaces;nonlibrary educated users;library systems;user activity pattern;mental search representation","","3","","18","","23 Jan 2014","","","IEEE","IEEE Conferences"
"Log Clustering Based Problem Identification for Online Service Systems","Q. Lin; H. Zhang; J. Lou; Y. Zhang; X. Chen","Microsoft Res., Beijing, China; Microsoft Res., Beijing, China; Microsoft Res., Beijing, China; Microsoft Corp., Redmond, WA, USA; Microsoft Res., Beijing, China","2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)","23 Mar 2017","2016","","","102","111","Logs play an important role in the maintenance of large-scale online service systems. When an online service fails, engineers need to examine recorded logs to gain insights into the failure and identify the potential problems. Traditionally, engineers perform simple keyword search (such as “error” and “exception”) of logs that may be associated with the failures. Such an approach is often time consuming and error prone. Through our collaboration with Microsoft service product teams, we propose LogCluster, an approach that clusters the logs to ease log-based problem identification. LogCluster also utilizes a knowledge base to check if the log sequences occurred before. Engineers only need to examine a small number of previously unseen, representative log sequences extracted from the clusters to identify a problem, thus significantly reducing the number of logs that should be examined, meanwhile improving the identification accuracy. Through experiments on two Hadoop-based applications and two large-scale Microsoft online service systems, we show that our approach is effective and outperforms the state-of-the-art work proposed by Shang et al. in ICSE 2013. We have successfully applied LogCluster to the maintenance of many actual Microsoft online service systems. In this paper, we also share our success stories and lessons learned.","","978-1-4503-4205-6","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7883294","Logs;Problem Identification;Log Clustering;Diagnosis;Online Service System","Production;Keyword search;Containers;Testing;Servers;Maintenance engineering;Collaboration","data handling","Microsoft online service systems;Hadoop-based applications;log sequences;log-based problem identification;LogCluster;Microsoft service product teams;recorded logs;large-scale online service systems;maintenance;log clustering based problem identification","","5","","30","","23 Mar 2017","","","IEEE","IEEE Conferences"
"Identifying Candidate Routines for Robotic Process Automation from Unsegmented UI Logs","V. Leno; A. Augusto; M. Dumas; M. La Rosa; F. M. Maggi; A. Polyvyanyy","University of Melbourne,Australia; University of Melbourne,Australia; University of Tartu,Estonia; University of Melbourne,Australia; Free University of Bozen-Bolzano,Italy; University of Melbourne,Australia","2020 2nd International Conference on Process Mining (ICPM)","22 Oct 2020","2020","","","153","160","Robotic Process Automation (RPA) is a technology to develop software bots that automate repetitive sequences of interactions between users and software applications (a.k. a. routines). To take full advantage of this technology, organizations need to identify and to scope their routines. This is a challenging endeavor in large organizations, as routines are usually not concentrated in a handful of processes, but rather scattered across the process landscape. Accordingly, the identification of routines from User Interaction (UI) logs has received significant attention. Existing approaches to this problem assume that the UI log is segmented, meaning that it consists of traces of a task that is presupposed to contain one or more routines. However, a UI log usually takes the form of a single unsegmented sequence of events. This paper presents an approach to discover candidate routines from unsegmented UI logs in the presence of noise, i.e. events within or between routine instances that do not belong to any routine. The approach is implemented as an open-source tool and evaluated using synthetic and real-life UI logs.","","978-1-7281-9832-3","10.1109/ICPM49681.2020.00031","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9229975","Robotic process automation;robotic process mining;user interaction log","Task analysis;Data mining;Automation;Robots;Organizations;Pipelines;Payloads","business data processing;intelligent robots;learning (artificial intelligence);public domain software;software agents;software engineering;system monitoring;user interfaces","robotic process automation;UI logs;software applications;user interaction logs;candidate routine identification;RPA;software development;open-source tool","","","","21","","22 Oct 2020","","","IEEE","IEEE Conferences"
"Software and machine learning tools for monitoring railway track switch performance","N. P. Wright; R. Gan; C. McVae","MPEC Technology Ltd, United Kingdom; Sydney Trains, Australia; Translink, Northern Ireland","7th IET Conference on Railway Condition Monitoring 2016 (RCM 2016)","8 May 2017","2016","","","1","7","Trackside data logging hardware is often used in the UK, and increasingly elsewhere in the world, to record and transmit processed condition data from track switching equipment (points) in order to gauge asset health. This paper presents a novel implementation of three tools which can be used together to make the analysis and handling of this data easier. The first of these tools is a statistical classifier which automatically assigns labels to the process data. The classifier is trained using historical data containing examples of events of interest, such as recordings taken when maintenance activity or failures have developed. In practice, the labels are used to pre-filter the data, to bring swift attention to events of interest, and to automatically create categorised datasets which can be used to analyse historical performance. Two different types of classifier are presented: a Gaussian Naïve Bayes classifier and a neural network classifier. The second tool is a simple pattern recognition algorithm which can determine when the different phases of mechanical operation in a single track switch movement occur, for example locking, unlocking, and moving. The final tool is a statistical technique which is used to extract simple features from the data and raise alarms if they indicate poor track switch performance. The effectiveness of these tools is tested using real world data taken from three different railways.","","978-1-78561-345-6","10.1049/cp.2016.1210","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7920684","Track switch;condition monitoring;statistical FDI methods;classification","","Bayes methods;belief networks;condition monitoring;Gaussian processes;learning (artificial intelligence);neural nets;pattern classification;railway engineering","railway track switch performance monitoring;machine learning tool;software tool;trackside data logging hardware;UK;track switching equipment;asset health;statistical classifier;Gaussian Naive Bayes classifier;neural network classifier;pattern recognition algorithm;feature extraction","","","","","","8 May 2017","","","IET","IET Conferences"
"A Universal Tool for Mobile Long-Time Recording, Monitoring and Analysis of Bio-Signals","C. Rasim; S. Geisler; J. Silny","Res. Center for Bioelectromagnetic Interaction (femu), RWTH Aachen Univ., Aachen, Germany; Inf. Syst., RWTH Aachen Univ., Aachen, Germany; Res. Center for Bioelectromagnetic Interaction (femu), RWTH Aachen Univ., Aachen, Germany","2011 IEEE 12th International Conference on Mobile Data Management","3 Nov 2011","2011","2","","58","60","While monitoring and analysis of bio-signals are essential in today's medical diagnostics, it is gaining even more importance with modern computer systems allowing the use of complex real-time algorithms and analysis of huge amounts of data. To ease development of such algorithms, a tool is needed which allows the mobile long-term recording of signals at reasonably high frequencies and at the same time provides a framework for a straightforward implementation of all kinds of real-time monitoring, analysis and visualization algorithms. The Measure tool developed at femu, RWTH Aachen University, consisting of a mobile data logging device and an easily extendable PC software, addresses this problem. The system has been evaluated in a case study analyzing gastrointestinal motility.","2375-0324","978-0-7695-4436-6","10.1109/MDM.2011.30","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6068496","","Software;Real time systems;Data visualization;Monitoring;Mobile communication;Indexes","bioelectric potentials;data analysis;data recording;data visualisation;medical signal processing;mobile computing;patient diagnosis","medical diagnostics;bio-signals monitoring;complex real-time algorithms;data analysis;mobile long-term recording;real-time monitoring;visualization algorithms;mobile data logging device;PC software;Universal Tool","","","","4","","3 Nov 2011","","","IEEE","IEEE Conferences"
"Analysis of Test Log Information through Interactive Visualizations","D. Castro; M. Schots",Rio de Janeiro State University; Rio de Janeiro State University,"2018 IEEE/ACM 26th International Conference on Program Comprehension (ICPC)","30 Jan 2020","2018","","","156","15610","A fundamental activity to achieve software quality is software testing, whose results are typically stored in log files. These files contain the richest and more detailed source of information for developers trying to understand failures and identify their potential causes. Analyzing and understanding the information presented in log files, however, can be a complex task, depending on the amount of errors and the variety of information. Some previously proposed tools try to visualize test information, but they have limited interaction and present a single perspective of such data. This paper presents ASTRO, an infrastructure that extracts information from a number of log files and presents it in multi-perspective interactive visualizations that aim at easing and improving the developers' analysis process. A study carried out with practitioners from 3 software test factories indicated that ASTRO helps to analyze information of interest, with less accuracy in tasks that involved assimilation of information from different perspectives. Based on their difficulties, participants also provided feedback for improving the tool.","2643-7171","978-1-4503-5714-2","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8973047","software testing;information visualization;test log analysis","","data visualisation;program testing;software quality","software quality;software testing;log files;multiperspective interactive visualizations;software test factories;ASTRO infrastructure","","","","28","","30 Jan 2020","","","IEEE","IEEE Conferences"
"Time independent query recommendations from search engine query logs","R. Umagandhi; A. V. S. Kumar","Department of Computer Technology, Kongunadu Arts and Science College, Coimbatore, India; Hindustan College of Arts and Science, Coimbatore, India","International Conference on Software Engineering and Mobile Application Modelling and Development (ICSEMA 2012)","1 Jul 2013","2012","","","1","6","Search Engine retrieves significant and essential information from the web based on the query term given by the user. The query log file contains log entry for every request given by the user to the search engine and it is maintained in the system desktop or in the proxy server. The process of mining the query log file improves the performance of the search engine. The proposed algorithm mines the query log file to discover the similarity between the query keywords and URLs in its first phase. In the second phase, the query cluster and the URL cluster is created by using the combined similarity measure generated from the first phase. The clusters recommend query to the user to frame their future queries based on their previous search histories and click through data. This combined similarity based approach also recommends the user about the URL selection for the future queries.","","978-1-84919-736-6","10.1049/ic.2012.0158","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6549326","Similarity;Query term;Search Engine;Query Cluster;URL","","data mining;Internet;pattern clustering;query processing;recommender systems;search engines","time independent query recommendations;search engine query logs;Web;query term;system desktop;proxy server;query log file mining;performance improvement;query keywords;URL cluster;similarity measure;search histories;click through data;URL selection","","","","","","1 Jul 2013","","","IET","IET Conferences"
"Heuristics-Based Process Mining on Extracted Philippine Public Procurement Event Logs","M. J. Sangil","Layertech Software Labs, Inc.,Daraga,Philippines","2020 7th International Conference on Behavioural and Social Computing (BESC)","16 Feb 2021","2020","","","1","4","Public procurement is a business process that is prone to corruption and administrative inefficiency, affecting quality of service delivery to the public. Using Bicol University's three-year procurement data as a sample, this paper explores the use of process mining on publicly-available procurement data to discover underlying structure of procurement processes of government entities in the Philippines, check for conformance with the prescribed process in the procurement law, and identify potentially problematic nodes. In this paper, event logs were generated from official public procurement data and mined with heuristics-based process mining algorithm, using free, open-sourced tools. The discovered processes revealed a concept drift in publication of contract award, a point for inspection and improvement for the agencies involved.","","978-1-7281-8605-4","10.1109/BESC51023.2020.9348306","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9348306","Procurement;Process mining;Conformance checking;Business process","Procurement;Social computing;Heuristic algorithms;Quality of service;Tools;Inspection;Data mining","business data processing;data mining;law administration;procurement;public administration;system monitoring","business process;procurement processes;Philippines;procurement law;official public procurement data;Bicol University three-year procurement data;heuristics-based process mining;process discovery;philippine public procurement event log extraction;official public procurement mining;open-sourced tool;free sourced tool","","","","20","","16 Feb 2021","","","IEEE","IEEE Conferences"
"Towards a better assessment of event logs quality","M. O. Kherbouche; N. Laga; P. Masse","Orange Gardens, 44 Avenue de la République, 92320 Châtillon, France; Orange Gardens, 44 Avenue de la République, 92320 Châtillon, France; Orange Gardens, 44 Avenue de la République, 92320 Châtillon, France","2016 IEEE Symposium Series on Computational Intelligence (SSCI)","13 Feb 2017","2016","","","1","8","It is widely observed that the poor event logs quality poses a significant challenge to the process mining project both in terms of choice of process mining algorithms and in terms of the quality of the discovered process model. Therefore, it is important to control the quality of event logs prior to conducting a process mining analysis. In this paper, we propose a qualitative model which aims to assess the quality of event logs before applying process mining algorithms. Our ultimate goal is to give process mining practitioners an overview of the quality of event logs which can help to indicate whether the event log quality is good enough to proceed to process mining and in this case, to suggest both the needed preprocessing steps and the process mining algorithm that is most tailored under such a circumstance. The qualitative model has been evaluated using both artificial and real-life case studies.","","978-1-5090-4240-1","10.1109/SSCI.2016.7849946","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7849946","event logs;process mining;process mining algorithms;qualitative model","Data mining;Complexity theory;Measurement;Algorithm design and analysis;Software algorithms;Heuristic algorithms;Finite element analysis","data mining","event logs quality;process mining project;process mining algorithms;process mining analysis","","2","","22","","13 Feb 2017","","","IEEE","IEEE Conferences"
"A Preliminary Study on Sensitive Information Exposure Through Logging","C. Zhi; J. Yin; J. Han; S. Deng","Zhejiang University,Hangzhou,China; Zhejiang University,Hangzhou,China; Zhejiang University,Hangzhou,China; Zhejiang University,Hangzhou,China","2020 27th Asia-Pacific Software Engineering Conference (APSEC)","1 Mar 2021","2020","","","470","474","Logging is a common practice to collect valuable runtime information about software systems. However, information written to log files can be sensitive and give valuable guidance to attackers. In fact, information exposure through logging is not uncommon. Even large-scale online services (e.g., Facebook and Twitter) have reported exposing sensitive information via log files, and hundreds of millions of users are affected. Despite the severity of such vulnerabilities, there is no existing work that studies such vulnerabilities in the real-world context, and we have little knowledge about them. To fill this gap, we conduct a preliminary study on 413 real-world vulnerabilities to investigate the exploitability and root causes of such vulnerabilities. By analyzing these vulnerabilities, we find that 1) about two-third (67.8%) vulnerabilities can be exploited via the network, and a significant amount (89.3%) of vulnerabilities can be exploited with low efforts; 2) malicious users and insiders can use about half of (46.9%) proof-of-concept exploits to launch attacks without any expertise; 3) the top three common root causes for the vulnerabilities are insecure whole-object logging (43.4%), incorrect permission assignment (17.5%), and improper implementation of sanitization (11.2%). Based on the findings, we also discuss the implications for researchers and practitioners. We believe our work can inspire further work on detecting and fixing the vulnerabilities.","2640-0715","978-1-7281-9553-7","10.1109/APSEC51365.2020.00058","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9359283","Information exposure;information leakage;logging;vulnerabilities","Runtime;Social networking (online);Blogs;Software systems;Open source software","authorisation;data privacy;program testing;security of data;social networking (online);system monitoring;Web services","sensitive information exposure;valuable runtime information;log files;valuable guidance;real-world vulnerabilities;exploitability;whole-object logging","","","","7","","1 Mar 2021","","","IEEE","IEEE Conferences"
"Internet of Things: Remote Patient Monitoring Using Web Services and Cloud Computing","J. Mohammed; C. Lung; A. Ocneanu; A. Thakral; C. Jones; A. Adler","Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, ON, Canada; Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, ON, Canada; Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, ON, Canada; Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, ON, Canada; Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, ON, Canada; Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, ON, Canada","2014 IEEE International Conference on Internet of Things (iThings), and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom)","16 Mar 2015","2014","","","256","263","The focus on this paper is to build an Android platform based mobile application for the healthcare domain, which uses the idea of Internet of Things (IoT) and cloud computing. We have built an application called 'ECG Android App' which provides the end user with visualization of their Electro Cardiogram (ECG) waves and data logging functionality in the background. The logged data can be uploaded to the user's private centralized cloud or a specific medical cloud, which keeps a record of all the monitored data and can be retrieved for analysis by the medical personnel. Though the idea of building a medical application using IoT and cloud techniques is not totally new, there is a lack of empirical studies in building such a system. This paper reviews the fundamental concepts of IoT. Further, the paper presents an infrastructure for the healthcare domain, which consists of various technologies: IOIO microcontroller, signal processing, communication protocols, secure and efficient mechanisms for large file transfer, data base management system, and the centralized cloud. The paper emphasizes on the system and software architecture and design which is essential to overall IoT and cloud based medical applications. The infrastructure presented in the paper can also be applied to other healthcare domains. It concludes with recommendations and extensibilities found for the solution in the healthcare domain.","","978-1-4799-5967-9","10.1109/iThings.2014.45","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7059670","Internet of Things;cloud computing;healthcare applications;system and software infrastructure","Servers;Electrocardiography;Medical services;Mobile communication;Mobile handsets;Androids;Humanoid robots","cloud computing;data loggers;data privacy;data visualisation;electrocardiography;health care;Internet of Things;medical signal processing;patient monitoring;smart phones;software architecture;telemedicine;Web services","Internet of Things;remote patient monitoring;Web services;cloud computing;Android platform;mobile application;healthcare domain;IoT;ECG Android app;electro cardiogram waves visualization;ECG waves visualization;data logging functionality;user private centralized cloud;medical cloud;monitored data records;medical personnel;cloud techniques;IOIO microcontroller;signal processing;communication protocols;file transfer;data base management system;software architecture;cloud based medical applications","","68","","9","","16 Mar 2015","","","IEEE","IEEE Conferences"
"Reinforcement Learning based Background Segment Cleaning for Log-structured File System on Mobile Devices","C. Wu; C. Ji; C. J. Xue","Department of Computer Science, City University of Hong Kong, Hong Kong, China; Department of Computer Science, City University of Hong Kong, Hong Kong, China; Department of Computer Science, City University of Hong Kong, Hong Kong, China","2019 IEEE International Conference on Embedded Software and Systems (ICESS)","1 Aug 2019","2019","","","1","8","With the adoption of Log-structured file system in mobile devices, the impact of background segment cleaning on system performance and storage lifetime becomes notable. Aggressive background segment cleaning solution generates excessive block migrations and impairs the endurance of NAND storage device, while a lazy solution cannot reclaim enough segments for subsequent I/O requests thus leading to the occurrence of foreground segment cleaning and prolonging I/O latency. In this paper, a reinforcement learning based approach is proposed to balance the trade-off. Through learning the behaviors of I/O workloads and the statuses of logical address space, the proposed approach can adaptively reduce the frequency of foreground segment cleaning by 68.57% on average, and decrease the number of block migrations by 71.10% over existing approaches.","","978-1-7281-2437-7","10.1109/ICESS.2019.8782508","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8782508","Log-structured file system;mobile device;segment cleaning;reinforcement learning;performance;endurance","Cleaning;Mobile handsets;Writing;Reinforcement learning;System performance;User experience;Performance evaluation","cache storage;learning (artificial intelligence);NAND circuits","mobile devices;storage lifetime;NAND storage device;foreground segment cleaning;log-structured file system;reinforcement learning;block migrations;background segment cleaning;I/O requests;I/O latency;I/O workloads;logical address space","","1","","22","","1 Aug 2019","","","IEEE","IEEE Conferences"
"Debugging Revisited: Toward Understanding the Debugging Needs of Contemporary Software Developers","L. Layman; M. Diep; M. Nagappan; J. Singer; R. Deline; G. Venolia","Fraunhofer Center for Exp. Software Eng., College Park, MD, USA; Fraunhofer Center for Exp. Software Eng., College Park, MD, USA; Software Anal. & Intell. Lab. (SAIL), Queen's Univ., Kingston, ON, Canada; Nat. Res. Council, Ottawa, ON, Canada; Microsoft Res., Redmond, WA, USA; Microsoft Res., Redmond, WA, USA","2013 ACM / IEEE International Symposium on Empirical Software Engineering and Measurement","12 Dec 2013","2013","","","383","392","We know surprisingly little about how professional developers define debugging and the challenges they face in industrial environments. To begin exploring professional debugging challenges and needs, we conducted and analyzed interviews with 15 professional software engineers at Microsoft. The goals of this study are: 1) to understand how professional developers currently use information and tools to debug, 2) to identify new challenges in debugging in contemporary software development domains (web services, multithreaded/multicore programming), and 3) to identify the improvements in debugging support desired by these professionals that are needed from research. The interviews were coded to identify the most common information resources, techniques, challenges, and needs for debugging as articulated by the developers. The study reveals several debugging challenges faced by professionals, including: 1) the interaction of hypothesis instrumentation and software environment as a source of debugging difficulty, 2) the impact of log file information on accurate debugging of web services, and 3) the mismatch between the sequential human thought process and the non-sequential execution of multithreaded environments as source of difficulty. The interviewees also describe desired improvements to tools to support debugging, many of which have been discussed in research but not transitioned to practice.","1949-3789","978-0-7695-5056-5","10.1109/ESEM.2013.43","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6681382","debugging;software engineering;interview;professionals;qualitative analysis;program comprehension","Debugging;Interviews;Encoding;Computer bugs;Web services;Testing","multi-threading;program debugging;Web services","debugging needs;contemporary software developers;professional developers;industrial environments;professional software engineers;Microsoft;information resources;hypothesis instrumentation;software environment;Web services;log file information;sequential human thought process;nonsequential multithreaded environments execution","","11","","21","","12 Dec 2013","","","IEEE","IEEE Conferences"
"Remote Automatic Test Equipment software management - Information Assurance Vulnerability Alert management","C. Koepping; P. Rajcok; C. Yoon","NAVAIR, Highway 547, Lakehurst, NJ 08733, USA; NAVAIR, Highway 547, Lakehurst, NJ 08733, USA; NAVAIR, Highway 547, Lakehurst, NJ 08733, USA","2011 IEEE AUTOTESTCON","24 Oct 2011","2011","","","440","442","Information Assurance Vulnerability Alerts (lAVAs) have become an important part of protecting and securing our systems. Operating systems and their applications are all susceptible to bugs/problems that need to be fixed. Virus definitions, which are released daily, are another important piece of IAVA compliance. IAVA updates are released almost weekly to ensure the integrity of the operation systems and its applications. Systems that are already fielded need to be updated with these approved IAVA updates. These fielded systems aren't always connected to the World Wide Web, so obtaining updates on their own isn't a viable option. They are however, connected to approved servers. A service was needed to obtain these updates on a weekly basis with little user interaction. A user reboot of the system might be needed to ensure that the updates take effect. For the most part the update service is free from user interaction. The application could also be run manually at a fielded site if needed. An application with Secure File Transfer Portocol (SFTP) capabilities was used to solve this problem and keep the remote, fielded, systems up to date with the latest IAVA patches. The remote systems are able to connect to the SFTP server, download the approved IAVA updates and install them all without user intervention. The remote computer might need to be rebooted for some of the IAVA patches, but this can be done at the user's convenience. The download and installation status are kept in a log file and database on the remote computer for future reference. If an IAVA patch fails during installation it is marked in the database as an attempted install and another attempt will be made to install it during the next automated update. During the next update this failed patch will be redownloaded and reinstalled. If this process fails a total of three times, it will be marked as failed and no more attempts to download or install will be made. Some updates require a reboot but this is not done automatically for fear that it could possibly affect a user that is currently using the system. Newly fielded sites already have all the latest patches, they are updated before being sent out, so the application could be run manually to update the database accordingly reflecting that the patches have already been installed. The ability to manually connect to SFTP server and download the updates was also necessary, in case we needed the updates sooner than the weekly update. Consolidated Automated Support System (CASS) Operations Management Software (OMS) contains this update service for the Navy and Marine CASS ATE and all of this functionally is needed to keep our systems secure/protected and up-to-date with the latest security patches. This paper will provide an overview of how we provide these IAVA updates to our clients.","1558-4550","978-1-4244-9363-0","10.1109/AUTEST.2011.6058790","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6058790","","Databases;Servers;Software;US Department of Defense;Maintenance engineering;Computers;Logistics","automatic test equipment;database management systems;Internet;operating systems (computers);software management","remote automatic test equipment software management;information assurance vulnerability alert management;operating system;virus definition;IAVA compliance;fielded system;world wide Web;user interaction;secure file transfer protocol;IAVA patch;SFTP server;log file;consolidated automated support system;operation management software;CASS ATE;security patch","","1","","3","","24 Oct 2011","","","IEEE","IEEE Conferences"
"What Logs Should You Look at When an Application Fails? Insights from an Industrial Case Study","M. Cinque; D. Cotroneo; R. D. Corte; A. Pecchia","Dipt. di Ing. Elettr. e Tecnol. dell'Inf., Univ. degli Studi di Napoli Federico II, Naples, Italy; Dipt. di Ing. Elettr. e Tecnol. dell'Inf., Univ. degli Studi di Napoli Federico II, Naples, Italy; Dipt. di Ing. Elettr. e Tecnol. dell'Inf., Univ. degli Studi di Napoli Federico II, Naples, Italy; Dipt. di Ing. Elettr. e Tecnol. dell'Inf., Univ. degli Studi di Napoli Federico II, Naples, Italy","2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks","22 Sep 2014","2014","","","690","695","Event logs are the first place where to find useful information about application failures. Event logs are available at different system levels, such as application, middleware and operating system. In this paper we analyze the failure reporting capability of event logs collected at different levels of an industrial system in the Air Traffic Control (ATC) domain. The study is based on a data set of 3,159 failures induced in the system by means of software fault injection. Results indicate that the reporting ability of event logs collected at a given level is strongly affected by the type of failure observed at runtime. For example, even if operating system logs catch almost all application crashes, they are strongly ineffective in face of silent and erratic failures in the considered system.","2158-3927","978-1-4799-2233-8","10.1109/DSN.2014.69","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6903626","event log;fault injection;failure analysis;middleware;air traffic control","Radio frequency;Computer crashes;Middleware;Operating systems;Logistics;Failure analysis","air traffic control;middleware;operating systems (computers);software fault tolerance;system monitoring","event logs;application failures;middleware;failure reporting capability;industrial system;air traffic control domain;ATC domain;software fault injection;operating system logs","","2","","22","","22 Sep 2014","","","IEEE","IEEE Conferences"
"Boot Log Anomaly Detection with K-Seen-Before","J. Garcia; T. Vehkajarvi","Karlstad University, Sweden; Karlstad University, Sweden","2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)","22 Sep 2020","2020","","","1005","1010","Software development for embedded systems, in particular code which interacts with boot-up procedures, can pose considerable challenges. In this work we propose the K-Seen-Before (KSB) approach to detect and highlight anomalous boot log messages, thus relieving developers from repeatedly having to manually examine boot log files of 1000+ lines. We describe the KSB instance based anomaly detection system and its relation to KNN. An industrial data set related to development of high-speed networking equipment is utilized to examine the effects of the KSB parameters on the amount of detected anomalies. The obtained results highlight the utility of KSB and provide indications of suitable KSB parameter settings for obtaining an appropriate trade-off for the cognitive workload of the developer with regards to log file analysis.","0730-3157","978-1-7281-7303-0","10.1109/COMPSAC48688.2020.0-140","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9201933","Fault localization, coincidentally correct, random forests, ensemble learning","Anomaly detection;Training;Software;Data models;Training data;Dictionaries;Computational modeling","computer bootstrapping;embedded systems;learning (artificial intelligence);system monitoring","K-Seen-Before approach;boot log anomaly detection;software development;embedded systems;boot-up procedures;anomalous boot log messages;boot log files;KSB instance;industrial data set;high-speed networking equipment;KSB parameters;KSB parameter settings;cognitive workload;log file analysis;instance-based learning","","","","15","","22 Sep 2020","","","IEEE","IEEE Conferences"
"Internet worms identification through serial episodes mining","Ming-Yang Su","Department of Computer Science and Information Engineering, Ming Chuan University, 5 The Ming Road, Gwei Shan District, Taoyuan 333, Taiwan","ECTI-CON2010: The 2010 ECTI International Confernce on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology","24 Jun 2010","2010","","","132","136","An Internet worm is a typical Internet attack that can rapidly pervade a computer without user intervention. In the frequent episodes mining, data is regarded as a sequence of events, where each event has an associated time of occurrence, thus, it has significant effect on the discovery of sophisticated Internet attacks. The method proposed in this paper can be used to detect abnormal Internet episodes from the log files of a honeypot system in order to discover known or unknown attack episodes. The experiment successfully identified sophisticated Internet attack episodes, which were caused by Internet worms, such as Sasser, Shelp, Korgo, etc.","","978-1-4244-5607-9","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5491518","","Internet;Telecommunication traffic;Tellurium;Computer worms;Protocols;Intrusion detection;Computer science;Data mining;File servers;Network servers","data mining;Internet;invasive software","Internet worm identification;serial episode mining;Internet attack;log files;honeypot system;frequent episodes mining","","","","15","","24 Jun 2010","","","IEEE","IEEE Conferences"
"How Far Have We Come in Detecting Anomalies in Distributed Systems? An Empirical Study with a Statement-level Fault Injection Method","Y. Yang; Y. Wu; K. Pattabiraman; L. Wang; Y. Li","Peking University,School of Software and Microelectronics; Peking University,School of Software and Microelectronics; University of British Columbia; IBM TJ Watson Research; Peking University,National Engineering Center for Software Engineering","2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE)","11 Nov 2020","2020","","","59","69","Anomaly detection in distributed systems has been a fertile research area, and a range of anomaly detectors have been proposed for distributed systems. Unfortunately, there is no systematic quantitative study of the efficacy of different anomaly detectors, which is of great importance to reveal the deficiencies of existing anomaly detectors and shed light on future research directions. In this paper, we investigate how various anomaly detectors behave on anomalies of different types and the reasons for the same, by extensively injecting software faults into three widely-used distributed systems. We use a statementlevel fault injection method to observe the anomalies, characterize these anomalies, and analyze the detection results from anomaly detectors of three categories. We find that: (1) the distributed systems' own error reporting mechanisms are able to report most of the anomalies (from 82.1% to 92.8%) but they incur a high false alarm rate of 26.6%. (2) State-of-the-art anomaly detectors are able to detect the existence of anomalies with 99.08% precision and 90.60% recall, but there is still a long way to go to pinpoint the accurate location of the detected anomalies, and (3) Log-based anomaly detection techniques outperform other anomaly detection techniques, but not for all anomaly types.","2332-6549","978-1-7281-9870-5","10.1109/ISSRE5003.2020.00015","Research and Development; Science and Engineering Research Council; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9251065","anomaly detection;distributed systems;fault injection;dependability","Systematics;Detectors;Software;Software reliability;Anomaly detection","distributed processing;security of data;software fault tolerance","distributed systems;anomaly detectors;detected anomalies;anomaly types;statement-level fault injection method;error reporting mechanisms;false alarm rate","","1","","38","","11 Nov 2020","","","IEEE","IEEE Conferences"
"Mobile malware detection in sandbox with live event feeding and log pattern analysis","W. Lin; J. Pan","Department of Communications Engineering, National Chung Cheng University, Chia-Yi, Taiwan, R.O.C.; Department of Communications Engineering, National Chung Cheng University, Chia-Yi, Taiwan, R.O.C.","2016 18th Asia-Pacific Network Operations and Management Symposium (APNOMS)","10 Nov 2016","2016","","","1","6","In recent years, the use of smart devices is becoming increasingly popular. All kinds of mobile applications are emerging. In addition to the official market, there are also many ways to allow users to download the mobile app. As unidentified instances of malware grow day by day, off-the-shelf malware detection methods identify malicious programs mainly with extracted signatures of codes, which only can effectively identify already known malwares, but not new malwares in initial spread. If no samples of these malwares are reported and the virus code library is not patched, users won't be alerted to the malwares. Therefore, this paper proposed a new detection method by live log analysis. A sandbox is conducted to mimic human operations and monitor responses from APPs. Feeding these manual events can excite deactivated malwares and improve the accuracy of log analysis, even though these malware are unknown yet. This study takes recent malwares and benign programs to conduct experiments, and then verifies the effectiveness of the proposed method comparing with those in other papers. The experimental results show that the proposed method outperforms in both hit rate and pass rate.","","978-4-88552-304-5","10.1109/APNOMS.2016.7737204","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7737204","Sandbox;Malware Detection;Data Mining;live event feeding","Malware;Smart devices;Smart phones;Mobile communication;Databases;Security","invasive software;mobile computing;program diagnostics;source code (software)","mobile malware detection;Sandbox;live event feeding;log pattern analysis;official market;mobile app;off-the-shelf malware detection;malicious program identification;code signature extraction;virus code library;live log analysis;human operations","","1","","20","","10 Nov 2016","","","IEEE","IEEE Conferences"
"The Bones of the System: A Case Study of Logging and Telemetry at Microsoft","T. Barik; R. DeLine; S. Drucker; D. Fisher","North Carolina State Univ., Raleigh, NC, USA; Microsoft Res., Redmond, WA, USA; Microsoft Res., Redmond, WA, USA; Microsoft Res., Redmond, WA, USA","2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)","23 Mar 2017","2016","","","92","101","Large software organizations are transitioning to event data platforms as they culturally shift to better support data-driven decision making. This paper offers a case study at Microsoft during such a transition. Through qualitative interviews of 28 participants, and a quantitative survey of 1,823 respondents, we catalog a diverse set of activities that leverage event data sources, identify challenges in conducting these activities, and describe tensions that emerge in data-driven cultures as event data flow through these activities within the organization. We find that the use of event data span every job role in our interviews and survey, that different perspectives on event data create tensions between roles or teams, and that professionals report social and technical challenges across activities.","","978-1-4503-4205-6","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7883293","boundary object;collaboration;logging;telemetry;developer tools;practices","Interviews;Operating systems;Software engineering;Encoding;Telemetry;Data science","software engineering","large software organizations;Microsoft;event data sources;data-driven cultures;event data flow;logging;telemetry","","1","","25","","23 Mar 2017","","","IEEE","IEEE Conferences"
"Predicting Issue Handling Process using Case Attributes and Categorical Variable Encoding Techniques","S. Baskoro; W. D. Sunindyo","Bandung Institute of Technology,School of Electrical Engineering and Informatics,Bandung,Indonesia; Bandung Institute of Technology,School of Electrical Engineering and Informatics,Bandung,Indonesia","2019 International Conference on Data and Software Engineering (ICoDSE)","14 May 2020","2019","","","1","5","Software development process deals with the increasing complexity, needs for enhancement, and the reduction of bugs. The software project managers have a responsibility to maintain the software quality, e.g., by establishing Service Level Agreement (SLA) to the software development. However, there are difficulties to monitor SLA and to predict the software quality, due to a lot of issues should be handled. This work proposed prediction techniques for handling issues in the software development by using historical data from software repositories. There are two types of predictions in this work, namely 1) prediction of the remaining duration, and 2) prediction of the next activities. Event logs extracted from historical data in the software repositories exploited case and event attributes as predictors. Furthermore, the categorical variable encoding technique used in the preprocessing data phase. Some categorical variable encoding techniques also proposed in this study. The results showed that the use of case attributes as predictors could improve performance by 8.57% in the next activity prediction and 1.9% in the remaining duration prediction. Of the 4 (four) categorical variable encoding techniques used, one-hot encoding and sum encoding techniques can provide the best remaining duration prediction with a MAE of 19.72 and one-hot encoding technique can provide the best next activity prediction with an accuracy of 65.38%. All of these methods are widely evaluated using datasets from the Google Chromium Project. Further work including utilization of other attributes outside case and event attributes, e.g., component attributes, as predictor variables.","2640-0227","978-1-7281-4992-9","10.1109/ICoDSE48700.2019.9092617","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9092617","software repositories;prediction;process mining;encoding technique","Encoding;Software;Data mining;Computer bugs;Business;Hidden Markov models;Monitoring","contracts;data handling;encoding;program debugging;software development management;software quality","historical data;software repositories;categorical variable encoding technique;case attributes;activity prediction;software development process;software project managers;service level agreement;SLA;software quality;issue handling process","","","","14","","14 May 2020","","","IEEE","IEEE Conferences"
"Configurable Event Correlation for Process Discovery from Object-Centric Event Data","G. Li; R. Medeiros de Carvalho; W. M. P. van der Aalst","Dept. of Math. & Comput. Sci., Eindhoven Univ. of Technol., Eindhoven, Netherlands; Dept. of Math. & Comput. Sci., Eindhoven Univ. of Technol., Eindhoven, Netherlands; Dept. of Math. & Comput. Sci., Eindhoven Univ. of Technol., Eindhoven, Netherlands","2018 IEEE International Conference on Web Services (ICWS)","6 Sep 2018","2018","","","203","210","Many modern enterprises are employing serviceoriented systems to execute their transactions. These systems generate an abundance of events, which can be analyzed to diagnose and improve business processes. However, the events are distributed over different data sources, as service-oriented systems are often implemented asWeb services involving different departments in an organization. They need to be consolidated in a process view since it makes no sense to analyze individual events. Existing approaches depend on an explicit case notion to correlate events. They work well on data from process-aware systems (e.g., BPM/WFM systems) which record events explicitly and each event is attached with a case id (a global identifier) to indicate its related process instance. However, most serviceoriented systems (e.g., ERP and CRM) produce object-centric data (e.g., database tables), which record events implicitly (e.g., through redo logs) and separately without a common case id. In this paper, we propose a novel approach to correlate events by the data perspective (e.g., by a so-called ""object path""). Besides, we define the concepts of correlation patterns and evaluation metrics, which enable users to evaluate and select the best way to correlate events based on their needs.","","978-1-5386-7247-1","10.1109/ICWS.2018.00033","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8456350","event correlation;object-centric data;service-oriented information systems;databases;process discovery","Databases;Data models;Correlation;Data mining;Web services;Customer relationship management","business data processing;data mining;database management systems;service-oriented architecture;software architecture;Web services","process-aware systems;serviceoriented systems;object-centric data;configurable event correlation;process discovery;object-centric event data;modern enterprises;business processes;service-oriented systems;Web services","","","","21","","6 Sep 2018","","","IEEE","IEEE Conferences"
"A Toolkit for Event Analysis and Logging","J. Carey; P. Sanders","IBM, 3605 Hwy 52 N, Rochester, MN 55901; IBM, 3605 Hwy 52 N, Rochester, MN 55901","SC '11: Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis","29 Dec 2011","2011","","","1","7","This report describes the Toolkit for Event Analysis and Logging that came out of IBM's effort to converge its support for low level HPC event analysis. The toolkit is designed as a pluggable processing pipeline that allows different components to use connectors to log events, have analyzers evaluate the events to get closer to the root cause, report and log these as alerts, and deliver the alerts to interested parties via filters and listeners. A number of plug-ins are provided with the toolkit, including a filter that removes duplicate alerts and a rule driven event analyzer, which has a new rules language that allows for a higher level location centric specification of event analysis. In order to work with the HPC community to enhance the toolkit and expand it to more platforms, it is available as open source on sourceforge.","2167-4337","978-1-4503-0771-0","10.1145/2063348.2063381","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6114478","System Management","Gears;Connectors;Pipelines;Monitoring;XML;Engines;Hardware","formal specification;pipeline processing;public domain software;software tools;system monitoring","toolkit-for-event analysis-and-logging;TEAL;low level HPC event analysis;pluggable processing pipeline;root cause;event evaluation;rules language;location centric specification;open source;sourceforge","","","31","9","","29 Dec 2011","","","IEEE","IEEE Conferences"
"Semantic logging in a distributed multi-agent system","S. Ilie; M. Scafeş; C. Bădică; T. Neidhart; R. Pinchuk","University of Craiova, Software Engineering Dept., Bvd. Decebal 107, Craiova, 200440, Romania; University of Craiova, Software Engineering Dept., Bvd. Decebal 107, Craiova, 200440, Romania; University of Craiova, Software Engineering Dept., Bvd. Decebal 107, Craiova, 200440, Romania; Space Applications Services NV, Leuvensesteenweg 325, 1932, Zaventem, Belgium; Space Applications Services NV, Leuvensesteenweg 325, 1932, Zaventem, Belgium","2010 International Joint Conference on Computational Cybernetics and Technical Informatics","21 Jun 2010","2010","","","265","270","The paper presents a semantic logging framework which allows structured information logging in an agent-based distributed system for chemical incident response. The logging framework is “semantic” because it allows semantic interpretation of logs according to relationships defined between logging events. For example, this approach could help the reconstruction of the order of events that occurred during the response to an incident, thus giving a detailed view of system execution trace, as well as of agents' decisions taken at various decision points during the incident management workflow. We intend to use semantic logs (i) for helping experts to analyze and explain system actions and thus improving system response to future incidents, as well as (ii) for training stakeholders by setting the system to run replay-like simulations of past incident management workflows.","","978-1-4244-7433-2","10.1109/ICCCYB.2010.5491285","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5491285","","Multiagent systems;Management training;Analytical models;Humans;Chemical industry;Environmental management;Information analysis;Software engineering;Application software;Disaster management","decision making;distributed processing;multi-agent systems;workflow management software","semantic logging framework;structured information logging;distributed multi agent system;system response;incident management workflow","","2","","14","","21 Jun 2010","","","IEEE","IEEE Conferences"
"Efficient Alignment Between Event Logs and Process Models","W. Song; X. Xia; H. Jacobsen; P. Zhang; H. Hu","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Middleware Systems Research Group; College of Computer and Information, Hohai University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China","IEEE Transactions on Services Computing","20 May 2017","2017","10","1","136","149","The aligning of event logs with process models is of great significance for process mining to enable conformance checking, process enhancement, performance analysis, and trace repairing. Since process models are increasingly complex and event logs may deviate from process models by exhibiting redundant, missing, and dislocated events, it is challenging to determine the optimal alignment for each event sequence in the log, as this problem is NP-hard. Existing approaches utilize the cost-based A* algorithm to address this problem. However, scalability is often not considered, which is especially important when dealing with industrial-sized problems. In this paper, by taking advantage of the structural and behavioral features of process models, we present an efficient approach which leverages effective heuristics and trace replaying to significantly reduce the overall search space for seeking the optimal alignment. We employ real-world business processes and their traces to evaluate the proposed approach. Experimental results demonstrate that our approach works well in most cases, and that it outperforms the state-of-the-art approach by up to 5 orders of magnitude in runtime efficiency.","1939-1374","","10.1109/TSC.2016.2601094","National Basic Research Program of China; National Natural Science Foundation of China; China Scholarship Council; NSERC; Alexander von Humboldt Foundation; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7546937","Event logs;process models;alignment;process decomposition;trace segmentation;trace replaying","Petri nets;Business;Analytical models;Electronic mail;Computational modeling;Jacobian matrices;Performance analysis","computational complexity;data mining;system monitoring","event logs;process models;trace replaying;business processes;process mining;NP-hard problem","","17","","33","","17 Aug 2016","","","IEEE","IEEE Journals"
"Mining Telecom System Logs to Facilitate Debugging Tasks","A. Larsson; A. Hamou-Lhadj","R&D, PLF Syst. Manage., Ericsson, Stockholm, Sweden; SBA Res. Lab., Concordia Univ., Montreal, QC, Canada","2013 IEEE International Conference on Software Maintenance","2 Dec 2013","2013","","","536","539","Telecommunication systems are monitored continuously to ensure quality and continuity of service. When an error or an abnormal behaviour occurs, software engineers resort to the analysis of the generated logs for troubleshooting. The problem is that, even for a small system, the log data generated after running the system for a period of time can be considerably large. There is a need to automatically mine important information from this data. There exist studies that aim to do just that, but their focus has been mainly on software applications, paying little attention to network information used by telecom systems. In this paper, we show how data mining techniques, more particularly the ones based on mining frequent itemsets, can be used to extract patterns that characterize the main behaviour of the traced scenarios. We show the effectiveness of our approach through a representative study conducted in an industrial setting.","1063-6773","978-0-7695-4981-1","10.1109/ICSM.2013.90","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6676951","System logs;event correlation;troubleshooting of telecom systems;mining algorithms","Itemsets;Data mining;Software;Correlation;Noise;Member and Geographic Activities Board committees","data mining;program debugging;quality of service;system monitoring","telecom system log mining;program debugging tasks;telecommunication system troubleshooting;quality of service;continuity of service;software engineers;log data;data mining techniques;itemset mining;pattern extraction;system logs","","1","","12","","2 Dec 2013","","","IEEE","IEEE Conferences"
"Comparison of File Integrity Monitoring (FIM) techniques for small business networks","B. Wilbert; L. Chen","Department of Computer Science, Sam Houston State University, Huntsville, Texas, United States; Department of Computer Science, Sam Houston State University, Huntsville, Texas, United States","Fifth International Conference on Computing, Communications and Networking Technologies (ICCCNT)","20 Nov 2014","2014","","","1","7","File Integrity Monitoring (FIM) can provide the ability to track changes which have been made to an operating system or software as a result of malicious behavior. For small business environments, the need for these tools is present; however, because of the difficulty of managing such software many businesses may ignore using them. This paper will discuss how a small business should create criteria for comparing file integrity monitoring tools in order to locate the most effective product for their environment.","","978-1-4799-2696-1","10.1109/ICCCNT.2014.6963090","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6963090","log file integrity monitoring;small business environments;auditing;log management","Monitoring;Business;Virtual machining;Security;Kernel","business data processing;file organisation;operating systems (computers);system monitoring","file integrity monitoring techniques;FIM;small business networks;operating system;small business environments;malicious behavior;file integrity monitoring tools","","1","","23","","20 Nov 2014","","","IEEE","IEEE Conferences"
"Performance Evaluation of Data Mining Frameworks in Hadoop Cluster Using Virtual Campus Log Files","F. Xhafa; D. Ramirez; D. Garcia; S. Caballé","Univ. Politec. de Catalunya, Barcelona, Spain; Univ. Politec. de Catalunya, Barcelona, Spain; Univ. Politec. de Catalunya, Barcelona, Spain; Open Univ. of Catalonia, Barcelona, Spain","2015 International Conference on Intelligent Networking and Collaborative Systems","2 Nov 2015","2015","","","217","222","With the fast development in Cloud computing technologies, most computing platforms and stand alone applications are being deployed in Cloud platforms and offered as a service (SaaS). Likewise, Data Mining Frameworks (DMFs) such as Weka and R, are being ported to Cloud platforms, while other frameworks properly designed for Cloud platforms are emerging such as Mahout. For existing DMFs, which were designed before Cloud computing appeared, the main issue is if porting them to Cloud platforms would bring any benefits. One the one hand, by porting them to Cloud, it is possible to offer them as Cloud service, which would alleviate the final user from the burden of installing and configuring DMFs at local computer or local networking infrastructure. On the other hand, porting DMFs to Cloud should allow to tackle mining of very large data sets, i.e. Big Data. In this work we evaluate some DMFs, including Weka and Mahout, under a Hadoop cluster and show that while there are improvements in time efficiency to a certain scale, some mining functions, which are part of DMFs, were not able to finalize for data sets of more than 20Gb, namely, mining log files of a virtual campus. The study revealed that indeed porting DMFs to Cloud might not necessarily help tackling Big Data, as such DMFs were conceived without Big Data requirements.","","978-1-4673-7695-2","10.1109/INCoS.2015.82","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7312074","Cloud Computing;Data Mining;Evaluation;Virtual Campus;Log Files;Weka;Mahout","Data mining;Clustering algorithms;Cloud computing;Big data;Databases;Algorithm design and analysis;Education","Big Data;cloud computing;data mining;parallel processing;software performance evaluation","performance evaluation;data mining framework;DMF;Hadoop cluster;virtual campus log file;cloud computing;Big Data","","1","","12","","2 Nov 2015","","","IEEE","IEEE Conferences"
"FPGA implementation of iterative decoder for turbo codes for software defined radio","S. Mishra; A. Mishra; A. Shastri; S. Madhekar","PGAD, Research Centre Imarat, DRDO, Hyderabad-500058, India; School of Electronics and Communication Engineering, VIT University, Vellore-632014, India; Department of Electronics and Communication Engineering, MANIT Bhopal-462003, India; PGAD, Research Centre Imarat, DRDO, Hyderabad-500058, India","2016 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET)","15 Sep 2016","2016","","","2357","2361","Implementation of iterative decoder for turbo codes is a challenging task; however, various algorithms have been proposed to enable the implementation of iterative decoder for turbo codes in a hardware device. For faster prototype development of any wireless communication system, software defined radio (SDR) is the best choice. This paper discusses the design and FPGA implementation of an iterative decoder for turbo codes using the MAX-LOG-MAP algorithm for an SDR. The entire implementation is carried out using the Xilinx block set available in MATLAB and SIMULINK. Xilinx System Generator is used to generate bit file which is used to configure Virtex-4 FPGA device in an SDR platform. BER plot and output results for the implementation are also presented.","","978-1-4673-9338-6","10.1109/WiSPNET.2016.7566563","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7566563","Error correcting codes;Turbo codes;Iterative decoding;FPGA;Software defined radio","Decoding;Field programmable gate arrays;Iterative decoding;Turbo codes;Conferences;Software algorithms;Software radio","error statistics;field programmable gate arrays;iterative decoding;software radio;turbo codes","iterative decoder;turbo codes;software defined radio;SDR;hardware device;wireless communication system;MAX-LOG-MAP algorithm;Xilinx block set;MATLAB;SIMULINK;Xilinx system generator;Virtex-4 FPGA device;bit error rate;BER","","1","","16","","15 Sep 2016","","","IEEE","IEEE Conferences"
"Intelligent loading of code modules based on the users' historical operating log in GIS software","H. Li; Y. Gao; H. Yu; L. Liu; X. Guo","Institute of Remote Sensing and Geographical Information Systems, Peking University, Beijing, PR China; Institute of Remote Sensing and Geographical Information Systems, Peking University, Beijing, PR China; Institute of Remote Sensing and Geographical Information Systems, Peking University, Beijing, PR China; Institute of Remote Sensing and Geographical Information Systems, Peking University, Beijing, PR China; Institute of Remote Sensing and Geographical Information Systems, Peking University, Beijing, PR China","2013 21st International Conference on Geoinformatics","10 Oct 2013","2013","","","1","4","The intelligent loading is a technology which is widely used in GIS Software. The paper designed a method which is based on the users' preference that is extracted and analysed from users' historical operating records. A matrix is used for describing the relation between code modules and users' behavior. Two vectors are used to formalize the users' behavior in the dimension of code modules. The two vectors record the information to judge whether the code module needs to be loaded or not. By a series of the calculation, the code modules which are very likely used by the project are selected. Through this method, GIS Software is loaded more efficient.","2161-0258","978-1-4673-6228-3","10.1109/Geoinformatics.2013.6626124","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6626124","Users' Prefermance;Code Modules;Intelligent Loading;Geographic Information System (GIS)","Software;Vectors;Loading;Heuristic algorithms;Geographic information systems;Algorithm design and analysis;Software algorithms","geographic information systems;information retrieval;matrix algebra;recommender systems","matrix method;user behavior;historical operating records;code modules;GIS software;intelligent loading","","","","13","","10 Oct 2013","","","IEEE","IEEE Conferences"
"Assessing Agile Software Development Processes with Process Mining: A Case Study","R. Marques; M. Mira da Silva; D. R. Ferreira","Inst. Super. Tecnico, Lisbon, Portugal; Inst. Super. Tecnico, Lisbon, Portugal; Inst. Super. Tecnico, Lisbon, Portugal","2018 IEEE 20th Conference on Business Informatics (CBI)","2 Sep 2018","2018","01","","109","118","Software development is a growing industry with increasingly more complex projects. Agile methodologies, such as Scrum, have emerged in part to address this challenge. Agile focuses on a set of best practices rather than a standardized process, which makes it difficult to assess whether it is being properly implemented in an organization. In this work, we analyze the feasibility of using process mining to evaluate the implementation of Scrum practices based on event logs collected from a case-handling system. A case study was conducted in an IT organization that uses Jira Software to manage its projects. With process mining, it was possible to extract the workflow behaviour in two different projects. While Scrum standards like role distribution could be discovered, other practices like customer collaboration could not be detected in the log. This case study provided important insights regarding the application of process mining in case-handling systems and Scrum processes.","2378-1971","978-1-5386-7016-3","10.1109/CBI.2018.00021","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8452664","Agile software development, Scrum methodology, process mining, case-handling systems","Data mining;Software;Organizations;Scrum (Software development);Tools;Planning;Standards organizations","data mining;project management;software development management;software prototyping","process mining;Scrum practices;case-handling system;Jira Software;Scrum standards;Scrum processes;agile methodologies;agile software development processes;event logs","","","","","","2 Sep 2018","","","IEEE","IEEE Conferences"
"An Introspection Mechanism to Debug Distributed Systems","T. Araújo; C. Wanderley; A. v. Staa","Dept. de Inf., PUC-Rio, Rio de Janeiro, Brazil; Dept. de Inf., PUC-Rio, Rio de Janeiro, Brazil; NA","2012 26th Brazilian Symposium on Software Engineering","25 Oct 2012","2012","","","21","30","Distributed systems are hard to debug due to the difficulty to collect, organize and relate information about their behavior. When a failure is detected the task to infer the system's state and the operations that have some connection with the problem is often quite difficult and usual debugging techniques often do not apply and, when they do, they are not very effective. This work presents a mechanism based on event logs annotated with contextual information, allowing visualization tools to organize events according to the context of interest for the system operator. We applied this mechanism to a real system and its the effort and cost to detect and diagnose the cause of problems was dramatically reduced.","","978-0-7695-4868-5","10.1109/SBES.2012.13","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6337890","Software quality;Software debugability;Software maintenance;Software recovery","Instruments;Visualization;Web servers;Maintenance engineering;Context modeling","distributed processing;program debugging;program visualisation;software maintenance;software quality;system recovery","introspection mechanism;system debugging;distributed system;failure detection;event log;visualization tool;system operator;system diagnose;software quality;software maintenance;software recovery","","","2","30","","25 Oct 2012","","","IEEE","IEEE Conferences"
"Insider Threat Identification Using the Simultaneous Neural Learning of Multi-Source Logs","L. Liu; C. Chen; J. Zhang; O. De Vel; Y. Xiang","School of Software and Electrical Engineering, Swinburne University of Technology, Hawthorn, VIC, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Hawthorn, VIC, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Hawthorn, VIC, Australia; Department of Defence, Defence Science and Technology Group, Edinburgh, SA, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Hawthorn, VIC, Australia","IEEE Access","25 Dec 2019","2019","7","","183162","183176","Insider threat detection has drawn increasing attention in recent years. In order to capture a malicious insider's digital footprints that occur scatteredly across a wide range of audit data sources over a long period of time, existing approaches often leverage a scoring mechanism to orchestrate alerts generated from multiple sub-detectors, or require domain knowledge-based feature engineering to conduct a one-off analysis across multiple types of data. These approaches result in a high deployment complexity and incur additional costs for engaging security experts. In this paper, we present a novel approach that works with a variety of security logs. The security logs are transformed into texts in the same format and then arranged as a corpus. Using the model trained by Word2vec with the corpus, we are enabled to approximate the posterior probabilities for insider behaviours. Accordingly, we label the transformed events as suspicious if their behavioural probabilities are smaller than a given threshold, and a user is labelled as malicious if he/she is associated with multiple suspicious events. The experiments are undertaken with the Carnegie Mellon University (CMU) CERT Programs insider threat database v6.2, which not only demonstrate that the proposed approach is effective and scalable in practical applications but also provide a guidance for tuning the parameters and thresholds.","2169-3536","","10.1109/ACCESS.2019.2957055","Australia Defence Science and Technology (DST) Group; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8918248","Cybersecurity;data analytics;insider threats;word embedding","Security;Feature extraction;Machine learning;Software;Databases;Transforms;Anomaly detection","database management systems;learning (artificial intelligence);probability;security of data","Carnegie Mellon University CERT Programs insider threat database;multiple suspicious events;behavioural probabilities;transformed events;insider behaviours;posterior probabilities;Word2vec;security logs;security experts;domain knowledge-based feature engineering;scoring mechanism;audit data sources;malicious insider;insider threat detection;multisource logs;simultaneous neural learning;insider threat identification","","7","","44","CCBY","2 Dec 2019","","","IEEE","IEEE Journals"
"Learning-Based Anomaly Cause Tracing with Synthetic Analysis of Logs from Multiple Cloud Service Components","Y. Yuan; H. Anu; W. Shi; B. Liang; B. Qin",Renmin University of China; Renmin University of China; Renmin University of China; Renmin University of China; Renmin University of China,"2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC)","9 Jul 2019","2019","1","","66","71","It is critical for a reliable cloud to effectively find out root causes of the cloud service anomalies for efficacious treatment. System logs are widely used for anomaly detection and analysis. Many efforts have been made to handle massive cloud logs automatically. However, existing work can still not effectually make comprehensive use of logs from multiple cloud service components to locate the causes of cloud service anomalies automatically. In this paper, we propose a learning-based approach for fine-grained deep cloud service anomaly cause tracing by synthetically utilizing logs from multiple service components of a cloud. We focus on uncovering root causes of anomalies corresponding to system executions of each user operation rather than roughly taking various system tasks as a whole. Log patterns are learned from past experience of system runs with anomalies occurred before, where the mined log event sequences to represent system behaviors related to each user operation are treated as natural language sequences. When an anomaly is to be diagnosed, the corresponding log patterns can be recognized for root cause identification. We implemented and evaluated our approach in OpenStack. Experimental results show that our approach can effectively trace the root causes for anomalies in cloud environments.","0730-3157","978-1-7281-2607-4","10.1109/COMPSAC.2019.00019","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8754032","IaaS cloud;problem diagnosis;across service component;log analysis;machine learning","Cloud computing;Task analysis;Knowledge based systems;Databases;Operating systems;Correlation","cloud computing;data mining;learning (artificial intelligence);software fault tolerance;system monitoring","root cause identification;cloud environments;cloud service anomalies;system logs;anomaly detection;cloud service components;learning-based anomaly cause tracing;logs analysis;deep cloud service anomaly cause tracing;log event sequences mining;OpenStack","","1","","39","","9 Jul 2019","","","IEEE","IEEE Conferences"
"TABLEFS: Embedding a NoSQL database inside the local file system","K. Ren; G. Gibson","Carnegie Mellon University, USA; Carnegie Mellon University, USA","2012 Digest APMRC","10 Jan 2013","2012","","","1","6","Conventional file systems are optimzed for large file transfers instead of workloads that are dominated by metadata and small file accesses. This paper examines using techniques adopted from NoSQL databases to manage file system metadata and small files, which feature high rates of change and efficient out-of-core data representation. A FUSE file system prototype was built by storing file system metadata and small files into a modern key-value store: LevelDB. We demonstrate that such techniques can improve the performance of modern local file systems in Linux for workloads dominated by metadata and tiny files.","","978-9-8107-2056-8","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6407525","System software;File systems;Metadata representation;Log-structure Merge Tree","Linux;Bars;Computers;SDRAM;Benchmark testing","data structures;distributed databases;Linux;meta data;peer-to-peer computing;storage management","TABLEFS;NoSQL database;local file system;large file transfers;small file access;file system metadata management;out-of-core data representation;FUSE file system prototype;LeveiDB;Linux","","","","15","","10 Jan 2013","","","IEEE","IEEE Conferences"
"Investigating on the Impact of Software Clones on Technical Debt","A. Lerina; L. Nardi","University of Sannio; Department of Engineering, University of Sannio, Italy","2019 IEEE/ACM International Conference on Technical Debt (TechDebt)","5 Aug 2019","2019","","","108","112","Code reuse by copying a code fragment with or without modification generates duplicate copies of exact or similar code fragments in a software system, known as code clones. The debate about the harmfulness of clone in ongoing in the literature, nevertheless, it is widely recognized that clones needs special considerations during software evolution. In this paper, it is proposed a quantitative analysis of technical debt values to understand if it is higher with cloned code than those without cloned code. Moreover, changes performed on these files have been analyzed by analyzing commit logs. According to our inspection on four subject systems, the technical debt of files with cloned code is significantly higher than those without cloned code. Moreover, as expected, files with cloned code are more impacted by changes.","","978-1-7281-3371-3","10.1109/TechDebt.2019.00029","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8786015","software clones, software maintenance, software evolution, technical debt","Cloning;Computer architecture;Software systems;Tools;FCC;Atmospheric measurements","software maintenance","software clones;technical debt;code reuse;code fragment;code clones","","1","","13","","5 Aug 2019","","","IEEE","IEEE Conferences"
"A Hadoop Based Weblog Analysis System","C. H. Wang; C. T. Tsai; C. C. Fan; S. M. Yuan","Dept. of Comput. Sci., Nat. Chiao-Tung Univ., Hsinchu, Taiwan; NA; Dept. of Comput. Sci., Nat. Chiao-Tung Univ., Hsinchu, Taiwan; Dept. of Comput. Sci., Nat. Chiao-Tung Univ., Hsinchu, Taiwan","2014 7th International Conference on Ubi-Media Computing and Workshops","7 Oct 2014","2014","","","72","77","In recent years, cloud computing has been an important issue in the field of research. Cloud computing employs distributed storage and distributed computing technology to achieve a large number of stored data, as well as fast data analysis and processing. As the rapid development of Internet technology, digital data showing explosive growth, the face of massive data processing, the traditional text software and relational database technology has been facing a bottleneck, presented the results are not very satisfactory. For this problem, the concept of cloud computing is a more appropriate choice. In this paper, based on the architecture of Hadoop with HDFS (Hadoop Distributed File System) and Hadoop MapReduce software framework and Pig Latin language, we design and implement an enterprise Web log analysis system. Experimental results, by analyzing daily Web log records, we get Application Server traffic trends, performance of program statistical reports, and performance reports of different intervals and different actions of program by user request. The main purpose of this system is to assist system administrators to quickly capture and analyze data hidden in the massive potential value, thus providing an important basis for business decisions.","","978-1-4799-4266-4","10.1109/U-MEDIA.2014.9","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6916328","Cloud Computing;Distributed File System;Pig programming language","Computer architecture;Conferences;Abstracts;File systems;Computer languages;Servers;Market research","cloud computing;data analysis;parallel processing;relational databases","Hadoop based Weblog analysis system;cloud computing;distributed storage;distributed computing technology;fast data analysis;Internet technology;digital data;massive data processing;traditional text software;relational database technology;HDFS;Hadoop distributed file system;Hadoop MapReduce software framework;pig Latin language;enterprise Web log analysis system;business decisions;application server traffic trends;program statistical reports","","6","","15","","7 Oct 2014","","","IEEE","IEEE Conferences"
"The Distribution of Time to Recovery of Enterprise IT Services","U. Franke; H. Holm; J. König","FOI—the Swedish Defence Research Agency, Stockholm, Sweden; FOI—the Swedish Defence Research Agency, Stockholm, Sweden; Department of Industrial Information and Control Systems, KTH—the Royal Institute of Technology, Stockholm, Sweden","IEEE Transactions on Reliability","25 Nov 2014","2014","63","4","858","867","The context of this article is the availability of enterprise IT services, a key concern for many enterprises. While there is a plethora of literature concerned with service availability, there is no previous systematic empirical study on IT service time to recovery following outages. The existing literature typically assumes a distribution, or builds on analogies to related areas such as software engineering. Therefore, our objective is to find the statistical distribution of IT service time to recovery. Method-wise, this investigation is based on logs of more than 1800 incidents in a large Nordic bank, corresponding to more than 11000 hours of recorded downtime. Five possible distributions of time to recovery from the literature were investigated using the Akaike Information Criterion to find the distribution offering the best fit. The results show that the log-normal distribution outperformed the others for all tested service channels (collections of IT services). It is concluded that the log-normal distribution offers the best fit of IT service time to recovery. Using this distribution in simulation and decision-support tools offers the prospect of better predictions of downtime and downtime costs to the practitioner community.","1558-1721","","10.1109/TR.2014.2336051","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6872605","Enterprise IT services;incident logs;log-normal distribution","Maintenance engineering;Availability;Software;Decision making;Log-normal distribution;Software reliability","banking;decision support systems;information technology;log normal distribution","enterprise IT services;service availability;IT service time;software engineering;statistical distribution;Nordic bank;Akaike information criterion;log normal distribution;decision support tools;practitioner community","","12","","58","","6 Aug 2014","","","IEEE","IEEE Journals"
"Kernel-based Behavior Analysis for Android Malware Detection","T. Isohara; K. Takemori; A. Kubota","KDDI R&D Labs.. Saitama, Saitama, Japan; KDDI R&D Labs.. Saitama, Saitama, Japan; KDDI R&D Labs.. Saitama, Saitama, Japan","2011 Seventh International Conference on Computational Intelligence and Security","12 Jan 2012","2011","","","1011","1015","The most major threat of Android users is malware infection via Android application markets. In case of the Android Market, as security inspections are not applied for many users have uploaded applications. Therefore, malwares, e.g., Geimini and Droid Dream will attempt to leak personal information, getting root privilege, and abuse functions of the smart phone. An audit framework called log cat is implemented on the Dalvik virtual machine to monitor the application behavior. However, only the limited events are dumped, because an application developers use the log cat for debugging. The behavior monitoring framework that can audit all activities of applications is important for security inspections on the market places. In this paper, we propose a kernel-base behavior analysis for android malware inspection. The system consists of a log collector in the Linux layer and a log analysis application. The log collector records all system calls and filters events with the target application. The log analyzer matches activities with signatures described by regular expressions to detect a malicious activity. Here, signatures of information leakage are automatically generated using the smart phone IDs, e.g., phone number, SIM serial number, and Gmail accounts. We implement a prototype system and evaluate 230 applications in total. The result shows that our system can effectively detect malicious behaviors of the unknown applications.","","978-1-4577-2008-6","10.1109/CIS.2011.226","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6128277","smartphone security;malware;Android","Smart phones;Malware;Humanoid robots;Androids;Mobile communication;Operating systems","invasive software;Linux;mobile computing;operating system kernels;program debugging;smart phones;system monitoring;systems analysis;virtual machines","kernel-based behavior analysis;Android malware detection;malware infection;security inspection;Geimini malware;Droid Dream malware;personal information leakage;smart phone;audit framework;log cat;Dalvik virtual machine;application behavior monitoring;debugging;behavior monitoring framework;kernel-base behavior analysis;log collector;Linux layer;log analysis application;event filtering;malicious activity detection;information leakage signature;malicious behavior detection","","90","114","9","","12 Jan 2012","","","IEEE","IEEE Conferences"
"Using apache spark to collect analytic from the streaming data processing application logs","G. M. Evgenyevich; B. A. Valerievich; B. M. Alekseevna","Sberbank Technology, Moscow, Russia; Ryazan State Radio Engineering University, RSREU, Ryazan, Russia; Ryazan State Radio Engineering University, RSREU, Ryazan, Russia","2018 7th Mediterranean Conference on Embedded Computing (MECO)","9 Jul 2018","2018","","","1","4","This paper presents the approach to design of Apache Spark application architecture to collect analytic from the application logs. Results of implementation those approach, atop of the already existing event streaming system, without of doing any modification inside of it are demonstrated. Also, in the paper discusses the accompanying results obtained in the process of organizing performance tuning and unit testing Apache Spark application both on local and server side.","","978-1-5386-5683-9","10.1109/MECO.2018.8406048","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8406048","big data;lambda architecture;apache spark;log processing;distributed computations","Computer architecture;Sparks;Task analysis;Data processing;Embedded computing;Servers","cluster computing;data analysis;software architecture","application logs;Apache Spark application architecture;streaming data processing;event streaming system","","","","5","","9 Jul 2018","","","IEEE","IEEE Conferences"
"DualFS: A Coordinative Flash File System with Flash Block Dual-mode Switching","B. Wu; M. Peng; D. Feng; W. Tong","Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, Engineering Research Center of Data Storage Systems and Technology; Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, Engineering Research Center of Data Storage Systems and Technology; Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, Engineering Research Center of Data Storage Systems and Technology; Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, Engineering Research Center of Data Storage Systems and Technology","2020 IEEE 38th International Conference on Computer Design (ICCD)","21 Dec 2020","2020","","","65","72","As users' demand for large-capacity storage continues to grow, the widely used NAND flash memory always adopts multi-bit per cell and 3D-stacking technology to improve the storage density however impairing flash performance. Modern flash chips allow flash blocks to switch between multi-bit per cell and one-bit per cell, which gives the potential to benefit from the high performance of Single-Level Cells (SLC, i.e. one-bit per cell). However, the semantic gap caused by the block I/O interface and flash translation layer (FTL) prevents the performance of the underlying flash memory from being fully utilized. In this paper, we propose a log-structured file system, called DualFS. DualFS allows flash blocks to switch between the original-mode and SLC-mode in a free manner and uses these SLC blocks to accelerate critical requests. DualFS dynamically adjusts the capacity of the SLC-mode area according to the total size of valid data to maintain the device a negligible capacity degradation. Besides, DualFS draws advantages from the Open-Channel solid-state disk (SSD) by using software semantic information to guide data placement and management. In particular, since writing the same amount of data to the SLC block will impair more endurance, DualFS proposes a novel life management scheme which throttles the ratio of writing to SLC-mode area within a certain window, thereby accurately control the endurance of SSD. Last, DualFS integrates the garbage collection (GC) procedure, which is directly driven by the file system and merges the GC of the SLC-mode area and the original-mode area. The experiment results show that DualFS offers an average 22.2% performance improvement than the state of art flash file system based on the open-channel SSD. Compared with other storage systems based on flash block dual-mode switching feature, the average performance of DualFS also exhibits considerable improvements by 13.3% to 24.6%. Moreover, DualFS effectively reduces the read response time under different read/write ratios and precisely manage the device endurance.","2576-6996","978-1-7281-9710-4","10.1109/ICCD50377.2020.00028","National Natural Science Foundation of China; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9283510","Flash;Open Channel SSD;Dual Mode","File systems;Semantics;Switches;Writing;Software;Time factors;Flash memories","file organisation;flash memories;NAND circuits;storage management","coordinative flash file system;flash block dual-mode;large-capacity storage;widely used NAND flash memory;storage density however impairing flash performance;modern flash chips;flash blocks;Single-Level Cells;flash translation layer;underlying flash memory;log-structured file system;called DualFS;SLC block;SLC-mode area;original-mode area;22.2% performance improvement;art flash file system;storage systems","","","","21","","21 Dec 2020","","","IEEE","IEEE Conferences"
"A simple logging system for safe internet use","D. Demirol; G. Tuna; R. Das","Bingol University, Department of Computer Programming Bingol, Turkey; Trakya University, Department of Computer Programming Edirne, Turkey; Firat University, Department of Software Engineering Elazig, Turkey","2017 International Artificial Intelligence and Data Processing Symposium (IDAP)","2 Nov 2017","2017","","","1","5","Although the Internet offers numerous advantages, it raises many information security risks, especially against young people and children, who are today amongst the largest user groups of mobile and online technologies all around the world. Therefore, to empower and protect Internet users, it is necessary to develop proper strategies and tools to encapsulate their needs, and identify and prevent all types of the information security risks that may arise during the use of the Internet. In this study, a tracking system to ensure the safe use of the Internet is proposed. Considering the distribution of its potential users, the system has an easy-to-use graphical user interface. By recognizing dangerous web sites and IP addresses, the proposed system blocks access to those sites and this way provides reliable Internet access to its users. During the Internet access, it analyzes accessed IP addresses and port numbers in terms of access type and time and informs the user of the corresponding port numbers which must be closed for safe Internet access. Moreover, by continuously checking the host redirection file of the computer it runs on, it identifies redirections from web addresses to specific IP addresses and this way provides protection against phishing attacks which are becoming one of the most common Internet threats. Although the proposed system is a simple application since it is an open source, freeware application designed for children, it can be improved to consider more sophisticated attack types.","","978-1-5386-1880-6","10.1109/IDAP.2017.8090252","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8090252","Internet;Internet Use;Information Security;Logging","Internet;Web sites;IP networks;Ports (Computers);Browsers","authorisation;graphical user interfaces;Internet;public domain software","simple logging system;safe internet use;information security risks;tracking system;easy-to-use graphical user interface;dangerous web sites;safe Internet access;accessed IP addresses;phishing attacks;open source freeware application","","","","9","","2 Nov 2017","","","IEEE","IEEE Conferences"
"A cross platform intrusion detection system using inter server communication technique","R. Priyadarshini; D. Jagadiswaree; A. Fareedha; M. Janarthanan","Department of Information Technology, B.S. Abdur Rahman University Seethakathi Estate, Vandalur, Chennai, India; Department of Information Technology, B.S. Abdur Rahman University Seethakathi Estate, Vandalur, Chennai, India; Department of Information Technology, B.S. Abdur Rahman University Seethakathi Estate, Vandalur, Chennai, India; Department of Information Technology, B.S. Abdur Rahman University Seethakathi Estate, Vandalur, Chennai, India","2011 International Conference on Recent Trends in Information Technology (ICRTIT)","4 Aug 2011","2011","","","1259","1264","In recent years, web applications have become tremendously popular. However, vulnerabilities are pervasive resulting in exposure of organizations and firms to a wide array of risks. SQL injection attacks, which has been ranked at the top in web application attack mechanisms used by hackers can potentially result in unauthorized access to confidential information stored in a backend database and the hackers can take advantages due to flawed design, improper coding practices, improper validations of user input, configuration errors, or other weaknesses in the infrastructure. Whereas using cross-site scripting techniques, miscreants can hijack Web sessions, and craft credible phishing sites. In this paper we have made a survey on different techniques to prevent SQLi and XSS attacks and we proposed a solution to detect and prevent against the malicious attacks over the developer's Web Application written in programming languages like PHP, ASP.NET and JSP also we have created an API (Application Programming Interface) in native language through which transactions and interactions are sent to IDS Server through Inter Server Communication Mechanism. This IDS Server which is developed from PHPIDS, a purely PHP based intrusion detection system and has a system architecture meant only for PHP application detects and prevents attacks like SQLi (SQL Injection) and XSS(Cross-site scripting), LFI(Local File Inclusion), and RFE(Remote File Execution) and returns back the result to the Web Application and logs the intrusions. In addition to this behavioural pattern of Web Logs is analysed using WAPT algorithm (Web Access Pattern Tree), which helps in recording the activity of the web application and examines any suspicious behaviour, uncommon patterns of behaviour over a period of time, and it also monitors the increased activity and known attack variants. Based on this an report is generated dynamically using P-Chart which can help the Website owner to increase the security measures, and also used to improve the quality of the Web Application.","","978-1-4577-0590-8","10.1109/ICRTIT.2011.5972284","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5972284","SQLi;XSS;PHPIDS;API;Intrusion Log;WAPT","Servers;Databases;Algorithm design and analysis;Security;Monitoring;Application programming interfaces;Arrays","application program interfaces;authoring languages;computer crime;Internet;software architecture;SQL;Web sites","cross platform intrusion detection system;inter server communication technique;Web application attack mechanisms;SQL injection attacks;backend database;cross-site scripting techniques;craft credible phishing sites;SQLi attacks;XSS attacks;malicious attacks;programming languages;PHP;ASP.NET;JSP;API;application programming interface;IDS server;intrusion detection system;system architecture;SQL Injection;cross-site scripting;LFI;local file inclusion;RFE;remote file execution;Web Logs;WAPT algorithm;Web access pattern tree;P-Chart;Web site","","9","","14","","4 Aug 2011","","","IEEE","IEEE Conferences"
"Data Cleaning for Process Mining with Smart Contract","B. Ekici; A. Tarhan; A. Ozsoy","Software Engineering Research Group, Hacettepe University,Ankara,Turkey; Software Engineering Research Group, Hacettepe University,Ankara,Turkey; Blockchain Research Group, Hacettepe University,Ankara,Turkey","2019 4th International Conference on Computer Science and Engineering (UBMK)","21 Nov 2019","2019","","","1","6","Process Mining (PM) is a special data mining technique that allows extracting information from data of critical transactions (i.e. event logs) carried out in Information Systems and monitors the patterns in these transactions. When we start to process event logs with process mining tools, we face with data quality problems such as incorrect and insufficient logging and timing. Thus, data cleaning operations must be applied to event logs before applying process mining on these logs. Being an innovative medium of distributed data processing and storage with the features of enhanced security, traceability, automated transaction verification and integration, Blockchain Technology and Smart Contracts might be a good option to process and store event logs for process mining. In this paper, we focused on the cleaning of the event logs by smart contract as data is flowing from the information systems into the blockchain, and used Hyperledger Composer by IBM to develop our solution. We tested our proposal on an open process data of 1555 records, and compared the cleaning performance of our proposal with that of DataWrangler by Stanford University. Our proposal not only cleaned all 1313 records identified and cleaned by DataWrangler, it also saved 12 additional records with a different date format that was caught and corrected by our smart contract implementation.","","978-1-7281-3964-7","10.1109/UBMK.2019.8907140","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8907140","Data cleaning;process mining;blockchain;smart contract;hyperledger","","contracts;cryptocurrencies;data handling;data mining;program diagnostics;program verification;security of data;storage management","smart contract;data mining technique;information systems;event logs;process mining tools;data quality problems;data cleaning operations;distributed data processing;Smart Contracts;open process data;information extraction;distributed data storage;enhanced security;automated transaction verification;blockchain technology;Hyperledger composer","","","","15","","21 Nov 2019","","","IEEE","IEEE Conferences"
"Xevdriver: A Software System Supporting XML-based Source-to-Source Code Transformations on Fortran Programs","R. Suda; H. Takizawa","Grad. Sch. of Inf. Sci. & Technol., Univ. of Tokyo, Tokyo, Japan; Grad. Sch. of Inf. Sci., Tohoku Univ., Sendai, Japan","2016 Fourth International Symposium on Computing and Networking (CANDAR)","19 Jan 2017","2016","","","522","528","The Xevolver framework is a code transformation framework for supporting evolutional modifications of high performance computing codes. This paper introduces a set of software modules that facilitates administrative tasks about multiple code transformations on multiple source codes. We call this set of software modules the xevdriver, because it drives transformations using Xevolver framework. First, xevdriver provides an abstract view of temporary files. Parsing and unparsing between Fortran and XML are done automatically, and the user do not have to keep track of temporary file names during a series of applications of code transformations. Second, xevdriver also provides an abstract view of combinations of transformations. Xevdriver provides a script language, in which users can define combinations of code transformations as procedures. Third, the logging functions and a log-viewer are provided in accordance with the above abstractions. Users can check how the source codes are transformed on the log-viewer from the high level abstractions of code transformation procedures, and can inspect the intermediate results down into an arbitrary level of concreteness of the implementations of transformations. Our toolset will help development, management and applications of complex code transformations based on Xevolver framework.","2379-1896","978-1-5090-2655-5","10.1109/CANDAR.2016.0096","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7818666","","XML;Data structures;Software;Data mining;Syntactics;Parallel processing","authoring languages;FORTRAN;parallel programming;program compilers;software process improvement;source code (software);XML","Xevdriver;software system;XML-based source-to-source code transformations;Fortran programs;Xevolver framework;evolutional modifications;high performance computing codes;software modules;temporary files;parsing;unparsing;script language;logging functions;log-viewer;high level abstractions","","2","","5","","19 Jan 2017","","","IEEE","IEEE Conferences"
"Using process mining in software development process management: A case study","A. M. Lemos; C. C. Sabino; R. M. F. Lima; C. A. L. Oliveira","Center for Informatics, Federal University of Pernambuco, Recife, Brazil; Center for Informatics, Federal University of Pernambuco, Recife, Brazil; Center for Informatics, Federal University of Pernambuco, Recife, Brazil; Center for Informatics, Federal University of Pernambuco, Recife, Brazil","2011 IEEE International Conference on Systems, Man, and Cybernetics","21 Nov 2011","2011","","","1181","1186","In this paper we describe the application of process mining techniques to analyze a software development process. Software engineering practitioners often conduct quality auditing of the development process to assure conformance with organizational standards. Despite some works have explored process mining techniques for the conformance analysis of general business processes, it is not of our knowledge any study that applies process mining to conformance checking of software development processes. Under a practical perspective, this paper explores a real database with event logs generated in the past five years of execution of a software development process. The database was gently provided by a Brazilian software house with annual revenue of more than US$ 500 million and includes more than 2,000 cases (process instances). The results show that process mining can be effectively employed as a supporting tool for the management of software development processes and for the improvement of the maturity level of software engineering organizations.","1062-922X","978-1-4577-0653-0","10.1109/ICSMC.2011.6083858","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6083858","process mining;software development process;conformance checking","Data mining;PROM;Programming;Process control;Software;Databases;Companies","software development management;software houses","process mining;software development process management;conformance analysis;business process;conformance checking;event log;Brazilian software house;software engineering organization","","6","","13","","21 Nov 2011","","","IEEE","IEEE Conferences"
"Congestion Detection in Mobile Network towards Complex Event Processing","T. Takahashi; H. Yamamoto; N. Fukumoto; S. Ano; K. Yamazaki","Nagaoka Univ. of Technol., Nagaoka, Japan; Nagaoka Univ. of Technol., Nagaoka, Japan; KDDI R&D Labs. Inc., Fujimino, Japan; KDDI R&D Labs. Inc., Fujimino, Japan; Nagaoka Univ. of Technol., Nagaoka, Japan","2013 IEEE 37th Annual Computer Software and Applications Conference","31 Oct 2013","2013","","","459","462","With a wide spread of smartphones and tablets, a mobile network becomes frequently congested when many users concentrate to the same place. Especially when a large-scale event is held, a heavy network congestion interferes with the communication of the participants and local residents. In order to detect the network congestion, a large amount of traffic log should be analyzed in real time. In this paper, the proposed system attempts to detect a sign of the congestion by using a Complex Event Processing (CEP). We analyze network status when the large-scale event, Nagaoka Fireworks festival, is held. As a result, it is concluded that a sign of the network congestion can be detected by combination of the RTT and the abnormal termination of TCP sessions.","0730-3157","978-0-7695-4986-6","10.1109/COMPSAC.2013.77","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6649864","Network Congestion;Streaming Data Processing;Complex Event Processing","Real-time systems;Servers;Data mining;Mobile computing;Mobile communication;Cloud computing;Data processing","computer network management;data handling;mobile communication;transport protocols","network congestion detection;mobile network;complex event processing;smart phones;tablet computers;traffic log;CEP;Nagaoka Fireworks festival;TCP sessions;transport control protocol","","2","2","10","","31 Oct 2013","","","IEEE","IEEE Conferences"
"Securing Internet Information Services (IIS) configuration files","S. Khalid; H. Abbas; M. Pasha; A. Raza","Department of Information Security, Military College of Signals, National University of Sciences & Technology, Islamabad, Pakistan; Centre of Excellence in Information Assurance (COEIA), King Saud University, Riyadh, Saudi Arabia; Department of Information Technology, Bahauddin Zakariya University, Pakistan; Faculty of Information Technology, Majan University College, Muscat, Oman","2012 International Conference for Internet Technology and Secured Transactions","11 Mar 2013","2012","","","726","729","Internet Information Services (IIS) is a modular TCP/IP network server application and a Software Development Kit from Microsoft. As a web server, it provides a platform for hosting and managing web applications and as a software development kit, it facilitates the developers to create applications to manage IIS server, or web applications that run on an IIS server. IIS stores all its configuration settings (server and site level) in plaintext XML files. The reliable functioning of IIS relies heavily on the integrity and confidentiality of these files. The protection provided to these files is; they can be accessed under the administrator's account only and the passwords are stored in encrypted form. But all other configurations relating to sites and the server are present in plaintext and are always accessible to the logged-in administrator. As there is no other protection layer except the administrator account login, therefore if someone manages to get into the system by some means, he can easily modify the files the way, he wants. As the web server is always running (or runs for long time intervals), these files are almost; constantly subjected to threats of integrity and confidentiality. This paper proposes and presents that another security layer be applied on these files, so that the threats to integrity and confidentially be minimized when the configuration files are not being edited by the administrator.","","978-1-908320-08-7","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6470913","Web Server;Internet Information Services;DPAPI","IP networks;Encryption;Software;Force;Entropy","computer network security;Internet;transport protocols","IIS configuration file security;internet information service;TCP-IP network server application;transfer control protocol;Internet protocol;software development kit;Microsoft;Web server;configuration setting;plaintext XML file;extensible markup language;IIS function;administrator account login;security layer;file integrity;file confidentiality","","1","","7","","11 Mar 2013","","","IEEE","IEEE Conferences"
"Design and development of wireless stethoscope with data logging function","S. Nur Hidayah Malek; W. Suhaimizan Wan Zaki; A. Joret; M. Mahadi Abdul Jamil","Department of Electronic Engineering, Faculty of Electrical and Electronic Engineering, UniversitiTun Hussein Onn, UTHM, 86400, Parit Raja, BatuPahat, Johor, Malaysia; Department of Electronic Engineering, Faculty of Electrical and Electronic Engineering, UniversitiTun Hussein Onn, UTHM, 86400, Parit Raja, BatuPahat, Johor, Malaysia; Department of Communication Engineering, Faculty of Electrical and Electronic Engineering, UniversitiTun Hussein Onn, UTHM, 86400, Parit Raja, BatuPahat, Johor, Malaysia; Department of Electronic Engineering, Faculty of Electrical and Electronic Engineering, UniversitiTun Hussein Onn, UTHM, 86400, Parit Raja, BatuPahat, Johor, Malaysia","2013 IEEE International Conference on Control System, Computing and Engineering","23 Jan 2014","2013","","","132","135","Stethoscope is a special device to hear heartbeat sound and monitor pulmonary disease. The most type of stethoscope used these days is the acoustic stethoscope. However, the problem with this acoustic stethoscope is the sound level very low. It is hard to analyze the heart sound and difficult to be diagnosed by a medical doctor. Therefore, this project was developed to monitor and display heartbeat sound using wireless digital stethoscope. The condenser microphone is used as a sensor to capture the low sensitivity of heart sound signal. ZigBee Pro Series 1 wireless module is used to send the heart beat signal wirelessly in 100 meters range. Microcontroller Arduino Nano and Arduino Mega were used as a platform to process the signal and sent the result to the computer. Graphical User Interface (GUI) was developed using MATLAB software to monitor real time electrocardiogram (ECG) waveform and for data logging purpose. The result shows that this device able to transmit and receive ECG waveform wirelessly. The ECG signal can be recorded through data logging application for further analysis by the medical personnel.","","978-1-4799-1508-8","10.1109/ICCSCE.2013.6719946","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6719946","Stethoscope;Arduino;Condenser;Heartbeat;Graphical User Interface;Data Logging","Heart beat;Stethoscope;Graphical user interfaces;Wireless communication;Monitoring;Biomedical monitoring","biomedical equipment;data loggers;electrocardiography;graphical user interfaces;medical signal processing;microcontrollers;microphones","data logging function;pulmonary disease monitoring;acoustic stethoscope;heartbeat sound monitoring;wireless digital stethoscope;condenser microphone;ZigBee Pro Series 1 wireless module;Arduino nano microcontroller;Arduino mega microcontroller;signal processing;graphical user interface;GUI;MATLAB software;electrocardiogram waveform monitoring;ECG waveform monitoring;ECG signal recording","","3","","11","","23 Jan 2014","","","IEEE","IEEE Conferences"
"Big Data Based Security Analytics for Protecting Virtualized Infrastructures in Cloud Computing","T. Y. Win; H. Tianfield; Q. Mair","Faculty of Business, Computing & Applied Sciences, University of Gloucestershire, Cheltenham, United Kingdom; Department of Computer, Communications and Interactive Systems, Glasgow Caledonian University, Glasgow, United Kingdom; Department of Computer, Communications and Interactive Systems, Glasgow Caledonian University, Glasgow, United Kingdom","IEEE Transactions on Big Data","27 Feb 2018","2018","4","1","11","25","Virtualized infrastructure in cloud computing has become an attractive target for cyberattackers to launch advanced attacks. This paper proposes a novel big data based security analytics approach to detecting advanced attacks in virtualized infrastructures. Network logs as well as user application logs collected periodically from the guest virtual machines (VMs) are stored in the Hadoop Distributed File System (HDFS). Then, extraction of attack features is performed through graph-based event correlation and MapReduce parser based identification of potential attack paths. Next, determination of attack presence is performed through two-step machine learning, namely logistic regression is applied to calculate attack's conditional probabilities with respect to the attributes, and belief propagation is applied to calculate the belief in existence of an attack based on them. Experiments are conducted to evaluate the proposed approach using well-known malware as well as in comparison with existing security techniques for virtualized infrastructure. The results show that our proposed approach is effective in detecting attacks with minimal performance overhead.","2332-7790","","10.1109/TBDATA.2017.2715335","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7949076","Virtualized infrastructure;virtualization security;cloud security;malware detection;rootkit detection;security analytics;event correlation;logistic regression;belief propagation","Security;Malware;Feature extraction;Big Data;Correlation;Databases;Cloud computing","Big Data;cloud computing;feature extraction;graph theory;invasive software;learning (artificial intelligence);parallel processing;random functions;virtual machines;virtualisation","guest virtual machines;Hadoop Distributed File System;MapReduce parser based identification;potential attack paths;cloud computing;advanced attacks;security analytics approach;big data based security analytics;virtualized infrastructures;HDFS;attack features extraction;two-step machine learning;logistic regression;graph-based event correlation;network logs;user application logs","","5","","27","","15 Jun 2017","","","IEEE","IEEE Journals"
"An automatic modeling method of team-based organization structure in e-business","Yan Li","School of Economics and Management, Shanghai Maritime University, China, 200135","2010 International Conference on Networking and Digital Society","7 Jun 2010","2010","1","","342","345","Realization of e-business is based on business process management system (BPMS). In BPMS there are a large number of event logs recording the performers who initiate or complete the activities. Team-based organization structure is so flexible as to be more suitable for the business implementing e-business. However the existing organizational structure modeling methods are difficult to model team-based structure. This paper design an automatic modeling method of team-based organization structure based on event logs from BPMS. Firstly, six kinds of performer relations are analyzed, i.e. PR1-PR6; Secondly, Five kinds of modeling rules are designed, i.e. MR1-MR5; At last, an actual case are given to verify feasibility of this method.","","978-1-4244-5162-3","10.1109/ICNDS.2010.5479200","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5479200","economic business;business process management system;team-based organization structure modeling;event logs","Information analysis;Performance analysis;Supply chain management;Enterprise resource planning;Conference management;Resource management;Technology management;Information technology;Process planning;Workflow management software","corporate modelling;enterprise resource planning;organisational aspects","automatic modeling method;team based organization structure;e-business;business process management system","","","","11","","7 Jun 2010","","","IEEE","IEEE Conferences"
"Extracting weblog of Siam University for learning user behavior on MapReduce","W. Premchaiswadi; W. Romsaiyud","Graduate School of Information Technology, Siam University, 38 Petkasem rd., Bangkok, 10160, Thailand; Graduate School of Information Technology, Siam University, 38 Petkasem rd., Bangkok, 10160, Thailand","2012 4th International Conference on Intelligent and Advanced Systems (ICIAS2012)","20 Sep 2012","2012","1","","149","154","MapReduce is a framework that allows developers to write applications that rapidly process and analyze large volumes of data in a massively parallel scale. Moreover, a clickstream is a record of a user's activity on the Internet. Using a clickstream analysis we can collect, analyze, and report aggregate data about which pages visitors visit in what order - and which are the result of the succession of mouse clicks each visitor makes. Clickstream analysis can reveal usage patterns leading to a heightened understanding of users' behavior. In this paper, we introduced a novel and efficient web log mining model for web users clustering. In general, our model consists of three main steps; 1) Computing the similarity measure of any path in a web page, 2) Defining the k-mean clustering for group customerID 3) Generating the report based on the Hadoop MapReduce Framework. Consequently, our experiments were run on real world data derived from weblogs of Siam University at Bangkok, Thailand (www.siam.edu).","","978-1-4577-1967-7","10.1109/ICIAS.2012.6306177","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6306177","MapReduce;Hadoop Distributed File System (HDFS);Clickstream Data;K-mean Clustering;Web log Analytics","Educational institutions;Web pages;File systems;Data mining;Web servers;Artificial intelligence;Computational modeling","computer aided instruction;data analysis;data mining;information retrieval;parallel programming;pattern clustering;public domain software;storage management;Web services","WebLog extraction;user behavior learning;data analysis;data processing;Internet;clickstream analysis;Web log mining model;Web user clustering;similarity measure computing;Web page;k-mean clustering;group customerID;Hadoop;MapReduce","","9","","24","","20 Sep 2012","","","IEEE","IEEE Conferences"
"git2net - Mining Time-Stamped Co-Editing Networks from Large git Repositories","C. Gote; I. Scholtes; F. Schweitzer",ETH Zürich; University of Zürich; ETH Zürich,"2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)","29 Aug 2019","2019","","","433","444","Data from software repositories have become an important foundation for the empirical study of software engineering processes. A recurring theme in the repository mining literature is the inference of developer networks capturing e.g. collaboration, coordination, or communication, from the commit history of projects. Most of the studied networks are based on the co-authorship of software artefacts defined at the level of files, modules, or packages. While this approach has led to insights into the social aspects of software development, it neglects detailed information on code changes and code ownership, e.g. which exact lines of code have been authored by which developers, that is contained in the commit log of software projects. Addressing this issue, we introduce git2net, a scalable python software that facilitates the extraction of fine-grained co-editing networks in large git repositories. It uses text mining techniques to analyse the detailed history of textual modifications within files. This information allows us to construct directed, weighted, and time-stamped networks, where a link signifies that one developer has edited a block of source code originally written by another developer. Our tool is applied in case studies of an Open Source and a commercial software project. We argue that it opens up a massive new source of high-resolution data on human collaboration patterns.","2574-3864","978-1-7281-3412-3","10.1109/MSR.2019.00070","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8816744","network analysis;data analysis;collaboration network;repository mining;git;co editing networks;temporal networks;social networks;open source;empirical software engineering","Software;Data mining;Tools;Collaboration;Social networking (online);History;Databases","data mining;Python;software maintenance;source code (software);text analysis","commercial software project;git repositories;software repositories;software engineering processes;repository mining literature;software artefacts;social aspects;software development;code changes;commit log;fine-grained co-editing networks;text mining techniques;time-stamped networks;source code;git2net;scalable Python software;time-stamped co-editing networks","","2","","57","","29 Aug 2019","","","IEEE","IEEE Conferences"
"Contextual anomaly detection for a critical industrial system based on logs and metrics","M. Farshchi; I. Weber; R. Della Corte; A. Pecchia; M. Cinque; J. Schneider; J. Grundy","Swinburne Univ. of Technol., Melbourne, VIC, Australia; Data61, CSIRO, Sydney, NSW, Australia; Federico II Univ. of Naples, Naples, Italy; Federico II Univ. of Naples, Naples, Italy; Federico II Univ. of Naples, Naples, Italy; Swinburne Univ. of Technol., Melbourne, VIC, Australia; Monash Univ., Melbourne, VIC, Australia","2018 14th European Dependable Computing Conference (EDCC)","11 Nov 2018","2018","","","140","143","Recent advances in contextual anomaly detection attempt to combine resource metrics and event logs to uncover unexpected system behaviors at run-time. This is highly relevant for critical software systems, where monitoring is often mandated by international standards and guidelines. In this paper, we analyze the effectiveness of a metrics-logs contextual anomaly detection technique in a middleware for Air Traffic Control systems. Our study addresses the challenges of applying such techniques to a new case study with a dense volume of logs, and finer monitoring sampling rate. Guided by our experimental results, we propose and evaluate several actionable improvements, which include a change detection algorithm and the use of time windows on contextual anomaly detection.","","978-1-5386-8060-5","10.1109/EDCC.2018.00033","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8530774","Anomaly detection, contextual anomaly, system monitoring, log analysis, change detection","Anomaly detection;Measurement;Middleware;Monitoring;Mathematical model;Computer crashes;Predictive models","air traffic control;middleware;safety-critical software;software metrics","critical industrial system;resource metrics;event logs;unexpected system behaviors;run-time;critical software systems;international standards;metrics-logs contextual anomaly detection;Air Traffic Control systems;change detection algorithm;middleware","","2","","10","","11 Nov 2018","","","IEEE","IEEE Conferences"
"Detecting Falsified Timestamps in Evidence Graph via Attack Graph","Y. Zhang; J. He; J. Xu","Coll. of Comput. Sci., Beijing Univ. of Technol., Beijing, China; Sch. of Software Eng., Beijing Univ. of Technol., Beijing, China; Coll. of Comput. Sci., Beijing Univ. of Technol., Beijing, China","2015 8th International Symposium on Computational Intelligence and Design (ISCID)","12 May 2016","2015","2","","369","374","Network forensics investigations aims to find a chain of evidences that helps reconstructing the alleged attack scenario. This often requires the check of timestamps of the logs to reconstruct the event. Yet, it is relatively easy for criminals to tamper with the event logs, which results in the evidence graph with falsified timestamps and hence hinders the event reconstruction. The aim of this work paper is to propose an algorithm detects these falsified timestamps and re-creates the true evidence graph. Our algorithm relies on attack graphs of the system environment which models known vulnerability sequences that were exploited to launch the attack. We demonstrate the effectiveness and performance of our algorithm via a possible attack scenario in a network environment running a file server and a database server.","","978-1-4673-9587-8","10.1109/ISCID.2015.111","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7469152","Falsified timestamps;Evidence graph;Attack graph;Network forensic","Computational intelligence","computer network security;digital forensics;file servers;graph theory;sequences","attack graph;evidence graph;event reconstruction;known vulnerability sequences;network environment;file server;database server;falsified timestamp detection;network forensics","","1","","10","","12 May 2016","","","IEEE","IEEE Conferences"
"Is Your Upgrade Worth It? Process Mining Can Tell","M. van Genuchten; R. Mans; H. Reijers; D. Wismeijer","VitalHealth Software, Ede; Eindhoven University of Technology, Eindhoven; Eindhoven University of Technology, Eindhoven; ACTA, Amsterdam","IEEE Software","15 Sep 2014","2014","31","5","94","100","Software vendors typically release updates and upgrades of their software once or twice a year. Users are then faced with the question of whether the upgrade is worth the price and the trouble. The software industry doesn't provide much evidence that it's worthwhile to upgrade to new releases. The authors propose the use of process mining to prove that upgrading to the next release provides quantifiable benefits to the end user. Process mining capitalizes on the fact that event logs capture information about processes. These events can be used to make processes visible and show the benefits of using a software product's next release. Three groups benefits from this process: end users, software suppliers, and researchers. The authors applied process mining to a medical software product and captured empirical data from 1,400 cases. The data shows that the new version was 11 percent more efficient than the old release.","1937-4194","","10.1109/MS.2014.20","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6728931","usability;process mining;product metrics","Usability;Data mining;Software measurements","data mining;software maintenance","process mining;software vendors;software updates;software upgrades;software industry;event logs;software product release;medical software product;empirical data","","6","","15","","30 Jan 2014","","","IEEE","IEEE Magazines"
"SLA guarantee real-time monitoring system with soft deadline constraint","P. Kittipipattanathaworn; N. Nupairoj","Department of Computer Engineering, Chulalongkorn University, Bangkok, Thailand, 10330; Department of Computer Engineering, Chulalongkorn University, Bangkok, Thailand, 10330","2014 11th International Joint Conference on Computer Science and Software Engineering (JCSSE)","26 Jun 2014","2014","","","52","57","Real time monitoring of data streams can be used for monitoring performance, availability or system anomaly detection which can analyze and correlate events from logs file. However, due to the increasing complexity of modern distributed systems, the size of logs can become very large. Monitoring with timing constraint becomes more important in facilitating the enforcement of conditional guarantees. In general, real-time systems are usually classified into soft and hard which need to comply the term of Service Level Agreements (SLAs). In this paper, we focused on the probabilistic deadline in order to guarantee the achievement of SLA deadline for verification of soft deadlines in real-time system by adopt the Central limit theorem in order to approximate the distributions and calculate the probability of a violation of SLA guarantee which can be used to determine the deadline constraint for the system.","","978-1-4799-5822-1","10.1109/JCSSE.2014.6841841","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6841841","Monitoring system;real time;SLAs;Soft deadlines;log correlation;Availability","","contracts;real-time systems;statistical distributions;system monitoring","SLA guarantee real-time monitoring system;soft deadline constraint;service level agreements;probabilistic deadline;real-time system;central limit theorem","","1","","22","","26 Jun 2014","","","IEEE","IEEE Conferences"
"Conformance checking of electronic business processes to secure distributed transactions","M. Talamo; F. Arcieri; C. H. Schunck; A. C. D'Iddio","Department of Business Engineering, University of Rome Tor Vergata, Italy 00133; Nestor Lab, University of Rome Tor Vergata, Italy 00133; Nestor Lab, University of Rome Tor Vergata, Italy 00133; Department of Business Engineering, University of Rome Tor Vergata, Italy 00133","2013 47th International Carnahan Conference on Security Technology (ICCST)","16 Oct 2014","2013","","","1","6","Advances in computer technologies facilitate the implementation of inter-organizational business processes. At the same time, managing the security of these processes is increasingly difficult. Compliance with high level specifcations, like normatives and pre-agreed protocols, rules and requirements, is difficult to validate. Here we discuss how Conformance Checking, a specific area of Process Mining, can be adapted for this purpose. Its role is to verify if an execution of a business process satisfies specifications represented by formal models (e.g. Petri Nets, Transition Systems, structures based on partial orders, etc). In the process mining literature, few efforts have been dedicated to online checking of business processes and choreographies for security purposes. The main requirement is high precision and reliability of event logs. They should record, precisely and unambiguously, all security-relevant activities of the analyzed process. Mantaining high-level logs becomes difficult with choreographies: log data are distributed, and must be related to events. Important metadata of event logs, like timestamps, can be ambiguous. Moreover, some data cannot be distributed due to security or privacy issues. These problems result in security-relevant ambiguities in event logs. Here we define a framework to create high-level event logs for online inter-organizational compliance checking using a Validation Authority. The system described here has been implemented in the issuing infrastructure for the Italian Electronic Identity card.","2153-0742","978-1-4799-0889-9","10.1109/CCST.2013.6922056","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6922056","","Security;Business;Software agents;Computational modeling;Data mining;Automata;Petri nets","business data processing;data mining;distributed processing;security of data","conformance checking;electronic business process;distributed transaction security;computer technology;process mining;security purpose;event logs precision;event logs reliability;validation authority;Italian Electronic Identity card","","3","","21","","16 Oct 2014","","","IEEE","IEEE Conferences"
"Scalable Run-Time Correlation Engine for Monitoring in a Cloud Computing Environment","M. Wang; V. Holub; T. Parsons; J. Murphy; P. O'Sullivan","Performance Eng. Lab., Univ. Coll. Dublin, Dublin, Ireland; Performance Eng. Lab., Univ. Coll. Dublin, Dublin, Ireland; Performance Eng. Lab., Univ. Coll. Dublin, Dublin, Ireland; Performance Eng. Lab., Univ. Coll. Dublin, Dublin, Ireland; IBM Software Group, Dublin, Ireland","2010 17th IEEE International Conference and Workshops on Engineering of Computer Based Systems","3 May 2010","2010","","","29","38","Monitoring the status of running applications is a real life requirement and important research area. In particular log analysis is often required to understand how the system is behaving during execution. For example it is common for system administrators to collect and view logs from different hardware and software components to gain an understanding into system behavior, especially during activities such as problem determination. A recent research project in this area, the Run Time Correlation Engine (RTCE), provides a framework for run-time correlation of distributed log files in a scalable manner for enterprise applications. The framework has been designed for enterprise applications consisting of distributed software components and is in use in real industry environments. The purpose of this paper is to explore how the RTCE can scale for cloud computing environments where providers of cloud services will require large architectures (e.g. data centers) to deploy such services.","","978-1-4244-6538-5","10.1109/ECBS.2010.11","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5457787","Scalable;Event;Correlation","Runtime environment;Monitoring;Cloud computing;Testing;Educational institutions;Application software;Search engines;Computer architecture;Hardware;Sun","distributed processing;Internet;ubiquitous computing","scalable run-time correlation engine;cloud computing;run time correlation engine;distributed log file;RTCE","","9","1","18","","3 May 2010","","","IEEE","IEEE Conferences"
"A Co-Training Strategy for Multiple View Clustering in Process Mining","A. Appice; D. Malerba","Dipartimento di Informatica, Università degli Studi Aldo Moro di Bari, via Orabona, Bari, Italy; Dipartimento di Informatica, Università degli Studi Aldo Moro di Bari, via Orabona, Bari, Italy","IEEE Transactions on Services Computing","20 May 2017","2016","9","6","832","845","Process mining refers to the discovery, conformance, and enhancement of process models from event logs currently produced by several information systems (e.g. workflow management systems). By tightly coupling event logs and process models, process mining makes it possible to detect deviations, predict delays, support decision making, and recommend process redesigns. Event logs are data sets containing the executions (called traces) of a business process. Several process mining algorithms have been defined to mine event logs and deliver valuable models (e.g. Petri nets) of how logged processes are being executed. However, they often generate spaghetti-like process models, which can be hard to understand. This is caused by the inherent complexity of real-life processes, which tend to be less structured and more flexible than what the stakeholders typically expect. In particular, spaghetti-like process models are discovered when all possible behaviors are shown in a single model as a result of considering the set of traces in the event log all at once.To minimize this problem, trace clustering can be used as a preprocessing step. It splits up an event log into clusters of similar traces, so as to handle variability in the recorded behavior and facilitate process model discovery. In this paper, we investigate a multiple view aware approach to trace clustering, based on a co-training strategy. In an assessment, using benchmark event logs, we show that the presented algorithm is able to discover a clustering pattern of the log, such that related traces result appropriately clustered. We evaluate the significance of the formed clusters using established machine learning and process mining metrics.","1939-1374","","10.1109/TSC.2015.2430327","PON; VINCENTE—A Virtual collective INtelligenCe ENvironment to develop sustainable Technology Entrepreneurship ecosystems; Italian Ministry of University and Research (MIUR); ATENEO; Mining Complex Patterns; University of Bari Aldo Moro; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7102743","Clustering;co-training;multiple view learning;process mining","Clustering algorithms;Computational modeling;Data mining;Training;Partitioning algorithms","data mining;decision making;information systems;learning (artificial intelligence);pattern clustering;Petri nets;workflow management software","decision making;process redesign;business process trace;Petri nets;spaghetti-like process model generation;trace clustering;variability handling;benchmark event logs;log pattern clustering;delay prediction;deviation detection;workflow management systems;information systems;process model enhancement;process model conformance;process model discovery;process mining;multiple view clustering;cotraining strategy;machine learning","","23","","39","","6 May 2015","","","IEEE","IEEE Journals"
"Assessing time coalescence techniques for the analysis of supercomputer logs","C. Di Martino; M. Cinque; D. Cotroneo","Center for Reliable and High-Performance Computing, University of Illinois at Urbana-Champaign, 1308 W. Main Street, 61801, USA; Dipartimento di Informatica e Sistemistica, Università degli Studi di Napoli Federico II, Via Claudio 21, 80125, Italy; Dipartimento di Informatica e Sistemistica, Università degli Studi di Napoli Federico II, Via Claudio 21, 80125, Italy","IEEE/IFIP International Conference on Dependable Systems and Networks (DSN 2012)","9 Aug 2012","2012","","","1","12","This paper presents a novel approach to assess time coalescence techniques. These techniques are widely used to reconstruct the failure process of a system and to estimate dependability measurements from its event logs. The approach is based on the use of automatically generated logs, accompanied by the exact knowledge of the ground truth on the failure process. The assessment is conducted by comparing the presumed failure process, reconstructed via coalescence, with the ground truth. We focus on supercomputer logs, due to increasing importance of automatic event log analysis for these systems. Experimental results show how the approach allows to compare different time coalescence techniques and to identify their weaknesses with respect to given system settings. In addition, results revealed an interesting correlation between errors caused by the coalescence and errors in the estimation of dependability measurements.","2158-3927","978-1-4673-1625-5","10.1109/DSN.2012.6263946","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6263946","Event Log Analysis;supercomputer dependability;data coalescence;dependability assessment","Supercomputers;Logic gates;Generators;Writing;Computational modeling;Libraries;Software","parallel machines;storage management;system recovery","time coalescence technique assessment;supercomputer log analysis;failure process reconstruction;dependability measurement estimation;automatically generated logs;ground truth;supercomputer dependability;automatic event log analysis;errors;data coalescence","","20","1","36","","9 Aug 2012","","","IEEE","IEEE Conferences"
"Near-Omniscient Debugging for Java Using Size-Limited Execution Trace","K. Shimari; T. Ishio; T. Kanda; K. Inoue",Osaka University; Nara Institute of Science and Technology; Osaka University; Osaka University,"2019 IEEE International Conference on Software Maintenance and Evolution (ICSME)","5 Dec 2019","2019","","","398","401","Logging is an important feature for a software system to record its run-time information. Detailed logging allows developers to collect information in situations where they cannot use an interactive debugger, such as continuous integration and web application server cases. However, extensive logging leads to larger execution traces because few instructions could be repeated many times. To record detailed program behavior within limited storage space constraints, we propose Near-Omniscient Debugging, a methodology that records an execution trace using fixed size buffers for each observed instruction. Our tool monitors a Java program's execution and annotates source code with observed values in an HTML format. Developers can easily investigate the execution and share the report on a web server. In case of DaCapo benchmark applications, our tool requires fewer than 1% of the complete execution traces to visualize all runtime values used by 66% of instructions that are executed less than 64 times. Developers also can obtain data dependencies with precision 91.8% and recall 79.0% using this tool.","2576-3148","978-1-7281-3094-1","10.1109/ICSME.2019.00068","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8919216","Dynamic Analysis;Logging;Software Visualization","Tools;Debugging;Java;Benchmark testing;Monitoring;Runtime;Data visualization","Internet;Java;program debugging;program diagnostics","complete execution;DaCapo benchmark applications;web server;Java program;observed instruction;size buffers;Near-Omniscient Debugging;storage space constraints;detailed program behavior;larger execution;extensive logging;web application server cases;continuous integration;interactive debugger;detailed logging;run-time information;software system;execution trace","","3","","15","","5 Dec 2019","","","IEEE","IEEE Conferences"
"Lightweight Task Graph Inference for Distributed Applications","B. Xin; P. Eugster; X. Zhang; J. Yang","Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN, USA; Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN, USA; Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN, USA; Center for Software Excellence, Microsoft Corp., Redmond, IN, USA","2010 29th IEEE Symposium on Reliable Distributed Systems","9 Nov 2010","2010","","","100","110","Recent paradigm shifts in distributed computing such as the advent of cloud computing pose new challenges to the analysis of distributed executions. One important new characteristic is that the management staff of computing platforms and the developers of applications are separated by corporate boundaries. The net result is that once applications go wrong, the most readily available debugging aids for developers are the visible output of the application and any log files collected during their execution. In this paper, we propose the concept of task graphs as a foundation to represent distributed executions, and present a low overhead algorithm to infer task graphs from event log files. Intuitively, a task represents an autonomous segment of computation inside a thread. Edges between tasks represent their interactions and preserve programmers' notion of data and control flows. Our technique leverages existing logging support where available or otherwise augments it with aspect-based instrumentation to collect events of a set of predefined types. We show how task graphs can improve the precision of anomaly detection in a request-oriented analysis of field software and help programmers understand the running of the Hadoop Distributed File System (HDFS).","1060-9857","978-0-7695-4250-8","10.1109/SRDS.2010.20","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5623382","task graphs;happens-before;distributed computing;log analysis;anomaly detection","Clocks;Instruction sets;Sockets;Synchronization;Programming;Java;Distributed databases","aspect-oriented programming;data flow analysis;graph theory;inference mechanisms;Internet","task graph inference;distributed computing;cloud computing;debugging aids;data flows;control flows;aspect based instrumentation;field software;Hadoop distributed file system","","2","","23","","9 Nov 2010","","","IEEE","IEEE Conferences"
"CBR: Controlled Burst Recording","O. Cornejo; D. Briola; D. Micucci; L. Mariani","University of Luxembourg,SnT Centre for Security, Reliability and Trust,Luxembourg; University of Milano Bicocca,Department of Informatics, Systems and Communication,Milan,Italy,20126; University of Milano Bicocca,Department of Informatics, Systems and Communication,Milan,Italy,20126; University of Milano Bicocca,Department of Informatics, Systems and Communication,Milan,Italy,20126","2020 IEEE 13th International Conference on Software Testing, Validation and Verification (ICST)","5 Aug 2020","2020","","","243","253","Collecting traces from software running in the field is both useful and challenging. Traces may indeed help revealing unexpected usage scenarios, detecting and reproducing failures, and building behavioral models that reflect how the software is actually used. On the other hand, recording traces is an intrusive activity that may annoy users, negatively affecting the usability of the applications, if not properly designed.In this paper we address field monitoring by introducing Controlled Burst Recording, a monitoring solution that can collect comprehensive runtime data without compromising the quality of the user experience. The technique encodes the knowledge extracted from the monitored application as a finite state model that both represents the sequences of operations that can be executed by the users and the corresponding internal computations that might be activated by each operation.Our initial assessment with information extracted from ArgoUML shows that Controlled Burst Recording can reconstruct behavioral information more effectively than competing sampling techniques, with a low impact on the system response time.","2159-4848","978-1-7281-5778-8","10.1109/ICST46399.2020.00033","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9159074","field monitoring;tracing;logging","Monitoring;Software;Runtime;Computational modeling;Software reliability;Electronic mail","software reliability","monitored application;finite state model;Controlled Burst Recording;software running;behavioral models;field monitoring;user experience;CBR;ArgoUML","","","","48","","5 Aug 2020","","","IEEE","IEEE Conferences"
"A Technique for Measuring Data Persistence Using the Ext4 File System Journal","K. D. Fairbanks","Electr. & Comput. Eng. Dept., United States Naval Acad., Annapolis, MD, USA","2015 IEEE 39th Annual Computer Software and Applications Conference","24 Sep 2015","2015","3","","18","23","In this paper, we propose a method of measuring data persistence using the Ext4 journal. Digital Forensic tools and techniques are commonly used to extract data from media. A great deal of research has been dedicated to the recovery of deleted data, however, there is a lack of information on quantifying the chance that an investigator will be successful in this endeavor. To that end, we suggest the file system journal be used as a source to gather empirical evidence of data persistence, which can later be used to formulate the probability of recovering deleted data under various conditions. Knowing this probability can help investigators decide where to best invest their resources. We have implemented a proof of concept system that interrogates the Ext4 file system journal and logs relevant data. We then detail how this information can be used to track the reuse of data blocks from the examination of file system metadata structures. This preliminary design contributes a novel method of tracking deleted data persistence that can be used to generate the information necessary to formulate probability models regarding the full and/or partial recovery of deleted data.","0730-3157","978-1-4673-6564-2","10.1109/COMPSAC.2015.164","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7273317","Ext4;File System Forensics;Digital Forensics;Journal;Data Persistence;Data Recovery;Persistence Measurement","File systems;Media;Metadata;Data mining;Digital forensics;Data structures;Operating systems","digital forensics;file organisation;probability","data persistence;Ext4 file system journal;digital forensic tools;data extraction;probability;proof of concept system","","2","","12","","24 Sep 2015","","","IEEE","IEEE Conferences"
"Process Mining of Logged Gaming Behavior","S. Ramadan; H. Ibrahim Baqapuri; E. Roecher; K. Mathiak",RWTH Aachen University; RWTH Aachen University; RWTH Aachen University; RWTH Aachen University,"2019 International Conference on Process Mining (ICPM)","5 Aug 2019","2019","","","57","64","Video gaming takes a significant position in contemporary social life. It also constitutes as an elegant scientific model to study social behavior. By applying this model, behavioral patterns and their underlying physiological processes can be precisely observed at millisecond resolutions, e.g. with functional brain imaging. To date, a detailed behavioral analysis on an event-level was only performed in a few studies. Process mining (PM) is an efficient methodology to assess and systematize these complex behavioral patterns. We describe the application of PM to semi-naturalistic behavior during gameplay of a first-person shooter (FPS). Behavioral data were collected from 18 participants performing three 10-min gaming sessions of a customized FPS during functional brain imaging. An event log recorded during gameplay represented 10 types of game events in chronological order. In total 54 sessions were entered in the analysis serving as process indicators (cases). ProM software (version 6.8 TU Eindhoven, NL) provided data summary, heuristic and fuzzy PM. Event summaries revealed that being under attack early on prevented environmental exploration and resulted in shorter behavioral cycles, in contrast to other behaviors such as navigation and active attacking. Furthermore, the Fuzzy Miner generated a general overview of the player behavior by defining gaming pattern sequences. Upon further investigation, the Heuristic Miner revealed examples of how PM could highlight important patterns in player behavior. These patterns reflect the uninfluenced behavior ('free will') of the player and the player's tendency towards aggressive or defensive behavior. PM was used to analyze and systematize behavioral patterns in complex semi-naturalistic behavior during game play and showed capability to give an overview of this behavior and then further describe detailed information about them. This can allow for more in-depth investigations of interactive social behavior in the future. Furthermore, the capability of inspecting behavioral cycle characteristics (length, performance) in varied situations enables the use of PM as a testing tool for the preparation of game related paradigms in functional brain imaging. Finally, the description of such patterns and their alterations in psychiatric disorders may be applied to investigate therapeutic targets for social impairments.","","978-1-7281-0919-0","10.1109/ICPM.2019.00019","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8786064","Process mining, fMRI, Behavioral patterns, First-person shooter, ProM, Semi-naturalistic behavior","Games;PROM;Imaging;Data mining;Software;Brain modeling","behavioural sciences computing;brain;computer games;data mining;fuzzy set theory;medical image processing","process mining;logged gaming behavior;video gaming;contemporary social life;elegant scientific model;functional brain imaging;complex behavioral patterns;player behavior;gaming pattern sequences;aggressive behavior;defensive behavior;complex seminaturalistic behavior;interactive social behavior;behavioral cycle characteristics;game related paradigms;physiological processes;behavioral analysis;gaming sessions;first-person shooter gameplay;FPS;fuzzy PM","","","","21","","5 Aug 2019","","","IEEE","IEEE Conferences"
"DeepLink: A Code Knowledge Graph Based Deep Learning Approach for Issue-Commit Link Recovery","R. Xie; L. Chen; W. Ye; Z. Li; T. Hu; D. Du; S. Zhang","National Engineering Research Center for Software Engineering, Peking University, Beijing, 100871, China; National Engineering Research Center for Software Engineering, Peking University, Beijing, 100871, China; National Engineering Research Center for Software Engineering, Peking University, Beijing, 100871, China; National Engineering Research Center for Software Engineering, Peking University, Beijing, 100871, China; National Engineering Research Center for Software Engineering, Peking University, Beijing, 100871, China; National Engineering Research Center for Software Engineering, Peking University, Beijing, 100871, China; National Engineering Research Center for Software Engineering, Peking University, Beijing, 100871, China","2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER)","18 Mar 2019","2019","","","434","444","Links between issue reports and corresponding code commits to fix them can greatly reduce the maintenance costs of a software project. More often than not, however, these links are missing and thus cannot be fully utilized by developers. Current practices in issue-commit link recovery extract text features and code features in terms of textual similarity from issue reports and commit logs to train their models. These approaches are limited since semantic information could be lost. Furthermore, few of them consider the effect of source code files related to a commit on issue-commit link recovery, let alone the semantics of code context. To tackle these problems, we propose to construct code knowledge graph of a code repository and generate embeddings of source code files to capture the semantics of code context. We also use embeddings to capture the semantics of issue- or commit-related text. Then we use these embeddings to calculate semantic similarity and code similarity using a deep learning approach before training a SVM binary classification model with additional features. Evaluations on real-world projects show that our approach DeepLink can outperform the state-of-the-art method.","1534-5351","978-1-7281-0591-8","10.1109/SANER.2019.8667969","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8667969","Issue-Commit Link;Semantic Similarity;Code Knowledge Graph;Code Context;Code Embeddings;Deep Learning","Semantics;Feature extraction;Deep learning;Software;Maintenance engineering;Context modeling;Data mining","graph theory;information retrieval;learning (artificial intelligence);natural language processing;pattern classification;software maintenance;support vector machines;text analysis","code knowledge graph;deep learning approach;issue-commit link recovery;source code files;code context;code repository;semantic similarity;code similarity;SVM binary classification model;commit-related text;issue-related text;software project;maintenance cost reduction","","1","","29","","18 Mar 2019","","","IEEE","IEEE Conferences"
"Software application for QTS24 test strip","S. Ali; N. Afshan; I. A. Jafri","CESAT, Islamabad, Pakistan; CESAT, Islamabad, Pakistan; CESAT, Islamabad, Pakistan","2018 15th International Bhurban Conference on Applied Sciences and Technology (IBCAST)","12 Mar 2018","2018","","","240","242","The API (Analytical Profile Index) - a well-established method for bacterial identification to the species level-is a bacterial classification based on biochemical tests On the same grounds, QTS24 test trip (an indigenous gramve test strip) was developed by CESAT for the identification of bacteria organisms which has shown promising comparative results with API. The test strip contains 24 bacterial identification tests which are comparatively more in numbers than API. In QTS24 test strip, the process of bacterial identification is based on the positive and negative laboratory test results of the test strip which can be interpreted by manual interpretation dichotomous elimination tree/key[2] This simplifies the bacterial identification procedure while working with the loads of isolates in the lab. To further aid the interpretation and recording of results, an easy to install windows based software application for QTS-24 Test Strips has been developed. This software application will automate the interpretation dichotomous elimination tree / key technique to further simplify the procedure of working with loads of isolates in the lab. The software application accepts the positives and negative lab results of QTS strip for 27 biochemical tests to identify 75 different organisms of bacteria. QTS software application features include, logging of interpretation results into database, searching of logged results, modification and deletion of logged results. It also features on taking backup of logged result database and restoring the old database of logged results back into software application. Display of 75 nomenclature and percentage positivity and negativity pivot chart is further added to the software application features. The software application is developed in Microsoft Technologies at front and Microsoft SQL Compact database at the backend to store the process results.","2151-1411","978-1-5386-3564-3","10.1109/IBCAST.2018.8312231","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8312231","API;QTS24 Software Application;Test Strip;Bacteria Identification;Dichotomous Elimination","Software;Microorganisms;Strips;Databases;Testing;Manuals","biology computing;microorganisms;relational databases;SQL","QTS24 test strip;API;indigenous gramve test strip;bacterial identification procedure;QTS strip;QTS software application features;laboratory test results;biochemical tests;bacterial identification tests;logged result database;Microsoft Technologies;Microsoft SQL Compact database;bacteria organism","","","","11","","12 Mar 2018","","","IEEE","IEEE Conferences"
"Spam Domain Detection Method Using Active DNS Data and E-Mail Reception Log","K. Dan; N. Kitagawa; S. Sakuraba; N. Yamai","Tokyo University of Agriculture and Technology; Tokyo University of Agriculture and Technology; Internet Initiative Japan Inc., Japan; Tokyo University of Agriculture and Technology","2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC)","9 Jul 2019","2019","1","","896","899","E-mail is widespread and an essential communication technology in modern times. Since e-mail has problems with spam mails and spoofed e-mails, countermeasures are required. Although SPF, DKIM and DMARC have been proposed as sender domain authentication, these mechanisms cannot detect non-spoofing spam mails. To overcome this issue, this paper proposes a method to detect spam domains by supervised learning with features extracted from e-mail reception log and active DNS data, such as the result of Sender Authentication, the Sender IP address, the number of each DNS record, and so on. As a result of the experiment, our method can detect spam domains with 88.09% accuracy and 97.11% precision. We confirmed that our method can detect spam domains with detection accuracy 19.40% higher than the previous study by utilizing not only active DNS data but also e-mail reception log in combination.","0730-3157","978-1-7281-2607-4","10.1109/COMPSAC.2019.00133","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8754369","spam mail, spam detection, active DNS, e-mail log analysis, supervised learning","IP networks;Feature extraction;Unsolicited e-mail;Postal services;Training;Authentication","electronic mail;feature extraction;Internet;message authentication;system monitoring;unsolicited e-mail","active DNS data;e-mail reception log;spoofed e-mails;sender domain authentication;spam domain detection;feature extraction;sender IP address;SPF;DKIM;DMARC","","","","10","","9 Jul 2019","","","IEEE","IEEE Conferences"
"Pushing the Limits in Event Normalisation to Improve Attack Detection in IDS/SIEM Systems","A. Azodi; D. Jaeger; F. Cheng; C. Meinel","Hasso Plattner Inst. (HPI), Univ. of Potsdam, Potsdam, Germany; Hasso Plattner Inst. (HPI), Univ. of Potsdam, Potsdam, Germany; Hasso Plattner Inst. (HPI), Univ. of Potsdam, Potsdam, Germany; Hasso Plattner Inst. (HPI), Univ. of Potsdam, Potsdam, Germany","2013 International Conference on Advanced Cloud and Big Data","5 Jun 2014","2013","","","69","76","The current state of affairs regarding the way events are logged by IT systems is the source of many problems for the developers of Intrusion Detection Systems (IDS) and Security Information and Event Management (SIEM) systems. These problems stand in the way of the development of more accurate security solutions that draw their results from the data included within the logs they process. This is mainly caused by a lack of standards that can encapsulate all events in a coherent way. As a result, correlating between logs produced by different systems that use different log formats has been difficult and infeasible in many cases. In order to solve the challenges faced by Correlation Based Intrusion Detection Systems, we provide a platform for normalising events1 into a unified super event loosely based on the Common Event Expression standard (CEE) developed by the Mitre corporation. We show how our solution is able to normalise seemingly unrelated events into a unified format. Additionally, we demonstrate queries that can detect attacks on collections of normalised logs from different sources.","","978-1-4799-3261-0","10.1109/CBD.2013.27","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6824575","Event Normalisation;Intrusion Detection;Event Management;Knowledge base","Standards;Software;Databases;Data mining;Intrusion detection;Servers","security of data","event normalisation;attack detection;IDS-SIEM systems;IT systems;security information and event management systems;log formats;correlation based intrusion detection systems;common event expression standard;CEE;query","","9","","31","","5 Jun 2014","","","IEEE","IEEE Conferences"
"On Automatic Parsing of Log Records","J. Rand; A. Miranskyy","Ryerson University,Department of Computer Science,Toronto,Canada; Ryerson University,Department of Computer Science,Toronto,Canada","2021 IEEE/ACM 43rd International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER)","7 May 2021","2021","","","41","45","Software log analysis helps to maintain the health of software solutions and ensure compliance and security. Existing software systems consist of heterogeneous components emitting logs in various formats. A typical solution is to unify the logs using manually built parsers, which is laborious.Instead, we explore the possibility of automating the parsing task by employing machine translation (MT). We create a tool that generates synthetic Apache log records which we used to train recurrent-neural-network-based MT models. Models’ evaluation on real-world logs shows that the models can learn Apache log format and parse individual log records. The median relative edit distance between an actual real-world log record and the MT prediction is less than or equal to 28%. Thus, we show that log parsing using an MT approach is promising.","","978-1-6654-0140-1","10.1109/ICSE-NIER52604.2021.00017","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9402230","","Tools;Software systems;Security;Machine translation;Task analysis;Software engineering","","","","","","36","","7 May 2021","","","IEEE","IEEE Conferences"
"Assessing Big Data SQL Frameworks for Analyzing Event Logs","M. Hinkka; T. Lehto; K. Heljanko","Dept. of Comput. Sci., Aalto Univ., Aalto, Finland; Dept. of Comput. Sci., Aalto Univ., Aalto, Finland; Dept. of Comput. Sci., Aalto Univ., Aalto, Finland","2016 24th Euromicro International Conference on Parallel, Distributed, and Network-Based Processing (PDP)","4 Apr 2016","2016","","","101","108","Performing Process Mining by analyzing event logs generated by various systems is a very computation and I/O intensive task. Distributed computing and Big Data processing frameworks make it possible to distribute all kinds of computation tasks to multiple computers instead of performing the whole task in a single computer. This paper assesses whether contemporary structured query language (SQL) supporting Big Data processing frameworks are mature enough to be efficiently used to distribute computation of two central Process Mining tasks to two dissimilar clusters of computers providing BPM as a service in the cloud. Tests are performed by using a novel automatic testing framework detailed in this paper and its supporting materials. As a result, an assessment is made on how well selected Big Data processing frameworks manage to process and to parallelize the analysis work required by Process Mining tasks.","2377-5750","978-1-4673-8776-7","10.1109/PDP.2016.26","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7445319","automatic business process discovery;process mining;event log analysis;distributed computing framework;distributed SQL;Hadoop;Hive;Presto;Spark","Big data;Data mining;Algorithm design and analysis;Computers;Performance evaluation;Software","Big Data;business data processing;cloud computing;data mining;relational databases;SQL","automatic testing framework;cloud computing;BPM;central process mining tasks;structured query language;computation task distribution;distributed computing;event log analysis;Big Data SQL framework assessment","","5","","21","","4 Apr 2016","","","IEEE","IEEE Conferences"
"Intelligent high-frame-rate video recording with imagebased trigger","Y. Wang; T. Takaki; I. Ishii","Robotics Laboratory, Graduate School of Engineering, Hiroshima University, 1-4-1 Kagamiyama, Higashi-hiroshima, 739-8527, JAPAN; Robotics Laboratory, Graduate School of Engineering, Hiroshima University, 1-4-1 Kagamiyama, Higashi-hiroshima, 739-8527, JAPAN; Robotics Laboratory, Graduate School of Engineering, Hiroshima University, 1-4-1 Kagamiyama, Higashi-hiroshima, 739-8527, JAPAN","2010 World Automation Congress","10 Dec 2010","2010","","","1","6","This paper introduces an intelligent high-frame-rate video recording system that can automatically detect strong acceleration in images and record corresponding images with dimensions of 512 × 512 pixels at 1000 fps. To capture high-frame-rate images of crash behavior, a contact detection algorithm is software-implemented on a high-speed vision platform, IDP Express, which can process and record input images at 1000 fps. The algorithm can estimate the external force acting on the target by calculating its position, velocity, and acceleration in real time. Several experiments were performed using image-based triggers for contact detection, and a video of the crash behavior was automatically recorded to verify the effectiveness of our system.","2154-4824","978-1-889335-42-1","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5665703","High-Speed Vision;High-Frame-Rate Video Logging;Motion Detection","Cameras;Force;Acceleration;Head;Pixel;Computer crashes;Real time systems","computer vision;image motion analysis;object detection;video recording","intelligent high-frame-rate video recording system;contact detection algorithm;high-speed vision platform;IDP Express;image based trigger;image pixel;motion detection;crash behavior","","","","7","","10 Dec 2010","","","IEEE","IEEE Conferences"
"A Scalable Distributed Framework for Efficient Analytics on Ordered Datasets","J. Yin; Y. Liao; M. Baldi; L. Gao; A. Nucci",NA; NA; NA; NA; NA,"2013 IEEE/ACM 6th International Conference on Utility and Cloud Computing","5 May 2014","2013","","","131","138","One of the most common datasets used by many corporations to gain business intelligence is event log files. Oftentimes, the records in event log files are temporally ordered, and need to be grouped by user ID with the temporal ordering preserved to facilitate mining user behaviors. This kind of analytical workload, here referred to as Relative Order-preserving based Grouping (RE-ORG), is quite common in big data analytics. Using MapReduce/Hadoop for executing RE-ORG tasks on ordered datasets is not efficient due to its internal sort-merge mechanism. In this paper, we propose a distributed framework that adopts an efficient group-order-merge mechanism to provide faster execution of RE-ORG tasks. We demonstrate the advantage of our framework by comparing its performance with Hadoop through extensive experiments on real-world datasets. The evaluation results show that our framework can achieve up to 6.3× speedup over Hadoop in executing RE-ORG tasks.","","978-0-7695-5152-4","10.1109/UCC.2013.35","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6809349","MapReduce;Hadoop;distributed framework;big data analytics;ordered dataset","Indexes;Sorting;Business;Merging;Programming;Open source software;Instruments","Big Data;competitive intelligence;data analysis;distributed databases;real-time systems","scalable distributed framework;ordered dataset analytics;event log files;temporal ordering;relative order-preserving based grouping;RE-ORG;big data analytics;MapReduce/Hadoop;group-order-merge mechanism;real-world datasets;business intelligence;user behavior mining","","6","","28","","5 May 2014","","","IEEE","IEEE Conferences"
"Extracting reusable services from legacy object-oriented systems","Liang Bao; Chao Yin; Weigang He; Jun Ge; Ping Chen","Software Engineering Institute, Xidian University, Xi'an, ShaanXi, China, 710071; Software Engineering Institute, Xidian University, Xi'an, ShaanXi, China, 710071; Software Engineering Institute, Xidian University, Xi'an, ShaanXi, China, 710071; Software Engineering Institute, Xidian University, Xi'an, ShaanXi, China, 710071; Software Engineering Institute, Xidian University, Xi'an, ShaanXi, China, 710071","2010 IEEE International Conference on Software Maintenance","25 Oct 2010","2010","","","1","5","Migrating legacy object-oriented system functionalities to SOA environment is a important problem that frequently arises in many system maintenance and integration tasks. A service is often implemented by complex collaborations of many objects in an object-oriented system. Such complexity brings impedance mismatch between service and object. Moreover, the delocalized nature of object-oriented system, where the code associated with a service is distributed across many interrelated objects, make this problem even more challenging. This paper presents a four-staged approach that extracts services from legacy object-oriented systems with source code and documents. In the first stage source code of legacy system is loaded and preprocessed to form different modules according to the explicit dependencies among classes. While preprocessing, some aspect code is also embedded to intercept and log execution traces of system as well as to store states of involved objects. In the second stage, services, which represent system-level business functionalities, are modeled with use cases. Useful test cases are in turn generated from these use cases to identify services. In the third stage, the modularized and intercepted system is executed driven by generated test cases and the execution logs and object states are recorded. In the last stage, services are located and extracted by analyzing the execution logs and restoring the object states. The approach is supported by an integrated tool and the evaluation on five open-source systems yields encouraging result and demonstrates the practical applicability of the approach.","1063-6773","978-1-4244-8629-8","10.1109/ICSM.2010.5609744","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5609744","","Service oriented architecture;Business;Object oriented modeling;XML;Unified modeling language;Software;Programming","object-oriented programming;open systems;software architecture;software reusability;source coding","reusable service;legacy object oriented system;SOA environment;source code;log execution;system level business functionality;open source system","","","2","15","","25 Oct 2010","","","IEEE","IEEE Conferences"
"An Evaluation Study on Log Parsing and Its Use in Log Mining","P. He; J. Zhu; S. He; J. Li; M. R. Lyu","Dept. of Comput. Sci. & Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Dept. of Comput. Sci. & Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Dept. of Comput. Sci. & Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Dept. of Comput. Sci. & Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Dept. of Comput. Sci. & Eng., Chinese Univ. of Hong Kong, Hong Kong, China","2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)","3 Oct 2016","2016","","","654","661","Logs, which record runtime information of modern systems, are widely utilized by developers (and operators) in system development and maintenance. Due to the ever-increasing size of logs, data mining models are often adopted to help developers extract system behavior information. However, before feeding logs into data mining models, logs need to be parsed by a log parser because of their unstructured format. Although log parsing has been widely studied in recent years, users are still unaware of the advantages of different log parsers nor the impact of them on subsequent log mining tasks. Thus they often re-implement or even re-design a new log parser, which would be time-consuming yet redundant. To address this issue, in this paper, we study four log parsers and package them into a toolkit to allow their reuse. In addition, we obtain six insightful findings by evaluating the performance of the log parsers on five datasets with over ten million raw log messages, while their effectiveness on a real-world log mining task has been thoroughly examined.","2158-3927","978-1-4673-8891-7","10.1109/DSN.2016.66","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7579781","","Data mining;Runtime;Vocabulary;Computer science;Maintenance engineering;Data models;Software systems","data mining;system monitoring","log parsing;log mining;runtime information recording;system development;system maintenance;data mining;system behavior information extraction","","55","1","41","","3 Oct 2016","","","IEEE","IEEE Conferences"
"BugMarks: A tool for mapping bugs to source code components","M. K. Chawla; I. Chhabra","Department of Computer Science & Applications, MCMDAV College for Women, Chandigarh, India; Department of Computer Science & Applications, Panjab University, Chandigarh, India","2015 IEEE International Conference on Research in Computational Intelligence and Communication Networks (ICRCICN)","17 Mar 2016","2015","","","466","469","Bugs reports and version archives are two useful sources of information for quality analysts, with the goal of mapping the bugs fixes to source code files. Manually performing this task is as arduous as it is time consuming. We propose a tool which programmatically performs this job in two phases-First, it filters relevant log entries by traversing CVS/SVN log files and then it parses the textual information into tokens to finally store required attributes in a database for future analyses. To users, it offers the flexibility and the liberty with which information can be utilized further, in any way, specific to their studies.","","978-1-4673-6735-6","10.1109/ICRCICN.2015.7434284","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7434284","Bugzilla;version archives;SVN logs;Apache Tomcat;BugMarks","Computer bugs;Databases;Software;Filtering;Testing;Visualization;Servers","configuration management;program debugging;software tools;source code (software)","BugMarks;bug mapping;source code components;version archives;relevant log entry filtering;CVS/SVN log files;textual information parsing","","","","7","","17 Mar 2016","","","IEEE","IEEE Conferences"
"AppAngio: Revealing Contextual Information of Android App Behaviors by API-Level Audit Logs","Z. Meng; Y. Xiong; W. Huang; F. Miao; J. Huang","School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China","IEEE Transactions on Information Forensics and Security","8 Jan 2021","2021","16","","1912","1927","Android users are now suffering severe threats from unwanted behaviors of various apps. The analysis of apps' audit logs is one of the essential methods for the security analysts of various companies to unveil the underlying maliciousness within apps. We propose and implement AppAngio, a novel system that reveals contextual information in Android app behaviors by API-level audit logs. Our goal is to help security analysts understand how the target apps worked and facilitate the identification of the maliciousness within apps. The key module of AppAngio is identifying the path matched with the logs on the app's control-flow graphs (CFGs). The challenge, however, is that the limited-quantity logs may incur high computational complexity in the log matching, where there are a large number of candidates caused by the coupling relation of successive logs. To address the challenge, we propose a divide and conquer strategy that precisely positions the nodes matched with log records on the corresponding CFGs and connects the nodes with as few backtracks as possible. Our experiments show that AppAngio reveals contextual information of behaviors in real-world apps. Moreover, the revealed results assist the analysts in identifying the maliciousness of app behaviors and complement existing analysis schemes. Meanwhile, AppAngio incurs negligible performance overhead on the real device in the experiments.","1556-6021","","10.1109/TIFS.2020.3044867","National Key Research and Development Program of China; National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9293262","Contextual reveal;log matching;divide and conquer;Android security","Runtime;Security;Computational complexity;Performance evaluation;Malware;Feature extraction;Couplings","Android (operating system);application program interfaces;computational complexity;invasive software;mobile computing","app maliciousness identification;security analysts;Android users;API-level audit logs;Android app behaviors;real-world apps;contextual information;log records;successive logs;log matching;limited-quantity logs;AppAngio","","","","48","IEEE","14 Dec 2020","","","IEEE","IEEE Journals"
"Vulnerability Discovery Strategies Used in Software Projects","F. A. Bhuiyan; A. Rahman; P. Morrison","Department of Computer Science,Cookeville,Tennessee,USA; Department of Computer Science,Cookeville,Tennessee,USA; IBM,Durham,North Carolina,USA","2020 35th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW)","20 Jan 2021","2020","","","13","18","Malicious users can exploit undiscovered software vulnerabilities i.e., undiscovered weaknesses in software, to cause serious consequences, such as large-scale data breaches. A systematic approach that synthesizes strategies used by security testers can aid practitioners to identify latent vulnerabilities. The goal of this paper is to help practitioners identify software vulnerabilities by categorizing vulnerability discovery strategies using open source software bug reports. We categorize vulnerability discovery strategies by applying qualitative analysis on 312 OSS bug reports. Next, we quantify the frequency and evolution of the identified strategies by analyzing 1,632 OSS bug reports collected from five software projects spanning across 2009 to 2019. The five software projects are Chrome, Eclipse, Mozilla, OpenStack, and PHP. We identify four vulnerability discovery strategies: diagnostics, malicious payload construction, misconfiguration, and pernicious execution. For Eclipse and OpenStack, the most frequently used strategy is diagnostics, where security testers inspect source code and build/debug logs. For three web-related software projects namely, Chrome, Mozilla, and PHP, the most frequently occurring strategy is malicious payload construction i.e., creating malicious files, such as malicious certificates and malicious videos.","2151-0830","978-1-4503-8128-4","10.1145/3417113.3422153","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9319308","bug report;empirical study;strategy;taxonomy;vulnerability","Security;Software;Computer bugs;Payloads;Tools;Open source software;Encoding","Internet;program debugging;program testing;public domain software;security of data","vulnerability discovery strategies;undiscovered software vulnerabilities;latent vulnerabilities;open source software bug reports;identified strategies;OSS bug reports;web-related software projects;Chrome;Eclipse;Mozilla;OpenStack;PHP;malicious payload construction;malicious files;malicious certificates;malicious videos","","","","55","","20 Jan 2021","","","IEEE","IEEE Conferences"
"Experience Report: System Log Analysis for Anomaly Detection","S. He; J. Zhu; P. He; M. R. Lyu","Dept. of Comput. Sci. & Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Dept. of Comput. Sci. & Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Dept. of Comput. Sci. & Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Dept. of Comput. Sci. & Eng., Chinese Univ. of Hong Kong, Hong Kong, China","2016 IEEE 27th International Symposium on Software Reliability Engineering (ISSRE)","8 Dec 2016","2016","","","207","218","Anomaly detection plays an important role in management of modern large-scale distributed systems. Logs, which record system runtime information, are widely used for anomaly detection. Traditionally, developers (or operators) often inspect the logs manually with keyword search and rule matching. The increasing scale and complexity of modern systems, however, make the volume of logs explode, which renders the infeasibility of manual inspection. To reduce manual effort, many anomaly detection methods based on automated log analysis are proposed. However, developers may still have no idea which anomaly detection methods they should adopt, because there is a lack of a review and comparison among these anomaly detection methods. Moreover, even if developers decide to employ an anomaly detection method, re-implementation requires a nontrivial effort. To address these problems, we provide a detailed review and evaluation of six state-of-the-art log-based anomaly detection methods, including three supervised methods and three unsupervised methods, and also release an open-source toolkit allowing ease of reuse. These methods have been evaluated on two publicly-available production log datasets, with a total of 15,923,592 log messages and 365,298 anomaly instances. We believe that our work, with the evaluation results as well as the corresponding findings, can provide guidelines for adoption of these methods and provide references for future development.","2332-6549","978-1-4673-9002-6","10.1109/ISSRE.2016.21","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7774521","","Feature extraction;Open source software;Runtime;Industries;Manuals;Inspection;Large-scale systems","distributed processing;public domain software;security of data;system monitoring","experience report;system log analysis;anomaly detection;large-scale distributed systems;system runtime information;keyword search;rule matching;automated log analysis;open-source toolkit;production log datasets;log messages;anomaly instances","","94","","49","","8 Dec 2016","","","IEEE","IEEE Conferences"
"Scientific Workflow Protocol Discovery from Public Event Logs in Clouds","W. Song; H. -A. Jacobsen; F. Chen","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Middleware Systems Research Group, Technische Universität München, Garching, Germany; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Transactions on Knowledge and Data Engineering","6 Nov 2020","2020","32","12","2453","2466","With the advancement of cloud computing, many challenging scientific problems can be solved using scientific workflow technology which integrates geo-distributed instruments, applications, and big data effectively and efficiently. For workflow collaboration, the workflow protocols of all participants are needed. However, workflow protocols are not always available and are often outdated as the workflow evolve frequently. To address this problem, we propose a novel workflow discovery approach which can extract up-to-date scientific workflow protocols from public event logs in clouds, without the need to access the full-fledged event logs involving private events. Our approach leverages transitive precedence relations between events to achieve this. We implement our approach as a ProM plug-in, and evaluate it through extensive experiments on event logs of real-world scientific workflows. The experimental results demonstrate that our approach requires a weaker completeness notion of event logs than the state-of-the-art do, and our approach derives the same workflow protocol from the public event log as that discovered from the original event log, and thus the private events can be protected.","1558-2191","","10.1109/TKDE.2019.2922183","National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; Collaborative Innovation Center of Novel Software Technology and Industrialization; Deutsche Forschungsgemeinschaft; Alexander von Humboldt-Stiftung; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8734698","Scientific workflow;event log;process discovery;workflow protocol;privacy preservation;transitive precedence","Protocols;Cloud computing;Collaboration;PROM;Privacy;Jacobian matrices;Big Data","Big Data;cloud computing;data analysis;data mining","scientific workflow protocol discovery;cloud computing;scientific workflow technology;workflow collaboration;transitive precedence relations;scientific workflow protocols;workflow discovery approach","","1","","54","IEEE","11 Jun 2019","","","IEEE","IEEE Journals"
"Towards IoT-driven Process Event Log Generation for Conformance Checking in Smart Factories","R. Seiger; F. Zerbato; A. Burattin; L. García-Bañuelos; B. Weber","University of St. Gallen (HSG),School of Computer Science,St. Gallen,Switzerland; University of St. Gallen (HSG),School of Computer Science,St. Gallen,Switzerland; Technical University of Denmark (DTU),Lyngby,Denmark; Tecnologico de Monterrey,School of Engineering and Sciences,Puebla,Mexico; University of St. Gallen (HSG),School of Computer Science,St. Gallen,Switzerland","2020 IEEE 24th International Enterprise Distributed Object Computing Workshop (EDOCW)","22 Oct 2020","2020","","","20","26","The Internet of Things (IoT) enables software-based access to vast amounts of data streams from sensors measuring physical and virtual properties of smart devices and their surroundings. While sophisticated means for the control and data analysis of single IoT devices exist, a more process-oriented view of IoT systems is often missing. Such a lack of process awareness hinders the development of process-based systems on top of IoT environments and the application of process mining techniques for process analysis and optimization in IoT. We propose a framework for the stepwise correlation and composition of raw IoT sensor streams with events and activities on a process level based on Complex Event Processing (CEP). From this correlation we derive refined process event logs-possibly with ambiguities-that can be used for process analysis at runtime (i. e., online). We discuss the framework using examples from a smart factory.","2325-6605","978-1-7281-6471-7","10.1109/EDOCW49879.2020.00016","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9233283","Conformance Checking;Complex Event Processing;Internet of Things;Process Mining;Smart Factories","Sensors;Internet of Things;Smart manufacturing;Production;Intelligent sensors;Grippers;Correlation","data analysis;data mining;factory automation;Internet of Things;production engineering computing;sensor fusion","Internet of Things;software-based access;data streams;physical properties;virtual properties;smart devices;IoT systems;process mining techniques;process analysis;complex event processing;smart factory;conformance checking;IoT sensor streams;IoT-driven process event log generation","","","","44","","22 Oct 2020","","","IEEE","IEEE Conferences"
"Software Architecture for Adaptation and Recommendation of Course Content and Activities Based on Learning Analytics","A. Aleksieva-Petrova; V. Gancheva; M. Petrov","Technical University of Sofia,Faculty of Computer Systems and Technologies,Sofia,Bulgaria; Technical University of Sofia,Faculty of Computer Systems and Technologies,Sofia,Bulgaria; Sofia University,Faculty of Mathematics and Informatics,Sofia,Bulgaria","2020 International Conference on Mathematics and Computers in Science and Engineering (MACISE)","14 Sep 2020","2020","","","16","19","Nowadays the main challenge in learning analytics is to suggest efficient methods and technologies in order to achieve better learner results. This paper presents a software architecture for adaptation and recommendation of course content and activities based on learning analytics. It is comprised of layers for ingestion layer, aggregation layer, storage layer and big data processing and analyses layer. An algorithm for prediction of student learning based on machine learning for processing and analysis of data and knowledge discovery with respect to main learner and teacher activities is presented. The proposed algorithm for student learning classification is implemented by using Averaged Perceptron method. Experimental results are presented and discussed. The purpose of the study is to apply the software architecture on learning analytics by practical experiments for specific case study identifying event elements in sequenced learners' and courses' activities logs, and student learning prediction.","","978-1-7281-6695-7","10.1109/MACISE49704.2020.00010","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9195591","adaptation;activities logs;averaged perceptron;course;learning analytic;recommendation system","Machine learning algorithms;Data mining;Software architecture;Data models;Prediction algorithms;Machine learning;Big Data","Big Data;computer aided instruction;data mining;learning (artificial intelligence);software architecture","knowledge discovery;data analysis;averaged perceptron method;student learning prediction;courses;student learning classification;teacher activities;machine learning;big data processing;storage layer;ingestion layer;learning analytics;course content;software architecture","","","","11","","14 Sep 2020","","","IEEE","IEEE Conferences"
"The Process of Metadata Management for Radar Target Classification Algorithm Development","V. Valenta; J. Pidanic; O. Nemec","University of Pardubice,Department of Electrical Engineering,Pardubice,Czech Republic; University of Pardubice,Department of Electrical Engineering,Pardubice,Czech Republic; University of Pardubice,Department of Electrical Engineering,Pardubice,Czech Republic","2020 New Trends in Signal Processing (NTSP)","2 Nov 2020","2020","","","1","4","This paper presents an approach to manage metadata (target class labels) for the recorded primary Doppler radar data. This information is necessary for further research and development of target classification algorithms. For this purpose, a labelling methodology and an application radar data analysis and target labelling was developed. The application includes radar records file processing, Doppler filtering, tag creation, data visualizationand tag database. For the better context of analysed data, an interface to Geographic Information Software (GIS) program is included as well. GIS program allows overlaying radar Plan Position Indicator (PPI) data with map data, airplane transponder tracks, drone flight path log and other support data. Finally, the application notes and observations are presented.","","978-1-7281-6155-6","10.1109/NTSP49686.2020.9229528","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9229528","primary radar;target classification;radar recordings;target type labelling;radar data analysis","","data analysis;data visualisation;Doppler radar;geographic information systems;meta data;radar tracking;signal classification","drone flight path log;airplane transponder tracks;GIS program;tag database;geographic information software program;radar plan position indicator data;radar target classification algorithm;Doppler radar data;target class labels;metadata management;map data;data visualization;tag creation;Doppler filtering;radar records file processing;target labelling;radar data analysis","","","","7","","2 Nov 2020","","","IEEE","IEEE Conferences"
"Mining Timing Constraints from Event Logs for Process Model","Z. Zhang; C. Guo; S. Ren","San Diego State University; California State University, Los Angeles; San Diego State University","2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)","22 Sep 2020","2020","","","1011","1016","Process mining is a technique for extracting process models from event logs. Event logs contain abundant information related to an event such as the timestamp of the event, the actions that triggers the event, etc. Much of existing process mining research has been focused on discoveries of process models behind event logs. How to uncover the timing constraints from event logs that are associated with the discovered process models is not well-studied. In this paper, we present an approach that extends existing process mining techniques to not only mine but also integrate timing constraints with process models discovered and constructed by existing process mining algorithms. The approach contains three major steps, i.e., first, for a given process model constructed by an existing process mining algorithm and represented as a workflow net, extract a time dependent set for each transition in the workflow net model. Second, based on the time dependent sets, develop an algorithm to extract timing constraints from event logs for each transition in the model. Third, extend the original workflow net into a time Petri net where the discovered timing constraints are associated with their corresponding transitions. A real-life road traffic fine management process scenario is used as a case study to show how timing constraints in the fine management process can be discovered from event logs with our approach.","0730-3157","978-1-7281-7303-0","10.1109/COMPSAC48688.2020.0-139","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9202794","Process Mining;Timing Constraints;Petri Net;time Petri net","Petri nets;Timing;Task analysis;Data mining;Roads;Semantics;Computational modeling","data mining;Petri nets;road traffic","event logs;process models;process mining research;time dependent set;timing constraints mining;real-life road traffic fine management process;time Petri net","","1","","32","","22 Sep 2020","","","IEEE","IEEE Conferences"
"Design Log Management System of Computer Network Devices Infrastructures Based on ELK Stack","A. F. Rochim; M. A. Aziz; A. Fauzi","Diponegoro University,Department of Computer Engineering,Semarang,Indonesia,50275; Diponegoro University,Department of Computer Engineering,Semarang,Indonesia,50275; Diponegoro University,Department of Computer Engineering,Semarang,Indonesia,50275","2019 International Conference on Electrical Engineering and Computer Science (ICECOS)","6 Feb 2020","2019","","","338","342","Device monitoring is an important thing to manage networks. Information-related network state or condition can be gathered through the device monitoring for administrators to take decisions regarding occurred events. Logs can be useful information to monitor network devices. Network administrator of Diponegoro University needs a centralized the logs, so that can receive, manage, and analyze logs. This research identifies functional requirements of the log management system. DSR Method was used to design topology and software of the log management system. The next step is implementation of the topology, software, and application. The last step is testing the system and log management application. The results show that collecting centralized logs and processing these logs into information in the form of dashboards using ELK Stack application successfully implemented. The dashboard resulted by ELK Stack Application will be implemented on the web application using PHP programming language and Code Igniter framework. The test results show that system can receive logs and group the log according to the device location and the severity level of the log.","","978-1-7281-4714-7","10.1109/ICECOS47637.2019.8984494","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8984494","Monitoring;log;ELK Stack;PHP;Codeigniter;Dashboard","","educational institutions;Internet;system monitoring","device monitoring;network administrator;log management application;centralized logs;ELK stack application;computer network device infrastructures;DSR method;Diponegoro University;PHP programming language;Code Igniter framework","","","","11","","6 Feb 2020","","","IEEE","IEEE Conferences"
"LiDSec- A Lightweight Pseudonymization Approach for Privacy-Preserving Publishing of Textual Personal Information","R. Rawassizadeh; J. Heurix; S. Khosravipour; A. M. Tjoa","Inst. of Software Technol. & Interactive Syst., Vienna Univ. of Technol., Vienna, Austria; SBA Res., Vienna, Austria; Inst. of Software Technol. & Interactive Syst., Vienna Univ. of Technol., Vienna, Austria; Inst. of Software Technol. & Interactive Syst., Vienna Univ. of Technol., Vienna, Austria","2011 Sixth International Conference on Availability, Reliability and Security","17 Oct 2011","2011","","","603","608","Sharing personal information benefits both data providers and data consumers in many ways. Recent advances in sensor networks and personal archives enable users to record personal information including emails, social networking activities, or life events (life logging). These information objects are usually privacy sensitive and thus need to be protected adequately when being shared. In this work, we present a lightweight pseudonymization framework which allows users to benefit from sharing their personal information while still preserving their privacy. Furthermore, this approach increases the data owners' awareness of what information they are sharing, thus rendering data publishing more transparent.","","978-1-4577-0979-1","10.1109/ARES.2011.93","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6046047","Pseudonymization;Personal Information;Privacy;Security","Privacy;Data privacy;Graphical user interfaces;Social network services;Usability;Data structures;Prototypes","data privacy;security of data","LiDSec;lightweight pseudonymization approach;privacy-preserving publishing;textual personal information;personal information sharing;data providers;data consumers;sensor network;personal archive;personal information recording;emails;social networking activities;life events;life logging","","1","","23","","17 Oct 2011","","","IEEE","IEEE Conferences"
"Relational database approach for execution trace analysis","S. Alouneh; S. Abed; B. J. Mohd; A. Al-Khasawneh","Computer Engineering Department, German-Jordanian University, Amman, Jordan; Computer Engineering Department, Hashemite University, Zarqa, Jordan; Computer Engineering Department, Hashemite University, Zarqa, Jordan; Computer Information System Department, Hashemite University, Zarqa, Jordan","2012 International Conference on Computer, Information and Telecommunication Systems (CITS)","21 Jun 2012","2012","","","1","4","Software maintenance is an important part of software engineering life cycle especially in large software systems due to the size and complexity of typical traces. In fact, the changes made to the implementation of systems are usually not reflected in the design documentation and hence the gap between a system's implementation and its design models becomes large. An essential step in software maintenance is to explore the content of large execution traces. The understanding of software structure can be simplified by choosing important information of execution traces files. In this paper, we propose a novel approach that improves program comprehension using relational database concepts.","","978-1-4673-1550-0","10.1109/CITS.2012.6220394","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6220394","Software maintenance;execution traces;comprehension;relational database;security;logs","Relational databases;Algebra;Security;Software systems;Receivers","program diagnostics;relational databases;software maintenance;system documentation","relational database;execution trace analysis;software maintenance;software engineering life cycle;software systems;typical traces;design documentation;system implementation;design models;software structure;program comprehension","","","","9","","21 Jun 2012","","","IEEE","IEEE Conferences"
"Improvement and Evaluation of a Method to Manage Multiple Types of Logs","A. Tomono; M. Uehara; Y. Shimada","Dept. of Open Inf. Syst., Toyo Univ., Saitama, Japan; Dept. of Open Inf. Syst., Toyo Univ., Saitama, Japan; Dept. of Open Inf. Syst., Toyo Univ., Saitama, Japan","2011 IEEE Workshops of International Conference on Advanced Information Networking and Applications","5 May 2011","2011","","","601","606","In recent years, many accounting scandals have been reported in companies not only in the United States, but also in Japan. The need for internal control is growing steadily. In particular, auditing logs are important for internal control, since internal control without audit evidence is incomplete. Moreover, logs are necessary not only as a defense mechanism, but also since they contain much information that can lead to improvements in the company. Consequently, the correct use of logs can be beneficial to a company. However, the cost of an information system is dependent on the amount of data, which in the case of log data can be very large. There are many different kinds of logs and storing them long term is necessary to realize an internal control system based on logs. Previously, we proposed a low cost system to store logs semi-permanently using a Virtual Large Scale Disk. However, this method has problems with cross-sectional searches of different formats and its overall effectiveness. Therefore, we proposed a log that can cope with changing schema on demand by integrating several kinds of logs into YAML format. We also proposed a log format able to search across several kinds of logs by consolidating the log format and combining the logs into a single file. However, this proposal is not usable in practice, instead an integrated log is needed. Thus, in this paper, we implement a method that ensures consistency when a log is converted into YAML format from a raw log and vice versa and a command to search the integrated log. We also present an evaluation of the proposed method.","","978-1-61284-829-7","10.1109/WAINA.2011.51","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5763567","internal control;log management;long term data management","Databases;Companies;Servers;Control systems;Security;File servers;Software","accounting;auditing;system monitoring;virtual storage","logs management;accounting scandals;logs auditing;virtual large scale disk;YAML format","","","","5","","5 May 2011","","","IEEE","IEEE Conferences"
"Record Skipping in Parallel Data Processing Systems","M. Höger; O. Kao",NA; NA,"2016 International Conference on Cloud and Autonomic Computing (ICCAC)","8 Dec 2016","2016","","","107","110","Cloud computing with massive parallel execution engines is a common method to handle big data. But cloud computing and the characteristics of the data (semi structured, from different sources, probably cluttered) cause the development and execution of data processing jobs to be failure prone. This asks for processing engines to be fault tolerant. This paper introduces an approach to skip faulty or already processed records after a failure. This enables the execution engine to recover from a failure if the data is flawed. Additionally skipping records allows a recovery with less restarts. The introduced record skipping solution relies on the possibility to reproduce a previous data distribution pattern of a job run. This is done by logging the order of consumed data. The evaluation shows that the overhead of the described technique is negligibly small.","","978-1-5090-3536-6","10.1109/ICCAC.2016.15","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7774965","fault tolerance;record skipping","Engines;Fault tolerance;Fault tolerant systems;Runtime;Pipelines;Cloud computing;Optical character recognition software","Big Data;cloud computing;parallel processing","record skipping;parallel data processing systems;cloud computing;massive parallel execution engines;big data;data processing jobs;processing engines;faulty records;processed records;data distribution pattern","","","","9","","8 Dec 2016","","","IEEE","IEEE Conferences"
"Hadoop Distributed File System for Big data analysis","H. T. Almansouri; Y. Masmoudi","Saudi Electronic Univeristy,Saudi Arabia; Saudi Electronic Univeristy,Saudi Arabia","2019 4th World Conference on Complex Systems (WCCS)","12 Dec 2019","2019","","","1","5","Hadoop is framework that is processing data with large volume that cannot be processed by conventional systems. Hadoop has management file system called Hadoop Distributed File System (HDFS) that has NameNode and DataNode where the data is divided into blocks based on the total size of dataset. In addition, Hadoop has MapReduce where the dataset is processed in Mapping phase and then reducing phase. Using Hadoop for big data analysis has been revealed important information that can be used for analytical purpose and enabling new products. Big data could be found in many different resources such as social networks, web server logs, broadcast audio streams and banking transactions. In this paper, we illustrated the main steps to setup Hadoop and MapReduce. The illustrated version in this work is the latest released of Hadoop 3.1.1 for big data analysis. A simplified pseudo code is provided to show the functionality of Map class and reduce class. The developed steps are applied with a given example that could be generalized with bigger data.","","978-1-7281-1232-9","10.1109/ICoCS.2019.8930804","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8930804","Hadoop;MapReduce;HDFS;DataNode;NameNode;Big Data Analysis","Training;Data collection;Software;Software algorithms;Task analysis;Industrial engineering;Virtual reality","Big Data;cloud computing;data analysis;distributed databases;parallel processing;public domain software","Hadoop Distributed File System;big data analysis;management file system;Hadoop 3.1.1;NameNode;DataNode;MapReduce;mapping phase","","","","12","","12 Dec 2019","","","IEEE","IEEE Conferences"
"Filtering noise in mixed-purpose fixing commits to improve defect prediction and localization","H. A. Nguyen; A. T. Nguyen; T. N. Nguyen",Electrical and Computer Engineering Department Iowa State University; Electrical and Computer Engineering Department Iowa State University; Electrical and Computer Engineering Department Iowa State University,"2013 IEEE 24th International Symposium on Software Reliability Engineering (ISSRE)","2 Jan 2014","2013","","","138","147","In open-source software projects, during fixing software faults, developers sometimes also perform other types of non-fixing code changes such as functionality enhancement, code restructuring/improving, or documentation. They commit non-fixing changes together with the fixing ones in the same transaction. We call them mixed-purpose fixing commits (MFCs). We have conducted an empirical study on MFCs in several popular open-source projects. Our results showed that MFCs are about 11%-39% of total fixing commits. In 3%-41% of MFCs, developers performed other change types without indicating them in the commit logs. Our study also showed that mining software repositories (MSR) approaches that rely on the recovery of the history of fixed/buggy files are affected by the noisy data where non-fixing changes in MFCs are considered as fixing ones. The results of our study motivated us to develop Cardo, a tool to identify MFCs and filter non-fixing changed files in the change sets of the fixing commits. It uses natural language processing to analyze the sentences in commit logs and program analysis to cluster the changes in the change sets to determine if a changed file is for non-fixing. Our empirical evaluation on several open-source projects showed that Cardo achieves on average 93% precision, and existing MSR approaches can be relatively improved up to 32% with data filtered by Cardo.","2332-6549","978-1-4799-2366-3","10.1109/ISSRE.2013.6698913","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6698913","","Noise;Feature extraction;History;Open source software;Documentation;Data mining","program debugging;program diagnostics;public domain software","noise filtering;defect prediction;defect localization;mixed-purpose fixing commits;MFCs;open-source software projects;mining software repositories approaches;MSR approaches;fixed files;Cardo;nonfixing changed files filtering;natural language processing;commit logs;program analysis;buggy files","","9","","22","","2 Jan 2014","","","IEEE","IEEE Conferences"
"HDFSx: An Enhanced Model to Handle Small Files in Hadoop with a Simulating Toolkit","P. M. El Kafrawy; A. M. Sauber; M. M. Hafez; A. F. Shawish","Faculty of Science Menoufia University, Egypt; Faculty of Science Menoufia University, Egypt; Faculty of Science Menoufia University, Egypt; Faculty of Science Menoufia University, Egypt","2018 1st International Conference on Computer Applications & Information Security (ICCAIS)","23 Aug 2018","2018","","","1","8","We live in Big Data era, where all data about our lives is captured, stored, processed and used to change the world around us. This data is generated by different sources such as Web, IoT sensors, application server logs, social media, traffic surveillance, and mobile data. The Hadoop distributed file system (HDFS) is established to handle large data files. Handling large number of small files with HDFS is considered a messy process. Thus, HDFSx is a proposed new distribute filesystem architecture that supports big data in small files format. It will enable Hadoop to store and process large number of small files without blocking Namenode memory or flooding network overheads. In order to prove the effectiveness of HDFSx model and algorithms, its performance needed to be evaluated under real scenarios. Implementing such a model in Hadoop environment is very hard, because of inadequacy of Hadoop core classes and libraries documentation which don't have official Class-diagram or Code-Map. In this paper, we propose an open source simulation toolkit for HDFSx called HDFSxSim. It provides a modeling and simulation tool for the main operations of HDFSx model. In addition to, we present a simulation toolkit for HDFS source that could be used to test and evaluate any future modification or adaptation. The experimental results indicated that HDFSx limits the metadata footprint and reduces the network overflow on Namenode.","","978-1-5386-4427-0","10.1109/CAIS.2018.8442036","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8442036","Big Data;Hadoop;HDFSxSim;HDFS","Metadata;File systems;Sockets;Big Data;Adaptation models;Programming","Big Data;cloud computing;data analysis;data mining;distributed databases;file organisation;Internet of Things;meta data;network operating systems;parallel processing;public domain software;storage management","simulating toolkit;Big Data era;IoT sensors;application server;social media;traffic surveillance;mobile data;file system;data files;messy process;flooding network overheads;HDFSx model;Hadoop environment;libraries documentation;open source simulation toolkit;simulation tool;HDFS source;filesystem architecture;metadata footprint;network overflow;Namenode","","","","13","","23 Aug 2018","","","IEEE","IEEE Conferences"
"Jedno rešenje programske podrške za parsiranje i vizuelizaciju signala sadržanih u TTL datoteci","U. Kovačević; J. Kovačević; S. Vukosavljev","NA; NA; RT-RK Computer Based Systems, Novi Sad, Serbia","2013 21st Telecommunications Forum Telfor (TELFOR)","20 Jan 2014","2013","","","963","966","The task is the realization the program for parsing XML files and TTL as well as graphical representation of the signal contained therein. TTL is a log file. Information exchanged in the car on different types of car system such as CAN, FlexRay, LIN, and similar are collected in TTL file. The toolkit parses messages arriving from different interfaces where each interface has a unique message content. Data Logger, the device with support for a wide variety of buses, collects information from the vehicle during the exchange of data between different control units (ECU) of the vehicle. A Data Logger stores data in TTL file. By analyzing the plotted graphs users can detect the correctness of signals and make decision whether it will further do analyze. The goal of developing application is to enable all users, of Data Logger application, graphical representation of recorded signals from vehicles without use of licensed and expensive applications. Also, application's goal is to simplify control of different safety devices in vehicle over graphical representation of their signals.","","978-1-4799-1420-3","10.1109/TELFOR.2013.6716391","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6716391","","XML;Vehicles;Payloads;Electronic mail;Abstracts;Automotive engineering;Telecommunications","automotive electronics;data loggers;program compilers;safety devices;vehicles;XML","software solution;TTL signal parsing;TTL signal visualization;XML files;graphical representation;log file;car system;CAN;FlexRay;LIN;TTL file;message content;data logger;control units;safety devices","","","","5","","20 Jan 2014","","","IEEE","IEEE Conferences"
"A DAX-enabled mmap mechanism for log-structured in-memory file systems","Z. Mao; S. Zheng; L. Huang; Y. Shen","Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University","2017 IEEE 36th International Performance Computing and Communications Conference (IPCCC)","5 Feb 2018","2017","","","1","8","Emerging byte-addressable Non-Volatile Memory(NVM) technologies offer fine-grained access to persistent data with latency comparable to DRAM. This presents both challenges and opportunities to system software designers. To utilize the fascinating features of NVM, a lot of works have been done to develop NVM-based file systems. Direct access (DAX) is a key feature provided by most NVM-based file systems, it enables applications to directly access NVM without going through the page cache layer. However, DAX-style mmap may cause serious consistency issues in versioning in-memory file systems that adopt CoW to solve block sharing problem among snapshots. In this paper, we propose a new mmap mechanism called versioning-mmap which solves the consistency issue of memory-mapped I/O in DAX-enabled versioning in-memory file systems. We implement our proposed mechanism in HMVFS and do a series of experiments based on our implementation. Results show that our mechanism achieves consistency while imposing negligible performance overhead compared to PMFS and NOVA.","2374-9628","978-1-5090-6468-7","10.1109/PCCC.2017.8280456","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8280456","","Nonvolatile memory;Random access memory;Cows;File systems;Layout;Computational modeling","random-access storage","NonVolatile Memory technologies;system software designers;fine-grained access;versioning-mmap;mmap mechanism;versioning in-memory file systems;DAX-style mmap;direct access;NVM-based file systems","","1","","22","","5 Feb 2018","","","IEEE","IEEE Conferences"
"BUILDFAST: History-Aware Build Outcome Prediction for Fast Feedback and Reduced Cost in Continuous Integration","B. Chen; L. Chen; C. Zhang; X. Peng","School of Computer Science and Shanghai, Key Laboratory of Data Science, Fudan University,Shanghai,China; School of Computer Science and Shanghai, Key Laboratory of Data Science, Fudan University,Shanghai,China; School of Computer Science and Shanghai, Key Laboratory of Data Science, Fudan University,Shanghai,China; School of Computer Science and Shanghai, Key Laboratory of Data Science, Fudan University,Shanghai,China","2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)","24 Dec 2020","2020","","","42","53","Long build times in continuous integration (CI) can greatly increase the cost in human and computing resources, and thus become a common barrier faced by software organizations adopting CI. Build outcome prediction has been proposed as one of the remedies to reduce such cost. However, the state-of-the-art approaches have a poor prediction performance for failed builds, and are not designed for practical usage scenarios. To address the problems, we first conduct an empirical study on 2,590,917 builds to characterize build times in realworld projects, and a survey with 75 developers to understand their perceptions about build outcome prediction. Then, motivated by our study and survey results, we propose a new history-aware approach, named BUILDFAST, to predict CI build outcomes cost-efficiently and practically. We develop multiple failure-specific features from closely related historical builds via analyzing build logs and changed files, and propose an adaptive prediction model to switch between two models based on the build outcome of the previous build. We investigate a practical online usage scenario of BUILDFAST, where builds are predicted in chronological order, and measure the benefit from correct predictions and the cost from incorrect predictions. Our experiments on 20 projects have shown that BUILDFAST improved the state-of-the-art by 47.5% in F1-score for failed builds.","2643-1572","978-1-4503-6768-4","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9286064","• Software and its engineering →Maintaining software;Continuous Integration;Build Failures;Failure Prediction","Analytical models;Adaptation models;Switches;Predictive models;Software;Time measurement;Software engineering","buildings (structures);data mining;project management;software development management;software metrics;software tools;system recovery","poor prediction performance;continuous integration;history-aware build outcome prediction;incorrect predictions;correct predictions;BUILDFAST;practical online usage scenario;previous build;adaptive prediction model;build logs;CI build outcomes cost-efficiently;history-aware approach;practical usage scenarios","","","","62","","24 Dec 2020","","","IEEE","IEEE Conferences"
"Failure Avoidance through Fault Prediction Based on Synthetic Transactions","M. Shatnawi; M. Ripeanu","Microsoft Online Ads Platform, Microsoft Corp., Redmond, WA, USA; Electr. & Comput. Eng. Dept., Univ. of British Columbia, Vancouver, BC, Canada","2011 11th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing","11 Jul 2011","2011","","","324","331","System logs are an important tool in studying the conditions (e.g., environment misconfigurations, resource status, erroneous user input) that cause failures. However, production system logs are complex, verbose, and lack structural stability over time. These traits make them hard to use, and make solutions that rely on them susceptible to high maintenance costs. Additionally, logs record failures after they occur: by the time logs are investigated, users have already experienced the failures' consequences. To detect the environment conditions that are correlated with failures without dealing with the complexities associated with processing production logs, and to prevent failure-causing conditions from occurring before the system goes live, this research suggests a three step methodology: (i) using synthetic transactions, i.e., simplified workloads, in pre-production environments that emulate user behavior, (ii) recording the result of executing these transactions in logs that are compact, simple to analyze, stable over time, and specifically tailored to the fault metrics of interest, and (iii) mining these specialized logs to understand the conditions that correlate to failures. This allows system administrators to configure the system to prevent these conditions from happening. We evaluate the effectiveness of this approach by replicating the behavior of a service used in production at Microsoft, and testing the ability to predict failures using a synthetic workload on a 650 million events production trace. The synthetic prediction system is able to predict 91% of real production failures using 50-fold fewer transactions and logs that are 10,000-fold more compact than their production counterparts.","","978-1-4577-0129-0","10.1109/CCGrid.2011.61","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5948623","Failure prediction;failure avoidance;system logs;synthetic transactions;data analysis;data mining","Production;Data mining;Time factors;Servers;Software;Time measurement;Reliability","data mining;fault diagnosis;transaction processing","failure avoidance;fault prediction;synthetic transaction;system log;structural stability;maintenance cost;user behavior;fault metric;synthetic prediction system;online service","","2","","11","","11 Jul 2011","","","IEEE","IEEE Conferences"
"Towards Highly Available Complex Event Processing Deployments in the Cloud","P. Carbone; K. Vandikas; F. Zaloshnja",NA; NA; NA,"2013 Seventh International Conference on Next Generation Mobile Apps, Services and Technologies","11 Nov 2013","2013","","","153","158","Recent advances in distributed computing have made it possible to achieve high availability on traditional systems and thus serve them as reliable services. For several offline computational applications, such as fine grained batch processing, their parallel nature in addition to weak consistency requirements allowed a more trivial transition. On the other hand, on-line processing systems such as Complex Event Processing (CEP) still maintain a monolithic architecture, being able to offer high expressiveness and vertical scalability at the expense of low distribution. Despite attempts to design dedicated distributed CEP systems there is potential for existing systems to benefit from a sustainable cloud deployment. In this work we address the main challenges of providing such a CEP service with a focus on reliability, since it is the most crucial aspect of that transition. Our approach targets low average detection latency and sustain-ability by leveraging event delegation mechanisms present on existing stream execution platforms. It also introduces redundancy and transactional logging to provide improved fault tolerance and partial recovery. Our performance analysis illustrates the benefits of our approach and shows acceptable performance costs for on-line CEP exhibited by the fault tolerance mechanisms we introduced.","2161-2897","978-1-4799-2010-5","10.1109/NGMAST.2013.35","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6658116","complex event processing;distributed stream processing;fault tolerance;fraud detection;SIP;CDR","Parallel processing;Engines;Fault tolerance;Fault tolerant systems;Availability;Cloning","cloud computing;software fault tolerance;software reliability;transaction processing","highly available complex event processing deployment;distributed computing;monolithic architecture;vertical scalability;CEP systems;sustainable cloud deployment;CEP service;reliability;low average detection latency;event delegation mechanisms;stream execution platforms;redundancy logging;transactional logging;partial recovery;performance costs;fault tolerance mechanism","","2","1","15","","11 Nov 2013","","","IEEE","IEEE Conferences"
"A Study on Efficient Log Visualization Using D3 Component against APT: How to Visualize Security Logs Efficiently?","J. Lee; J. Jeon; C. Lee; J. Lee; J. Cho; K. Lee","Dept. of Inf. Security, Korea Univ., Seoul, South Korea; Dept. of Comput. Sci. & Eng., Sogang Univ., Seoul, South Korea; Dept. of Comput. Sci. & Eng., Sogang Univ., Seoul, South Korea; Dept. of Inf. & Telecommun. Eng., Univ. of Suwon, Seoul, South Korea; Dept. of Convergence Secuirty, Kyonggi Univ., Seoul, South Korea; Dept. of Inf. Security, Korea Univ., Seoul, South Korea","2016 International Conference on Platform Technology and Service (PlatCon)","21 Apr 2016","2016","","","1","6","APT attack has caused chaos in society since 2006. Especially, the vulnerability of the infrastructure is exposed to the outside a lot due to the development of the IT infrastructure in Korea. In addition, APT attacks targeting companies' major confidential information are increasing every year. APT attack causes negative publicity for the company and financial damage. APT is completely different from the problem which most organizations have been dealt with. Cyber-attack threats were visible in the past. But currently, APT attacks were invisible and focused on confidential data. Therefore, we need a new approach to solve this problem. We have to find traces of prejudice in the circumstances, everything seems normal. If we perform a correlation analysis of the log acquired from all the devices, systems and applications, we can easily understand the problems which occur in our information systems. Current commercial SIEM has the ability to visualize the correlation analysis and the log. But the security officer takes a lot of time to understand the visualized security logs. Moreover, due to expensive cost of SIEM solution, small companies have difficulty introducing SIEM solution. For these reasons, we have developed a SIEM solution based on open-source program such as D3 component which results in decreasing the cost of the program. In addition, we analyzed the D3 components which can visualize the security logs, and matched D3 components with the security logs. In this paper, we propose the visualization methods using D3 components for analyzing the security logs efficiently.","","978-1-4673-8685-2","10.1109/PlatCon.2016.7456778","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7456778","","Security;Data visualization;Correlation;Companies;Monitoring;Risk management;Retirement","data visualisation;information systems;public domain software;security of data","efficient log visualization;D3 component;APT attack;Korea;financial damage;cyber-attack threats;current commercial SIEM;information systems;correlation analysis;open-source program;security logs;security information-and-event management","","5","","20","","21 Apr 2016","","","IEEE","IEEE Conferences"
"flexfringe: A Passive Automaton Learning Package","S. Verwer; C. A. Hammerschmidt","Cyber Security Group, Delft Univ. of Technol., Delft, Netherlands; SEDAN group, Univ. of Luxembourg, Luxembourg, Luxembourg","2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)","7 Nov 2017","2017","","","638","642","Finite state models, such as Mealy machines or state charts, are often used to express and specify protocol and software behavior. Consequently, these models are often used in verification, testing, and for assistance in the development and maintenance process. Reverse engineering these models from execution traces and log files, in turn, can accelerate and improve the software development and inform domain experts about the processes actually executed in a system. We present name, an open-source software tool to learn variants of finite state automata from traces using a state-of-the-art evidence-driven state-merging algorithm at its core. We embrace the need for customized models and tailored learning heuristics in different application domains by providing a flexible, extensible interface.","","978-1-5386-0992-7","10.1109/ICSME.2017.58","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8094471","machine learning tool;automata learning;software package;finite state machines","Learning automata;Tools;Software algorithms;Software;Heuristic algorithms;Machine learning algorithms;Algorithm design and analysis","finite state machines;learning (artificial intelligence);program testing;program verification;public domain software;reverse engineering;software maintenance;software tools","flexfringe;finite state models;Mealy machines;state charts;software behavior;maintenance process;reverse engineering;execution traces;software development;open-source software tool;finite state automata;learning heuristics;passive automaton learning package;evidence-driven state-merging algorithm;verification;testing","","1","","37","","7 Nov 2017","","","IEEE","IEEE Conferences"
"Web user navigation patterns discovery from WWW server log files","P. Weichbroth; M. Owoc; M. Pleszkun","PHD Student, Katowice University of Economics, Katowice, Poland; Department of Artificial Intelligence Systems, Wroclaw University of Economics, Wroclaw, Poland; Institute of Computer Science, Wroclaw University of Technology, Wroclaw, Poland","2012 Federated Conference on Computer Science and Information Systems (FedCSIS)","20 Nov 2012","2012","","","1171","1176","Continued growth of user number and size of shared content on Web sites cause the necessity of automatic adjusting content to users' needs. In the literature of Web Mining, such actions are referred to personalization and recommendation which led to improve the visibility of presented content. To perform adequacy actions which correspond to the expected users' needs we can utilize web server log files. Mining such data with accurate constraints can lead to the discovery of web user navigation patterns. Such knowledge is used by personalization and recommendation systems (PRS) due to performed actions against user behavior during a visit on the web portal. In these paper we present the system framework for mining web user navigation patterns in order to knowledge management. We focus on constraints which are critical factors to evaluate the effectiveness of the implemented algorithm. On the other hand, these constraints can be perceived as knowledge validation criteria due to its adequacy. Thus only adequate knowledge can be added to existing in PRS knowledge base.","","978-83-60810-48-4","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6354427","","Navigation;Electronic mail;Association rules;Web servers;Software algorithms","data mining;file servers;information needs;information retrieval;knowledge management;portals;Web sites","Web user navigation pattern discovery;WWW server log files;shared content;Web sites;automatic content adjustment;user needs;Web Mining;data Mining;personalization and recommendation systems;user behavior;Web portal;knowledge management;PRS knowledge base","","1","","36","","20 Nov 2012","","","IEEE","IEEE Conferences"
"Implementation of Alpha Miner Algorithm in Process Mining Application Development for Online Learning Activities Based on MOODLE Event Log Data","P. Nafasa; I. Waspada; N. Bahtiar; A. Wibowo","Faculty of Mathematics and Science, Universitas Diponegoro,Department of Computer Science,Semarang,Indonesia; Faculty of Mathematics and Science, Universitas Diponegoro,Department of Computer Science,Semarang,Indonesia; Faculty of Mathematics and Science, Universitas Diponegoro,Department of Computer Science,Semarang,Indonesia; Faculty of Mathematics and Science, Universitas Diponegoro,Department of Computer Science,Semarang,Indonesia","2019 3rd International Conference on Informatics and Computational Sciences (ICICoS)","6 Feb 2020","2019","","","1","6","Moodle is one of the widely used Learning Management Systems in the field of education. Moodle stores all online learning activities to the database in the form of event log. These event logs can be used to improve the quality of learning through process analysis. One of the fields of science that can be used to discover the process model based on event log is Process Mining. The problem arise when an instructor willing to use the Moodle event log data to do a Process Mining activities. There are some preprocessing issues need to be done to the Moodle event log data as prerequisite to continue with Process Mining algorithm. As the solution, Moodle need to be integrated with the Process Mining. In this study an application was developed to integrate the Moodle event log data with the activities of Process Mining, especially to facilitate the preprocessing tools. The alpha miner algorithm was used here as the process model discovery algorithm. As the result, we successfully develop the application to discover process model from Moodle log event data. Instructors can use some functional features of the application to meet their need in process mining analysis. Experiments using real and artificial case studies have been conducted and it is proven that the implementation of the alpha miner algorithm can work correctly on the Moodle event log data.","","978-1-7281-4610-2","10.1109/ICICoS48119.2019.8982384","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8982384","process mining;alpha miner algorithm;moodle","","data analysis;data mining;learning management systems;public domain software","alpha miner algorithm;Process Mining application development;online learning activities;MOODLE event log data;learning management systems;Moodle stores;process analysis;Process Mining activities;Process Mining algorithm;process model discovery algorithm;process mining analysis","","1","","12","","6 Feb 2020","","","IEEE","IEEE Conferences"
"Augmented reality serious game framework for rehabilitation with personal health records","Jia-Kuan Lin; P. -H. Cheng; Yen Su; Jer-Junn Luh; Shao-Yu Wang; Hsiang-Wen Lin; Hsiao-Chi Hou; Wen-Cheng Chiang; Ssu-Wei Wu; Mei-Ju Su","Department of Software Engineering, Kaohsiung Normal University, Taiwan; Department of Software Engineering, Kaohsiung Normal University, Taiwan; Department of Software Engineering, Kaohsiung Normal University, Taiwan; School of Physical Therapy, National Taiwan University, Taipei, Taiwan; Department of Software Engineering, Kaohsiung Normal University, Taiwan; Department of Software Engineering, Kaohsiung Normal University, Taiwan; Department of Software Engineering, Kaohsiung Normal University, Taiwan; Department of Software Engineering, Kaohsiung Normal University, Taiwan; Department of Software Engineering, Kaohsiung Normal University, Taiwan; Department of Biomedical Engineering, Yuanpei University, HsinChu, Taiwan","2011 IEEE 13th International Conference on e-Health Networking, Applications and Services","22 Sep 2011","2011","","","197","200","Users interact with most electronic entertainment games via some specific electronic sensors, which are not part of the existing equipment in most household. As well, some electronic entertainment games claim to have medical rehabilitative effects. This research designs a series of web-based serious games (SGs) for rehabilitation utilization with a proposed framework that lets family users easily connect through the Internet without the need for special sensors. This system includes diverse SGs for different rehabilitation levels. Users can operate these SGs via mouse or hand movements. Therefore, it is not necessary for patients to buy or rent any extra rehabilitation equipment; all that is required is a personal computer at home. Some of these SGs are adapted to augmented reality technology. This system also connects directly with personal health records in order to log every rehabilitation activity. In addition, social networks, such as Facebook, can be used for connecting rehabilitation patients at home with each other. By simplifying access, this rehabilitation SG system increases the frequency of rehabilitative activity of patients at home.","","978-1-61284-697-2","10.1109/HEALTH.2011.6026743","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6026743","Augment reality;cybercare;digital camera;health information management;medical information system;patient rehabilitation;personal health records","Biomedical imaging;Humanoid robots;Image color analysis;Irrigation;Games;Androids","augmented reality;computer games;Internet;medical information systems;patient rehabilitation;personal information systems;social networking (online);telemedicine","augmented reality serious game framework;personal health record;electronic entertainment game;electronic sensor;medical rehabilitative effect;Web-based serious game;rehabilitation utilization;Internet;hand movement;mouse movement;rehabilitation equipment;personal computer;social network;Facebook;patient rehabilitative activity","","2","","15","","22 Sep 2011","","","IEEE","IEEE Conferences"
"Distributed Honeypot log management and visualization of attacker geographical distribution","V. Visoottiviseth; U. Jaralrungroj; E. Phoomrungraungsuk; P. Kultanon","Faculty of Information and Communication Technology, Mahidol University, Nakhon Pathom, Thailand 73170; Faculty of Information and Communication Technology, Mahidol University, Nakhon Pathom, Thailand 73170; Faculty of Information and Communication Technology, Mahidol University, Nakhon Pathom, Thailand 73170; Faculty of Information and Communication Technology, Mahidol University, Nakhon Pathom, Thailand 73170","2011 Eighth International Joint Conference on Computer Science and Software Engineering (JCSSE)","23 Jun 2011","2011","","","23","28","Honeypot is a prominent technology that helps us learn new hacking techniques from attackers and intruders. The much information from multiple Honeypot servers, the more appropriate signatures we can generate. To ease the administrator to manage and monitor trace files from multiple Honeypot servers that are distributed in various locations at the same time, in this paper we design and implement a prototype of log management server to automatically and periodically collect log files from them. Information reported by each Honeypot server will be sent in secure manner to the log management server. The log management server then parses the information into the database server, where users can search for specific information through the web interface, such as searching based on one or two Honeypot servers. Moreover, the geographical distribution of attackers is visualized in the world map by utilizing the WHOIS database and GeoPlot software.","","978-1-4577-0687-5","10.1109/JCSSE.2011.5930083","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5930083","Honeypot;Honeynet;Management Server;Honeyd;Distributed Server;Network Security","","database management systems;file servers;Internet;security of data","distributed Honeypot Log management;geographical distribution attacker;log management server;database server;Web interface;WHOIS database;GeoPlot software","","2","1","17","","23 Jun 2011","","","IEEE","IEEE Conferences"
"Analysis of runtime data-log for software fault localization","C. Zhou; R. Kumar; S. Jiang","Magnatech LLC, East Granby, CT, USA; Department of Elec. & Comp. Eng., Iowa State Univ., Ames, USA; General Motors R&D, Warren, MI, USA","Proceedings of the 2011 American Control Conference","18 Aug 2011","2011","","","5127","5132","Software can contain faults that remain undetected prior to its release. It is then important to determine the plausible root-cause of fault, namely, the faulty lines of code, or indicators for any missing lines of code. To localize a software fault to its ""root-cause"", we introduce the notion of a fault-seed, a fragment of a faulty-run, and propose a model based automated approach that analyzes the observed faulty run of the software, recorded during its runtime operation, to determine the fault-seed. Owing to resource constraints in certain system such as embedded system, the run-time data logging can be incomplete, resulting in partial observation of software runs. A feature of our analysis is to localize the possible root cause in presence of such partial observability of data variables.","2378-5861","978-1-4577-0081-1","10.1109/ACC.2011.5989966","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5989966","","Software;Monitoring;Computational modeling;Radiation detectors;Fault diagnosis;Temperature sensors;Software algorithms","data flow analysis;software fault tolerance","runtime data log analysis;software fault localization;model based automated approach;resource constraints;observability;data variables","","1","","15","","18 Aug 2011","","","IEEE","IEEE Conferences"
"Process-aware Event Log Datacubes for Workflow Process and Knowledge Mining, Predicting and Analyzing Frameworks","S. Ham; D. Pham; K. Kim; K. P. Kim","KYONGGI UNIVERSITY,Div. of Comp. Sci. & Eng.,Suwon,South Korea; KYONGGI UNIVERSITY,Div. of Comp. Sci. & Eng.,Suwon,South Korea; KYONGGI UNIVERSITY,Div. of Comp. Sci. & Eng.,Suwon,South Korea; KYONGGI UNIVERSITY,Div. of Comp. Sci. & Eng.,Suwon,South Korea","2020 22nd International Conference on Advanced Communication Technology (ICACT)","9 Apr 2020","2020","","","651","657","The issue of workflow process mining and analytics is beginning to make its appearance in the workflow-supported enterprise intelligence and systems literature. In order to improve the quality of workflow process intelligence, it is essential for an efficient and effective data center storing workflow enactment event logs to be provisioned in carrying out the workflow process mining and analytics. In this paper, we propose a three-dimensional datacube, which is named as a process-aware dat-acube, for organizing workflow-supported enterprise data centers to efficiently as well as effectively store the workflow process enactment event logs in the IEEE XES format, and also we carry out an experimental process mining and analytics to show how much perfectly the process-aware datacubes are suitable for discovering workflow process patterns and its analytical knowledge, like enacted proportions and enacted work transferences, from the workflow process enactment event histories. From the process-aware datacubes, the workflow process mining must be able to properly discover all the workflow process patterns based upon the four types of control-flow primitives such as linear (sequential) routing, disjunctive (selective) routing, conjunctive (parallel) routing, and iterative (loop) routing patterns, whereas the workflow process analytics is to discover the enacted pro-protions of each of the process pattherns and the enacted work transferences of each of the workflow performers.","1738-9445","979-11-88428-04-5","10.23919/ICACT48636.2020.9061449","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9061449","process-aware datacubes;workflow process mining;workflow process analytics;temporal workcase;temporal worktransference;XES format;workflow-supported enterprise data center","Data centers;Data mining;Xenon;History;Process control;Routing;Engines","data mining;workflow management software","workflow-supported enterprise data centers;workflow process enactment event histories;workflow process mining;process-aware event log datacubes;workflow-supported enterprise intelligence;systems literature;data center storing workflow enactment event","","","","23","","9 Apr 2020","","","IEEE","IEEE Conferences"
"Hierarchy testing for Web applications performance using Web log statistics","Xiaolu Li; Cheng Zhong; Rui Lin; Dexia Cai","School of Computer and Electronics and Information, Guangxi University, Nanning, China; School of Computer and Electronics and Information, Guangxi University, Nanning, China; School of Computer and Electronics and Information, Guangxi University, Nanning, China; School of Computer and Electronics and Information, Guangxi University, Nanning, China","2011 International Conference on Computer Science and Service System (CSSS)","4 Aug 2011","2011","","","1304","1307","To simulate user's behaviors more realistically, this paper proposes a novel hierarchy testing framework and method for Web applications performance. It extracts the access information from Web log files and builds a hierarchical structure model (HSM) by statistical analysis, obtains the test path with higher probability based on HSM model, and tests Web applications performance by using this path as a test case. The experiments show that the presented method can reveal more effectively the characteristics of Web applications performance than the traditional Web applications performance testing approach.","","978-1-4244-9763-8","10.1109/CSSS.2011.5974445","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5974445","Web Applications;Performance Testing;Web Log;User Behaviors","Testing;Central Processing Unit;Computers;Servers;Analytical models;Computational modeling;Statistical analysis","Internet;program testing;software performance evaluation;statistical analysis;user interfaces","Web hierarchy testing;Web log statistics;user behavior;hierarchical structure model;statistical analysis;Web application performance testing;hierarchy testing framework;access information extraction","","","","","","4 Aug 2011","","","IEEE","IEEE Conferences"
"Atomic multi-database transaction of WAL joumaling mode in SQLite","S. Kim; M. Kim; D. Q. Tuan; Y. Won","Dept. of Computer Software, Hanyang University, Seoul, Korea; Dept. of Computer Software, Hanyang University, Seoul, Korea; Dept. of Computer Software, Hanyang University, Seoul, Korea; Dept. of Computer Software, Hanyang University, Seoul, Korea","2017 19th International Conference on Advanced Communication Technology (ICACT)","30 Mar 2017","2017","","","874","878","This works is to propose a solution for multi-database atomicity problem of WAL journaling mode in SQLite. SQLite is the most widely deployed and used DBMS in mobile system [1]. SQLite has several journaling modes. WAL (Write-Ahead Logging) is the one of those journaling modes included from version 3.7.0. WAL is significantly faster in most scenarios and provides more concurrency as reading and writing can proceed concurrently. However, transactions that involve changes with multiple attached databases do not guarantee atomicity across all databases as a set [2]. We modify transaction and recovery procedure of WAL to solve this problem. This work consists of three parts: (i) Enabling Use of Master Journal for WAL. (ii) Creation of `mj-stored' File in Multi-Database Transaction. (iii) Rollback with `mj-stored' File. In multi-database transaction, we create `mj-stored' file for each WAL file to save the state before the transaction begins and the master journal file name. If crash occurs during the transaction, database with WAL journaling mode try to find the `mj-stored' file and corresponding master journal file to roll back to the state before the transaction began in recovery time. With this solution, multi-database transaction with WAL journaling mode can guarantee atomicity.","","978-89-968650-9-4","10.23919/ICACT.2017.7890219","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7890219","Atomicity;Database;Multi-database;SQLite;Write-Ahead Logging","Computer crashes;Concurrent computing;Indexes;Software;Mobile communication;Writing","distributed databases;mobile computing;transaction processing","WAL transaction procedure;WAL recovery procedure;mj-stored file rollback;master journal file name;recovery time;WAL version 3.7.0;write-ahead logging;mobile system;DBMS;WAL journaling mode;multidatabase atomicity problem;SQLite;atomic multidatabase transaction","","","","15","","30 Mar 2017","","","IEEE","IEEE Conferences"
"Insights into the Diagnosis of System Failures from Cluster Message Logs","E. Chuah; A. Jhumka; J. C. Browne; B. Barth; S. Narasimhamurthy","Univ. of Texas at Austin, Austin, TX, USA; Univ. of Warwick, Coventry, UK; Singapore Polytech., Singapore, Singapore; Texas Adv. Comput. Center, TX, USA; Seagate Technol., Havant, UK","2015 11th European Dependable Computing Conference (EDCC)","7 Jan 2016","2015","","","225","232","Large cluster systems are composed of complex, interacting hardware and software components. Components, or the interactions between components, may fail due to many different reasons, leading to the eventual failure of executing jobs. This paper investigates an open question about failure diagnosis: What are the characteristics of the errors that lead to cluster system failures? To this end, this paper gives a systematic process for identifying and characterizing the root-causes of failures. We applied an extended version of the FDiagV3 diagnostics toolkit to the log-files of the Ranger and Lonestar supercomputers. Our results show that: (i) failures were a result of recurrent issues and errors, (ii) a small set of nodes are associated with these issues and errors, and (iii) Ranger and Lonestar display similar sets of problems. FDiagV3 will be put in the public domain for support of failure diagnosis for large cluster systems in May, 2015.","","978-1-4673-9289-1","10.1109/EDCC.2015.19","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7371970","Large cluster systems;Failure diagnosis toolkit;Linux O/S;Lustre file-system;Cluster message logs","Software;Supercomputers;Production;Linux;Correlation;Electronic mail;Analytical models","failure analysis;mainframes;parallel machines;system monitoring;workstation clusters","system failure diagnosis;cluster message logs;FDiagV3 diagnostics toolkit;Lonestar supercomputer;Ranger supercomputer;log-files;cluster systems","","2","","17","","7 Jan 2016","","","IEEE","IEEE Conferences"
"Black Block Recorder: Immutable Black Box Logging for Robots via Blockchain","R. White; G. Caiazza; A. Cortesi; Y. I. Cho; H. I. Christensen","University of California San Diego, San Diego, CA, USA; Ca’ Foscari University of Venice, Venice, Italy; Ca’ Foscari University of Venice, Venice, Italy; Gachon University Seoul, Seoul, South Korea; University of California San Diego, San Diego, CA, USA","IEEE Robotics and Automation Letters","1 Aug 2019","2019","4","4","3812","3819","Event data recording is crucial in robotics research, providing prolonged insights into a robot's situational understanding, progression of behavioral state, and resulting outcomes. Such recordings are invaluable when debugging complex robotic applications or profiling experiments ex post facto. As robotic developments mature into production, both the roles and requirements of event logging will broaden, to include serving as evidence for auditors and regulators investigating accidents or fraud. Given the growing number of high profile public incidents involving self-driving automotives resulting in fatality and regulatory policy making, it is paramount that the integrity, authenticity and non-repudiation of such event logs are maintained to ensure accountability. Being mobile cyber-physical systems, robots present new threats, and vulnerabilities beyond traditional IT: unsupervised physical system access or postmortem collusion between robot and OEM could result in the truncation or alteration of prior records. In this letter, we address immutablization of log records via integrity proofs and distributed ledgers with special considerations for mobile and public service robot deployments.","2377-3766","","10.1109/LRA.2019.2928780","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8764004","Robot safety;networked robots;software middleware;cryptobotics;distributed ledgers","Robots;Blockchain;Distributed ledger;Bitcoin;Middleware","","","","3","","29","Traditional","15 Jul 2019","","","IEEE","IEEE Journals"
"A novel mechanism to continuously scan field logs and gain real-time feedback","K. Vinod; M. Ramachandra; P. Pai; S. Yalawar","Philips Electronics India Ltd, Bengaluru - 560045, India; Philips Electronics India Ltd, Bengaluru - 560045, India; Philips Electronics India Ltd, Bengaluru - 560045, India; Philips Electronics India Ltd, Bengaluru - 560045, India","2013 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)","19 Dec 2013","2013","","","50","52","Reliability is characteristic of the system which begins during the concept development phase of a product realization process and continuously or iteratively improved, until its end-of-life. Reliability data along with availability and serviceability (RAS) [1] can commonly be retrieved using the system logs through various data mining techniques. The size of the logs for a typical healthcare modality like the Philips Magnetic Resonance (MR) would be of the order of 3-digit megabyte number per day per installed base. Given the humongous size, various clustering techniques as used in big data processing algorithms [2], grind the data to seek the correct results in a timely and efficient fashion. This post-processing step introduces a temporal shift in analyzing the data much after the events have occurred. For the state of affairs that affects reliability and serviceability, it is important that the condition of the deployed systems is notified to actors who can resolve such issues, meeting shrinking timelines demanded by the service level agreements. This would require the log information to be processed directly at the deployment without causing a system performance regression. This paper talks about such a technique that is implemented within the system purview to improve the lead time and thus increase efficiency of the feedback into the research and development (R & D) department.","","978-1-4799-2552-0","10.1109/ISSREW.2013.6688866","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6688866","","Reliability;Monitoring;Information management;Data handling;Data storage systems;Software;Portable computers","Big Data;biomedical MRI;data mining;health care;medical computing;pattern clustering;regression analysis;research and development;system monitoring","field logs scanning;real-time feedback;system reliability;concept development phase;product realization process;end-of-life;serviceability;data mining techniques;healthcare modality;Philips magnetic resonance;MR;clustering techniques;Big Data processing algorithms;system performance regression;research and development department;R&D department","","","","1","","19 Dec 2013","","","IEEE","IEEE Conferences"
"Do Injected Faults Cause Real Failures? A Case Study of Linux","N. Kikuchi; T. Yoshimura; R. Sakuma; K. Kono","Keio Univ., Yokohama, Japan; Keio Univ., Yokohama, Japan; Keio Univ., Yokohama, Japan; Keio Univ., Yokohama, Japan","2014 IEEE International Symposium on Software Reliability Engineering Workshops","15 Dec 2014","2014","","","174","179","Software fault injection (SFI) has been used to intentionally cause ""failures"" in software components and assess their impacts on the entire software system. A key property that SFI should satisfy is the representative ness of injected failures, the failures caused by SFI should be as close as possible to failures in the wild. If injected failures do not represent realistic failures, the measured resilience or tolerance against failures of the investigated system is not trustworthy. To the best of the authors' knowledge, the representative ness of ""faults"" has been investigated. However, it is an open problem whether the failures caused by injected faults represent realistic failures. In this paper, we report the preliminary results of the investigation on the representative ness of injected failures. To compare injected failures with real failures, we have collected 43,742 real crash logs of Linux from the Red Hat repository, and conducted a fault injection campaign on Linux, using SAFE, a state-of-theatre injector of software faults. In the fault injection campaign, 50,000 faults are injected to the Linux file system and 71,470 runs of a workload are executed. The crash logs generated by SFI are compared with the real Red Hat logs with respect to crash causes, crashed system calls, and crashed modules. Our preliminary results suggest that failures caused by injected faults do not represent real failures, probably because injected faults are not representative enough or because the selected workload is not realistic.","","978-1-4799-7377-4","10.1109/ISSREW.2014.104","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6983834","software fault injection;failure analysis;operating systems","Computer crashes;Kernel;Linux;Software systems;Fault tolerant systems;Benchmark testing","failure analysis;Linux;software reliability;system recovery","software fault injection;SFI;crash logs;RedHat repository;SAFE;Linux file system;RedHat logs;crash causes;crashed system calls;crashed modules","","4","","20","","15 Dec 2014","","","IEEE","IEEE Conferences"
"A device for logging student driver's experience","A. Shaout; J. Alan Gleichman; S. Awad","The University of Michigan - Dearborn, The Electrical and Computer Engineering Department, 48128, USA; The University of Michigan - Dearborn, The Electrical and Computer Engineering Department, 48128, USA; The University of Michigan - Dearborn, The Electrical and Computer Engineering Department, 48128, USA","2013 9th International Computer Engineering Conference (ICENCO)","13 Feb 2014","2013","","","72","77","This paper describes a system for helping teen drivers track there experience while training to obtain their driver's license. The proposed system is a portable in-vehicle device run by an embedded microcontroller that logs student driver hours and mileage and a PC program to create reports from the recorded data. The proposed device uses a GPS receiver to track distance traveled and time spent driving.","","978-1-4799-3370-9","10.1109/ICENCO.2013.6736479","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6736479","Embedded software;Microcontrollers;GPS;Automotive applications;Student driver logger;Embedded systems","Global Positioning System;Vehicles;Computers;Microcontrollers;Receivers;Software;Monitoring","computer based training;computerised instrumentation;embedded systems;Global Positioning System;human factors;microcontrollers","student driver experience logging device;teen driver training;driver license;portable in-vehicle device;embedded microcontroller;student driver hour logs;student driver mileage logs;PC program;GPS receiver;distance tracking;time tracking","","","","7","","13 Feb 2014","","","IEEE","IEEE Conferences"
"A UML profile for dynamic execution persistence with monitoring purposes","E. Domínguez; B. Pérez; M. A. Zapata",Dpto. de Informática e Ingeniería de Sistemas. Universidad de Zaragoza. 50009 Zaragoza. Spain; Dpto. de Matemáticas y Computación. Universidad de La Rioja. 26004 Logroño. Spain; Dpto. de Informática e Ingeniería de Sistemas. Universidad de Zaragoza. 50009 Zaragoza. Spain,"2013 5th International Workshop on Modeling in Software Engineering (MiSE)","12 Sep 2013","2013","","","55","61","System monitoring is typically performed by means of log files storing the sequential trace of the system events. However, these files constitute a poor solution when improved auditing features are required. For this reason, we advocate defining specific persistence structures for registering a more complete system trace. In particular, when the system behaviour is specified by means of a UML statechart, we propose to automatically generate a stereotyped UML class diagram containing information for tracing the system behaviour without loosing the statechart dynamic semantics. To do this, we have formally defined a Statechart Execution Persistence (SEP) UML profile which (1) eases audit and process improvement, (2) ensures accuracy and consistency of data, and (3) guides during the automatic generation of the system's storage mechanisms, following a MDD approach.","2156-7891","978-1-4673-6447-8","10.1109/MiSE.2013.6595297","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6595297","Model Transformation;System Monitoring;Process Improvement;UML Profile;Health Care Context","Unified modeling language;Guidelines;Semantics;Proposals;Monitoring;Context;Standards","file organisation;program diagnostics;system monitoring;Unified Modeling Language","dynamic execution persistence;system monitoring;log file storage;sequential trace;system events;system trace;system behaviour;stereotyped UML class diagram;statechart dynamic semantics;statechart execution persistence;SEP UML profile;system storage mechanisms;MDD approach;model driven development approach","","2","","33","","12 Sep 2013","","","IEEE","IEEE Conferences"
"Integration of EPICS subsystem control on FireSignal","A. S. Duarte; P. R. Carvalho; B. Santos; B. B. Carvalho; T. Pereira; J. Fortunato; J. Sousa; H. Fernandes","Associação EURATOM/IST, Instituto de Plasmas e Fusão Nuclear - Laboratório Associado, Instituto Superior Técnico, Av. Rovisco Pais, P-1049-001 Lisboa, Portugal; Associação EURATOM/IST, Instituto de Plasmas e Fusão Nuclear - Laboratório Associado, Instituto Superior Técnico, Av. Rovisco Pais, P-1049-001 Lisboa, Portugal; Associação EURATOM/IST, Instituto de Plasmas e Fusão Nuclear - Laboratório Associado, Instituto Superior Técnico, Av. Rovisco Pais, P-1049-001 Lisboa, Portugal; Associação EURATOM/IST, Instituto de Plasmas e Fusão Nuclear - Laboratório Associado, Instituto Superior Técnico, Av. Rovisco Pais, P-1049-001 Lisboa, Portugal; Associação EURATOM/IST, Instituto de Plasmas e Fusão Nuclear - Laboratório Associado, Instituto Superior Técnico, Av. Rovisco Pais, P-1049-001 Lisboa, Portugal; Associação EURATOM/IST, Instituto de Plasmas e Fusão Nuclear - Laboratório Associado, Instituto Superior Técnico, Av. Rovisco Pais, P-1049-001 Lisboa, Portugal; Associação EURATOM/IST, Instituto de Plasmas e Fusão Nuclear - Laboratório Associado, Instituto Superior Técnico, Av. Rovisco Pais, P-1049-001 Lisboa, Portugal; Associação EURATOM/IST, Instituto de Plasmas e Fusão Nuclear - Laboratório Associado, Instituto Superior Técnico, Av. Rovisco Pais, P-1049-001 Lisboa, Portugal","2010 17th IEEE-NPSS Real Time Conference","15 Apr 2011","2010","","","1","3","This paper presents a slow control system based on Experimental Physics and Industrial Control System (EPICS) and FireSignal technology for fusion devices, that is being developed for ISTTOK and COMPASS tokamaks. In this approach, the machine sub-systems control is based in micro-controllers. These sub-systems can handle automatically the necessary procedures before, during and after a plasma discharge, including safety protocols. An EPICS based supervisory control software then integrates all machine subsystems for central monitoring, configuration, logging and alarm management. Instead of the usual EPICS MEDM client tools, remote monitoring and configuration is accomplished by means of a specifically developed Java application using the CAJ library for EPICS and Common Object Request Broker Architecture (CORBA). This application behaves also as FireSignal Client, allowing the subsystems to be completely integrated on a FireSignal-based CODAC with all Process Variables (PV) and relevant control events being recorded in the central database. Through the FireSignal operator console, the so called FS User Client, it is possible to present a dedicated GUI for each subsystem, thus providing the machine operator a complete overview of the state of the machine and full control capabilities.","","978-1-4244-7110-2","10.1109/RTC.2010.5750474","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5750474","EPICS;FireSignal;Remote Monitoring;Remote Configuration;dsPIC","Fires;Software;Monitoring;Protocols;Java;Tokamaks","client-server systems;control engineering computing;data acquisition;discharges (electric);distributed object management;graphical user interfaces;Java;microcontrollers;physical instrumentation control;physics computing;telecontrol;Tokamak devices","EPICS subsystem control;slow control system;Experimental Physics and Industrial Control System;FireSignal technology;fusion devices;ISTTOK tokamak;COMPASS tokamak;machine subsystem control;microcontroller;plasma discharge;safety protocol;supervisory control software;central monitoring;logging;alarm management;remote monitoring;remote configuration;Java application;CAJ library;Common Object Request Broker Architecture;CORBA;FireSignal Client;FireSignal-based CODAC;process variables;FireSignal operator console;subsystem GUI;machine operator","","2","","7","","15 Apr 2011","","","IEEE","IEEE Conferences"
"Differential snapshot algorithms based on Hadoop MapReduce","Wei Du; X. Zou","Department of Computer Science, GongDong Police College, Guangzhou 510232, China; Department of Computer Science, Jinan University, Guangzhou 510632, China","2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)","14 Jan 2016","2015","","","1203","1208","Change Data Capture from source system is the first step in the incremental maintenance of data warehouses and business intelligence and is a key component of ETL (Extract, Transform and Load) technique. Methods of CDC are currently available, namely, time stamps, differential snapshots, triggers, and archive log. Differential snapshots do not rely on the implementation mechanism of the information sources, and therefore demonstrates better universality and adaptability. Due to the lack of computing resources, the differential snapshots based on sort merge and hash partition are sometimes error and not effective. This paper proposes the differential snapshot of low cost and high efficiency which combines open source database and Hadoop MapReduce. The differential snapshot based data summary which is generated by the MD5 algorithm is very effective but I/O cost is very heavy. So the paper proposes the SQL statement which queries the database while generating the tuples summary only once I/O. We implement the SQL statement on the open source database MySQL. In addition the parallel programming of MapReduce is used to find difference of database files which improves the efficiency and avoids the error. Experiment verifies the different performances among differential snapshot algorithms difference algorithm.","","978-1-4673-7682-2","10.1109/FSKD.2015.7382113","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7382113","Change Data Capture (CDC);differential snapshot algorithm;Hadoop MapReduce;MD5 algorithm","Databases;Particle separators;Algorithm design and analysis;Partitioning algorithms;Data mining;Syntactics;Data warehouses","file organisation;merging;parallel programming;public domain software;query processing;SQL","Hadoop MapReduce;change data capture;source system;incremental data warehouse maintenance;business intelligence;ETL technique;extract-transform-and-load technique;CDC methods;time stamps;triggers;archive log;information sources;resource computation;sort merge;hash partition;open source database;differential snapshot based data summary;MD5 algorithm;I/O cost;SQL statement;database queries;tuple summary generation;MySQL database;parallel programming;database files","","","","12","","14 Jan 2016","","","IEEE","IEEE Conferences"
"Mining unstructured log files for recurrent fault diagnosis","T. Reidemeister; Miao Jiang; P. A. S. Ward","Shoshin Distributed Systems Group, E&CE Department, University of Waterloo, Ontario, Canada; Shoshin Distributed Systems Group, E&CE Department, University of Waterloo, Ontario, Canada; Shoshin Distributed Systems Group, E&CE Department, University of Waterloo, Ontario, Canada","12th IFIP/IEEE International Symposium on Integrated Network Management (IM 2011) and Workshops","18 Aug 2011","2011","","","377","384","Enterprise software systems are large and complex with limited support for automated root-cause analysis. Avoiding system downtime and loss of revenue dictates a fast and efficient root-cause analysis process. Operator practice and academic research have shown that about 80% of failures in such systems have recurrent causes; therefore, significant efficiency gains can be achieved by automating their identification. In this paper, we present a novel approach to modelling features of log files. This model offers a compact representation of log data that can be efficiently extracted from large amounts of monitoring data. We also use decision-tree classifiers to learn and classify symptoms of recurrent faults. This representation enables automated fault matching and, in addition, enables human investigators to understand manifestations of failure easily. Our model does not require any access to application source code, a specification of log messages, or deep application knowledge. We evaluate our proposal using fault-injection experiments against other proposals in the field. First, we show that the features needed for symptom definition can be extracted more efficiently than does related work. Second, we show that these features enable an accurate classification of recurrent faults using only standard machine learning techniques. This enables us to identify accurately up to 78% of the faults in our evaluation data set.","1573-0077","978-1-4244-9221-3","10.1109/INM.2011.5990536","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5990536","","Servers;Manuals;Particle measurements;Atmospheric measurements;Monitoring","business data processing;data mining;data structures;decision trees;fault diagnosis;learning (artificial intelligence);pattern classification","unstructured log file mining;recurrent fault diagnosis;enterprise software systems;automated root-cause analysis;revenue loss;decision-tree classifiers;automated fault matching;fault-injection experiments;machine learning techniques;log data representation","","11","","34","","18 Aug 2011","","","IEEE","IEEE Conferences"
"Toward a Big Data Architecture for Security Events Analytic","L. Fetjah; K. Benzidane; H. E. Alloussi; O. E. Warrak; S. Jai-Andaloussi; A. Sekkaki","Comput. Sci. Dept., Univ. Hassan II, Casablanca, Morocco; Comput. Sci. Dept., Univ. Hassan II, Casablanca, Morocco; Comput. Sci. Dept., Univ. Hassan II, Casablanca, Morocco; Comput. Sci. Dept., Univ. Hassan II, Casablanca, Morocco; Comput. Sci. Dept., Univ. Hassan II, Casablanca, Morocco; Comput. Sci. Dept., Univ. Hassan II, Casablanca, Morocco","2016 IEEE 3rd International Conference on Cyber Security and Cloud Computing (CSCloud)","18 Aug 2016","2016","","","190","197","Cloud Computing did come up with so many attractive advantages such as scalability, flexibility, accessibility, rapid application deployment, and user self service. However in hindsight, Cloud Computing makes ensuring security within these environments so much challenging. Therefore traditional security mechanisms such as firewalls and antivirus softwares have proven insufficient and incapable of dealing with the sheer amount of data and events generated within a Cloud infrastructure. Herein, we present a highly scalable module based system that relies upon Big Data techniques and tools providing a comprehensive solution to process and analyze relevant events (packets flow, logs files) in order to generate an informative decisions that will be handled accordingly and swiftly.","","978-1-5090-0946-6","10.1109/CSCloud.2016.53","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7545918","Cloud Computing;Security Information and Event Management (SIEM);Security Intelligence;Big Data;Hadoop;Spark;ITIL;SKMS","Security;Big data;Cloud computing;Sparks;Organizations;Context;Correlation","Big Data;security of data","Big Data architecture;security events analytic;cloud Computing;security mechanisms;cloud infrastructure;Big Data techniques;Big Data tools;packets flow;logs files;security information and event management;SIEM","","5","","16","","18 Aug 2016","","","IEEE","IEEE Conferences"
"Using Supervised Learning to Guide the Selection of Software Inspectors in Industry","M. Singh; G. S. Walia; A. Goswami","Dept. of Comput. Sci., North Dakota State Univ., Fargo, ND, USA; Dept. of Comput. Sci., North Dakota State Univ., Fargo, ND, USA; Dept. of Comput. Sci., Bennett Univ., Greater Noida, India","2018 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)","18 Nov 2018","2018","","","12","17","Software development is a multi-phase process that starts with requirement engineering. Requirements elicited from different stakeholders are documented in natural language (NL) software requirement specification (SRS) document. Due to the inherent ambiguity of NL, SRS is prone to faults (e.g., ambiguity, incorrectness, inconsistency). To find and fix faults early (where they are cheapest to find), companies routinely employ inspections, where skilled inspectors are selected to review the SRS and log faults. While other researchers have attempted to understand the factors (experience and learning styles) that can guide the selection of effective inspectors but could not report improved results. This study analyzes the reading patterns (RPs) of inspectors recorded by eye-tracking equipment and evaluates their abilities to find various fault-types. The inspectors' characteristics are selected by employing ML algorithms to find the most common RPs w.r.t each fault-types. Our results show that our approach could guide the inspector selection with an accuracy ranging between 79.3% and 94% for various fault-types.","","978-1-5386-9443-5","10.1109/ISSREW.2018.00-38","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8539156","Fault types;classifiers;eye tracking;reading patterns;inspector selection;machine learning","Inspection;Software;Classification algorithms;Prediction algorithms;Machine learning algorithms;Measurement;Training","formal specification;formal verification;learning (artificial intelligence);software fault tolerance;software maintenance;software quality;systems analysis","fault-types;inspector selection;supervised learning;software inspectors;software development;multiphase process;requirement engineering;natural language software requirement specification document;NL;SRS","","","","17","","18 Nov 2018","","","IEEE","IEEE Conferences"
"Tablet-type GPS tracking radiation detection system and viewer software","Y. Matsumoto; M. Satoh","Department of Applied Physics and Physico-Informatics, Faculty of Science and Technology, Keio University, 3-14-1 Hiyoshi, Kohoku-ku, Yokohama, Kanagawa, Japan; Yaguchi denshi corp., 301 Kaemon, Ishinomaki, Miyagi 986-1111, Japan","SENSORS, 2014 IEEE","15 Dec 2014","2014","","","229","232","A radiation detection system has been developed utilizing one board-type radiation sensor, a microprocessor and android tablet software. Information regarding the environmental radiation dose is saved in a CSV log file in the tablet associated with GPS position data. Furthermore, viewer software has been designed for the radiation heat map and time-series graph. The GPS tracking function has been very important for visualizing the radiation data in the heat map since the nuclear power plant accident in Fukushima Japan. This visualization helps users to understand the environmental radiation level, and the log data is used to check hot spots in the contaminated area. In order to develop a long-term measuring system, a low-power custom CMOS detection circuit has been designed using a 0.6-μm three-metal process. The power consumption of the developed circuit is 2mW.","1930-0395","978-1-4799-0162-3","10.1109/ICSENS.2014.6984975","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6984975","","Global Positioning System;Software;Microprocessors;Universal Serial Bus;Radiation detectors;Pollution measurement;Image color analysis","Android (operating system);CMOS integrated circuits;computerised instrumentation;data visualisation;detector circuits;Global Positioning System;microprocessor chips;notebook computers;radiation detection;sensors;time series","tablet-type GPS tracking radiation detection system;viewer software;board-type radiation sensor;microprocessor;android tablet software;environmental radiation dose level;CSV log file;GPS position data;radiation heat map;time-series graph;radiation data visualization;nuclear power plant;Fukushima Japan;long-term measuring system;low-power custom CMOS detection circuit;three-metal process;size 0.6 mum;power 2 mW","","5","","5","","15 Dec 2014","","","IEEE","IEEE Conferences"
"Malicious URL sequence detection using event de-noising convolutional neural network","T. Shibahara; K. Yamanishi; Y. Takata; D. Chiba; M. Akiyama; T. Yagi; Y. Ohsita; M. Murata","NTT Secure Platform Laboratories, Tokyo, Japan; Osaka University, Osaka, Japan; NTT Secure Platform Laboratories, Tokyo, Japan; NTT Secure Platform Laboratories, Tokyo, Japan; NTT Secure Platform Laboratories, Tokyo, Japan; NTT Secure Platform Laboratories, Tokyo, Japan; Osaka University, Osaka, Japan; NTT Secure Platform Laboratories, Tokyo, Japan","2017 IEEE International Conference on Communications (ICC)","31 Jul 2017","2017","","","1","7","Attackers have increased the number of infected hosts by redirecting users of compromised popular websites toward websites that exploit vulnerabilities of a browser and its plugins. To prevent damage, detecting infected hosts based on proxy logs, which are generally recorded on enterprise networks, is gaining attention rather than blacklist-based filtering because creating blacklists has become difficult due to the short lifetime of malicious domains and concealment of exploit code. Since information extracted from one URL is limited, we focus on a sequence of URLs that includes artifacts of malicious redirections. We propose a system for detecting malicious URL sequences from proxy logs with a low false positive rate. To elucidate an effective approach of malicious URL sequence detection, we compared three approaches: individual-based approach, convolutional neural network (CNN), and our newly developed event de-noising CNN (EDCNN). Our EDCNN is a new CNN to reduce the negative effect of benign URLs redirected from compromised websites included in malicious URL sequences. Our evaluation shows that the EDCNN lowers the operation cost of malware infection by reducing 47% of false alerts compared with a CNN when users access compromised websites but do not obtain exploit code due to browser fingerprinting.","1938-1883","978-1-4673-8999-0","10.1109/ICC.2017.7996831","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7996831","","Uniform resource locators;Feature extraction;IP networks;Malware;Browsers;Security;Neural networks","business communication;invasive software;neural nets;online front-ends;Web sites","malicious URL sequence detection;event denoising convolutional neural network;websites;infected hosts detection;proxy logs;enterprise networks;blacklist-based filtering;malicious domains;individual-based approach;event de-noising CNN;malware;browser fingerprinting","","7","1","23","","31 Jul 2017","","","IEEE","IEEE Conferences"
"Spatiotemporal Real-Time Anomaly Detection for Supercomputing Systems","Q. Kang; A. Agrawal; A. Choudhary; A. Sim; K. Wu; R. Kettimuthu; P. H. Beckman; Z. Liu; W. Liao","Northwestern Univeristy,ECE,Evanston,IL,USA; Northwestern Univeristy,ECE,Evanston,IL,USA; Northwestern Univeristy,ECE,Evanston,IL,USA; Lawrence Berkeley National Laboratory,Berkeley,IL,USA; Lawrence Berkeley National Laboratory,Berkeley,IL,USA; Argonne National Laboratory,Lemont,,IL,USA; Argonne National Laboratory,Lemont,,IL,USA; Argonne National Laboratory,Lemont,,IL,USA; Northwestern Univeristy,ECE,Evanston,IL,USA","2019 IEEE International Conference on Big Data (Big Data)","24 Feb 2020","2019","","","4381","4389","The demands of increasingly large scientific application workflows lead to the need for more powerful supercomputers. As the scale of supercomputing systems have grown, the prediction of fault tolerance has become an increasingly critical area of study, since the prediction of system failures can improve performance by saving checkpoints in advance. We propose a real-time failure detection algorithm that adopts an event-based prediction model. The prediction model is a convolutional neural network that utilizes both traditional event attributes and additional spatio-temporal features. We present a case study using our proposed method with six years of reliability, availability, and serviceability event logs recorded by Mira, a Blue Gene/Q supercomputer at Argonne National Laboratory. In the case study, we have shown that our failure prediction model is not limited to predict the occurrence of failures in general. It is capable of accurately detecting specific types of critical failures such as coolant and power problems within reasonable lead time ranges. Our case study shows that the proposed method can achieve a F1 score of 0.56 for general failures, 0.97 for coolant failures, and 0.86 for power failures.","","978-1-7281-0858-2","10.1109/BigData47090.2019.9006046","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9006046","Blue Gene/Q;system anomaly detection;RAS","Predictive models;Real-time systems;Feature extraction;Supercomputers;Prediction algorithms;Anomaly detection;Clustering algorithms","checkpointing;convolutional neural nets;failure analysis;mainframes;parallel machines;scientific information systems;software fault tolerance;spatiotemporal phenomena;system recovery;workflow management software","spatiotemporal real-time anomaly detection;power failures;coolant failures;critical failures;failure prediction model;serviceability event logs;convolutional neural network;event-based prediction model;real-time failure detection algorithm;system failures;fault tolerance;supercomputing systems;powerful supercomputers","","","","24","","24 Feb 2020","","","IEEE","IEEE Conferences"
"Co-Locating Virtual Machine Logging and Replay for Recording System Failures","J. Kawasaki; S. Oikawa","Dept. of Comput. Sci., Univ. of Tsukuba, Tsukuba, Japan; Dept. of Comput. Sci., Univ. of Tsukuba, Tsukuba, Japan","2010 10th IEEE International Conference on Computer and Information Technology","16 Sep 2010","2010","","","1352","1357","There can be more system failures in the near future because of the combination of increased software complexity and a wide variety of usage patters. It is, however, difficult or sometimes almost impossible to find the root causes of the failures only from the limited and unreliable information provided by customers. Therefore, it is important to equip a feature that enables the complete tracing of system failures. We propose a system that employs two virtual machines, one for the primary execution and the other for the backup execution. The backup virtual machine maintains the past state of the primary virtual machine along with the log to make the backup the same state as the primary. When a system failure occurs on the primary virtual machine, the VMM saves the backup state and the log. By replaying the backup virtual machine from the saved state following the saved log, the execution path to the failure can be completely traced. We developed such a logging and replaying feature in a VMM. It can log and replay the execution of the Linux operating system. The experiments show that the overhead of the primary execution is only fractional, and the overhead of the replaying execution on the backup is less than 2%.","","978-1-4244-7548-3","10.1109/CIT.2010.242","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5577854","dependability;virtual machine monitor;operating system","Virtual machining;Linux;Operating systems;Timing;Electric breakdown;Radiation detectors","Linux;software metrics;system recovery;virtual machines","co-locating virtual machine logging;recording system failure;system failures;software complexity;backup execution;primary execution;backup virtual machine;Linux operating system","","","","10","","16 Sep 2010","","","IEEE","IEEE Conferences"
"A fast, lightweight, and reliable file system for wireless sensor networks","B. Mazumder; J. O. Hallstrom","School of Computing, Clemson University, United States; Institute for Sensing and Embedded Network Systems Engineering, Florida Atlantic University, United States","2016 International Conference on Embedded Software (EMSOFT)","17 Nov 2016","2016","","","1","10","Sensor nodes are increasingly used in critical applications. A file storage system that is fast, lightweight, and reliable across device failures is important to safeguard the data that these devices record. A fast and lightweight file system should enable sensed data to be sampled and stored quickly for later transmission, while imposing a small resource footprint. A reliable file system should provide storage integrity in the face of hardware, software, and other failures. We present the design and implementation of LoggerFS, a fast, lightweight, and reliable file system for wireless sensor networks which uses a hybrid memory design consisting of RAM, FRAM, and flash. LoggerFS is engineered to provide fast data storage, have a small memory footprint, and provide data reliability across system failures. Additionally, LoggerFS is designed to be power efficient. LoggerFS adapts a log-structured file system approach, augmented with data persistence and reliability guarantees. A caching mechanism allows for flash wear-leveling and fast data buffering. We present a performance evaluation of LoggerFS using a prototypical in situ sensing platform, and demonstrate between 50% and 800% improvements for various workloads using the FRAM write-back cache.","","978-1-4503-4485-2","10.1145/2968478.2968486","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7743241","","Random access memory;Nonvolatile memory;Reliability;Ferroelectric films;Computer crashes;EPROM;Metadata","flash memories;random-access storage;wireless sensor networks","wireless sensor networks;sensor nodes;file storage system;reliable file system;LoggerFS;hybrid memory design;caching mechanism;flash wear-leveling;data buffering;FRAM write-back cache","","2","1","32","","17 Nov 2016","","","IEEE","IEEE Conferences"
"Falcon: A Practical Log-Based Analysis Tool for Distributed Systems","F. Neves; N. Machado; J. Pereira","HASLab, Univ. of Minho, Braga, Portugal; HASLab, Univ. of Minho, Braga, Portugal; HASLab, Univ. of Minho, Braga, Portugal","2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)","23 Jul 2018","2018","","","534","541","Programmers and support engineers typically rely on log data to narrow down the root cause of unexpected behaviors in dependable distributed systems. Unfortunately, the inherently distributed nature and complexity of such distributed executions often leads to multiple independent logs, scattered across different physical machines, with thousands or millions entries poorly correlated in terms of event causality. This renders log-based debugging a tedious, time-consuming, and potentially inconclusive task. We present Falcon, a tool aimed at making log-based analysis of distributed systems practical and effective. Falcon's modular architecture, designed as an extensible pipeline, allows it to seamlessly combine several distinct logging sources and generate a coherent space-time diagram of distributed executions. To preserve event causality, even in the presence of logs collected from independent unsynchronized machines, Falcon introduces a novel happens-before symbolic formulation and relies on an off-the-shelf constraint solver to obtain a coherent event schedule. Our case study with the popular distributed coordination service Apache Zookeeper shows that Falcon eases the log-based analysis of complex distributed protocols and is helpful in bridging the gap between protocol design and implementation.","2158-3927","978-1-5386-5596-2","10.1109/DSN.2018.00061","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8416513","distributed systems;log-based analysis;execution flow","Tools;Debugging;Clocks;Protocols;Runtime;Pipelines;Schedules","constraint handling;distributed processing;message passing;program debugging;public domain software;system monitoring","coherent space-time diagram;coherent event schedule;complex distributed protocols;practical log-based analysis tool;distributed systems;Apache Zookeeper service;Falcon tool;logging sources","","2","","15","","23 Jul 2018","","","IEEE","IEEE Conferences"
"A Software Chain Approach to Big Data Stream Processing and Analytics","F. Xhafa; V. Naranjo; S. Caballé; L. Barolli","Univ. Politec. de Catalunya, Barcelona, Spain; Univ. Politec. de Catalunya, Barcelona, Spain; Open Univ. of Catalonia, Barcelona, Spain; Fukuoka Inst. of Technol., Fukuoka, Japan","2015 Ninth International Conference on Complex, Intelligent, and Software Intensive Systems","13 Aug 2015","2015","","","179","186","Big Data Stream processing is among the most important computing trends nowadays. The growing interest on Big Data Stream processing comes from the need of many Internet-based applications that generate huge data streams, whose processing can serve to extract useful analytics and inform for decision making systems. For instance, an IoT-based monitoring systems for a supply-chain, can provide real time data analytics for the business delivery performance. The challenges of processing Big Data Streams reside on coping with real-time processing of an unbounded stream of data, that is, the computing system should be able to compute at high throughput to accommodate the high data stream rate generation in input. Clearly, the higher the data stream rate, the higher should be the throughput to achieve consistency of the processing results (e.g. Preserving the order of events in the data stream). In this paper we show how to map the data stream processing phases (from data generation to final results) to a software chain architecture, which comprises five main components: sensor, extractor, parser, formatter and out putter. We exemplify the approach using the Yahoo!S4 for processing the Big Data Stream from Flight Radar24 global flight monitoring system.","","978-1-4799-8870-9","10.1109/CISIS.2015.24","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7185183","Big Data Stream;Real-time Analytics;Software Chain Architecture;Massive Data Processing;Generic Log Adapter;Yahoo!S4;Global Flight Monitoring System;Flight Radar24","Big data;Monitoring;Data mining;Software;Real-time systems;Computer architecture;Distributed databases","Big Data;data integrity;Internet of Things;software architecture;supply chain management","software chain approach;FlightRadar24 global flight monitoring system;Yahoo!S4;outputter component;formatter component;parser component;extractor component;sensor component;software chain architecture;data consistency;data stream rate generation;throughput;unbounded data stream;business delivery performance;real time data analytics;supply-chains;IoT-based monitoring systems;decision making systems;Big Data stream analytics;Big Data stream processing","","7","","13","","13 Aug 2015","","","IEEE","IEEE Conferences"
"A novel approach for efficient accessing of small files in HDFS: TLB-MapFile","B. Meng; W. Guo; G. Fan; N. Qian","East China University of Science and Technology, School of Information Science and Engineering, Shanghai, China; East China University of Science and Technology, School of Information Science and Engineering, Shanghai, China; East China University of Science and Technology, School of Information Science and Engineering, Shanghai, China; East China University of Science and Technology, School of Information Science and Engineering, Shanghai, China","2016 17th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)","21 Jul 2016","2016","","","681","686","Hadoop distributed file system (HDFS) was originally designed for streaming access large files, but the access and storage efficiency is low for the mass small files. This paper presents an access optimization approach for HDFS small file based on MapFile: TLB-MapFile. TLB-MapFile merges massive small files into large files by MapFile mechanism to reduce NameNode memory consumption and add fast table structure (TLB) in DataNode, and to improve retrieval efficiency of small files. First, according to MapFile mechanism, small files are merged into large files and stored in HDFS. Second, the access frequency and the ordered queue of small files (per unit time) can be obtained through accessing system audit logs in HDFS, and the mapping information between block and small files are stored in the TLB table with regularly being updated. TLB-MapFile improves access efficiency of small files through the prefetching of priori strategies based on TLB table. Experiment results show that this method can effectively reduce NameNode memory consumption and improve the reading speed of small files.","","978-1-5090-2239-7","10.1109/SNPD.2016.7515978","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7515978","HDFS;TLB;Rapid retrieval;the storage of small files;prefetching","Prefetching;Indexes;Metadata;Memory management;Time-frequency analysis;File systems;Optimization","data handling;data structures;optimisation;parallel processing","Hadoop distributed file system;HDFS;TLB-MapFile;access optimization approach;TLB table structure;NameNode memory consumption reduction","","3","","13","","21 Jul 2016","","","IEEE","IEEE Conferences"
"Alarm association rules mining based on run log for civil aviation information system","W. Jing; W. Huaichao; Z. Jiyang","Institute of Computer Science and Technology, Civil Aviation University of China, Tianjin, Dongli district, Jinbing Road, No 2898, China; Institute of Computer Science and Technology, Civil Aviation University of China, Tianjin, Dongli district, Jinbing Road, No 2898, China; Hubei Chutian General Aviation Co., Ltd, Hubei, Jingzhou, Jingsha Road, No 170, China","2017 8th IEEE International Conference on Software Engineering and Service Science (ICSESS)","23 Apr 2018","2017","","","836","841","The new generation of Civil Aviation Passenger Service Information System is a national key information system, its stable and reliable operation must be ensured. For the great challenge of safety monitoring caused by the complexity of its function and application, an alarm association rules mining scheme for complex information system is proposed in this paper, and the algorithm of alarm association rules mining based on double constrained sliding time window model is studied emphatically. Firstly, the characteristics of the alarm data in monitoring log of Travelsky Passenger Information and Service system is analyzed to structure these data. Then, the sliding time window model based on the double constraint is introduced to divide the alarm events in the alarm database, and a matrix Apriori algorithm is used to mine the alarm association rules by using the divided log alarm event data, so as to obtain the anomalous event alarm association between complex applications. Finally, the experimental results show that the method has more efficient and reasonable mining results.","2327-0594","978-1-5386-0497-7","10.1109/ICSESS.2017.8343041","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8343041","civil aviation;monitoring log;alarm association;rule mining;sliding time window","Data mining;Monitoring;Databases;Information systems;Mathematical model;Time factors;Data models","aerospace computing;data mining","alarm association rules;civil aviation information system;national key information system;complex information system;double constrained sliding time window model;alarm events;alarm database;divided log alarm event data;anomalous event alarm association;civil aviation passenger service information system","","1","","16","","23 Apr 2018","","","IEEE","IEEE Conferences"
"Locus: Locating bugs from software changes","M. Wen; R. Wu; S. Cheung","Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China","2016 31st IEEE/ACM International Conference on Automated Software Engineering (ASE)","6 Oct 2016","2016","","","262","273","Various information retrieval (IR) based techniques have been proposed recently to locate bugs automatically at the file level. However, their usefulness is often compromised by the coarse granularity of files and the lack of contextual information. To address this, we propose to locate bugs using software changes, which offer finer granularity than files and provide important contextual clues for bug-fixing. We observe that bug inducing changes can facilitate the bug fixing process. For example, it helps triage the bug fixing task to the developers who committed the bug inducing changes or enables developers to fix bugs by reverting these changes. Our study further identifies that change logs and the naturally small granularity of changes can help boost the performance of IR-based bug localization. Motivated by these observations, we propose an IR-based approach Locus to locate bugs from software changes, and evaluate it on six large open source projects. The results show that Locus outperforms existing techniques at the source file level localization significantly. MAP and MRR in particular have been improved, on average, by 20.1% and 20.5%, respectively. Locus is also capable of locating the inducing changes within top 5 for 41.0% of the bugs. The results show that Locus can significantly reduce the number of lines needing to be scanned to locate the bug compared with existing techniques.","","978-1-4503-3845-5","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7582764","bug localization;software changes;information retrieval;software analytics","Computer bugs;Software;Debugging;History;Information retrieval;Natural languages;Manuals","information retrieval;program debugging","Locus;software changes;information retrieval;bug-fixing;bug inducing changes;IR-based bug localization;source file level localization;MAP;MRR","","","","46","","6 Oct 2016","","","IEEE","IEEE Conferences"
"A review of process mining algorithms","Dianmin Yue; Xiaodan Wu; Haiyan Wang; Junbo Bai","School of Management, Heibei University of Technology, Tianjin, China; School of Management, Heibei University of Technology, Tianjin, China; School of Management, Heibei University of Technology, Tianjin, China; RenAi College of Tianjin University, China","2011 International Conference on Business Management and Electronic Information","23 Jun 2011","2011","5","","181","185","Process mining has been widely applied in lots of fields, which can discover workflow models from event logs, helps to design or re-design process models, and brings convenience to workflow management system. The main function of process mining algorithms is to provide us with valuable objective information hidden in event logs, which plays a crucial important role on the implementation of new operation business process, and audit, analysis and improvement of the existing ones. In this paper, we firstly describe the application background of process mining technology, and then list the frame structure of the technology at home and abroad, followed by summarizing and analyzing the algorithm of important domestic and abroad research, and forecast the future direction for research at last.","","978-1-61284-109-0","10.1109/ICBMEI.2011.5914454","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5914454","Petri net;process mining algorithm;workflow net;event log","Data mining;Noise;Algorithm design and analysis;Computational modeling;Clustering algorithms;Business;Computers","data mining;workflow management software","workflow model;event log;workflow management system;objective information;business process;process mining algorithms","","","","41","","23 Jun 2011","","","IEEE","IEEE Conferences"
"The Reading-Life Log -- Technologies to Recognize Texts That We Read","T. Kimura; R. Huang; S. Uchida; M. Iwamura; S. Omachi; K. Kise","Kyushu Univ., Fukuoka, Japan; Kyushu Univ., Fukuoka, Japan; Kyushu Univ., Fukuoka, Japan; Osaka Prefecture Univ., Sakai, Japan; Tohoku Univ., Sendai, Japan; Osaka Prefecture Univ., Sakai, Japan","2013 12th International Conference on Document Analysis and Recognition","15 Oct 2013","2013","","","91","95","Reading life log is a type of techniques to automatically and unconsciously record people's reading intentions, interests and habits. Besides, it can also serve as various assistants in our daily life. In this paper, a reading-life log system is implemented by a head-mounted and unobtrusive video camera with a high resolution and a high shutter speed. We utilize DP matching, and propose a text-based frame mosaicing method to integrate multiple frames in a clip. The developed system is tested in the various environments indoor and outdoor. The experimental results show that our system can provide reliable outputs with respect to the most correct responses. The infrequent misregistration between lines also indicates the feasibility and validity of the text-based frame mosaicing.","2379-2140","978-0-7695-4999-6","10.1109/ICDAR.2013.26","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6628591","","Cameras;Text recognition;Tracking;Optical character recognition software;Image recognition;Conferences;Robustness","dynamic programming;image matching;image resolution;image segmentation;text analysis;video cameras","outdoor environment;indoor environment;multiple frame integration;text-based frame mosaicing method;DP matching;high-shutter speed camera;high-resolution camera;head-mounted unobtrusive video camera;automatic-unconscious people reading habit recording;automatic-unconscious people reading interest recording;reading-life log system;automatic-unconscious people reading intention recording;text recognition","","7","","19","","15 Oct 2013","","","IEEE","IEEE Conferences"
"The Use of Automatic Test Data Generation for Genetic Improvement in a Live System","S. O. Haraldsson; J. R. Woodward; A. I. E. Brownlee","Dept. of Comput. Sci. & Math., Univ. of Stirling, Stirling, UK; Dept. of Comput. Sci. & Math., Univ. of Stirling, Stirling, UK; Dept. of Comput. Sci. & Math., Univ. of Stirling, Stirling, UK","2017 IEEE/ACM 10th International Workshop on Search-Based Software Testing (SBST)","7 Jul 2017","2017","","","28","31","In this paper we present a bespoke live system in commercial use that has been implemented with self-improving properties. During business hours it provides overview and control for many specialists to simultaneously schedule and observe the rehabilitation process for multiple clients. However in the evening, after the last user logs out, it starts a self-analysis based on the day's recorded interactions and the self-improving process. It uses Search Based Software Testing (SBST) techniques to generate test data for Genetic Improvement (GI) to fix any bugs if exceptions have been recorded. The system has already been under testing for 4 months and demonstrates the effectiveness of simple test data generation and the power of GI for improving live code.","","978-1-5386-2789-1","10.1109/SBST.2017.10","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7967957","Search Based Software Engineering;Test data generation;Bug fixing;Real world application","Computer bugs;Testing;Software;Genetics;Graphical user interfaces;Software engineering;Tools","automatic test software;program testing;search problems","automatic test data generation;genetic improvement;bespoke live system;self-improving properties;multiple clients rehabilitation process;self-analysis;search-based software testing;SBST techniques;bug fixing;live code improvement","","2","","19","","7 Jul 2017","","","IEEE","IEEE Conferences"
"Experience Report: Log-Based Behavioral Differencing","M. Goldstein; D. Raz; I. Segall",NA; NA; NA,"2017 IEEE 28th International Symposium on Software Reliability Engineering (ISSRE)","16 Nov 2017","2017","","","282","293","Monitoring systems and ensuring the required service level is an important operation task. However, doing this based on external visible data, such as systems logs, is very difficult since it is very hard to extract from the logged data the exact state and the root cause to the actions taken by the system. Yet, identifying behavioral changes of complex systems can be used for early identification of problems and allow proactive correction measurements. Since it is practically impossible to perform this task manually, there is a critical need for a methodology that can analyze logs, automatically create a behavioral model, and compare the behavior to the expected behavior.In this paper we propose a novel approach for comparison between serviceexecutions as exhibited in their log files. The behavior is captured by FiniteState Automaton models (FSAs), enhanced with performance related data, bothmined from the logs. Our tool then computes the difference between the current model and behavioral models created when the service was known to operate well. A visual framework that graphically presents and emphasizes the changes in the behavior is then used to trace their root cause. We evaluate our approach over real telecommunication logs.","2332-6549","978-1-5386-0941-5","10.1109/ISSRE.2017.14","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8109094","behavioral differencing;anomaly detection;root cause analysis","Computational modeling;Tools;Data mining;Automata;Analytical models;Performance evaluation","finite state machines;program testing;system monitoring","monitoring systems;complex systems;proactive correction measurements;behavioral model;log files;telecommunication logs;system logs;logged data;FSA;finite state automaton models;log-based behavioral differencing","","5","","29","","16 Nov 2017","","","IEEE","IEEE Conferences"
"A Novel Model and a Simulation Tool for Churn of P2P Network","Q. Luo; Y. Li; W. Dong; G. Liu; R. Mao","Nat. High Performance Comput. Center, Shenzhen Univ., Shenzhen, China; Coll. of Inf. Eng., Shenzhen Univ., Shenzhen, China; Coll. of Comput. Sci. & Software Eng., Shenzhen Univ., Shenzhen, China; Coll. of Comput. Sci. & Software Eng., Shenzhen Univ., Shenzhen, China; Coll. of Comput. Sci. & Software Eng., Shenzhen Univ., Shenzhen, China","2010 International Conference on Parallel and Distributed Computing, Applications and Technologies","28 Jan 2011","2010","","","267","272","The prior studies setup the churn model by measuring the historical logs or records of a P2P network, and treat it as one whole black-box without understanding the inside of peer's population. The metrics used to characterize the churn is distributions of the node session lengths and arrival intervals. We investigate churn in a higher level point of view, and find that modeling it based on the global geographical distribution of peer nodes will result in a system which explain the fluctuation and cyclic phenomenon of network size. This model considers the user behavior pattern into account. Then we provide a Matlab tools that can provide churn events according to this model. From the output events of simulation, we do see some more future things than other models. We might expect or predict when and what nodes would return back, as well as when and what nodes would disappear at high possibility. So it is useful when designing a system optimized both to the pass and the future, which could reduce the overhead of the maintenance of underlying overlay network of DHTs and lower the redundant level of replications for P2P storage system.","2379-5352","978-1-4244-9110-0","10.1109/PDCAT.2010.12","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5704429","Churn;Model;Global Churn Pattern;Peer-to-peer;DHT","Peer to peer computing;Mathematical model;Computers;Equations;Fluctuations;Computational modeling;Measurement","peer-to-peer computing;storage management;table lookup","simulation tool;P2P network;churn model;historical logs;historical records;black-box;peer population;node session lengths;arrival intervals;global geographical distribution;peer nodes;user behavior pattern;Matlab tools;overlay network;DHT;redundant level;P2P storage system","","1","","6","","28 Jan 2011","","","IEEE","IEEE Conferences"
"Detecting Ransomware with Honeypot Techniques","C. Moore","Comput. & Media Services, Univ. of St. Mark & St. John, Plymouth, UK","2016 Cybersecurity and Cyberforensics Conference (CCC)","20 Oct 2016","2016","","","77","81","Attacks of Ransomware are increasing, this form of malware bypasses many technical solutions by leveraging social engineering methods. This means established methods of perimeter defence need to be supplemented with additional systems. Honeypots are bogus computer resources deployed by network administrators to act as decoy computers and detect any illicit access. This study investigated whether a honeypot folder could be created and monitored for changes. The investigations determined a suitable method to detect changes to this area. This research investigated methods to implement a honeypot to detect ransomware activity, and selected two options, the File Screening service of the Microsoft File Server Resource Manager feature and EventSentry to manipulate the Windows Security logs. The research developed a staged response to attacks to the system along with thresholds when there were triggered. The research ascertained that witness tripwire files offer limited value as there is no way to influence the malware to access the area containing the monitored files.","","978-1-5090-2657-9","10.1109/CCC.2016.14","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7600214","honeypot;ransomware;malware;computer security;cyber security;network;detect;activity","Malware;Monitoring;Electronic mail;Servers;Cryptography;Computers","file servers;invasive software","ransomware detection;honeypot techniques;ransomware attacks;malware form;social engineering methods;bogus computer resources;network administrators;decoy computers;illicit access detection;ransomware activity detection;file screening service;Microsoft file server resource manager feature;EventSentry;Windows security logs;witness tripwire files","","45","1","22","","20 Oct 2016","","","IEEE","IEEE Conferences"
"An approach towards backbone network congestion minimization in software defined network","R. Paudyal; S. Shakya","Backbone Transmission Directorate, Nepal Telecom, Kathmandu Nepal; Department of Electronics and Computer Engineering, Pulchowk Engineering Campus, Kathmandu, Nepal","2017 International Conference on Computing, Communication and Automation (ICCCA)","21 Dec 2017","2017","","","412","416","With Emergence of broadband access networks and powerful personal computer systems the demand for network-delivered full-motion streaming video is growing. People upload their home made videos. Different companies have started to offer software as service. Data traffic is increasing each day at an unprecedented rate creating congestion on the backbone part of the network. Backbone network congestion minimization approach is proposed to minimize the backbone network traffic by locally distributing the video files. Mininet emulator is used for simulation purpose. Custom network topology and Ryuretic framework for controller are used. Ryu controller pushes the rule when first time packet arrives to establish flow table on the switches. Server maintains the logs of video file downloaded by host. Based on its logs on database, server redirects the request towards host, P2P session is established and video file is uploaded. Uploaded data traffic flows through the access network part result on decreasing the network latency. File download rate is increased by 4 times and congestion is minimized by 66% in proposed congestion reduction approach than traditional client server approach in Software Defined Network based used topology. Validation of video file download from the server and from P2P host is verified by unique identifier MD5 checksum value.","","978-1-5090-6471-7","10.1109/CCAA.2017.8229855","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8229855","SDN;Ryu;OpenFlow;MD5;Backbone network;latency;P2P","Servers;Streaming media;Computer architecture;Control systems;Automation;Broadband communication;Network topology","broadband networks;client-server systems;peer-to-peer computing;software defined networking;telecommunication congestion control;telecommunication network topology;telecommunication traffic","backbone network congestion minimization approach;backbone network traffic;custom network topology;uploaded data traffic;access network part;file download rate;software defined network;broadband access networks;network latency;personal computer systems;network-delivered full-motion streaming video;home made videos;video file distribution;congestion reduction approach;client server approach","","1","","12","","21 Dec 2017","","","IEEE","IEEE Conferences"
"Intelligent System for Tracking and Logging the Zigzag Pantograph Motion","R. Rob; C. Panoiu; S. R. Anghel","Department of Electrical Engineering and Industrial Informatics, Politehnica University of Timisoara, Hunedoara, Romania; Department of Electrical Engineering and Industrial Informatics, Politehnica University of Timisoara, Hunedoara, Romania; Department of Electrical Engineering and Industrial Informatics, Politehnica University of Timisoara, Hunedoara, Romania","2018 Innovations in Intelligent Systems and Applications (INISTA)","20 Sep 2018","2018","","","1","6","In this paper a system which is able to track and log data obtained by measuring the zigzag motion of the pantograph used in railway transportation is presented. The horizontally movement of the pantograph (zigzag) is sensed in real time using a monitoring camera. The system is composed by a NI MyRIO controller equipped with FPGA and Real Time technologies which communicates with a laptop as the host computer. A LabVIEW application monitors the zigzag motion of pantograph, displays it in real time and logs the information into document files in order the information to be post processed. As well, the application can save the camera images into .avi file.","","978-1-5386-5150-6","10.1109/INISTA.2018.8466302","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8466302","intelligent system;pantograph motion;real time;image processing.","Real-time systems;Monitoring;Cameras;Wires;Software;Current measurement;Tracking","document handling;field programmable gate arrays;image motion analysis;optical tracking;pantographs;railways;real-time systems;virtual instrumentation","intelligent system;railway transportation;monitoring camera;NI MyRIO controller;FPGA;laptop;host computer;LabVIEW application;document files;camera images;avi file;zigzag pantograph motion tracking;zigzag pantograph motion logging;real time technologies","","1","","10","","20 Sep 2018","","","IEEE","IEEE Conferences"
"RDBMS Based Hadoop Metadata and Log Data Management Optimization","H. Che; O. Iradukunda; K. Shahin","Computer Science and Technology, Beijing Institute of Technology,Beijing,China; Computer Science and Technology, Beijing Institute of Technology,Beijing,China; Computer Science and Technology, Beijing Institute of Technology,Beijing,China","2019 IEEE International Conference on Signal, Information and Data Processing (ICSIDP)","21 Aug 2020","2019","","","1","7","At the moment, metadata is one of the fastest growing sub-segments of enterprise data management. While metadata is growing, it is not able to keep pace with the rapid increase of Big Data projects being currently initiated by organizations. Nowadays, it refers to this as the “Big Data Gap”. This paper introduces novel approach by bringing Apache Hadoop and Relational database together to minimize the query time, resource usage, and increase the fault tolerance, and efficiency. Hadoop's metadata and log files are synchronously being migrated to the PGSQL and easily controlled through the graphical user interface. The experiment part has used 100.000's of movie rates dataset and decreased the resource usage of NameNode by giving the task of log and metadata analysis to the PGSQL. The query time in PGSQL is 1.5 times faster than Hadoop and the data format is in structured format comparing to Hadoop. Although, the technique implemented on a single node, it outperformed existing hadoop on premise and on cloud. The technique makes the metadata and log data management easier through the GUI that uses charts and graphs. The results suggest that the proposed approach performs better than existing solution and sharply decreases the usage of Big Data hardware systems and budget as well.","","978-1-7281-2345-5","10.1109/ICSIDP47821.2019.9173057","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9173057","big data;Relational Databases;resource optimization;metadata;log data;log management;query optimization;hadoop;PostGresql","","Big Data;data handling;data mining;graphical user interfaces;meta data;parallel processing;public domain software;query processing;relational databases;SQL","RDBMS based hadoop metadata;data management optimization;fastest growing sub-segments;enterprise data management;Big Data projects;Big Data Gap;Apache Hadoop;query time;resource usage;log files;PGSQL;data format","","","","25","","21 Aug 2020","","","IEEE","IEEE Conferences"
"Time Performance Evaluation of Agile Software Development","A. H. M. Shani; R. Sarno; K. R. Sungkono; C. S. Wahyuni","Informatics Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Informatics Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Informatics Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Informatics Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia","2019 International Seminar on Application for Technology of Information and Communication (iSemantic)","28 Oct 2019","2019","","","202","207","Agile is a method in software development that demand the development teams to work fast and adapt to changes that happens along the way. However, in the fact, there are activities in software development process that take longer time to complete than others, which is called bottleneck. Bottlenecks are difficult to find if there is no actual event log of the process. To provide the event log this research focus on creating an agile process model which can be executed and generate the event log. Besides the agile process model, this research also discover bottleneck not only from the whole processes but also from processes that are executed frequently. This research creates the agile process model using a workflow management systems. Then the bottleneck from the whole process and processes that are executed frequently are obtained by using Alpha ++ Algorithm and Heuristic Miner Algorithm respectively. The occurrence of activities over time is also analyzed in this research by using dotted chart analysis. This experiment proves that this research can create event log automatically and detect bottlenecks in whole process and processes that are executed frequently.","","978-1-7281-3832-9","10.1109/ISEMANTIC.2019.8884304","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8884304","process mining;agile;bottleneck;alpha ++;heuristic miner algorithm","Heuristic algorithms;Petri nets;Software;Data mining;PROM;Business;Performance analysis","data mining;software prototyping;workflow management software","time performance evaluation;agile software development;agile process model;workflow management systems;Alpha ++ algorithm;heuristic miner algorithm;dotted chart analysis","","","","18","","28 Oct 2019","","","IEEE","IEEE Conferences"
"Pathidea: Improving Information Retrieval-Based Bug Localization by Re-Constructing Execution Paths Using Logs","A. R. Chen; T. -H. P. Chen; S. Wang","Gina Cody School of Engineering and Computer Science, Concordia University, 5618 Montreal, Quebec, Canada, (e-mail: archen94@gmail.com); Computer Science and Software Engineering, Concordia University, 5618 Montreal, Quebec, Canada, H3G 2W1 (e-mail: peterc@encs.concordia.ca); Computer Science, University of Manitoba, 8664 Winnipeg, Manitoba, Canada, R3T 2N2 (e-mail: shaoweiwang.2010@hotmail.com)","IEEE Transactions on Software Engineering","","2021","PP","99","1","1","To assist developers with debugging and analyzing bug reports, researchers have proposed information retrieval-based bug localization (IRBL) approaches. IRBL approaches leverage the textual information in bug reports as queries to generate a ranked list of potential buggy files that may need further investigation. Although IRBL approaches have shown promising results, most prior research only leverages the textual information that is visible in bug reports, such as bug description or title. However, in addition to the textual description of the bug, developers also often attach logs in bug reports. Logs provide important information that can be used to re-construct the system execution paths when an issue happens and assist developers with debugging. In this paper, we propose an IRBL approach, Pathidea, which leverages logs in bug reports to re-construct execution paths and helps improve the results of bug localization. Pathidea uses static analysis to create a file-level call graph, and re-constructs the call paths from the reported logs. We evaluate Pathidea on eight open source systems, with a total of 1,273 bug reports that contain logs. We find that Pathidea achieves a high recall (up to 51.9% for Top@5). On average, Pathidea achieves an improvement that varies from 8% to 21% and 5% to 21% over BRTracer in terms of Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) across studied systems, respectively. Moreover, we find that the re-constructed execution paths can also complement other IRBL approaches by providing a 10% and 8% improvement in terms of MAP and MRR, respectively. Finally, we conduct a parameter sensitivity analysis and provide recommendations on setting the parameter values when applying Pathidea.","1939-3520","","10.1109/TSE.2021.3071473","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9397337","bug localization;log;bug report;information retrieval","Computer bugs;Location awareness;Debugging;Static analysis;Information retrieval;History;Tools","","","","","","","IEEE","6 Apr 2021","","","IEEE","IEEE Early Access Articles"
"A Log Based Approach to Make Digital Forensics Easier on Cloud Computing","T. Sang","Shanghai Jiao Tong Univ., Shanghai, China","2013 Third International Conference on Intelligent System Design and Engineering Applications","7 Feb 2013","2013","","","91","94","Cloud computing is getting more and more attention from the information and communication technologies industry recently. Almost all the leading companies of the information area show their interesting and efforts on cloud computing and release services about cloud computing in succession. But if want to make it go further, we should pay more effort on security issues. Especially, the Internet environment now has become more and more unsecure. With the popularization of computers and intelligent devices, the number of crime on them has increased rapidly in last decades, and will be quicker on the cloud computing environment in future. No wall is wall in the world. We should enhance the cloud computing not only at the aspect of precaution, but also at the aspect of dealing with the security events to defend it from crime activities. In this paper, I propose a approach which using logs model to building a forensic-friendly system. Using this model we can quickly gather information from cloud computing for some kinds of forensic purpose. And this will decrease the complexity of those kinds of forensics.","","978-1-4673-4893-5","10.1109/ISDEA.2012.29","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6454779","cloud computing;digital forensic;log;security","Cloud computing;Software as a service;Digital forensics;Security;Computational modeling;Companies","cloud computing;digital forensics","log based approach;digital forensics;cloud computing;information and communication technology;security issue;Internet environment;forensic purpose","","26","","7","","7 Feb 2013","","","IEEE","IEEE Conferences"
"Database Structures for Accountable Flow-Net Logging","D. Takahashi; Y. Xiao; T. Li","Department of Computer Science, The University of Alabama, Tuscaloosa, AL, 35487-0290, USA; Navigation College, Dalian Maritime University 1 Linghai Road, Dalian, Gaoxinyuan District, China; Navigation College, Dalian Maritime University 1 Linghai Road, Dalian, Gaoxinyuan District, China","2018 10th International Conference on Communication Software and Networks (ICCSN)","11 Oct 2018","2018","","","254","258","Computer and network accountability is to make every action in computers and networks accountable. In order to achieve accountability, we need to answer the following questions: what did it happen? When did it happen? Who did it? In order to achieve accountability, the first step is to record what exactly happened. Therefore, an accountable logging is needed and implemented in computers and networks. Our previous work proposed a novel accountable logging methodology called Flow-Net. However, how to storage the huge amount of Flow-net logs into databases is not clear. In this paper, we try to answer this question.","2472-8489","978-1-5386-7223-5","10.1109/ICCSN.2018.8488249","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8488249","accountability;security;computer and networks","Relational databases;Containers;Computer security;Encapsulation;Structured Query Language","computer network management;database management systems","database structures;accountable Flow-Net logging;network accountability;Flow-net logs;databases;accountable logging methodology","","","","16","","11 Oct 2018","","","IEEE","IEEE Conferences"
"Implementing Dockerized Elastic Stack for Security Information and Event Management","F. Mulyadi; L. A. Annam; R. Promya; C. Charnsripinyo","Communication and Network Research Group, NECTEC, NSTDA,Pathum Thani,Thailand; Communication and Network Research Group, NECTEC, NSTDA,Pathum Thani,Thailand; Communication and Network Research Group, NECTEC, NSTDA,Pathum Thani,Thailand; Communication and Network Research Group, NECTEC, NSTDA,Pathum Thani,Thailand","2020 - 5th International Conference on Information Technology (InCIT)","11 Jan 2021","2020","","","243","248","In security information and event management (SIEM), real-time monitoring together with analysis of log data for correlation of events can provide anomaly detection and notification. A centralized log system with big-data databases is required for SIEM in order to store and manage the log data. In this paper, we present dockerized Elastic Stack for security information and event management. The main reasons are because of lightweight, simplicity and supporting features. One of important Elasticsearch features is security information. Elasticsearch can act as Intrusion Detection System (IDS) together with log/event management as SIEM. Our experiments show that dockerized elastic stack can be efficiently used for security information and event management.","","978-1-7281-6694-0","10.1109/InCIT50588.2020.9310950","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9310950","big data;data log system;elasticsearch;event management;security information;siem","Security;Servers;Tools;Containers;Monitoring;Open source software;Bridges","data analysis;security of data","dockerized elastic stack;security information;SIEM;log data;centralized log system;big-data databases","","","","10","","11 Jan 2021","","","IEEE","IEEE Conferences"
"Learning analytics to improve coding abilities: a fuzzy-based process mining approach","P. Ardimento; M. L. Bernardi; M. Cimitile; G. D. Ruvo","University of Bari Aldo Moro, Bari, Italy; Giustino Fortunato University, Benevento, Italy; Unitelma Sapienza, Rome, Italy; Independent IEEE Member, Rome, Italy","2019 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)","11 Oct 2019","2019","","","1","7","Comprehension of how students and developers head the development of software and what specific hurdles they face, have a strong potential to better support the coding workflow. In this paper, we present the CodingMiner environment to generate event logs from IDE usage enabling the adoption of fuzzy-based process mining techniques to model and to study the developers' coding process. The logs from the development sessions have been analyzed using the fuzzy miner to highlight emergent and interesting developers' and students' behaviors during coding. The mined processes show different IDE usage patterns for students with different skills and performances. To validate our approach, we describe the results of a study in which the CodingMiner environment is used to investigate the coding activities of twenty students of a CS2 course performing a given programming task during four assignments. Results also demonstrate that fuzzy-based process mining techniques can be effectively exploited to understand students and developers behavior during programming tasks providing useful insights to improve the way they code.","1558-4739","978-1-5386-1728-1","10.1109/FUZZ-IEEE.2019.8859009","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8859009","fuzzy process mining;development workflows;source code;IDE logging","Data mining;Encoding;Unified modeling language;Software systems;Task analysis;Tools","computer science education;data mining;software maintenance","coding workflow;CodingMiner environment;event logs;fuzzy miner;emergent developers;IDE usage patterns;coding activities;fuzzy-based process mining techniques;developers behavior;coding abilities;fuzzy-based process mining approach","","","","24","","11 Oct 2019","","","IEEE","IEEE Conferences"
"Ad hoc Test Generation Through Binary Rewriting","A. Saieva; S. Singh; G. Kaiser","Columbia University,Department of Computer Science,New York,NY,USA; Columbia University,Department of Computer Science,New York,NY,USA; Columbia University,Department of Computer Science,New York,NY,USA","2020 IEEE 20th International Working Conference on Source Code Analysis and Manipulation (SCAM)","11 Nov 2020","2020","","","115","126","When a security vulnerability or other critical bug is not detected by the developers' test suite, and is discovered post-deployment, developers must quickly devise a new test that reproduces the buggy behavior. Then the developers need to test whether their candidate patch indeed fixes the bug, without breaking other functionality, while racing to deploy before attackers pounce on exposed user installations. This can be challenging when factors in a specific user environment triggered the bug. If enabled, however, record-replay technology faithfully replays the execution in the developer environment as if the program were executing in that user environment under the same conditions as the bug manifested. This includes intermediate program states dependent on system calls, memory layout, etc. as well as any externally-visible behavior. Many modern record-replay tools integrate interactive debuggers, to help locate the root cause, but don't help the developers test whether their patch indeed eliminates the bug under those same conditions. In particular, modern record-replay tools that reproduce intermediate program state cannot replay recordings made with one version of a program using a different version of the program where the differences affect program state. This work builds on record-replay and binary rewriting to automatically generate and run targeted tests for candidate patches significantly faster and more efficiently than traditional test suite generation techniques like symbolic execution. These tests reflect the arbitrary (ad hoc) user and system circumstances that uncovered the bug, enabling developers to check whether a patch indeed fixes that bug. The tests essentially replay recordings made with one version of a program using a different version of the program, even when the the differences impact program state, by manipulating both the binary executable and the recorded log to result in an execution consistent with what would have happened had the the patched version executed in the user environment under the same conditions where the bug manifested with the original version. Our approach also enables users to make new recordings of their own workloads with the original version of the program, and automatically generate and run the corresponding ad hoc tests on the patched version, to validate that the patch does not break functionality they rely on.","2470-6892","978-1-7281-9248-2","10.1109/SCAM51674.2020.00018","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9252025","test generation;software patching;record-replay;binary rewriting;security vulnerabilities","Computer bugs;Tools;Test pattern generators;Security;Open source software;Monitoring;Testing","program debugging;program diagnostics;program testing;program verification;software maintenance","ad hoc test generation;binary rewriting;critical bug;candidate patch;exposed user installations;user environment;record-replay technology;developer environment;intermediate program state;modern record-replay tools;targeted tests;traditional test suite generation techniques;system circumstances;binary executable;recorded log;patched version;ad hoc tests;impact program state;record-replay tools","","","","66","","11 Nov 2020","","","IEEE","IEEE Conferences"
"Code Coverage and Postrelease Defects: A Large-Scale Study on Open Source Projects","P. S. Kochhar; D. Lo; J. Lawall; N. Nagappan","School of Information Systems, Singapore Management University, Singapore; School of Information Systems, Singapore Management University, Singapore; INRIA, Paris, France; Microsft Research, Redmond, WA, USA","IEEE Transactions on Reliability","29 Nov 2017","2017","66","4","1213","1228","Testing is a pivotal activity in ensuring the quality of software. Code coverage is a common metric used as a yardstick to measure the efficacy and adequacy of testing. However, does higher coverage actually lead to a decline in postrelease bugs? Do files that have higher test coverage actually have fewer bug reports? The direct relationship between code coverage and actual bug reports has not yet been analyzed via a comprehensive empirical study on real bugs. Past studies only involve a few software systems or artificially injected bugs (mutants). In this empirical study, we examine these questions in the context of open-source software projects based on their actual reported bugs. We analyze 100 large open-source Java projects and measure the code coverage of the test cases that come along with these projects. We collect real bugs logged in the issue tracking system after the release of the software and analyze the correlations between code coverage and these bugs. We also collect other metrics such as cyclomatic complexity and lines of code, which are used to normalize the number of bugs and coverage to correlate with other metrics as well as use these metrics in regression analysis. Our results show that coverage has an insignificant correlation with the number of bugs that are found after the release of the software at the project level, and no such correlation at the file level.","1558-1721","","10.1109/TR.2017.2727062","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8031982","Code coverage;empirical study;open-source;postrelease defects;software testing","Computer bugs;Software testing;Sonar measurements;Open source software","Java;program debugging;program testing;public domain software;regression analysis;software metrics;software quality","regression analysis;software quality;software testing;software metrics;open-source Java projects;bug reports;test coverage;open-source software projects;postrelease bugs;open source projects;code coverage","","2","","41","Traditional","12 Sep 2017","","","IEEE","IEEE Journals"
"CRUDE: Combining Resource Usage Data and Error Logs for Accurate Error Detection in Large-Scale Distributed Systems","N. Gurumdimma; A. Jhumka; M. Liakata; E. Chuah; J. Browne","Dept. of Comput. Sci., Univ. of Warwick, Coventry, UK; Dept. of Comput. Sci., Univ. of Warwick, Coventry, UK; Dept. of Comput. Sci., Univ. of Warwick, Coventry, UK; Dept. of Comput. Sci., Univ. of Warwick, Coventry, UK; Dept. of Comput. Sci., Univ. of Warwick, Coventry, UK","2016 IEEE 35th Symposium on Reliable Distributed Systems (SRDS)","22 Dec 2016","2016","","","51","60","The use of console logs for error detection in large scale distributed systems has proven to be useful to system administrators. However, such logs are typically redundant and incomplete, making accurate detection very difficult. In an attempt to increase this accuracy, we complement these incomplete console logs with resource usage data, which captures the resource utilisation of every job in the system. We then develop a novel error detection methodology, the CRUDE approach, that makes use of both the resource usage data and console logs. We thus make the following specific technical contributions: we develop (i) a clustering algorithm to group nodes with similar behaviour, (ii) an anomaly detection algorithm to identify jobs with anomalous resource usage, (iii) an algorithm that links jobs with anomalous resource usage with erroneous nodes. We then evaluate our approach using console logs and resource usage data from the Ranger Supercomputer. Our results are positive: (i) our approach detects errors with a true positive rate of about 80%, and (ii) when compared with the well-known Nodeinfo error detection algorithm, our algorithm provides an average improvement of around 85% over Nodeinfo, with a best-case improvement of 250%.","1060-9857","978-1-5090-3513-7","10.1109/SRDS.2016.017","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7794329","anomaly detection;resource usage data;faults;detection;large-scale HPC systems;unsupervised;event logs","Radiation detectors;Clustering algorithms;Entropy;Supercomputers;Mutual information;Detection algorithms;Resource management","distributed processing;software fault tolerance","CRUDE approach;resource usage data;error logs;error detection;large-scale distributed systems;error detection methodology;console logs;anomaly detection algorithm;Ranger Supercomputer;true positive rate;Nodeinfo error detection algorithm","","9","","23","","22 Dec 2016","","","IEEE","IEEE Conferences"
"Cloud Log Forensics Metadata Analysis","S. Thorpe; I. Ray; T. Grandison; A. Barbir","Fac. of Eng. & Comput., Univ. of Technol., Kingston, Jamaica; Dept. of Comput. Sci., Colorado State Univ., Fort Collins, CO, USA; IBM Res., Yorktown Heights, NY, USA; NA","2012 IEEE 36th Annual Computer Software and Applications Conference Workshops","10 Nov 2012","2012","","","194","199","The increase in the quantity and questionable quality of the forensic information retrieved from the current virtualized data cloud system architectures has made it extremely difficult for law enforcement to resolve criminal activities within these logical domains. This paper poses the question of what kind of information is desired from virtual machine (VM) hosted operating systems (OS) investigated by a cloud forensic examiner. The authors gives an overview of the information that exists on current VM OS by looking at it's kernel hypervisor logs and discusses the shortcomings. An examination of the role that the VM kernel hypervisor logs provide as OS metadata in cloud investigations is also presented.","","978-0-7695-4758-9","10.1109/COMPSACW.2012.44","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6341574","Hypervisor;Cloud;Metadata;Logs;Forensics","Virtual machine monitors;Cloud computing;File systems;Digital forensics;Kernel","cloud computing;computer forensics;information retrieval;law;meta data;operating systems (computers);virtual machines;virtualisation","cloud log forensics metadata analysis;forensic information retrieval;current virtualized data cloud system architectures;law enforcement;criminal activities;logical domains;virtual machine hosted operating systems;VM hosted OS;cloud forensic examiner;VM kernel hypervisor logs;OS metadata;cloud investigations","","11","","24","","10 Nov 2012","","","IEEE","IEEE Conferences"
"High performance database logging using storage class memory","R. Fang; H. Hsiao; B. He; C. Mohan; Y. Wang","IBM Almaden Research Center, 650 Harry Road, San Jose, CA, USA; IBM Almaden Research Center, 650 Harry Road, San Jose, CA, USA; IBM Almaden Research Center, 650 Harry Road, San Jose, CA, USA; IBM Almaden Research Center, 650 Harry Road, San Jose, CA, USA; IBM China Research Laboratory, Building 19, Zhongguancun Software Park, 8 Dongbeiwang West Road, Haidian District, Beijing, China, 100193","2011 IEEE 27th International Conference on Data Engineering","16 May 2011","2011","","","1221","1231","Storage class memory (SCM), a new generation of memory technology, offers non-volatility, high-speed, and byte-addressability, which combines the best properties of current hard disk drives (HDD) and main memory. With these extraordinary features, current systems and software stacks need to be redesigned to get significantly improved performance by eliminating disk input/output (I/O) barriers; and simpler system designs by avoiding complicated data format transformations. In current DBMSs, logging and recovery are the most important components to enforce the atomicity and durability of a database. Traditionally, database systems rely on disks for logging transaction actions and log records are forced to disks when a transaction commits. Because of the slow disk I/O speed, logging becomes one of the major bottlenecks for a DBMS. Exploiting SCM as a persistent memory for transaction logging can significantly reduce logging overhead. In this paper, we present the detailed design of an SCM-based approach for DBMSs logging, which achieves high performance by simplified system design and better concurrency support. We also discuss solutions to tackle several major issues arising during system recovery, including hole detection, partial write detection, and any-point failure recovery. This new logging approach is used to replace the traditional disk based logging approach in DBMSs. To analyze the performance characteristics of our SCM-based logging approach, we implement the prototype on IBM SolidDB. In common circumstances, our experimental results show that the new SCM-based logging approach provides as much as 7 times throughput improvement over disk-based logging in the Telecommunication Application Transaction Processing (TATP) benchmark.","2375-026X","978-1-4244-8960-2","10.1109/ICDE.2011.5767918","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5767918","","Phase change materials;Random access memory;Databases;Computer crashes;Hardware;Writing;Data structures","database management systems;disc drives;hard discs;transaction processing","high performance database logging;storage class memory;hard disk drives;main memory;DBMS logging;logging transaction actions;log records;SCM-based approach;hole detection;partial write detection;any-point failure recovery;IBM SolidDB;telecommunication application transaction processing benchmark","","53","31","20","","16 May 2011","","","IEEE","IEEE Conferences"
"Represent The Discovered Proportional Information Control Net Using Extended XPDL Format","D. -L. Pham; H. Ahn; K. -S. Kim; K. P. Kim","Data and Process Engineering Research Lab., KYONGGI UNIVERSITY,Division of Computer Science and Engineering,Gyeonggi-Do,Republic of Korea,16627; Data and Process Engineering Research Lab., KYONGGI UNIVERSITY,Division of Computer Science and Engineering,Gyeonggi-Do,Republic of Korea,16627; Data and Process Engineering Research Lab., KYONGGI UNIVERSITY,Division of Computer Science and Engineering,Gyeonggi-Do,Republic of Korea,16627; Data and Process Engineering Research Lab., KYONGGI UNIVERSITY,Division of Computer Science and Engineering,Gyeonggi-Do,Republic of Korea,16627","2020 22nd International Conference on Advanced Communication Technology (ICACT)","9 Apr 2020","2020","","","640","643","The XML Process Definition Language (XPDL) is a widely used XML format for storing and exchanging business process information from various workflow systems. In this paper, we introduce the methodology to represent the run-time proportional Information Control Net (pICN) discovered from the business process event log by using XPDL format. More precisely, we propose an approach and algorithm to export pICN data from the graphical visualization model to XPDL format (version 2.1) with several extended attributes to store run-time proportional information of activities in the system. By storing the discovered pICN in an XPDL format, we do not have to rediscover it again in the next mining time since using the information that has been recorded. With the popular and widespread use of the XPDL format, we can use the discovered pICN model in various business process management and workflow systems.","1738-9445","979-11-88428-04-5","10.23919/ICACT48636.2020.9061482","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9061482","Process mining;XPDL format;pICN;Workflow system;business process","Unified modeling language;Business;Process control;Petri nets;Logic gates;Standards;Tools","business data processing;data mining;workflow management software;XML","graphical visualization;pICN model;proportional information control net;XPDL format;XML process definition language;business process management;business process event log;workflow systems;business process information","","","","14","","9 Apr 2020","","","IEEE","IEEE Conferences"
"Dynamic Analysis for the Reconstruction of System Behavior Models","V. Koutsoumpas; K. Kontogiannis; P. Matthews","Nat. Tech. Univ. of Athens, Athens, Greece; Nat. Tech. Univ. of Athens, Athens, Greece; CA Labs., Datchet, UK","2010 IEEE 34th Annual Computer Software and Applications Conference Workshops","1 Nov 2010","2010","","","273","280","The analysis of large software systems is often a difficult and time consuming task because of the sheer complexity of such systems, and the limited access software engineers have to the system's source code. In this respect, it is important to be able to analyze the system based on the events that can be collected while the system performs its specified operations. Such analysis, can take the form of reconstructing system behavioral models, and identifying common operational patterns that may assist software engineers deduce important information on the properties and characteristics of the system being analyzed. In this paper, we present an analysis technique that allows for the hierarchical reconstruction of sequence diagrams and the identification of common event patterns from system traces. The technique has been applied for the analysis of different operations in the Session Initiation Protocol.","","978-0-7695-4105-1","10.1109/COMPSACW.2010.55","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5615806","Software Engineering;Software Analysis;Log Analysis;Software Monitoring","","signalling protocols;software maintenance","dynamic analysis;system behavior model reconstruction;system source code;sequence diagrams reconstruction;session initiation protocol;software system evolution;software system maintenance","","","","26","","1 Nov 2010","","","IEEE","IEEE Conferences"
"Software versioning quality parameters: Automated assessment tools based on the parameters","R. Elsen; I. Liem; S. Akbar","Knowledge and Sofware Engineering Research Group, School of Electrical Engineering and Informatics, Institut Teknologi Bandung (ITB), Lab Tek V, Lantai-2, Jl. Ganesha 10, 40132, Indonesia; Knowledge and Sofware Engineering Research Group, School of Electrical Engineering and Informatics, Institut Teknologi Bandung (ITB), Lab Tek V, Lantai-2, Jl. Ganesha 10, 40132, Indonesia; Knowledge and Sofware Engineering Research Group, School of Electrical Engineering and Informatics, Institut Teknologi Bandung (ITB), Lab Tek V, Lantai-2, Jl. Ganesha 10, 40132, Indonesia","2016 International Conference on Data and Software Engineering (ICoDSE)","1 Jun 2017","2016","","","1","6","Version Control System (VCS) is used by software developers during a software development project with the purpose for maintaining versions and collaborative work. It is a part of configuration management. Developers work together using the same environment. Though all developers must follow the same best practices, each developer may work individually with different practices due to different knowledge and experience of using VCS. Many best practices are available, but they are different. In this paper, we define quality parameters based on quality factors. The quality factors and associated metrics are extracted from the log file of VCS. These parameters bear out good software versioning activities. To prove the concept, we build a tool for automatically assessing the quality of versioning activities of a developer, based on the log file of a VCS. The tool has been tested and used for assessing the quality of software versioning of two open source projects. As the result, developers versioning activities can be analysed.","","978-1-5090-5671-2","10.1109/ICODSE.2016.7936139","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7936139","versioning quality parameter;versioning activity","Monitoring;Lead","configuration management;groupware;project management;public domain software;software quality","software versioning quality parameters;automated assessment tools;version control system;VCS;software development project;collaborative work;configuration management;quality factors;software versioning activities;open source projects;versioning activities","","","","31","","1 Jun 2017","","","IEEE","IEEE Conferences"
"Electrical network signal's waveform and frequency logging for forensic","V. A. Niţă; R. A. Dobre; A. Drumea; A. Ciobanu; C. Negrescu; D. Stanomir","University POLITEHNICA of Bucharest, Faculty of Electronics, Telecommunications and Information Technology, Blvd. Iuliu Maniu 1-3, Bucharest, Romania; University POLITEHNICA of Bucharest, Faculty of Electronics, Telecommunications and Information Technology, Blvd. Iuliu Maniu 1-3, Bucharest, Romania; University POLITEHNICA of Bucharest, Faculty of Electronics, Telecommunications and Information Technology, Blvd. Iuliu Maniu 1-3, Bucharest, Romania; University POLITEHNICA of Bucharest, Faculty of Electronics, Telecommunications and Information Technology, Blvd. Iuliu Maniu 1-3, Bucharest, Romania; University POLITEHNICA of Bucharest, Faculty of Electronics, Telecommunications and Information Technology, Blvd. Iuliu Maniu 1-3, Bucharest, Romania; University POLITEHNICA of Bucharest, Faculty of Electronics, Telecommunications and Information Technology, Blvd. Iuliu Maniu 1-3, Bucharest, Romania","2015 9th International Symposium on Advanced Topics in Electrical Engineering (ATEE)","25 Jun 2015","2015","","","156","161","In this paper we develop a framework for collecting the ENF (electric network frequency) variations in a database, which can be used in digital media authentication and time stamping. The proposed framework allows the recording of the ENF variations and of the hum signal. The discontinuities introduced in the hum signal by the probe used for data acquisition are studied and methods for minimizing these discontinuities are proposed. In the end a method for time stamping audio recordings is used, based on the created ENF database.","2068-7966","978-1-4799-7514-3","10.1109/ATEE.2015.7133756","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7133756","ENF;database;analog frequency measurement","Databases;Audio recording;Frequency measurement;Software;Band-pass filters;Discrete Fourier transforms;Power supplies","audio recording;audio signal processing;digital forensics;signal detection","electrical network signal waveform;frequency logging;forensics;electric network frequency variation recording;digital media authentication;ENF variation recording;hum signal;probes;data acquisition;discontinuity minimization;time stamping audio recordings;ENF database","","","","6","","25 Jun 2015","","","IEEE","IEEE Conferences"
"Towards a performance estimate in semi-structured processes","A. Wombacher; M. Iacob; M. Haitsma","University of Twente, Enschede, The Netherlands; University of Twente, Enschede, The Netherlands; University of Twente, Enschede, The Netherlands","2011 IEEE International Conference on Service-Oriented Computing and Applications (SOCA)","9 Mar 2012","2011","","","1","5","Semi-structured processes are business workflows, where the execution of the workflow is not completely controlled by a workflow engine, i.e., an implementation of a formal workflow model. Examples are workflows where actors potentially have interaction with customers reporting the result of the interaction in a process aware information system. Building a performance model for resource management in these processes is difficult since the information required for a performance model is only partially recorded. In this paper we propose a systematic approach for the creation of an event log that is suitable for available process mining tools. This event log is created by an incremental cleansing of data. The proposed approach is evaluated in a case study where the quality of the derived event log i assessed by domain experts.","2163-2871","978-1-4673-0319-4","10.1109/SOCA.2011.6166256","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6166256","","Data models;Histograms;Business;Data visualization;Information systems;Computational modeling;Software","business data processing;data mining;formal specification;software performance evaluation","performance estimate;semistructured processes;business workflows;formal workflow model;customers interaction;resource management;event log;process mining tools;incremental data cleansing","","6","","9","","9 Mar 2012","","","IEEE","IEEE Conferences"
"Error Monitoring for Legacy Mission-Critical Systems","M. Cinque; R. Della Corte; S. Russo","Dipt. di Ing. Elettr. e delle Tecnol. dell'Inf., Univ. degli Studi di Napoli Federico II, Naples, Italy; Dipt. di Ing. Elettr. e delle Tecnol. dell'Inf., Univ. degli Studi di Napoli Federico II, Naples, Italy; Dipt. di Ing. Elettr. e delle Tecnol. dell'Inf., Univ. degli Studi di Napoli Federico II, Naples, Italy","2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshop (DSN-W)","3 Oct 2016","2016","","","66","71","Error data collected at runtime play a key role for dependability analysis and improvement of software systems. The use of monitoring frameworks for legacy mission-critical systems is hindered by limited intervention degree and low intrusiveness requirements. We present the design and experimentation of an error monitoring service for a legacy large-scale critical system in the Air Traffic Control (ATC) domain. We describe the details of the API realized to collect both direct data (event logs, execution traces) and indirect data (system resources' utilization). We present experiments with the ATC industrial case study, showing the efficacy of combining different data sources for error detection and propagation analysis, with an acceptable overhead at high monitoring rates for such a class of systems.","","978-1-5090-3688-2","10.1109/DSN-W.2016.41","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7575352","Error monitoring;event logging;critical systems","Monitoring;Probes;Kernel;Data collection;Mission critical systems;Random access memory;Open source software","software maintenance;software reliability","error monitoring;legacy mission critical systems;error data;dependability analysis;software systems;monitoring frameworks;legacy mission-critical systems;error monitoring service;legacy large-scale critical system;air traffic control;direct data;event logs;execution traces;ATC industrial case study","","","","18","","3 Oct 2016","","","IEEE","IEEE Conferences"
"A QoS-aware Web Service Selection Method Based on Credibility Evaluation","L. Qi; R. Yang; W. Lin; X. Zhang; W. Dou; J. Chen","State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; Fac. of Inf. & Commun. Technol., Swinburne Univ. of Technol., Melbourne, VIC, Australia","2010 IEEE 12th International Conference on High Performance Computing and Communications (HPCC)","27 Sep 2010","2010","","","471","476","There exist so many web services that share same or similar functional properties, so it is often a challenging effort to select a credible and optimal web service based on their various history QoS records. In view of this challenge, in this paper, a novel QoS-aware web service selection method is put forward, based on credibility evaluation associated with negotiated QoS dimensions. More specifically, the historical empirical data, i.e., execution logs of a service, are used for evaluation purpose. At last, a case study is employed for illustration purpose, and the evaluations are presented to demonstrate the feasibility of our method.","","978-1-4244-8335-8","10.1109/HPCC.2010.20","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5581452","web service;QoS;service selection;credibility evaluation;execution log","Quality of service;Silicon;Service oriented architecture;Data mining;History;Computational modeling","quality of service;software architecture;software performance evaluation;Web services","QoS aware Web service selection method;credibility evaluation;execution log","","4","","11","","27 Sep 2010","","","IEEE","IEEE Conferences"
"Boundary Protection System Based on Software-Defined Networking","L. Cao; X. Zhu; S. Xu; L. Zhang","Science and Technology on Communication Networks Laboratory, The 54th Research Institute of China Electronics Technology Group Corporation, Shijiazhuang, China; Science and Technology on Communication Networks Laboratory, The 54th Research Institute of China Electronics Technology Group Corporation, Shijiazhuang, China; Science and Technology on Communication Networks Laboratory, The 54th Research Institute of China Electronics Technology Group Corporation, Shijiazhuang, China; Science and Technology on Communication Networks Laboratory, The 54th Research Institute of China Electronics Technology Group Corporation, Shijiazhuang, China","2018 IEEE 18th International Conference on Communication Technology (ICCT)","3 Jan 2019","2018","","","1291","1296","The framework of separated data and control planes in software-defined networking (SDN) with high programmability makes it more flexible to manage and control network traffic. In this paper we propose a boundary protection system based on software defined networking, which is composed of an intrusion detection module (IDM) and a boundary protection module (BPM). The IDM is embedded into the SDN switches on the data plane to realize the monitoring and warning of abnormal events. The BPM is deployed upon the Open Network Operating System (ONOS) controller to install flow rules on the SDN switches on the data plane with the assistance of ONOS controller. It accesses ONOS controller via a Restful API. Once an abnormal event is detected, the IDM reports it to the BPM, the BPM can make intelligent decisions to prevent abnormal packets in the whole network, this promote the defensive capacity of the boundary protection system. Also the BPM realizes functions such as protocol filtering, blacklist controlling, ACL controlling and warning logging.","2576-7828","978-1-5386-7635-6","10.1109/ICCT.2018.8599933","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8599933","boundary protection system;SDN;IDM;BPM;ONOS;ACL controlling;warning logging","Firewalls (computing);Control systems;Protocols;Blacklisting;IP networks;Filtering;Performance evaluation","application program interfaces;computer network security;operating systems (computers);software defined networking;telecommunication switching;telecommunication traffic","boundary protection system;control planes;SDN;software defined networking;intrusion detection module;IDM;boundary protection module;BPM;data plane;ONOS controller;control network traffic;open network operating system controller;blacklist control;SDN switches;Restful API;defensive capacity;intelligent decision making","","","","12","","3 Jan 2019","","","IEEE","IEEE Conferences"
"An approach for log analysis based failure monitoring in Hadoop cluster","M. Mohandas; P. M. Dhanya","Department of Computer Science & Engineering, Rajagiri School of Engineering & Technology, Kochi, India; Department of Computer Science & Engineering, Rajagiri School of Engineering & Technology, Kochi, India","2013 International Conference on Green Computing, Communication and Conservation of Energy (ICGCE)","2 Jun 2014","2013","","","861","867","Massive and gargantuan amount of data is produced on per day basis. Such scenario elevates the need for apposite storage, supervision and processing of data. The massive use of Distributed framework calls for faster analysis and diagnosis of failures. Due to the distributed nature of processing, it is difficult for cluster administrator to isolate the failures and failed nodes. Many contributions have been done for failure monitoring, analysis etc in the last few years. Apache Hadoop's Jobtracker, Namenode, Secondary Namenode, Datanode and Tasktracker all generate logs. This paper aims at building a failure monitoring system from the scratch, by parsing and analyzing the Hadoop log files generated in the cluster. The monitoring system gives all relevant details related to the application, and points out the specific reason for failure, that is, whether an application failure or a network failure (these are the most common failures in the cluster).","","978-1-4673-6126-2","10.1109/ICGCE.2013.6823555","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6823555","Hadoop;HDFS;Failure Monitoring;Log Analysis;BigData","Monitoring;File systems;Google;Computer architecture;Computational modeling;History","fault diagnosis;Java;program diagnostics;public domain software","log analysis based failure monitoring;Hadoop cluster;distributed framework;failure analysis;failure diagnosis;Apache Hadoop Jobtracker;Namenode;Secondary Namenode;Datanode;Tasktracker;failure monitoring system;open source Java software framework","","3","","23","","2 Jun 2014","","","IEEE","IEEE Conferences"
"Evaluating Programming Performance with Keystroke Characteristics: An Empirical Experiment","H. Liu; D. Liu; S. Xu","Changsha Univ., Changsha, China; GradientX, Santa Monica, CA, USA; Changsha Univ., Changsha, China","2013 14th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing","16 Sep 2013","2013","","","329","335","This study continued to investigate programming habits based on keystrokes, software quality, and code format. In previous work, we studied programmer's performance when they were working without pressure. In this work, we conducted an experiment by asking nineteen junior undergraduate students to complete a programming task under pressure. We also used a software tool to record the keystroke frequency and designed criteria to evaluation program quality. The experiment results demonstrate that while novice programmers are diverse in terms of programming styles, good ones tend to control execution in finer granularity and produce more solid code such as exception handling. Source code format seems to be a useful indicator for programming performance. It seems that there is still no direct correlation between the frequency of keystrokes and the quality of programs.","","978-0-7695-5005-3","10.1109/SNPD.2013.49","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6598485","Programming experiment;Programmer Performance;Cognitive Activity;Keystroke Logging","Productivity;Programming profession;Correlation;Software engineering;Solids;Sorting","computer science education;exception handling;further education;programming;software performance evaluation;software quality;software tools","programming performance evaluation;keystroke characteristics;programming habits;software quality;code format;junior undergraduate students;programming task;software tool;keystroke frequency;evaluation program quality;exception handling;source code format;software development","","","","20","","16 Sep 2013","","","IEEE","IEEE Conferences"
"Analyzing Long-Running Controller Applications for Specification Violations Based on Deterministic Replay","R. Schatz; H. Prähofer","Christian Doppler Lab. for Automated Software Eng., Johannes Kepler Univ., Linz, Austria; Christian Doppler Lab. for Automated Software Eng., Johannes Kepler Univ., Linz, Austria","2012 38th Euromicro Conference on Software Engineering and Advanced Applications","11 Oct 2012","2012","","","55","62","Deterministic replay debugging is a technique aimed at finding and debugging software failures occurring in field operation that are usually hard to reproduce. With deterministic replay debugging a software run is recorded, so that it can be reproduced deterministically in a debugger. While deterministic replay debugging is capable of reproducing a failure, in practice, especially in the case of a long-running application, it is still hard for the developer to locate the exact position of the failure in the trace log. Based on our previous work on deterministic replay debugging, we propose an approach to use behavior specifications in the form of test cases to search a recorded trace log. We first present a formal approach for the specification of test cases for PLC applications based on hybrid automata. Then we present a method for searching a recorded program trace for occurrences of the test scenario. That way, we can not only identify regions where a specification violation occurred, but also the corresponding regions in the trace where the test case passed, which can then be further used for comparison. We present the theoretical background of our approach, an implementation of the trace search algorithm, and an example application.","2376-9505","978-0-7695-4790-9","10.1109/SEAA.2012.10","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6328128","software tracing;software visualization;deterministic replay;debugging;PLC applications;testing;hybrid automata","Automata;Algorithm design and analysis;Debugging;Actuators;Semantics;Clocks;Software","automata theory;formal specification;program debugging;programmable controllers;system recovery","long running controller application analysis;specification violations;deterministic replay debugging;software failure debugging;PLC applications;hybrid automata;trace search algorithm","","2","","20","","11 Oct 2012","","","IEEE","IEEE Conferences"
"A novel framework for password securing system from key-logger spyware","G. Tyagi; K. Ahmad; M. N. Doja","CSE/IT Department, Swami Vivekanand Subharti University, India; CSE/IT Department, Swami Vivekanand Subharti University, India; Computer Engineering Department, Jamia Millia Islamia, India","2014 International Conference on Issues and Challenges in Intelligent Computing Techniques (ICICT)","3 Apr 2014","2014","","","70","74","Malwares are very precarious problem for Internet users. They are malicious software or programs, programmed by attackers to interrupt computer operation, gather delicate information, or gain access to client computer system. Key-logger is one of the massive threat among various malwares. It can be a hardware key-logger or software key-logger which records the keystrokes and store them in a log file and email that file to the attacker, if attacker wants to do so. It is baleful for system user or end users who use online banking rarely or ordinarily. In this paper, we propose a technique for prevention of password from a key-logger spywares and give assurance of security of password from key-logger spywares.","","978-1-4799-2900-9","10.1109/ICICICT.2014.6781255","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6781255","","Spyware;Web servers;Grippers;Unsolicited electronic mail;Monitoring","invasive software","password security system;key-logger spyware;malwares;Internet users;malicious software;malicious programs;hardware key-logger;software key-logger;password prevention","","3","1","16","","3 Apr 2014","","","IEEE","IEEE Conferences"
"A Systematic Approach for Privilege Escalation Prevention","F. Jaafar; G. Nicolescu; C. Richard","Polytech. Montreal, Montreal, QC, Canada; R&D Team, Ubitrak Inc., Montreal, QC, Canada; Polytech. Montreal, Montreal, QC, Canada","2016 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)","22 Sep 2016","2016","","","101","108","Information systems are designed to present services and functionalities for multiple users. Thus, it is used to have on one information system different levels of privilege for different users. Privileges describe what a user is permitted to do such as viewing files, modifying or deleting data. Privilege escalation takes place when a user gets access to more resources or services than they are normally allowed to perform unauthorized actions. Many studies have been presented to detect anomalies and vulnerabilities in information systems to discover security issues or attacks related to privilege escalation. In this context, we introduced a systematic methodology that uses pattern recognition and outliers identification to detect numerous abnormal events which can indicate anomalies and security issues related to privilege escalation. In this paper, we describe results from an empirical study in order to show the performance of a new outliers detection algorithm to identify unknown abnormal events and the ability of pattern recognition techniques to specify known abnormal events. Results show that the outlier detection algorithm, enables us to discover unknown abnormal events on synthetic data sets. In addition, results notice the existence and the usefulness of four patterns used the discover known privilege escalation scenarios and, potentially, reduce security issues in systems.","","978-1-5090-3713-1","10.1109/QRS-C.2016.17","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7573730","Privilege Escalation;Log Messages;Pattern Recognition;Outlier Detection;Abnormal Events;Misuse Detection;Anomaly Detection","Security;Pattern recognition;Detection algorithms;Information systems;Systematics;Context;Correlation","security of data","privilege escalation prevention;information systems;unauthorized actions;security issues;pattern recognition techniques;abnormal event detection;outlier detection algorithm;synthetic data sets","","1","","29","","22 Sep 2016","","","IEEE","IEEE Conferences"
"DPDA: A Moving Target Defense System Based on Detection and Prediction Mechanism","T. Jinglei; Z. Hongqi","Zhengzhou Information Science and Technology Institute, Zhengzhou, China; Zhengzhou Information Science and Technology Institute, Zhengzhou, China","2018 IEEE 4th International Conference on Computer and Communications (ICCC)","1 Aug 2019","2018","","","1371","1376","As a new technology to change the rules of cyberspace games, moving target defense provides a new idea for reversing the asymmetry between the attacker and defender. This paper proposes a moving target defense system based on detection and prediction mechanism--DPDA system (detection-prediction-defense-assessment system), which utilizes the centralized control and programming flexibility of the SDN network. DPDA system is modeled as a discrete event dynamic system (DEDS), and performs detection of existing attacks based on real-time events. On the other hand, the attack probability transformation is used to adjust the transformation period based on the log-normal distribution function for unknown attacks. Finally, the simulation test environment is built. The results show that DPDA can implement more efficient active defense than the system with fixed transformation period.","","978-1-5386-8339-2","10.1109/CompComm.2018.8781064","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8781064","moving target defense;software defined network;discrete event dynamic system;log-normal distribution function","Security;Real-time systems;Timing;Switches;Cyberspace;Games","computer network security;discrete event systems;normal distribution;probability","moving target defense system;detection-prediction-defense-assessment system;discrete event dynamic system;attack probability transformation;efficient active defense;DPDA system;SDN network;real-time events;log-normal distribution function;fixed transformation period","","","","12","","1 Aug 2019","","","IEEE","IEEE Conferences"
"SIEM implementation for global and distributed environments","I. Anastasov; D. Davcev","University “Ss. Cyril and Methodius”, the Faculty of Computer Science and Engineering (FCSE), Skopje, Macedonia; University “Ss. Cyril and Methodius”, the Faculty of Computer Science and Engineering (FCSE), Skopje, Macedonia","2014 World Congress on Computer Applications and Information Systems (WCCAIS)","7 Oct 2014","2014","","","1","6","Today's computer networks produce a huge amount of security log data. Handling this data is impossible without using Security Information and Event Management Systems (SIEM) to centralize the log management and increase the level of information security and data protection in the organization. SIEM collect and aggregate log data from various devices and applications through software called agents or connectors, filter uninteresting data and normalize to a proprietary format, analyses through correlation using contextual information and alert administrators in case of attack. SIEM provide proactive threat detection and real-time analysis of system activity. Handling these issues will be very hard without relying on consolidated, big data-powered SIEM. However, even having the most expensive SIEM solution, the organization should not expect the product to work great out of the box. The best SIEM solution does not guarantee success. The organization should focus on building various use cases to make their SIEM solution a success. In this paper, we propose a new model and architecture for SIEM implementation that is using multiple hierarchical SIEM Managers. The model is called “Hierarchical Managers Model”. We demonstrated how this model and architecture could be created and enabled in the leading SIEM system - ArcSight ESM [7]. We also provide examples of possible use cases that we have created and tested in our testing environment. These are meant to provide a good base starting point and should not be considered comprehensive for all situations. The use cases shown in this paper are created using the security event correlation framework from Hewlett-Packard - ArcSight ESM [7].","","978-1-4799-3351-8","10.1109/WCCAIS.2014.6916651","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6916651","information security;computer security;event management;SIEM;log management;ArcSight ESM [7];rules;use cases;Internet of Things","Organizations;Security;Servers;Monitoring;Connectors;Databases;Electronic mail","security of data;software agents","SIEM;global environment;distributed environment;computer network;security log data;security information;event management system;log management;data protection;proactive threat detection;hierarchical managers model;ArcSight ESM;security event correlation","","9","","9","","7 Oct 2014","","","IEEE","IEEE Conferences"
"Honeypots deployment for the analysis and visualization of malware activity and malicious connections","I. Koniaris; G. Papadimitriou; P. Nicopolitidis; M. Obaidat","Department of Informatics, Aristotle University of Thessaloniki, 54124 Thessaloniki, Greece; Department of Informatics, Aristotle University of Thessaloniki, 54124 Thessaloniki, Greece; Department of Informatics, Aristotle University of Thessaloniki, 54124 Thessaloniki, Greece; Department of Computer Science and Software Engineering, Monmouth University, W. Long Branch, NJ 07764, USA","2014 IEEE International Conference on Communications (ICC)","28 Aug 2014","2014","","","1819","1824","Honeypots are systems aimed at deceiving threat agents. In most of the cases the latter are cyber attackers with financial motivations, and malicious software with the ability to launch automated attacks. Honeypots are usually deployed as either production systems or as research units to study the methods employed by attackers. In this paper we present the results of two distinct research honeypots. The first acted as a malware collector, a device usually deployed in order to capture self-propagating malware and monitor their activity. The second acted as a decoy server, dropping but logging every malicious connection attempt. Both of these systems have remained online for a lengthy period of time to study the aforementioned malicious activity. During this assessment it was shown that human attackers and malicious software are constantly attacking servers, trying to break into systems or spread across networks. It was also shown that the usage of honeypots for malware monitoring and attack logging can be very effective and provide valuable data. Lastly, we present an open source visualization tool which was developed to help security professionals and researchers during the analysis and conclusion drawing phases, for use with one of the systems fielded in our study.","1938-1883","978-1-4799-2003-7","10.1109/ICC.2014.6883587","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6883587","honeypot;intrusion detection;malware;data visualization;log file analysis","Malware;Data visualization;Software;IP networks;Ports (Computers);Servers;Grippers","data visualisation;invasive software;public domain software","honeypots deployment;malware activity;malicious connections;threat agents;cyber attackers;financial motivations;malicious software;open source visualization tool","","6","","30","","28 Aug 2014","","","IEEE","IEEE Conferences"
"Reading Time Prediction Model on Chinese Technical Documentation","Z. Gao; F. Li; J. Yu","Peking University, University of Twente; Peking University; Peking University","2020 IEEE International Professional Communication Conference (ProComm)","21 Sep 2020","2020","","","161","167","This paper was presented at the Invited Panel session “Technical Communication in China”. There has been various research on the reading time and legibility of online texts with people's tendency to online materials. Text-related attributes like font size or letterspacing are commonly used variables in this field. The objective of this study is to investigate the influential factors on the reading time of Chinese technical documentation, and to build a Decision Tree model to predict its reading time. In the experiment, log data including information of over a million user visits from a cloud service provider's website are collected. User's visit time, stay time, visit step, visit device and many other data fields are recorded in a user session. In addition to user behavioral data from log files, data metrics concerning technical documentation itself are also collected. For all documents used in the experiment, their word counts, image counts, link counts and section counts are scraped using web crawlers. The linear correlation analysis is applied in order to explore the correlations between variables for predictions. The results show that a 75 percent accuracy is achieved using the Decision Tree model.","2158-1002","978-1-7281-5563-0","10.1109/ProComm48883.2020.00046","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9201244","Decision tree;online documentation;readability;technical communication","Data models;Documentation;Predictive models;Decision trees;Training;Machine learning;Correlation","cloud computing;correlation methods;decision trees;human factors;natural language processing;software agents;text analysis;Web sites","font size;Chinese technical documentation;decision tree model;user behavioral data;online texts reading;reading time prediction model;log data;cloud service provider Website;log files;data metrics;word counts;image counts;link counts;section counts;Web crawlers;linear correlation analysis","","","","23","","21 Sep 2020","","","IEEE","IEEE Conferences"
"The Design and Implementation of an Efficient Data Consistency Mechanism for In-Memory File Systems","X. Chen; E. H. -. Sha; Z. Sun; Q. Zhuge; W. Jiang","Coll. of Comput. Sci., Chongqing Univ., Chongqing, China; Coll. of Comput. Sci., Chongqing Univ., Chongqing, China; Coll. of Comput. Sci., Chongqing Univ., Chongqing, China; Coll. of Comput. Sci., Chongqing Univ., Chongqing, China; Coll. of Comput. Sci., Chongqing Univ., Chongqing, China","2016 13th International Conference on Embedded Software and Systems (ICESS)","19 Oct 2017","2016","","","170","175","The Non-Volatile Memory (NVM) based in-memory file systems show great potential in supporting real-time data processing for their extremely high performance. The reliability of file systems is ensured by data consistency mechanisms. The existing data consistency mechanisms, however, largely degrades the performance of the in-memory file system without fully exploiting the characteristics of NVM. In this paper, we propose an efficient data consistency mechanism, Amphibian Update Strategy (AUS), taking advantages of the virtual address space of NVM. In the proposed AUS technique, the backup spaces of file data are organized and accessed by the contiguous virtual address space of the kernel. We present the Direct-Copy and Exchanging approaches to efficiently update the primary file data for the requests with different sizes. We implemented different data consistency mechanisms in a real in-memory file system, SIMFS. Extensive experiments are conducted. The experimental results show that AUS achieves 2.4 times, 1.8 times, and 1.7 times faster than the legacy journaling, short-circuit shadow paging, and the state-of-the-art technique adaptive logging, respectively.","","978-1-5090-3727-8","10.1109/ICESS.2016.18","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8074461","Non-volatile memory;file systems;data consistency;performance;copy-on-write","Nonvolatile memory;Kernel;Metadata;Hardware;Adaptive systems;Reliability","flash memories;random-access storage;storage allocation;virtual storage","nonvolatile memory;primary file data;efficient data consistency mechanism;NVM;real-time data processing;in-memory file system","","","","20","","19 Oct 2017","","","IEEE","IEEE Conferences"
"GNSS Baseband Logging System","Chen Pei; Wang Sheng; Han Chao","Beijing University of Aeronautics & Astronautics, 1205 Tower 5, 48 Huayuanbei Road, China 100191; Beijing University of Aeronautics & Astronautics, 1205 Tower 5, 48 Huayuanbei Road, China 100191; Beijing University of Aeronautics & Astronautics, 1205 Tower 5, 48 Huayuanbei Road, China 100191","IEEE Aerospace and Electronic Systems Magazine","24 Feb 2011","2011","26","1","9","13","The software design, implementation, and preliminary test of a GNSS Baseband Logging System, which records baseband GNSS L1 signals in real-time, is presented herein. The RF Front-End of this GNSS Baseband Logger, centered at the GPS L1 frequency, has enough bandwidth to include (or receive) GPS, GLONASS, Galileo, and Compass L1 signals without truncation which is essential for baseband processing. Existing software defined radio open source GNSS projects have been proposed, developed, and implemented into a PC Data Processing Terminal of the system with several off-line functions such as FFT-based acquisition, GPS (or GNSS) positioning, and Carrier/Noise ratio analysis. A brief description of the design and performance of the GNSS logging system is supported by the observed experimental data.","1557-959X","","10.1109/MAES.2011.5719650","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5719650","","Software design;Baseband;Global Positioning System;Real time systems;Satellite navigation systems","satellite navigation;system monitoring;telecommunication computing","GNSS baseband logging system;software design;RF front-end;GPS;GLONASS;Galileo;compass LI signal;PC data processing terminal;FFT-based acquisition;carrier-noise ratio analysis","","","","","","24 Feb 2011","","","IEEE","IEEE Magazines"
"Aspect-Oriented Programming for MVC Framework","H. Li; M. Zhou; G. Xu; L. Si","Sch. of Inf. Eng., Handan Coll., Handan, China; Sch. of Inf. Eng., Handan Coll., Handan, China; Sch. of Inf. Eng., Handan Coll., Handan, China; Sch. of Inf. Eng., Handan Coll., Handan, China","2010 International Conference on Biomedical Engineering and Computer Science","6 May 2010","2010","","","1","4","In the field of J2EE, MVC framework exists crosscutting concerns across multiple modules (e.g. logging, validation, transaction etc.) causing the code scattering and code confusion and making the system difficult to maintain and to extend. Fortunately, Aspect-Oriented Programming aims at addressing the problems of them. Aspects can be defined to modularize such concerns. In this work, we introduce the aspect-oriented programming ideas into the MVC model, and propose a model of aspect-oriented MVC framework, which extracts crosscutting concerns of going through the system to form an aspect layer and uses the configuration file to statement the point of weaving. Finally, we report the results of the framework of the feasibility and superiority by an actual project development.","2165-9249","978-1-4244-5315-3","10.1109/ICBECS.2010.5462393","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5462393","","Object oriented modeling;Educational institutions;Logic programming;Maintenance engineering;Weaving;Scattering;Access control;Packaging;Prototypes;HTML","aspect-oriented programming;Java;software architecture","aspect-oriented programming;MVC framework;J2EE;MVC model;aspect layer;configuration file;project development;object-oriented programming;model-view controller;software architecture","","","","12","","6 May 2010","","","IEEE","IEEE Conferences"
"Parametric classification over multiple samples","B. Russo","Faculty of Computer Science, Free University of Bozen-Bolzano, Italy","2013 1st International Workshop on Data Analysis Patterns in Software Engineering (DAPSE)","19 Sep 2013","2013","","","23","25","This pattern was originally designed to classify sequences of events in log files by error-proneness. Sequences of events trace application use in real contexts. As such, identifying error-prone sequences helps understand and predict application use. The classification problem we describe is typical in supervised machine learning, but the composite pattern we propose investigates it with several techniques to control for data brittleness. Data pre-processing, feature selection, parametric classification, and cross-validation are the major instruments that enable a good degree of control over this classification problem. In particular, the pattern includes a solution for typical problems that occurs when data comes from several samples of different populations and with different degree of sparcity.","","978-1-4673-6296-2","10.1109/DAPSE.2013.6603805","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6603805","","Correlation;Sociology;Vectors;Training;Accuracy;Software","learning (artificial intelligence);pattern classification","parametric classification;error-prone sequences;supervised machine learning;classification problem;feature selection;data pre-processing;cross-validation","","2","","8","","19 Sep 2013","","","IEEE","IEEE Conferences"
"Anomaly Detection of System Logs Based on Natural Language Processing and Deep Learning","M. Wang; L. Xu; L. Guo","Key Laboratory of Space Utilization, Technology and Engineering Center for space Utilization, Chinese Academy of Sciences, Beijing, 100094, China; Key Laboratory of Space Utilization, Technology and Engineering Center for space Utilization, Chinese Academy of Sciences, Beijing, 100094, China; Key Laboratory of Space Utilization, Technology and Engineering Center for space Utilization, Chinese Academy of Sciences, Beijing, 100094, China","2018 4th International Conference on Frontiers of Signal Processing (ICFSP)","29 Nov 2018","2018","","","140","144","System logs record the execution trajectory of the system and exist in all components of the system. Nowadays, the systems are deployed in a distributed environment and they generate logs which contain complex format and rich semantic information. Simple statistical analysis methods cannot fully capture log information for effective abnormal detection of software systems. In this paper, we propose to analyze the logs by combining feature extraction methods from natural language processing and anomaly detection methods from deep learning. Two feature extraction algorithms, Word2vec and Term Frequency-Inverse Document Frequency (TF-IDF), are respectively adopted and compared here to obtain the log information, and then one deep learning method named Long Short-Term Memory (LSTM) is applied for the anomaly detection. To validate the effectiveness of the proposed method, we compare LSTM with other machine learning algorithms, including Gradient Boosting Decision Tree (GBDT) and Naïve Bayes, the results show that LSTM can perform the best for anomaly detection of system logs with both of the two feature extraction methods, indicating that LSTM can capture contextual semantic information effectively in log anomaly detection and will be a promising tool for log analysis.","","978-1-5386-7853-4","10.1109/ICFSP.2018.8552075","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8552075","anomaly detection;Word2vec;TF-IDF;LSTM;log analysis;natural language processing","Anomaly detection;Feature extraction;Logic gates;Semantics;Boosting;Decision trees","Bayes methods;decision trees;feature extraction;learning (artificial intelligence);natural language processing;pattern classification;statistical analysis;text analysis","long short-term memory;machine learning algorithms;deep learning method;anomaly detection methods;feature extraction methods;software systems;effective abnormal detection;log information;simple statistical analysis methods;rich semantic information;natural language processing;system logs;log anomaly detection;contextual semantic information;LSTM","","3","","17","","29 Nov 2018","","","IEEE","IEEE Conferences"
"Formal validation and verification of space flight software using statechart-assertions and runtime execution monitoring","M. C. Bergue Alves; D. Drusinsky; J. B. Michael; M. Shing","Computer Science, Naval Postgraduate School, 1411 Cunningham Road, Monterey, CA, USA; Computer Science, Naval Postgraduate School, 1411 Cunningham Road, Monterey, CA, USA; Computer Science, Naval Postgraduate School, 1411 Cunningham Road, Monterey, CA, USA; Computer Science, Naval Postgraduate School, 1411 Cunningham Road, Monterey, CA, USA","2011 6th International Conference on System of Systems Engineering","28 Jul 2011","2011","","","155","160","Systems of systems must rely on a sound validation and verification process due to their inherent complexity. This paper presents the results of a formal computer-aided validation and verification of critical time-constrained requirements of the Brazilian Satellite Launcher flight software. It describes the entire specification, validation, and verification process that begins with a system requirement as a natural language specification, followed by the creation and computer-aided validation of UML statechart-formal specification assertions, and ends with the log file based runtime verification. These log files were executed as JUnit tests against the assertions. The verification and validation of the flight software uncovered inaccuracies in the requirements understanding and implementation. The results also confirmed the importance of having computer-aided tools deeply integrated into the verification and validation process, supporting requirement behavioral validation and verification of requirements implementation on the hardware and software platforms on which these systems run.","","978-1-61284-782-5","10.1109/SYSOSE.2011.5966590","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5966590","flight software;statechart-assertions;runtime execution monitoring;JUnit tests;validation and verification;UML","Software;Runtime;Instruments;Monitoring;Testing;Java;USA Councils","aerospace computing;formal specification;formal verification;space vehicles","formal validation;formal verification;space flight software;statechart-assertion;runtime execution monitoring;Brazilian satellite launcher flight software;natural language specification;formal specification assertion;log file based runtime verification;JUnit test","","2","","12","","28 Jul 2011","","","IEEE","IEEE Conferences"
"PLELog: Semi-supervised Log-based Anomaly Detection via Probabilistic Label Estimation","L. Yang; J. Chen; Z. Wang; W. Wang; J. Jiang; X. Dong; W. Zhang","Tianjin University,College of Intelligence and Computing,Tianjin,China; Tianjin University,College of Intelligence and Computing,Tianjin,China; Tianjin University,College of Intelligence and Computing,Tianjin,China; Tianjin University,College of Intelligence and Computing,Tianjin,China; Tianjin University,College of Intelligence and Computing,Tianjin,China; Tianjin University,Information and Network Center,Tianjin,China; Tianjin University,Information and Network Center,Tianjin,China","2021 IEEE/ACM 43rd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)","7 May 2021","2021","","","230","231","PLELog is a novel approach for log-based anomaly detection via probabilistic label estimation. It is designed to effectively detect anomalies in unlabeled logs and meanwhile avoid the manual labeling effort for training data generation. We embed semantic information within log events as fixed-length vectors and apply HDBSCAN to automatically cluster log sequences. After that, we also propose a Probabilistic Label Estimation approach to automatically label log sequences, which can reduce the noises introduced by error labeling and put ""labeled"" instances into an attention-based GRU network for training. We conducted an empirical study to evaluate the effectiveness of PLELog on two open-source log data (i.e., HDFS and BGL). The results demonstrate the effectiveness of PLELog. In particular, PLELog has been applied to two real-world systems from a university and a large corporation, and the results further demonstrate its practicability.","2574-1926","978-1-6654-1219-3","10.1109/ICSE-Companion52605.2021.00106","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9402368","","Training;Estimation;Training data;Probabilistic logic;Labeling;Anomaly detection;Software engineering","","","","","","4","","7 May 2021","","","IEEE","IEEE Conferences"
"Integration of autonomous sender for hidden log data on kleptoware for supporting physical penetration testing","S. M. Nasution; Y. Purwanto; A. Virgono; M. R. Y. Tambunan","Electrical Engineering Faculty, Telkom University, Bandung, Indonesia; Electrical Engineering Faculty, Telkom University, Bandung, Indonesia; Electrical Engineering Faculty, Telkom University, Bandung, Indonesia; Electrical Engineering Faculty, Telkom University, Bandung, Indonesia","2015 1st International Conference on Wireless and Telematics (ICWT)","9 Apr 2016","2015","","","1","5","Keylogger is a dangerous device that can capture all word that typed on the keyboard. There are two kinds of keylogger, it is hardware and software keylogger. It is very easy to detect them because both of them already listed as a malware. There are a lot of antivirus application that can detect software keylogger and for the hardware keylogger, it is easily can be seen if there is a strange thing that attached to our computer. Kleptoware is one of the solutions to hardware keylogger main problem. Another problem comes when we want to take all data had been capture on the device, we must take the keylogger first. This paper discuss about how to gain data from a kleptoware autonomously with client-server design on a local area network. Result in this paper shows that data must be send at least had same file size with the buffer that already determine first.","","978-1-4673-8434-6","10.1109/ICWT.2015.7449205","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7449205","autonomous sender;keylogger;kleptoware","Servers;Keyboards;Hardware;Testing;Computers;Sockets;Universal Serial Bus","client-server systems;computer network security;computer viruses;invasive software;keyboards;local area networks","autonomous sender integration;hidden log data;kleptoware;physical penetration testing;hardware keylogger;word capture;keyboard;malware;antivirus application;software keylogger detection;client-server design;local area network","","1","","7","","9 Apr 2016","","","IEEE","IEEE Conferences"
"Understanding BitTorrent through real measurements","W. Mazurczyk; P. Kopiczko","Institute of Telecommunications, Warsaw University of Technology, Warsaw 00-665, Poland; Institute of Telecommunications, Warsaw University of Technology, Warsaw 00-665, Poland","China Communications","22 Nov 2013","2013","10","11","107","118","In this paper, we present the results of the BitTorrent measurement study. Two sources of BitTorrent data were utilised: metadata files and the logs of one of the currently most popular BitTorrent clients - μTorrent. Experimental data were collected for fifteen days from the popular torrent-discovery site thepiratebay.org (more than 30 000 torrents were captured and analysed). During this period the activity and logs of an unmodified version of μTorrent client downloading sessions were also captured. The obtained experimental results are swarm-oriented, which allows us to look at BitTorrent and its users from an exchanged resources perspective. Moreover, comparative analysis of the clients' connections with and without the μTP protocol is carried out to verify the extent to which μTP improves BitTorrent transmissions. To the authors' best knowledge, none of the previous studies have addressed these issues.","1673-5447","","10.1109/CC.2013.6674215","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6674215","BitTorrent traffic analysis;measurement study;metadata files;μTorrent;μTP","Telecommunication traffic;Metadata;Peer-to-peer computing;Software;Indexing","file organisation;peer-to-peer computing;protocols","BitTorrent measurement;BitTorrent data sources;metadata files;torrent-discovery site;μTorrent client downloading sessions;μTP protocol;P2P networking system","","1","","","","22 Nov 2013","","","IEEE","IEEE Magazines"
"Similarity-Based Bayesian Learning from Semi-structured Log Files for Fault Diagnosis of Web Services","X. Han; Z. Shi; W. Niu; K. Chen; X. Yang","Key Lab. of Intell. Inf. Process., Chinese Acad. of Sci., Beijing, China; Key Lab. of Intell. Inf. Process., Chinese Acad. of Sci., Beijing, China; Key Lab. of Intell. Inf. Process., Chinese Acad. of Sci., Beijing, China; Key Lab. of Intell. Inf. Process., Chinese Acad. of Sci., Beijing, China; Key Lab. of Intell. Inf. Process., Chinese Acad. of Sci., Beijing, China","2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology","1 Nov 2010","2010","1","","589","596","With the rapid development of XML language which has good flexibility and interoperability, more and more log files of software running information are represented in XML format, especially for Web services. Fault diagnosis by analyzing semi-structured and XML like log files is becoming an important issue in this area. For most related learning methods, there is a basic assumption that training data should be in identical structure, which does not hold in many situations in practice. In order to learn from training data in different structures, we propose a similarity-based Bayesian learning approach for fault diagnosis in this paper. Our method is to first estimate similarity degrees of structural elements from different log files. Then the basic structure of combined Bayesian network (CBN) is constructed, and the similarity-based learning algorithm is used to compute probabilities in CBN. Finally, test log data can be classified into possible fault categories based on the generated CBN. Experimental results show our approach outperforms other learning approaches on those training datasets which have different structures.","","978-1-4244-8482-9","10.1109/WI-IAT.2010.51","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5616413","Web service;fault diagnosis;Bayesian learning;similarity;CBN","","belief networks;fault diagnosis;fault tolerant computing;learning (artificial intelligence);open systems;Web services;XML","similarity based Bayesian learning;semistructured log file;fault diagnosis;Web service;XML language;combined Bayesian network","","6","","16","","1 Nov 2010","","","IEEE","IEEE Conferences"
"Message Correlation and Web Service Protocol Mining from Inaccurate Logs","K. Musaraj; T. Yoshida; F. Daniel; M. Hacid; F. Casati; B. Benatallah","LIRIS, Univ. de Lyon, Villeurbanne, France; Grad. Sch. of Inf. Sci. & Technol., Hokkaido Univ., Sapporo, Japan; Dept. of Inf. Eng. & Comput. Sci., Univ. of Trento, Trento, Italy; LIRIS, Univ. de Lyon, Villeurbanne, France; Dept. of Inf. Eng. & Comput. Sci., Univ. of Trento, Trento, Italy; Sch. of Comput. Sci. & Eng., Univ. of New South Wales, Sydney, NSW, Australia","2010 IEEE International Conference on Web Services","23 Aug 2010","2010","","","259","266","Business process management, service-oriented architectures and software back-engineering heavily rely on the fundamental processes of mining of processes and web service business protocols from log files. Model extraction and mining aim at the (re)discovery of the behavior of a running model implementation using solely its interaction and activity traces, and no a priori information on the target model. This paper presents an approach for correlating messages and extracting the business protocol of a web service in the realistic scenario in which correlation information is entirely absent from interaction and activity logs. Correlation is achieved through deterministic computations that result in an extremely efficient method whose extensive experiments have shown its solid reliability, robustness when dealing with complex structures, and very high performance and scalability. This approach and the underlying algorithms extend what is actually possible to achieve in the web service business protocol mining domain using incomplete and noisy data logs, and opens new horizons in back-engineering of web services. The theoretical and experimental results clearly show the leap forward achieved herein.","","978-1-4244-8146-0","10.1109/ICWS.2010.104","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5552821","knowledge extraction;business protocol;web service;protocol mining;log","Protocols;Correlation;Equations;Business;Web services;Data mining;Linear regression","business data processing;data mining;protocols;Web services","message correlation;Web service protocol mining;inaccurate logs;business process management;service-oriented architecture;software back-engineering;Web service business protocols;log files;model extraction;data mining;Web service business protocol mining domain","","10","","24","","23 Aug 2010","","","IEEE","IEEE Conferences"
"Big Data and Machine Learning for Forestalling Customer Churn Using Hybrid Software","L. Butgereit","Nelson Mandela University,School of ICT,Port Elizabeth,South Africa","2020 Conference on Information Communications Technology and Society (ICTAS)","30 Apr 2020","2020","","","1","4","The term customer churn is used to describe a situation where a customer leaves one merchant or supplier and moves to a competitor of that original merchant or supplier. This is also know as customer attrition. Prior to churning, however, there are often hints or clues in the customer's buying patterns that he or she is ready to leave the supplier. This paper looks at the use of Machine Learning algorithms to predict when customers are ready to churn or in the process of churning. These predictions are then used to look at free text unformatted log data to find any reasons why this customer might be churning. This free text log data would include textual error messages that the customer might have received or financial problems which might have arisen such as not having sufficient funds in his or her account. This merged information is then forwarded to an outbound call queue so that trained call center agents could make human-to-human voice calls to the customer and entice them to stay with the merchant or supplier by offering some financial incentive. All of the technicalities were orchestrated using Spring Boot microservices. Design Science Research was used for the this project and a number of iterations were executed until results were satisfactory. These iterations included changing from an AutoEncoder to a MultiLayerPerceptron, included changing from one Java library providing neural network objects to another Java library, included better searching of log files for possible reasons that customers were churning and included many experiments with the quantity of sales data required in order for the neural networks to create reasonable predictions.","","978-1-7281-3770-4","10.1109/ICTAS47918.2020.233972","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9082475","customer churn;machine learning;Neuroph;Weka;DeepLearning4J;SpringBoot;Microservices","","customer services;data mining;iterative methods;Java;learning (artificial intelligence);multilayer perceptrons;neural nets;public domain software;software maintenance;text analysis","forestalling customer churn;customer attrition;neural networks;Java library;multilayer perceptron;autoencoder;Spring Boot microservices;human-to-human voice calls;call center agents;financial incentive;text log data;textual error messages;hybrid software;Big Data;machine learning;iteration methods;sales data","","","","20","","30 Apr 2020","","","IEEE","IEEE Conferences"
"Redundancy Analysis and Elimination on Access Patterns of the Windows Applications Based on I/O Log Data","J. Lee; H. Kwon","Department of Industrial and Systems Engineering, Seoul National University of Science and Technology (SeoulTech), Seoul, South Korea; Department of Industrial and Systems Engineering, Seoul National University of Science and Technology (SeoulTech), Seoul, South Korea","IEEE Access","4 Mar 2020","2020","8","","40640","40655","In this paper, we analyze I/O log data monitored in the Windows operating system for improving the system performance. Especially, we focus on the I/O operations to the Windows registry. As a result, we identify redundant access patterns of the Windows applications. To find all the possible redundant patterns from the large-scale log data, we propose the redundancy detection algorithm. Then, we propose the two-level redundancy elimination method to remove unnecessary redundant operations. We also present an event-driven method that guarantees that the result of redundancy elimination is equivalent to that of the original program. Through experiments, we show that the proposed redundancy elimination method improves the performance of the original program having redundant access patterns by up to 90.25% for individual access patterns; by 8.93% ~ 26.21% when the multiple programs having combined access patterns are running concurrently.","2169-3536","","10.1109/ACCESS.2020.2964260","National Research Foundation of Korea; Ministry of Science, ICT and Future Planning; National Research Foundation of Korea; Ministry of Education; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8950307","I/O log data;access pattern analysis;redundancy elimination;Windows registry","Microsoft Windows;Redundancy;Monitoring;Internet;Binary codes;System performance","input-output programs;Microsoft Windows (operating systems);redundancy;software maintenance;system monitoring","large-scale log data;I/O operations;I/O log data;event-driven method;two-level redundancy elimination method;redundancy detection algorithm;redundant patterns;Windows registry;system performance;Windows operating system;Windows applications;redundancy analysis;redundant access patterns","","","","38","CCBY","6 Jan 2020","","","IEEE","IEEE Journals"
"Connecting the Dots: Reconstructing Network Behavior with Individual and Lossy Logs","J. Wang; X. Zheng; X. Mao; Z. Cao; D. Liu; Y. Liu","Sch. of Software & TNLIST, Tsinghua Univ., Beijing, China; Sch. of Software & TNLIST, Tsinghua Univ., Beijing, China; Sch. of Software & TNLIST, Tsinghua Univ., Beijing, China; Sch. of Software & TNLIST, Tsinghua Univ., Beijing, China; Sch. of CSE, Univ. of Electron. Sci. & Technol. of China, Chengdu, China; Sch. of Software & TNLIST, Tsinghua Univ., Beijing, China","2015 44th International Conference on Parallel Processing","10 Dec 2015","2015","","","170","179","In distributed networks such as wireless ad hoc networks, local and lossy logs are often available on individual nodes. We propose REFILL, which analyzes lossy and unsynchronized logs collected from individual nodes and reconstructs the network behaviors. We design an inference engine based on protocol semantics to abstract states on each node. Further we leverage inherent and implicit event correlations in and between nodes to connect interference engines and analyze logs from different nodes. Based on unsynchronized and incomplete logs, REFILL can reconstruct network behavior, recover the network scenario and understand what has happened in the network. We show that the result of REFILL can be used to guide protocol design, network management, diagnosis, etc. We implement REFILL and apply it to a large-scale wireless sensor network project. REFILL provides a detailed per-packet tracing information based on event flows. We show that REFILL can reveal and verify fundamental issues, like locating packet loss positions and root causes. Further, we present implications and demonstrate how to leverage REFILL to enhance network performance.","0190-3918","978-1-4673-7587-0","10.1109/ICPP.2015.26","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7349572","","Correlation;Wireless sensor networks;Ad hoc networks;Protocols;Synchronization;Receivers;Wireless communication","computer network performance evaluation;inference mechanisms;protocols;wireless sensor networks","distributed network;REFILL;unsynchronized log collection;protocol semantics;interference engine;protocol design;network management;large-scale wireless sensor network project;per-packet tracing information;network performance enhancement;network diagnosis","","","","21","","10 Dec 2015","","","IEEE","IEEE Conferences"
"Autonomous real time requirements tracing","G. Plattsmier; H. Stetson","Marshall Space Flight Center, Huntsville, AL 35812, USA; Teledyne Brown Engineering, Huntsville, AL 35812, USA","2014 IEEE Aerospace Conference","19 Jun 2014","2014","","","1","9","One of the more challenging aspects of software development is the ability to verify and validate the functional software requirements dictated by the Software Requirements Specification (SRS) and the Software Detail Design (SDD). Insuring the software has achieved the intended requirements is the responsibility of the Software Quality team and the Software Test team. The utilization of Timeliner-TLX™ Auto-Procedures for relocating ground operations positions to ISS automated on-board operations has begun the transition that would be required for manned deep space missions with minimal crew requirements. [1] This transition also moves the auto-procedures from the procedure realm into the flight software arena and as such the operational requirements and testing will be more structured and rigorous. The auto-procedures would be required to meet NASA software standards as specified in the Software Safety Standard (NASA-STD-8719), the Software Engineering Requirements (NPR 7150), the Software Assurance Standard (NASA-STD-8739) and also the Human Rating Requirements (NPR-8705). The Autonomous Fluid Transfer System (AFTS) test-bed utilizes the Timeliner-TLX™ Language for development of autonomous command and control software. The Timeliner-TLX™ system has the unique feature of providing the current line of the statement in execution during real-time execution of the software. The feature of execution line number internal reporting unlocks the capability of monitoring the execution autonomously by use of a companion Timeliner-TLX™ sequence as the line number reporting is embedded inside the Timeliner-TLX™ execution engine. This negates I/O processing of this type data as the line number status of executing sequences is built-in as a function reference. This paper will outline the design and capabilities of the AFTS Autonomous Requirements Tracker, which traces and logs SRS requirements as they are being met during real-time execution of the targeted system. It is envisioned that real time requirements tracing will greatly assist the movement of auto-procedures to flight software enhancing the software assurance of auto-procedures and also their acceptance as reliable commanders.","1095-323X","978-1-4799-1622-1","10.1109/AERO.2014.6836215","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6836215","","Fluids;Software systems;Real-time systems;Valves;Standards;Encoding","aerospace computing;formal specification;formal verification;software quality","autonomous real time requirements tracing;software development;functional software requirements;software requirements specification;SRS;software detail design;SDD;software quality team;software test team;ground operations positions;space missions;flight software arena;software engineering requirements;software assurance standard;NASA-STD;human rating requirements;autonomous fluid transfer system;AFTS;autonomous command and control software;autonomous requirements tracker;real-time execution","","","","1","","19 Jun 2014","","","IEEE","IEEE Conferences"
"Mining Developer's Behavior from Web-Based IDE Logs","P. Ardimento; M. L. Bernardi; M. Cimitile; G. De Ruvo",University of Bari 'Aldo Moro'; Giustino Fortunato University; Unitelma Sapienza University; Independent IEEE Member,"2019 IEEE 28th International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE)","15 Aug 2019","2019","","","277","282","The birth of cloud-based development environments makes available an increasing number of data coming out from the interaction of different developers with a diverse level of expertise. This data, if opportunely captured and analyzed, can be useful to understand how developers head the coding activities and can suggest members of developers community how to improve their performances. This paper presents a framework allowing to generate event logs from cloud-based IDE. These event logs are then examined using a process mining technique to extract the developers' coding processes and compare them in the shared coding environment. The approach can be used to discover emergent and interesting developers' behavior. Thus, we compare the coding process extracted by developers with different skills. To validate our approach, we describe the results of a study in which we investigate the coding activities of forty students of an advanced Java programming course performing a given programming task-during four assignments. Results also prove that users with different performances possess distinct attitudes highlighting that the adopted process mining technique can be useful to comprehend how developers can improve their coding skills.","2641-8169","978-1-7281-0676-2","10.1109/WETICE.2019.00065","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8795421","process mining;development workflows;source code;IDE logging","Data mining;Encoding;Java;Programming;Software systems;Human computer interaction","behavioural sciences computing;computer science education;data mining;educational courses;Java;programming;programming environments;project management","programming task;Web-based IDE logs;coding process;shared coding environment;cloud-based IDE;event logs;cloud-based development environments;mining developer;coding skills;adopted process mining technique;coding activities","","","","23","","15 Aug 2019","","","IEEE","IEEE Conferences"
"Hierarchical Multi-log Cloud-Based Search Engine","A. Singh; H. Gonzalez Velez","Cloud Competency Centre, Nat. Coll. of Ireland, Dublin, Ireland; Cloud Competency Centre, Nat. Coll. of Ireland, Dublin, Ireland","2014 Eighth International Conference on Complex, Intelligent and Software Intensive Systems","2 Oct 2014","2014","","","211","219","Having become the leading trend in IT infrastructure, service delivering, and multi-layered resource sharing, cloud services typically include SaaS (Software as a service), PaaS (Platform as a service) and Iaas (Infrastructure as a service). With the increasing popularity of cloud computing, users store large amounts of data as documents, text files, databases, and more relevant to this work, system logs. Current cloud services are getting more decoupled with each layer in the cloud stack generating different logs for network, applications, database, and programming interfaces on different machines. At any point in time, cloud providers, users, or application developers arguably require to understand the status of different components, monitor business processes, and analyse machine logs in real time. However, there are no specialised search engines for the systematic analysis of logs by different cloud providers. Hence, this paper presents Simha, an agent-based document search service for cloud platforms. It implements a proof of concept system to analyse user documents, logs, and folders in real time from different virtual machines. Based on an Elastic search server, our overall search process has been extended to distributively search data stored into cloud. So, we propose an application which looks for data in private cloud and public clouds. In this paper, we describe its design and implementation. We have obtained initial encouraging results, and we further discuss how to extend our scheme in several ways.","","978-1-4799-4325-8","10.1109/CISIS.2014.30","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6915519","BigData;Cloud Computing;Hybrid Cloud;ElasticSearch;Log;Search Engine;Data","Cloud computing;Servers;Rivers;Indexes;Virtual machining;Monitoring","cloud computing;search engines","public cloud;private cloud;elastic search server;virtual machine;agent-based document search service;Simha;programming interface;system logs;cloud computing;infrastructure as a service;Iaas;platform as a service;PaaS;software as a service;SaaS;cloud services;search engine;hierarchical multilog cloud","","4","","28","","2 Oct 2014","","","IEEE","IEEE Conferences"
"Automatic classification of event logs sequences for failure detection in WfM/BPM systems","J. Jaramillo; J. Arias","Intelligent Information Systems Lab, Universidad de Antioquia, Medellín, Colombia; Intelligent Information Systems Lab, Universidad de Antioquia, Medellín, Colombia","2019 IEEE Colombian Conference on Applications in Computational Intelligence (ColCACI)","1 Aug 2019","2019","","","1","6","Process mining is a technical alternative focused on solving some of the difficulties presented when modeling business processes, particularly when Business Process Management (BPM) systems present inconsistencies with the real process. Currently, there is an increasing interest in predicting the behavior of active work items in any business process, which would make possible to monitor the behavior of such processes in a more accurate way. Given the complexity of current business processes, conventional techniques are not always effective in addressing this type of requirements; therefore, machine learning techniques are being increasingly more used for this task. This work deals with the problem of fail detection in a BPM system from event logs, based on machine learning methods. The paper explores the use of two structural learning models, Hidden Markov Models (HMM) and Hidden semi-Markov models (HSMM). Both models are suitable to model sequence data, but the last one can take into consideration the duration time that the underlying process remains in one state. The experiments are carried out using a real database of about 460,000 event logs sequences. The results show that for the given dataset, fail detection can be achieved with an accuracy of 86.70% using the HSMM model. In order to reduce the computational load of the proposed approach, the models were implemented in a distributed processing environment using Apache Spark, which guarantees the scalability of the solution.","","978-1-7281-1614-3","10.1109/ColCACI.2019.8781973","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8781973","Process Mining;Hidden Markov Models;Hidden semi-Markov Models;Apache Spark;Distributed System","Hidden Markov models;Computational modeling;Vector quantization;Task analysis;Stochastic processes","business data processing;data mining;distributed processing;hidden Markov models;learning (artificial intelligence);workflow management software","structural learning models;event logs sequences;HSMM model;machine learning techniques;hidden semiMarkov models;distributed processing;process mining;business process management;WfM-BPM systems;Apache Spark","","","","19","","1 Aug 2019","","","IEEE","IEEE Conferences"
"Detection of Bottleneck and Social Network in Business Process of Agile Development","N. Raitubu; K. R. Sungkono; R. Sarno; C. S. Wahyuni","Informatics Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Informatics Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Informatics Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Informatics Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia","2019 International Seminar on Application for Technology of Information and Communication (iSemantic)","28 Oct 2019","2019","","","208","213","Many software organizations are moving away from traditional methods to adopt Agile development methodologies with the increasing popularity of Agile. It was noted that Agile is more adaptive and people focused. It was created for small to medium and collaborative team that work closely together. When Agile is implemented, team size a common factor in turn constrained by people factor which are often overlooked. Many organizations and industries are migrant of Agile methodologies because of its popularity in today's technology. One of the main assets of any organization are the employees because they bring out the important success of the organization to make the best use of the resources available in the organization. for this, it will be easy to measure the job performance evaluation of an organization. The aims of this study are to investigate the use of Agile methodology for the performance analysis evaluation of software development. it is difficult to find the bottleneck and dependencies between members of the team if we do observe the process manually. This paper proposes an agile process model that can generate an event log and to analyzed the event log by using alpha ++ and social network analysis to find the bottle neck and dependencies of the member.","","978-1-7281-3832-9","10.1109/ISEMANTIC.2019.8884341","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8884341","Agile;Bottleneck;Performance analysis;Social network","Software;Social networking (online);Organizations;Task analysis;PROM;Nanoelectromechanical systems","business data processing;groupware;organisational aspects;personnel;project management;social network theory;software prototyping","business process;software organizations;agile development methodologies;collaborative team;job performance evaluation;performance analysis evaluation;software development;agile process model;social network analysis;event log","","","","15","","28 Oct 2019","","","IEEE","IEEE Conferences"
"A novel development and analysis solution to PaaS log by using CouchDB","Z. Liu; Y. Wang; R. Lin","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing 100876, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing 100876, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing 100876, China","2012 3rd IEEE International Conference on Network Infrastructure and Digital Content","24 Jan 2013","2012","","","251","255","Log for PaaS cloud computing system is important to provide information to manage systems. In this paper, we design and implement a PaaS log system by providing log recording ability to other parts of PaaS system as a webservice. This system uses Log4j as log frame and CouchDB as database to analyze logs due to its data structure, efficient log analysis using Map Reduce and fore-end visualization. Also, we analyze system load, webservice load, VM load and DB load based on this PaaS log system and real data. At last, future analysis using our PaaS log system and system optimizations are discussed.","2374-0272","978-1-4673-2204-1","10.1109/ICNIDC.2012.6418754","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6418754","PaaS;Log;CouchDB;Map reduce;Visualization","Servers;Databases;Cloud computing;Writing;Data structures;Data visualization;Optimization","cloud computing;data analysis;data structures;data visualisation;database management systems;distributed processing;Web services","PaaS log analysis;CouchDB;PaaS cloud computing system;PaaS log system;Web service;Log4j log frame;data structure;Map Reduce;fore-end visualization;VM load;DB load;system optimizations;platform-as-a-service software","","1","","12","","24 Jan 2013","","","IEEE","IEEE Conferences"
"Tracking the Ripple Impact of Code Changes through Version Control Systems","A. Irfan; A. U. Rahman",Bahria University; Bahria University,"2019 International Conference on Frontiers of Information Technology (FIT)","13 Feb 2020","2019","","","262","2623","Continuous updates and modifications are integral part of software development and maintenance process. With the advancements in technology, geographically distributed teams can contribute to code of a project shared with the help of integrated development environments (IDE) like Visual Studio and version control systems (VCS) like VSTS. In addition to managing shared code repository, VCS helps in keeping track of changes made in the source code. However, sometimes developers may detect the ripple impact of changes made in a specific code file. For this purpose, developers have to obtain the specific version of source code file from VCS and then use IDE to track the references of modified methods. This paper proposes an approach that enables VCSs to maintain a log of files and methods that have been affected by changes in a specific method within a project. This will assist developers in reviewing the impact of a specific change prior to merging the code files into their working directory. The proposed technique can be linked with any VCS like GIT, VSTS or Subversion (SVN) in the form of extensions. The effectiveness of having this capability withing the VCS is evaluated through a survey pertaining relevant questions. Survey result yields that having visibility of the source code impacts within the VCS will ease development and review process.","2334-3141","978-1-7281-6625-4","10.1109/FIT47737.2019.00056","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8991662","Version Control System, Integrated Development Environment, Visual Studio Team Services, Subversion, Change Impact Analysis","","configuration management;programming environments;public domain software;software development management;software maintenance;software quality","ripple impact;code changes;version control systems;software development;maintenance process;geographically distributed teams;integrated development environments;IDE;Visual Studio;VCS;specific code file;source code file;code files;source code impacts;shared code repository","","","","11","","13 Feb 2020","","","IEEE","IEEE Conferences"
"Assessing Direct Monitoring Techniques to Analyze Failures of Critical Industrial Systems","M. Cinque; D. Cotroneo; R. D. Corte; A. Pecchia","Dipt. di Ing. Elettr. e Tecnol. dell'Inf., Univ. degli Studi di Napoli Federico II, Naples, Italy; Dipt. di Ing. Elettr. e Tecnol. dell'Inf., Univ. degli Studi di Napoli Federico II, Naples, Italy; Dipt. di Ing. Elettr. e Tecnol. dell'Inf., Univ. degli Studi di Napoli Federico II, Naples, Italy; Dipt. di Ing. Elettr. e Tecnol. dell'Inf., Univ. degli Studi di Napoli Federico II, Naples, Italy","2014 IEEE 25th International Symposium on Software Reliability Engineering","15 Dec 2014","2014","","","212","222","The analysis of monitoring data is extremely valuable for critical computer systems. It allows to gain insights into the failure behavior of a given system under real workload conditions, which is crucial to assure service continuity and downtime reduction. This paper proposes an experimental evaluation of different direct monitoring techniques, namely event logs, assertions, and source code instrumentation, that are widely used in the context of critical industrial systems. We inject 12,733 software faults in a real-world air traffic control (ATC) middleware system with the aim of analyzing the ability of mentioned techniques to produce information in case of failures. Experimental results indicate that each technique is able to cover a limited number of failure manifestations. Moreover, we observe that the quality of collected data to support failure diagnosis tasks strongly varies across the techniques considered in this study.","2332-6549","978-1-4799-6033-0","10.1109/ISSRE.2014.30","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6982628","event logs;assertions;code instrumentation;failure detection;information entropy;critical systems","Monitoring;Middleware;Instruments;Context;Computer crashes;Operating systems","fault diagnosis;middleware;software reliability;system monitoring","direct monitoring technique assessment;critical industrial systems;failure analysis;event logs;assertions;source code instrumentation;software faults;air traffic control;ATC middleware system;failure diagnosis","","17","","33","","15 Dec 2014","","","IEEE","IEEE Conferences"
"Microservices Monitoring with Event Logs and Black Box Execution Tracing","M. Cinque; R. Della Corte; A. Pecchia","Dipartimento di Ingegneria Elettrica e delle Tecnologie dell'Informazione, Università degli Studi di Napoli Federico II, Naples, Italy Italy (e-mail: macinque@unina.it); Dipartimento di Ingegneria Elettrica e delle Tecnologie dell'Informazione, Università degli Studi di Napoli Federico II, Naples, Italy Italy (e-mail: raffaele.dellacorte2@unina.it); Dipartimento di Ingegneria Elettrica e delle Tecnologie dell'Informazione, Università degli Studi di Napoli Federico II, Naples, Italy Italy (e-mail: antonio.pecchia@unina.it)","IEEE Transactions on Services Computing","","2019","PP","99","1","1","Monitoring is a core practice in any software system. Trends in microservices systems exacerbate the role of monitoring and pose novel challenges to data sources being used for monitoring, such as event logs. Current deployments create a distinct log per microservice; moreover, composing microservices by different vendors exacerbates format and semantic heterogeneity of logs. Understanding and traversing the logs from different microservices demands for substantial cognitive work by human experts. This paper proposes a novel approach to accompany microservices logs with black box tracing to help practitioners in making informed decisions for troubleshooting. Our approach is based on the passive tracing of request-response messages of the REpresentational State Transfer (REST) communication model. Differently from many existing tools for microservices, our tracing is application transparent and non-intrusive. We present an implementation called MetroFunnel and conduct an assessment in the context of two case studies: a Clearwater IP Multimedia Subsystem (IMS) setup consisting of Docker microservices and a Kubernetes orchestrator deployment hosting tens of microservices. MetroFunnel allows making useful attributions in traversing the logs; more important, it reduces the size of collected monitoring data at negligible performance overhead with respect to traditional logs.","1939-1374","","10.1109/TSC.2019.2940009","Compagnia di San Paolo; Ministero dell Istruzione dell Universita e della Ricerca; Universita degli Studi di Napoli Federico II; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8826375","monitoring;microservices;REST;Docker;Clearwater;Kubernetes;log analysis","Monitoring;Instruments;Containers;Measurement;Tools;Runtime;Software systems","","","","2","","","","6 Sep 2019","","","IEEE","IEEE Early Access Articles"
"Search engine pictures: Empirical analysis of a web search engine query log","F. Shoeleh; M. S. Zahedi; M. Farhoodi","Iran Telecommunication Research Center, Tehran, Iran; Iran Telecommunication Research Center, Tehran, Iran; Iran Telecommunication Research Center, Tehran, Iran","2017 3th International Conference on Web Research (ICWR)","29 Jun 2017","2017","","","90","95","Since the use of internet has incredibly increased, it becomes an important source of knowledge about anything for everyone. Therefore, the role of search engine as an effective approach to find information is critical for internet's users. The study of search engine users' behavior has attracted considerable research attention. These studies are helpful in developing more effective search engine and are useful in three points of view: for users at the personal level, for search engine vendors at the business level, and for government and marketing at social society level. These kinds of studies can be done through analyzing the log file of search engine wherein the interactions between search engine and the users are captured. In this paper, we aim to present analyses on the query log of a well-known and most used Persian search engine. Our analyses are presented in three main categories: 1) Stats-based analyses, 2) Temporal- based analyses, and 3) Topic-based analyses. The obtained results are promising. Mobile users often posted queries in weekends, whereas Web users utilize the search engine in workweeks. The majority of queries posted form most-populated cities. Additionally, Iranians are mostly interested in political, social, and economical topics.","","978-1-5386-0420-5","10.1109/ICWR.2017.7959311","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7959311","Web Search engine;Query Log analyses;Information Search and Retrival","Search engines;Engines;Web search;Time-frequency analysis;Mobile communication;Europe;Software","information resources;Internet;query processing;search engines","search engine pictures;Web search engine query log;Internet;search engine user behavior;search engine vendors;social society level;log file analysis;Persian search engine;stats-based analyses;temporal-based analyses;topic-based analyses;mobile users;Web users;Iranians;information search;information retrieval","","1","","21","","29 Jun 2017","","","IEEE","IEEE Conferences"
"Lock-Free Decentralized Storage for Transactional Upgrade Rollback","B. Mejías; G. Gutiérrez; P. V. Roy; J. Thomson; P. Trezentos","Univ. catholique de Louvain, Louvain, Belgium; Univ. catholique de Louvain, Louvain, Belgium; Univ. catholique de Louvain, Louvain, Belgium; NA; NA","2010 19th IEEE International Workshops on Enabling Technologies: Infrastructures for Collaborative Enterprises","5 Aug 2010","2010","","","229","234","Installing and upgrading software may introduce conflicts and errors into a system. Transactional Rollback allows the system to return back to a previous, stable and known state. However, to perform such a rollback, it is necessary to store a large amount of information including configuration and installation logs, as well as different versions of software packages. Nevertheless, much of this information is common to several users using the same software and performing the same operations. We can reduce the total amount of storage by having a decentralized architecture using a Distributed Hash Table (DHT) to localise shared resources. We propose a lock-free key/value-set protocol to add and remove data from the DHT. The lock-free protocol is not limited to transactional rollback, and it can be used by other applications that also need value-sets as part of their stored data.","1524-4547","978-1-4244-7217-8","10.1109/WETICE.2010.43","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5541775","software distribution;transactinoal rollback;decentralized storage;peer-to-peer;lock-free protocols","Packaging;Protocols;Open source software;Collaborative software;Software packages;Application software;Operating systems;Linux;Software performance;Dynamic programming","file organisation;software packages;system recovery","lock free decentralized storage;transactional upgrade rollback;software package;decentralized architecture;distributed hash table;localise shared resource;lock free key value set protocol","","","","11","","5 Aug 2010","","","IEEE","IEEE Conferences"
"Online Detection of Spectre Attacks Using Microarchitectural Traces from Performance Counters","C. Li; J. Gaudiot","University of California Irvine, Electrical Engineering and Computer Science, Irvine, USA; University of California Irvine, Electrical Engineering and Computer Science, Irvine, USA","2018 30th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)","21 Feb 2019","2018","","","25","28","To improve processor performance, computer architects have adopted such acceleration techniques as speculative execution and caching. However, researchers have recently discovered that this approach implies inherent security flaws, as exploited by Meltdown and Spectre. Attacks targeting these vulnerabilities can leak protected data through side channels such as data cache timing by exploiting mis-speculated executions. The flaws can be catastrophic because they are fundamental and widespread and they affect many modern processors. Mitigating the effect of Meltdown is relatively straightforward in that it entails a software-based fix which has already been deployed by major OS vendors. However, to this day, there is no effective mitigation to Spectre. Fixing the problem may require a redesign of the architecture for conditional execution in future processors. In addition, a Spectre attack is hard to detect using traditional software-based antivirus techniques because it does not leave traces in traditional log files. In this paper, we proposed to monitor microarchitectural events such as cache misses, branch mispredictions from existing CPU performance counters to detect Spectre during attack runtime. Our detector was able to achieve 0% false negatives with less than 1 % false positives using various machine learning classifiers with a reasonable performance overhead.","1550-6533","978-1-5386-7769-8","10.1109/CAHPC.2018.8645918","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8645918","security;malware detection;microarchitectural features","Microarchitecture;Detectors;Time series analysis;Machine learning;Monitoring;Hardware;Microsoft Windows","cache storage;computer viruses;microprocessor chips;operating systems (computers)","Spectre attack;microarchitectural traces;processor performance;acceleration techniques;caching;data cache timing;software-based fix;traditional log files;microarchitectural events;CPU performance counters;speculative execution technique;software-based antivirus techniques;security flaws;OS vendors;Meltdown effect;speculative caching technique","","4","","15","","21 Feb 2019","","","IEEE","IEEE Conferences"
"TPS metrics extraction software for resource management","J. Luna; M. Morgan; C. Geiger","Frontier Technology, Inc., Beavercreek, OH; Naval Air Warfare Center, PMA-260, NAS Lakehurst, NJ; Lockheed Martin MST, Orlando, FL","2016 IEEE AUTOTESTCON","13 Oct 2016","2016","","","1","7","The Navy has a large set of Test Program Set (TPS) code and associated data that are an untapped resource in Automatic Test System (ATS) planning and support. The ability to relate test instrument capabilities to TPS source data and ATS usage data would provide a comprehensive look at how avionics maintenance is performed. This could identify economic targets of opportunity for the deployment of new and innovative test techniques. Currently the technology does not exist to tap into the available TPS code and associated data. The objective of this effort was to develop a software toolset that provides an innovative capability to extract usage metrics from TPS source code and ATS log data. This paper describes an effort focused on defining and developing a complete data metrics generation concept for the aggregation and analysis of ATE and TPS data. This is a new capability that parses TPS source code and extract metrics that leverages other sources of data, such as log data and UUT maintenance data. The result of the process is the creation of TPS and ATS component usage metrics that can be used to support usage analyses. As a result of this effort, concept software for metrics extraction and analysis from TPS source code and TPS log files was successfully demonstrated. A key goal of this capability is to show how standard Navy maintenance data for Weapon Replaceable Assembly (WRA) and Subsystem Replaceable Assembly (SRA) testing can be merged with TPS data to determine enterprise utilization and prediction of future utilization based on predicted maintenance demand data.","1558-4550","978-1-5090-0790-5","10.1109/AUTEST.2016.7589596","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7589596","ATS;ATE;TPS;metrics analysis;test resource management;avionics maintenance","Voltage measurement;Maintenance engineering;Instruments;Software;Current measurement;Runtime","automatic test equipment;data analysis;military computing","TPS metrics extraction software;test program set;resource management;ATS planning;ATS support;automatic test system;avionics maintenance;software toolset;data metrics generation concept;data aggregation;data analysis;UUT maintenance data;log data;TPS component usage metric;ATS component usage metric;navy maintenance data;weapon replaceable assembly;WRA;subsystem replaceable assembly;SRA;maintenance demand data","","","","4","","13 Oct 2016","","","IEEE","IEEE Conferences"
"Optimizing business processes by learning from monitoring results","M. L. Sebu; H. Ciocârlie","Computer and Software Engineering Department, Politehnica University of Timisoara, Romania; Computer and Software Engineering Department, Politehnica University of Timisoara, Romania","2016 IEEE 11th International Symposium on Applied Computational Intelligence and Informatics (SACI)","9 Jul 2016","2016","","","43","50","In the current paper we propose analyzing the recorded information about broken rules in business process execution and using it for enhancing business process models defined inside organizations for specific business cases. An alert system is designed as a Web Application for detecting deviations by verifying the event log data of the running cases. We consider it a Business Activity Monitoring solution implementing a custom rule definition language mainly created for ensuring the operational support inside organizations by providing guidance to people as task owners in real time about their actions not compliant with process definition. The current system is able to analyze real-time events with the scope of identifying problems, diagnosing them and generating alerts to recommend managerial action. The main effect should be a better performance of the organization and better business results. We consider that processes are meant to automatize and ease the work and should be commonly understood and accepted by the people contributing inside organizations. A rule constantly broken could indicate that a process difficult to follow in specific areas. Such statistical information about broken rules could represent the input for business process improvements. We also consider as a possibility referencing a process definition instead of a set of rules. The running cases are checked for conformance with the reference process model and if the case is not compliant, a trigger is activated and the information is used further for a possible evolution of business process definitions. The business process model is re-built from monitoring results and statistical information.","","978-1-5090-2380-6","10.1109/SACI.2016.7507417","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7507417","business process;learning;association rules;business activity monitoring","Organizations;Monitoring;Standards organizations;Collaboration;Data mining;Computational modeling","business data processing;Internet;learning (artificial intelligence)","business process model optimization;business cases;Web Application;event log data;business activity monitoring solution;task owners;managerial action;business process improvements;statistical information","","","","7","","9 Jul 2016","","","IEEE","IEEE Conferences"
"Toward full-text searching middleware over hierarchical documents","K. Ma; B. Yang; A. Abraham","Shandong Provincial Key Laboratory of Network Based Intelligent Computing, University of Jinan, China; Shandong Provincial Key Laboratory of Network Based Intelligent Computing, University of Jinan, China; Machine Intelligence Research Labs, Scientific Network for Innovation and Research Excellence, WA, USA","2013 13th International Conference on Intellient Systems Design and Applications","13 Oct 2014","2013","","","194","198","Currently, full-text searching can benefit from the emerging NoSQL databases and traditional indexing tools in the big data era. However, there are some drawbacks of current solutions. On one hand, the indexing documents lack of the hierarchy. On the other hand, big data have become the bottleneck of full-text searching. In the context of big data, we design a full-text searching middleware over hierarchical documents. We discuss the architecture of this middleware in detail. In addition, we propose a structure-independent hierarchical document model to present the hierarchical document. Moreover, the transformation engine is designed to translate the rich files into models. The core log event listener is responsible for capturing the changed documents and push them to the indexing storage at the same time. The experimental results show that our middleware is more advantageous than RDBMS with indexes and RDBMS with Lucene solutions.","2164-7151","978-1-4799-3516-1","10.1109/ISDA.2013.6920734","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6920734","Full-text searching;middleware;hierarchical documents;NoSQL","Middleware;Engines;Real-time systems;Indexes;Open source software","document handling;indexing;information retrieval;middleware;relational databases","full-text searching middleware;NoSQL databases;indexing tools;big data era;indexing documents;structure-independent hierarchical document model;log event listener;RDBMS;Lucene solutions","","2","","12","","13 Oct 2014","","","IEEE","IEEE Conferences"
"Towards Online Discovery of Data-Aware Declarative Process Models from Event Streams","N. Navarin; M. Cambiaso; A. Burattin; F. M. Maggi; L. Oneto; A. Sperduti","University of Padua,Department of Mathematics ""Tullio Levi-Civita"",Padova,Italy; University of Genoa,DIBRIS,Genova,Italy; Technical University of Denmark,Software and Process Engineering,Lyngby,Denmark; Free University of Bozen-Bolzano,Faculty of Computer Science,Bolzano,Italy; University of Genoa,DIBRIS,Genova,Italy; University of Padua,Department of Mathematics ""Tullio Levi-Civita"",Padova,Italy","2020 International Joint Conference on Neural Networks (IJCNN)","28 Sep 2020","2020","","","1","8","In recent years, several techniques have been made available to automatically discover declarative process models from event logs. These techniques are useful to provide a comprehensible picture of the process as opposed to full specifications of process behavior provided by procedural modeling languages. Since many modern systems produce ""big data"" from business process executions, in previous work, a framework for the discovery of LTL-based declarative process models from streaming event data has been proposed. This framework can be used to process events online, as they occur, as a way to deal with large and complex collections of datasets that are impossible to store and process altogether. However, the proposed framework does not take into account data attributes associated with events in the log, which can otherwise provide valuable insights into the rules that govern the process. This paper makes the first proposal to close this gap by presenting a technique for discovering declarative process models from event streams that incorporates both control-flow dependencies and data conditions. Specifically, we use Hoeffding trees to incrementally discover data-aware declarative process models, which are represented as conjunctions of first-order temporal logic expressions. The proposed technique has been validated on a synthetic event log, and on a real-life log of a cancer treatment process.","2161-4407","978-1-7281-6926-2","10.1109/IJCNN48605.2020.9207500","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9207500","online process discovery;data-aware process model;declarative process models","Data mining;Task analysis;Semantics;Data models;Standards;Approximation algorithms","cancer;data mining;medical computing;temporal logic","data-aware declarative process models;event streams;process behavior;procedural modeling languages;big data;business process executions;LTL-based declarative process models;event data;process events;account data attributes;control-flow dependencies;data conditions;synthetic event log;cancer treatment process;Hoeffding trees;first-order temporal logic expressions","","","","51","","28 Sep 2020","","","IEEE","IEEE Conferences"
"An Activity Rule Based Approach to Simulate ADL Sequences","S. Kristiansen; T. P. Plagemann; V. Goebel","Department of Informatics, University of Oslo, Oslo, Norway; Department of Informatics, University of Oslo, Oslo, Norway; Department of Informatics, University of Oslo, Oslo, Norway","IEEE Access","19 Mar 2018","2018","6","","12551","12572","The concept of activities of daily living (ADL) has for many years successfully been used in a broad range of health and health care applications. Recent hardware and software developments suggest that the future use of ADL will not only benefit from the transition from manually created ADL logs to automatic sensor-based activity recognition and logging but also from the transition from manual inspection of ADL sequences to their automatic software-driven analysis. This ADL sequence analysis software will be core part in mission critical systems, like ambient assisted living, to detect for example changing health status. Therefore, proper testing and evaluation of this software is mandatory before its deployment. However, testing requires data sets that include normal ADL sequences, hazards, and various kinds of long term behavioral changes; which means it might require weeks or even months to monitor individuals to capture such ADL sequences. Thus, collecting such data sets is very costly, if feasible at all; and very few data sets are available on-line. Therefore, we present an approach to create the necessary data sets for testing through simulation. The simulation of ADL sequences is based on existing ADL sequences and uses probabilistic activity instigation and durations with a novel concept called activity rules to create data sets for proper testing. Activity rules are used to model how individuals resolve activity conflicts. We implemented these concepts as a discrete event simulator, called ADLSim. The evaluation of ADLsim shows that the simulated ADL sequences are realistic and able to capture the variability and non-predictable behavior found in the real world, and that activity rules can impact simulation results significantly.","2169-3536","","10.1109/ACCESS.2018.2807761","Regional Research Fund Oslofjord through the TRIO Project; Research Council of Norway through the CESAR Project; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8294191","Activities of daily living;activity rules;discrete event simulation;evaluation;modelling","Software;Sequences;Testing;Activity recognition;Senior citizens;Monitoring;Hazards","behavioural sciences computing;computerised instrumentation;data analysis;geriatrics;health care;knowledge based systems;program testing;sensors;software engineering;system monitoring","proper testing;individuals resolve activity conflicts;simulated ADL sequences;simulate ADL;health care applications;automatic software;ADL sequence analysis software;data sets;normal ADL sequences;probabilistic activity instigation;durations;changing health status;activity rule based approach;activities-of-daily living;manually created ADL logs;automatic sensor-based activity recognition;automatic sensor-based activity logging;automatic software-driven analysis;ambient assisted living;ADLSim;long term behavioral changes","","","","32","","19 Feb 2018","","","IEEE","IEEE Journals"
"Analysis of design meetings for understanding software architecture decisions","G. Pedraza-Garcia; H. Astudillo; D. Correal","Departamento de Ingenieria de Sistemas у Computación, Universidad de los Andes, Bogotá, Colombia; Departamento de Informática, Universidad Técnica Federico Santa María, Valparaíso, Chile; Departamento de Ingeniería de Sistemas у Computación, Universidad de los Andes, Bogotá, Colombia","2014 XL Latin American Computing Conference (CLEI)","24 Nov 2014","2014","","","1","10","Software architecture teams use verbal and graphical contributions to evaluate and select alternative design decisions. Usually the rationale behind these decisions is not recorded in the architecture document itself, and many contributions and rationale are lost to future reviewers and builders. This article describes Design Verbal Interventions Analysis (DVIA), which applies verbal protocol analysis to transcribed meetings logs to classify verbal interventions as either issue, orientation, mandatory orientation, request for clarification, explanation, disagreement, constraint, assessment, choice, or assumption. The approach is illustrated with a case study using transcribed meetings from undergraduate student teams designing a command-and-control center for a pan-Andean spatial project. The study shows that much of the interaction supports coevolution of problem understanding and solution framing, with designers engaged in simultaneous resolution of several design issues, following a cyclical structure. This strategy will allow to retrieve software architecture decisions from architecture team meetings.","","978-1-4799-6130-6","10.1109/CLEI.2014.6965159","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6965159","Software architecture design;Architectural decisions;Design meetings;Design Verbal Interventions Analysis","Computer architecture;Protocols;Software architecture;Cognition;Software systems;Vehicles","command and control systems;software architecture","design meetings;software architecture design decisions;design verbal intervention analysis;DVIA;verbal protocol analysis;transcribed meeting logs;command-and-control center;pan-Andean spatial project;architecture team meetings","","4","","42","","24 Nov 2014","","","IEEE","IEEE Conferences"
"Mining Logs to Model the Use of a System","D. Gadler; M. Mairegger; A. Janes; B. Russo","Free Univ. of Bozen-Bolzano, Bolzano, Italy; Schwer Prazision, Rodeneck, Italy; Free Univ. of Bozen-Bolzano, Bolzano, Italy; Free Univ. of Bozen-Bolzano, Bolzano, Italy","2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","11 Dec 2017","2017","","","334","343","Background. Process mining is a technique to build process models from ""execution logs"" (i.e., events triggered by the execution of a process). State-of-the-art tools can provide process managers with different graphical representations of such models. Managers use these models to compare them with an ideal process model or to support process improvement. They typically select the representation based on their experience and knowledge of the system. Aim. This work studies how to automatically build process models representing the actual intents (or uses) of users while interacting with a software system. Such intents are expressed as a set of actions performed by a user to a system to achieve specific use goals. Method. This work applies the theory of Hidden Markov Models to mine use logs and automatically model the use of a system. Results. Unlike the models generated with process mining tools, the Hidden Markov Models automatically generated in this study provide the intents of a user and can be used to recommend managers with a faithful representation of the use of their systems. Conclusions. The automatic generation of the Hidden Markov Models can achieve a good level of accuracy in representing the actual user's intents provided the log dataset is carefully chosen. In our study, the information contained in one-month set of logs helped automatically build Hidden Markov Models with superior accuracy and similar expressiveness of the models built together with the company's stakeholder.","","978-1-5090-4039-1","10.1109/ESEM.2017.47","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8170120","Log analysis;Process modelling;Hidden Markov Chain","Hidden Markov models;Tools;Companies;Mobile communication","business data processing;data mining;hidden Markov models","mining logs;process models;process managers;ideal process model;process improvement;software system;Hidden Markov Models;process mining tools","","2","","15","","11 Dec 2017","","","IEEE","IEEE Conferences"
"Mass log data processing and mining based on Hadoop and cloud computing","H. Yu; D. Wang","State Key Laboratory of Software Architecture, Neusoft Corporation, Neusoft Park, No.2 Xinxiu Street, Hunnan New District, Shenyang 110179, China; State Key Laboratory of Software Architecture, Neusoft Corporation, Neusoft Park, No.2 Xinxiu Street, Hunnan New District, Shenyang 110179, China","2012 7th International Conference on Computer Science & Education (ICCSE)","6 Sep 2012","2012","","","197","202","With the rapid development of the Internet, SaaS applications delivered as services through internet become an important alternative of traditional software. While using the services, users need real time usage information, and they also need to dig out useful knowledge. As a result, data processing and data mining techniques are designed to cope with such problems, and using log data is an effective method to record the SaaS usage information in a standard format. However, as the size of data grows, traditional distributed log data processing systems are not able to processing massive log data from SaaS applications with millions of users. This paper proposes a mass log data processing and data mining methods based on Hadoop to achieve scalability and performance. The model, process, architecture, and implementation of the data processing and mining methods are proposed, and the experimental results is shown and analyzed to prove the effectiveness of the methods.","","978-1-4673-0242-5","10.1109/ICCSE.2012.6295056","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6295056","mass data processing;data mining;real time statistics;business intelligence;Hadoop","Data mining;Data processing;Algorithm design and analysis;Real time systems;Servers;Distributed databases","cloud computing;data mining;distributed processing","mass log data processing;mass log data mining;cloud computing;Hadoop computing;Internet;SaaS applications;distributed log data processing systems","","8","6","11","","6 Sep 2012","","","IEEE","IEEE Conferences"
"A Performance Analysis of Large Scale Scientific Computing Applications from Log Archives","L. Cao; X. Liu; X. Xu; Z. Liu",Institute of Applied Physics and Computational Mathematics; Institute of Applied Physics and Computational Mathematics; Institute of Applied Physics and Computational Mathematics; Institute of Applied Physics and Computational Mathematics,"2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)","29 Jul 2019","2019","","","406","411","A log archive for scientific computing applications is a set of logs for model and time of jobs in HPCs. We have developed light weight and fast performance analysis tools on top of log archives. We classify the job logs based on the similarity of the input models to form a model-based tree like archive. With linear regression, we analyze the relations of the step time of the jobs with the parameters in the model. We found that although there is some disturbance, the performance of most of the jobs showed good regularity. In one of the applications, we found the step time of job changes proportionally to the geometric parameters of model. And the most significant physical parameter determines step time up to 1.7 times. In another application, we find that the performance of each step scales 1.59 times with the number of process scales from 384 to 768.","","978-1-7281-3510-6","10.1109/IPDPSW.2019.00079","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8778217","performance analysis;log archive;linear regression;performance predict","Computational modeling;Scientific computing;Tools;Analytical models;Hardware;File systems","geometry;parallel processing;regression analysis;software performance evaluation;trees (mathematics)","performance analysis tools;log archives;model-based tree like archive;geometric parameters;parallel scientific computing applications;linear regression","","","","16","","29 Jul 2019","","","IEEE","IEEE Conferences"
"Discovering Business Processes in Legacy Systems Using Business Rules and Log Mining","G. S. D. Nascimento; C. Iochpe; L. Thom; A. C. Kalsing; G. S. d. Nascimento","NA; Inf. Dept., Fed. Univ. of Rio Grande do Sul, Porto Alegre, Brazil; Inf. Dept., Fed. Univ. of Rio Grande do Sul, Porto Alegre, Brazil; Inf. Dept., Fed. Univ. of Rio Grande do Sul, Porto Alegre, Brazil; NA","2013 IEEE 10th International Conference on e-Business Engineering","19 Dec 2013","2013","","","207","212","This paper shows a method to semi-automate the discovery of business processes implemented implicitly in the source code of legacy systems. The method identifies the encoding of the business activities through the business rules implemented in legacy source code. In this paper we propose a tool to executes the source code instrumentation, identifying the business rules implemented in the legacy system and enabling the creation of event logs. This allows using log mining techniques to discover the partial order of execution of the business rules. Thus it is possible to discover business processes implemented implicitly in the source code of legacy systems. This paper also presents a case study conducted in a medical information system.","","978-0-7695-5111-1","10.1109/ICEBE.2013.31","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6686264","business rules;legacy system;business processes;process mining","Conferences","business data processing;data mining;software maintenance;source code (software);system monitoring","medical information system;log mining techniques;event logs;source code instrumentation;legacy source code;business activity encoding;business rules;legacy systems;business processes discovery","","","","20","","19 Dec 2013","","","IEEE","IEEE Conferences"
"A Novel Approach of Unprivileged Keylogger Detection","A. Wajahat; A. Imran; J. Latif; A. Nazir; A. Bilal","School of Software Engineering, Beijing University of Technology, Beijing, 100124, China; School of Software Engineering, Beijing University of Technology, Beijing, 100124, China; School of Software Engineering, Beijing University of Technology, Beijing, 100124, China; School of Software Engineering, Beijing University of Technology, Beijing, 100124, China; School of Software Engineering, Beijing University of Technology, Beijing, 100124, China","2019 2nd International Conference on Computing, Mathematics and Engineering Technologies (iCoMET)","25 Mar 2019","2019","","","1","6","Nowadays, computers are used everywhere to carry out daily routine tasks. The input devices i.e. keyboard or mouse are used to feed input to computers. The surveillance of input devices is much important as monitoring the users logging activity. A keylogger also referred as a keystroke logger, is a software or hardware device which monitors every keystroke typed by a user. Keylogger runs in the background that user cannot identify its presence. It can be used as monitoring software for parents to keep an eye on children activity on computers and for the owner to monitor their employees. A keylogger (which can be either spyware or software) is a kind of surveillance software that has the ability to store every keystroke in a log file. It is very dangerous for those systems which use their system for daily transaction purpose i.e. Online Banking Systems. A keylogger is a tool, made to save all the keystroke generated through the machine which sanctions hackers to steal sensitive information without user's intention. Privileged also relies on the access for both implementation and placement by Kernel keylogger, the entire message transmitted from the keyboard drivers, while the programmer simply relies on kernel level facilities that interrupt. This certainly needs a large power and expertise for real and error-free execution. However, it has been observed that 90% of the current keyloggers are running in userspace so they do not need any permission for execution. Our aim is focused on detecting userspace keylogger. Our intention is to forbid userspace keylogger from stealing confidential data and information. For this purpose, we use a strategy which is clearly based on detection manner techniques for userspace keyloggers, an essential category of malware packages. We intend to achieve this goal by matching I/O of all processes with some simulated activity of the user, and we assert detection in case the two are highly correlated. The rationale behind this is that the more powerful stream of keystrokes, the more I/O operations are required by the keylogger to log the keystrokes into the file.","","978-1-5386-9509-8","10.1109/ICOMET.2019.8673404","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8673404","Keylogger userspace;privileged;keystroke simulation;security","Kernel;Keyboards;Computers;Microsoft Windows;Monitoring","authorisation;invasive software;mobile computing;operating system kernels","keystroke logger;monitoring software;spyware;surveillance software;Kernel keylogger;unprivileged keylogger detection;userspace keylogger detection;confidential data stealing;malware packages","","1","","17","","25 Mar 2019","","","IEEE","IEEE Conferences"
"Logging mechanism for Internet of Things: A Case Study of Patient Monitoring System","P. Maneenual; S. Vasupongayya","Management of Information Technology, Prince of Songkla University, Songkhla, Hat Yai, Thailand; Department of Computer Engineering, Prince of Songkla University, Songkhla, Hat Yai, Thailand","2018 15th International Joint Conference on Computer Science and Software Engineering (JCSSE)","11 Sep 2018","2018","","","1","6","A logging mechanism for NETPIE in the patient medical device monitoring task is proposed in this work. The logging mechanism aims to collect the communication log between things and NETPIE such that any communication issue or any attack can be detected. There are three main components in the proposed logging mechanism including the raw data, the analysis part, and the final result. The raw data is approximately less than 256 bytes per each communication. The raw data will be analyzed to generate the final result which will contain only the suspicion events including communication issue (i.e., packet lost) and possible attack (i.e., reply attack). The cost of the proposed mechanism includes an extra communication per each communication path and a storage space for the collected data at NETPIE, and things.","","978-1-5386-5538-2","10.1109/JCSSE.2018.8457390","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8457390","NETPIE;packet lost;log analysis;Apache Cassandra","Patient monitoring;Internet;Biomedical monitoring;Medical services;Computer science;Software engineering","biomedical communication;Internet of Things;medical computing;patient monitoring","logging mechanism;patient monitoring system;NETPIE;patient medical device monitoring task;communication issue;Internet of Things","","2","","18","","11 Sep 2018","","","IEEE","IEEE Conferences"
"Towards Flexible Event-Handling in Workflows through Data States","J. E. Ferreira; Q. Wu; S. Malkowski; C. Pu","Comput. Sci. Dept., Univ. of Sao Paulo, São Paulo, Brazil; Center for Exp. Res. in Comput. Syst., Georgia Inst. of Technol., Atlanta, GA, USA; Center for Exp. Res. in Comput. Syst., Georgia Inst. of Technol., Atlanta, GA, USA; Center for Exp. Res. in Comput. Syst., Georgia Inst. of Technol., Atlanta, GA, USA","2010 6th World Congress on Services","16 Sep 2010","2010","","","344","351","Despite recent advances in many real-time and workflow management systems (WFMS), event-handling is still a manual or semi-automated task. The integration of automated event processing with workflows remains an open research challenge to both academic and industrial communities. In this work, we propose a concrete approach that logs interactions between workflow component activities in the form of data states that accurately and efficiently store necessary information for event-handling. Our approach (called WED-flow) explicitly represents various dependencies and constraints of a WFMS in sophisticated data states. Due to the availability of this large amount of historic information, our approach is able to support a flexible event-handling in WFMS. In this paper we present definitions for workflow management systems that incorporate events, and characterize such systems using the WED-flow approach. We also present a scientific workflow example in genetic testing to illustrate the advantages of integrating events with workflow through the WED-flow approach.","2378-3818","978-1-4244-8199-6","10.1109/SERVICES.2010.60","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5577256","data-flow;data state;control-flow;event-handling;transaction model;scientific workflow","Business;DNA;Process control;Atrophy;Adaptation model;Workflow management software","data handling;real-time systems;workflow management software","flexible event-handling;workflow management systems;automated event processing;WED-flow approach;genetic testing","","7","","30","","16 Sep 2010","","","IEEE","IEEE Conferences"
"Revealing Students' UML Class Diagram Modelling Strategies with WebUML and LogViz","D. R. Stikkolorum; T. Ho-Quang; M. R. V. Chaudron","LIACS, Leiden Univ., Leiden, Netherlands; Chalmers & Gothenburg Univ., Gothenburg, Sweden; Chalmers & Gothenburg Univ., Gothenburg, Sweden","2015 41st Euromicro Conference on Software Engineering and Advanced Applications","26 Oct 2015","2015","","","275","279","This paper aims to reveal the most common strategies students use to create class designs. We show our approach of logging students' modelling activities while doing a software design task. We developed our own online modelling editor 'Web UML' and visualisation tool 'Log Viz' for the logging and interpretation of the log files. As follow-up students filled-in a brief questionnaire targeting their time spent and difficulties in performing the task. The results show that the students use different strategies for solving the tasks. We categorised these strategies into four main strategies: Depthless, Depth First, Breadth First and Ad Hoc. From our results Depth First indicates to support better layout and richness (detail). From the questionnaire students mention choosing the appropriate UML elements is a difficult and time consuming task. We want to use our insights to improve our educational programs and tools. In the future we want to test Web UML and Log Viz in larger educational contexts.","2376-9505","978-1-4673-7585-6","10.1109/SEAA.2015.77","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7302463","students' modelling strategies;education;software design;UML;activity logging;tracking","Unified modeling language;Visualization;Software design;Layout;Object oriented modeling;Analytical models","computer aided instruction;data visualisation;Internet;software engineering;student experiments;task analysis;Unified Modeling Language","UML class diagram modelling strategies;WebUML;LogViz;students modelling activities;software design task;visualisation tool;educational contexts","","5","","10","","26 Oct 2015","","","IEEE","IEEE Conferences"
"A CSP-theoretic Framework of Checking Conformance of Business Processes","S. Roy; S. Bihary; J. A. C. Laos","Infosys Labs., Infosys Ltd., Bangalore, India; Infosys Labs., Infosys Ltd., Bangalore, India; Dept. of Ind. Eng., Pontificia Univ. Catolica De Chile, Santiago, Chile","2012 19th Asia-Pacific Software Engineering Conference","18 Feb 2013","2012","1","","30","39","In this paper, we tackle the problem of conformance checking which verifies if the event logs (observed) match/fit the reference (arbitrary) process. We use concepts from Communicating Sequential Processes (CSP), which facilitates automated analysis using PAT toolkit. By this technique one can identify all the logs which cannot be properly replayed on the process. We illustrate our approach with an example. Finally, we introduce some metrics based on conformance checking. They are related to fitness, closeness, and appropriateness of the event logs vis-a-vis reference process models.","1530-1362","978-1-4673-4930-7","10.1109/APSEC.2012.110","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6462635","Formal methods;Software verification and validation;conformance checking;BPMN;Business Processes;CSP;PAT toolkit;Trace refinement","Business;Unified modeling language;Synchronization;Process control;Logic gates;System recovery;Standards","business process re-engineering;communicating sequential processes;formal verification;information systems","CSP theoretic framework;business process checking conformance;conformance checking;communicating sequential processes;PAT toolkit;event log fitness;event log closeness;event log appropriateness","","1","","23","","18 Feb 2013","","","IEEE","IEEE Conferences"
"On the Practical Feasibility of Software Monitoring: a Framework for Low-Impact Execution Tracing","J. Mertz; I. Nunes",Universidade Federal do Rio Grande do Sul (UFRGS); Universidade Federal do Rio Grande do Sul (UFRGS),"2019 IEEE/ACM 14th International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS)","5 Aug 2019","2019","","","169","180","In order for a software system to self-adapt, it often needs to be aware of its behavior. A typical way of achieving this is by means of the runtime collection of execution traces, which requires the interception of the execution of, e.g. methods, and record information about them. Although this is simple in theory, in practice, it can be very costly because it might have a significant impact on the application performance or require huge amounts of memory or storage. This becomes a significant issue mainly in real-time applications, which are time-sensitive and must often meet deadlines in resource-constrained environments. We thus in this paper propose a two-phase tracing framework to cope with the monitoring overhead. In its first phase, the application is monitored in a lightweight fashion providing information for the second phase, which identifies an adaptive configuration and samples traces according to the current configuration. The adaptive configuration is determined by a set of criteria, specified through a proposed domain-specific language. We empirically evaluate our framework by instantiating it as a reduced-overhead monitoring solution, integrated into an existing automated application-level caching approach. We demonstrate that our solution reduces the overhead caused by monitoring, without compromising the performance improvements provided by the caching approach.","2157-2321","978-1-7281-3368-3","10.1109/SEAMS.2019.00030","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8787058","monitoring;logging;execution trace;sampling;caching;performance","Monitoring;Runtime;Software;Measurement;Synthetic aperture sonar;Instruments;Computer bugs","cache storage;specification languages;system monitoring","adaptive configuration;reduced-overhead monitoring solution;software monitoring;low-impact execution tracing;software system;runtime collection;execution traces;record information;application performance;real-time applications;resource-constrained environments;two-phase tracing framework;monitoring overhead;lightweight fashion;automated application-level caching approach","","","","45","","5 Aug 2019","","","IEEE","IEEE Conferences"
"PULSE: A Framework for Protocol Based Utility to Log Student Engagement","R. Majumdar; A. Kothiyal",NA; NA,"2013 IEEE Fifth International Conference on Technology for Education (t4e 2013)","3 Mar 2014","2013","","","159","162","Educational researchers use observations to collect information about classroom activities as they occur. Currently the most prevalent methods of logging classroom observations are pen-and-paper or video recording. The former is inefficient, especially in large classes while the latter requires expensive infrastructure. In this paper we propose the design framework for the development of a software application for protocol-based multi-observer data logging and analysis called PULSE. Following design-based research we developed the four aspects of this framework. They were iteratively improved over the research cycles based on the feedback of the previous cycles. In this work-in progress paper, we present an ongoing design prototype of the application.","","978-0-7695-5141-8","10.1109/T4E.2013.46","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6751085","Observation protocol;Design framework;Mobile Application;Educational Research","Protocols;Observers;Usability;Computer architecture;Prototypes;Interviews","behavioural sciences computing;data recording;graphical user interfaces","PULSE framework;information collection;classroom activities;classroom observation logging;software application;protocol-based multiobserver data analysis;design-based educational research;research cycle improvement;protocol-based utility-to-log student engagement;protocol-based multiobserver data logging","","","","17","","3 Mar 2014","","","IEEE","IEEE Conferences"
"Antecedence Graph Approach to Checkpointing for Fault Tolerance in Mobile Agent Systems","R. Singh; M. Dave","National Institute of Technology, Kurukshetra; National Institute of Technology, Kurukshetra","IEEE Transactions on Computers","20 Dec 2012","2013","62","2","247","258","The flexibility offered by mobile agents is quite noticeable in distributed computing environments. However, the greater flexibility of the mobile agent paradigm compared to the client/server computing paradigm comes at an additional threats since agent systems are prone to failures originating from bad communication, security attacks, agent server crashes, system resources unavailability, network congestion, or even deadlock situations. In such events, mobile agents either get lost or damaged (partially or totally) during execution. In this paper, we propose parallel checkpointing approach based on the use of antecedence graphs for providing fault tolerance in mobile agent systems. During normal computation message transmission, the dependency information among mobile agents is recorded in the form of antecedence graphs by participating mobile agents of mobile agent group. When a checkpointing procedure begins, the initiator concurrently informs relevant mobile agents, which minimizes the identifying time. The proposed scheme utilizes the checkpointed information for fault tolerance which is stored in form of antecedence graphs. In case of failures, using checkpointed information, the antecedence graphs and message logs are regenerated for recovery and then normal operation continued. Moreover, compared with the existing schemes, our algorithm involves the minimum number of mobile agents during the identifying and checkpoiting procedure, which leads to the improvement of the system performance. In addition, the proposed algorithm is a domino-free checkpointing algorithm, which is especially desirable for mobile agent systems. Quantitative analysis and experimental simulation show that our algorithm outperforms other coordinated checkpointing schemes in terms of the identifying time and the number of blocked mobile agents and then can provide a better system performance. The main contribution of the proposed checkpointing scheme is the enhancement of graph-based approach in terms of considerable improvement by reducing message overhead, execution, and recovery times.","1557-9956","","10.1109/TC.2011.235","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6109235","Mobile agents;fault tolerance;reliability;failure;antecedence graphs;checkpointing;message logs","Mobile agents;Checkpointing;Fault tolerance;Fault tolerant systems;Servers;Protocols","distributed processing;formal verification;graph theory;mobile agents;parallel processing;software fault tolerance","antecedence graph approach;fault tolerance checkpointing;mobile agent systems;distributed computing environments;client/server computing;bad communication;security attacks;agent server crashes;network congestion;deadlock situations;parallel checkpointing approach;message transmission;message logs","","9","","30","","20 Dec 2011","","","IEEE","IEEE Journals"
"A Read-Optimized Index Structure for Distributed Log-Structured Key-Value Store","I. Kang; B. Kim; D. Lee","Dept. of Comput. Sci. & Eng., Hanyang Univ., Seoul, South Korea; Dept. of Comput. Sci. & Eng., Hanyang Univ., Seoul, South Korea; Dept. of Comput. Sci. & Eng., Hanyang Univ., Seoul, South Korea","2015 IEEE 39th Annual Computer Software and Applications Conference","24 Sep 2015","2015","3","","650","651","Recently, Big Data processing is becoming a necessary technique to efficiently store, manage, and analyze massive data obtained by social media contents. NoSQL is one of databases that efficiently handle Big Data compared to the traditional database that has the limitation to manipulate Big Data. Log-structured key/value store, widely used in NoSQL, basically stores data into the disk storage in batch writing. Since this batch writing of the key/value store does not overwrite data in place, many data are accumulated in several places. Although it improves the write performance, the read performance decreases because the key/value store requires many accesses to widely-spread data. In order to address this problem, we propose T-tree index structure to reduce the search time by avoiding exploring contents stored in distributed many files. Finally, we show the performance improvement through the experimental results.","0730-3157","978-1-4673-6564-2","10.1109/COMPSAC.2015.85","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7273449","Bigdata;NoSQL;key-value store;index;T-tree;database","Indexes;Big data;Distributed databases;Conferences;Writing;Computers","Big Data;data analysis;social networking (online);tree data structures","read-optimized index structure;distributed log-structured key-value store;Big Data processing;data analysis;social media contents;NoSQL;disk storage;batch writing;T-tree index structure","","","","6","","24 Sep 2015","","","IEEE","IEEE Conferences"
"A Method to Measure User Influence in Social Network Based on Process Log","H. Wang; S. Liu; H. Yu; Y. Lu","Coll. of Manage., ShenZhen Univ., Shenzhen, China; Coll. of Manage., ShenZhen Univ., Shenzhen, China; Coll. of Comput. Sci. & Software Eng., ShenZhen Univ., Shenzhen, China; Coll. of Comput. Sci. & Software Eng., ShenZhen Univ., Shenzhen, China","2014 IEEE 11th International Conference on e-Business Engineering","11 Dec 2014","2014","","","338","343","Social network is a graph composed by actors and relationships between these actors. In the workflow management systems, there are a large number of logs which record the activity related information during process execution. The executors' relationships between these activities can be considered as a kind of social relations to establish the corresponding social network models. In this paper, we summarize up four different kinds of social network models based on the characteristics of process logs. Then we propose a user influence measure method to better understand the organization structure of the company. Finally, we use our method to analyze the situation based on practical process logs of a company. Our method gives us a new perspective to analyze organization structure from workflow process logs.","","978-1-4799-6563-2","10.1109/ICEBE.2014.65","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6982103","Organization Structure;Social Network;Workflow;Process Log","Social network services;Measurement;Companies;Educational institutions;Runtime","graph theory;organisational aspects;social networking (online)","workflow management systems;activity related information;process execution;social relations;social network models;user influence measure method;organization structure;workflow process log characteristics","","","","15","","11 Dec 2014","","","IEEE","IEEE Conferences"
"Survivability prediction of web system based on log statistics","J. Zhou; H. Miao; J. Kai; K. Zhao; H. Gao","School of Computer Engineering and Science, Shanghai University, Shanghai 200072, China; School of Computer Engineering and Science, Shanghai University, Shanghai 200072, China; School of Computer Engineering and Science, Shanghai University, Shanghai 200072, China; School of Computer Engineering and Science, Shanghai University, Shanghai 200072, China; School of Computer Engineering and Science, Shanghai University, Shanghai 200072, China","2015 IEEE/ACIS 16th International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)","6 Aug 2015","2015","","","1","6","Widely applied and quickly developed as the SOA theory has been, the instability of distributed Web services will lead to services composition failure. Currently, a hot research topic is that when does the system can make an appropriate adjustment of the system structure dynamically to ensure the system runs at the best performance while the runtime environment or requirement is changed. To address this problem, this paper proposes an approach to predicting the system survivability which bases on log statistic. The method gets the system usage model by monitoring Web log files, and then constructs value model and adopts quantitative model checking to forecast the system survivability to estimate whether the system is survivable in a certain period of time.","","978-1-4799-8676-7","10.1109/SNPD.2015.7176170","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7176170","Web service;prediction;Web log;survivability;quantitative model checking","Model checking;Business;Markov processes;Data models;Web services;Predictive models;Probabilistic logic","formal verification;reliability;service-oriented architecture;Web services","survivability prediction;Web system;SOA theory;Web service;Web log file monitoring;quantitative model checking","","","","15","","6 Aug 2015","","","IEEE","IEEE Conferences"
"Discovery and Quality Evaluation of Software Component Behavioral Models","C. Liu","School of Computer Science and Technology, Shandong University of Technology, Zibo 255000, China (e-mail: liucongchina@sdust.edu.cn)","IEEE Transactions on Automation Science and Engineering","","2020","PP","99","1","12","Tremendous amounts of execution data are collected during software execution. These data provide rich information for software runtime behavior comprehension. The unstructured execution data may be too complex, involving multiple interleaved components and so on. Applying existing process discovery techniques results in spaghetti-like models with no clear structure and no valuable information that can be easily understood by end users. In this article, we start with the observation that a software system is composed of a group of components, and we use this information to decompose the problem into smaller independent ones by discovering a behavioral model per component. To this end, we first distill a software event log for each component from the raw software execution data. Then, we construct the hierarchical software event log by recursively applying caller-and-callee relation detection. Next, component behavioral models, represented as hierarchical Petri nets, are discovered by recursively applying existing process discovery techniques. To measure the quality of discovered models against the execution data, we transform hierarchical Petri nets to flat ones, and the quality metrics, e.g., fitness, precision, and complexity, are applied. All proposed approaches have been implemented in the open-source process mining toolkit ProM. Through the experimental evaluation using both synthetic software systems and open-source software systems, we illustrate that the proposed approach facilitates the discovery of more understandable and high-quality software behavioral models.","1558-3783","","10.1109/TASE.2020.3008897","National Natural Science Foundation of China; Taishan Scholars Program of Shandong Province; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9190056","Component behavioral model;component instance identification;hierarchical Petri nets;process discovery;software execution data.","Software systems;Data mining;Petri nets;Data models;Analytical models;Runtime","","","","","","","","9 Sep 2020","","","IEEE","IEEE Early Access Articles"
"Cloud Storage Behavior Analysis Using Time Series Clustering","W. Yao; S. Ma; J. Lv; Y. Liu","State Key Laboratory of Software Development Environment, Beihang University, Beijing 100083, China; State Key Laboratory of Software Development Environment, Beihang University, Beijing 100083, China; State Key Laboratory of Software Development Environment, Beihang University, Beijing 100083, China; State Key Laboratory of Software Development Environment, Beihang University, Beijing 100083, China","2018 5th IEEE International Conference on Cloud Computing and Intelligence Systems (CCIS)","14 Apr 2019","2018","","","90","95","To meet with different storage requirements, lots of products are provided by cloud service providers. These products vary greatly in price and performance. Choosing different storage services to build tiered storage systems could reduce overall costs of using of cloud services. But choosing of storage services is not easy because it must meet storage performance requirements and minimize the costs. We use machine leaning methods to solve this problem. Log files that keep storage access traits are used to analyze storage access patterns. Firstly, we process log files and generate access frequency time series, which are translated to N-Hot time series later. Then we extract feathers from N-Hot time series and use K-Means clustering method to classify storage objects. Different migration policies could be made to optimize storage usages according to these different classes of storage objects. The experiments show that our approach is useful, well performance and scalable.","","978-1-5386-6005-8","10.1109/CCIS.2018.8691369","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8691369","big data analytics;time series;machine learning;clustering;tiered storage","","cloud computing;pattern clustering;storage management;time series","cloud service providers;tiered storage systems;storage performance requirements;log files;storage access traits;storage access patterns;access frequency time series;N-Hot time series;storage objects;storage usages;cloud storage behavior analysis;time series clustering;storage requirements;storage services;migration policies;K-Means clustering method","","","","22","","14 Apr 2019","","","IEEE","IEEE Conferences"
"Detecting and thwarting hardware trojan attacks in cyber-physical systems","V. Venugopalan; C. D. Patterson; D. M. Shila","United Technologies Research Center, E Hartford, CT USA; Bradley Department of ECE, Virginia Tech, Blacksburg, USA; United Technologies Research Center, E Hartford, CT USA","2016 IEEE Conference on Communications and Network Security (CNS)","23 Feb 2017","2016","","","421","425","Cyber-physical system integrity requires both hardware and software security. Many of the cyber attacks are successful as they are designed to selectively target a specific hardware or software component in an embedded system and trigger its failure. Existing security measures also use attack vector models and isolate the malicious component as a counter-measure. Isolated security primitives do not provide the overall trust required in an embedded system. Trust enhancements are proposed to a hardware security platform, where the trust specifications are implemented in both software and hardware. This distribution of trust makes it difficult for a hardware-only or software-only attack to cripple the system. The proposed approach is applied to a smart grid application consisting of third-party soft IP cores, where an attack on this module can result in a blackout. System integrity is preserved in the event of an attack and the anomalous behavior of the IP core is recorded by a supervisory module. The IP core also provides a snapshot of its trust metric, which is logged for further diagnostics.","","978-1-5090-3065-1","10.1109/CNS.2016.7860530","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7860530","Design;Security;Trust;Reliability;Verification","Security;IP networks;Hardware;Trojan horses;Measurement;Field programmable gate arrays","cyber-physical systems;invasive software;trusted computing","thwarting hardware trojan attacks;cyber-physical system integrity;software security;hardware component;software component;embedded system;security measures;attack vector models;counter-measure;isolated security primitives;trust enhancements;hardware security platform;trust specifications;hardware-only attack;software-only attack;smart grid application;third-party soft IP cores;supervisory module;trust metric;diagnostics","","3","","26","","23 Feb 2017","","","IEEE","IEEE Conferences"
"Enabling Patient Information Handoff from Pre-hospital Transport Providers to Hospital Emergency Departments: Design-Science Approach to Field Testing","A. Murad; B. Schooley; T. Horan; Y. Abed","Claremont Grad. Univ., Claremont, CA, USA; Univ. of South Carolina, Columbia, SC, USA; Claremont Grad. Univ., Claremont, CA, USA; Claremont Grad. Univ., Claremont, CA, USA","2014 47th Hawaii International Conference on System Sciences","10 Mar 2014","2014","","","2665","2674","The transfer of complete patient information between EMS personnel and hospitals ED staff is a major challenge in emergency medical care. This study used a design science methodology to design, demonstrate in use, and field test a mobile and web based EMS software solution that provides textual and multimedia information for emergency responses. The system was field tested for a period of three months in rural Minnesota. A mixed method approach was employed to assess the system use and perceptions of value in patient handoff. Data was drawn from system log files over a 3-month period and in-depth interviews conducted at the end of the study with an equal number of representatives from EMS personnel and ED staff. Findings suggest the use of mobile and web based EMS solutions may be more appropriate in rural settings with long transport times and for more sever incidents - where participants found value in the use of information for patient pre-registrations and early notifications. A systematic longer-term testing of clinical use of the system is suggested as the next step in further demonstrating the value of such a mobile solution.","1530-1605","978-1-4799-2504-9","10.1109/HICSS.2014.336","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6758936","mHealth;emergency medical services;mobile computing;multimedia;Design Science Research;Field Testing","Mobile communication;Context;Standards;Hospitals;Audio recording","health care;hospitals;Internet;medical information systems;mobile computing;patient care;personnel","patient information handoff;pre-hospital transport providers;hospital emergency departments;design-science approach;field testing;EMS personnel;hospitals ED staff;emergency medical care;design science methodology;Web based EMS software solution;mobile EMS software solution;multimedia information;emergency responses;textual information;rural Minnesota;mixed method approach;system log files;systematic longer-term testing;pre-hospital emergency medical services","","1","","43","","10 Mar 2014","","","IEEE","IEEE Conferences"
"High Resolution Program Flow Visualization of Hardware Accelerated Hybrid Multi-core Applications","D. Hackenberg; G. Juckeland; H. Brunst","Center for Inf. Services & High Performance Comput. (ZIH), Tech. Univ. Dresden, Dresden, Germany; Center for Inf. Services & High Performance Comput. (ZIH), Tech. Univ. Dresden, Dresden, Germany; Center for Inf. Services & High Performance Comput. (ZIH), Tech. Univ. Dresden, Dresden, Germany","2010 10th IEEE/ACM International Conference on Cluster, Cloud and Grid Computing","24 Jun 2010","2010","","","786","791","The advent of multi-core processors has made parallel computing techniques mandatory on main stream systems. With the recent rise of hardware accelerators, hybrid parallelism adds yet another dimension of complexity to the process of software development. This article presents a tool for graphical program flow analysis of hardware accelerated parallel programs. It monitors the hybrid program execution to record and visualize many performance relevant events along the way. Representative real-world applications written for both IBM's Cell processor and NVIDIA's CUDA API are studied exemplarily. To the best of our knowledge, this approach is the first that visualizes the parallelism in hybrid multi-core systems at the presented level of detail.","","978-1-4244-6988-8","10.1109/CCGRID.2010.27","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5493388","event logging;tracing;monitoring;multi-core;many-core;Cell/BE;GPU;CUDA;MPI;performance visualization","Visualization;Hardware;Acceleration;Parallel processing;Application software;Multicore processing;Programming;Concurrent computing;Monitoring","application program interfaces;data analysis;data visualisation;multiprocessing systems;parallel programming","program flow visualization;hybrid multicore applications;multicore processors;parallel computing techniques;hardware accelerators;graphical program flow analysis;IBM Cell processor;NVIDIA CUDA API;computer unified device architecture;application program interfaces;software development","","5","","15","","24 Jun 2010","","","IEEE","IEEE Conferences"
"Using Resource Use Data and System Logs for HPC System Error Propagation and Recovery Diagnosis","E. Chuah; A. Jhumka; S. Alt; J. J. Villalobos; J. Fryman; W. Barth; M. Parashar",The Alan Turing Institute & The University of Warwick; The University of Warwick & The Alan Turing Institute; Intel Corporation; Rutgers University; Intel Corporation; Texas Advanced Computing Center; Rutgers University,"2019 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)","26 Mar 2020","2019","","","458","467","Analyzing failures is important for the reliability of HPC systems and failure diagnosis based only on system logs is incomplete. Resource use data - made available recently - is another potential source of data for failure analysis. Recent work that combines analysis of system logs with resource use data show promising results. In this paper, we describe a new workflow for combining system resource usage and failure logs for diagnosis. The workflow - called EXERMEST - identifies significant system counters and events then correlates them to failures and recovery. We apply EXERMEST on the Ranger HPC system cluster log-data and show that it improves diagnosis over previous research. EXERMEST: (i) show that more system counters and errors can be identified only by applying more feature extractors, (ii) identify CPU I/O bottlenecks and Lustre client eviction, (iii) identify network packet drops and Lustre I/O errors, (iv) identify virtual memory and harddisk I/O errors, (v) show that time-bins of different granularities are required for identifying the errors. EXERMEST is available on the public domain for supporting system administrators in failure diagnosis.","","978-1-7281-4328-6","10.1109/ISPA-BDCloud-SustainCom-SocialCom48970.2019.00072","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9047253","HPC;Feature extraction;Correlation;Error propagation and recovery;Resource use data and system logs","Correlation;Feature extraction;Tools;Principal component analysis;Memory management;Data centers;Cloud computing","failure analysis;fault diagnosis;feature extraction;input-output programs;parallel processing;software reliability;storage management;system monitoring;system recovery","system logs;HPC system error propagation;recovery diagnosis;HPC systems;failure diagnosis;failure analysis;system resource usage;EXERMEST;Ranger HPC system cluster log-data;system administrators;Lustre I/O errors;network packet drops","","","","36","","26 Mar 2020","","","IEEE","IEEE Conferences"
"Efficient Transaction-Based Deterministic Replay for Multi-threaded Programs","E. Pobee; X. Mei; W. K. Chan",City University of Hong Kong; City University of Hong Kong; City University of Hong Kong,"2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)","9 Jan 2020","2019","","","760","771","Existing deterministic replay techniques propose strategies which attempt to reduce record log sizes and achieve successful replay. However, these techniques still generate large logs and achieve replay only under certain conditions. We propose a solution based on the division of the sequence of events of each thread into sequential blocks called transactions. Our insight is that there are usually few to no atomicity violations among transactions reported during a program execution. We present TPLAY, a novel deterministic replay technique which records thread access interleavings on shared memory locations at the transactional level. TPLAY also generates an artificial pair of interleavings when an atomicity violation is reported on a transaction. We present an experiment using the Splash2x extension of the PARSEC benchmark suite. Experimental results indicate that TPLAY experiences a 13-fold improvement in record log sizes and achieves a higher replay probability in comparison to existing work.","2643-1572","978-1-7281-2508-4","10.1109/ASE.2019.00076","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8952181","Concurrency;Deterministic Replay;Transactions;Multi-threading","Instruction sets;Runtime;Clocks;Synchronization;Computer science;Urban areas;Benchmark testing","multi-threading;program debugging;shared memory systems;system monitoring","multithreaded programs;deterministic replay techniques;record log sizes;sequential blocks;atomicity violation;program execution;TPLAY;thread access interleavings;transactional level;replay probability;deterministic replay technique;transaction-based deterministic replay","","","","31","","9 Jan 2020","","","IEEE","IEEE Conferences"
"Unified debugging of distributed systems with Recon","K. H. Lee; N. Sumner; X. Zhang; P. Eugster","Department of Computer Science, Purdue University, West Lafayette, IN 47907, USA; Department of Computer Science, Purdue University, West Lafayette, IN 47907, USA; Department of Computer Science, Purdue University, West Lafayette, IN 47907, USA; Department of Computer Science, Purdue University, West Lafayette, IN 47907, USA","2011 IEEE/IFIP 41st International Conference on Dependable Systems & Networks (DSN)","18 Jul 2011","2011","","","85","96","To scale to today's complex distributed software systems, debugging and replaying techniques mostly focus on single facets of software, e.g., local concurrency, distributed messaging, or data representation. This forces developers to tediously combine different technologies such as instruction-level dynamic tracing, event log analysis, or global state reconstruction to gradually explain non-trivial defects. This paper proposes Recon, a debugging system that provides iterative and interactive homogeneous debugging services. As related systems, Recon promotes SQL-like queries for debugging distributed systems. Unlike other approaches, however, Recon allows for all system artifacts including nodes, communication channels, events, or instructions to be uniformly described by relations. Also, an application in Recon originally runs with a lightweight logger that only collects replay logs for individual nodes. Developers debug a complete program by replaying the execution with fine-grained instrumentation that is capable of exposing instruction-level information. We illustrate the effectiveness of Recon on programs as diverse as BerkeleyDB, i3/Chord, RandTree, and Pastry. Our evaluation includes executions in local clusters as well as in Amazon EC2 and exhibits an unreported bug in RandTree.","2158-3927","978-1-4244-9233-6","10.1109/DSN.2011.5958209","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5958209","Software reliability;distributed systems;debugging;replay;instrumentation","Instruments;Debugging;Runtime;Protocols;Computer bugs;Distributed databases","program debugging;program diagnostics;software reliability;SQL;systems software","program debugging;complex distributed software systems;software development;instruction-level dynamic tracing;event log analysis;global state reconstruction;nontrivial defects;Recon;interactive homogeneous debugging services;SQL-like queries;communication channels;lightweight logger;BerkeleyDB;i3/Chord;RandTree;Pastry;Amazon EC2;unreported bug","","8","","32","","18 Jul 2011","","","IEEE","IEEE Conferences"
"Identifying Crashing Fault Residence Based on Cross Project Model","Z. Xu; T. Zhang; Y. Zhang; Y. Tang; J. Liu; X. Luo; J. Keung; X. Cui","Wuhan University, and The Hong Kong Polytechnic University; Harbin Engineering University; Wuhan University; The Hong Kong Polytechnic University; Wuhan University and Chinese Academy of Sciences; The Hong Kong Polytechnic University; City University of Hong Kong; Wuhan University","2019 IEEE 30th International Symposium on Software Reliability Engineering (ISSRE)","10 Feb 2020","2019","","","183","194","Analyzing the crash reports recorded upon software crashes is a critical activity for software quality assurance. Predicting whether or not the fault causing the crash (crashing fault for short) resides in the stack traces of crash reports can speed-up the program debugging process and determine the priority of the debugging efforts. Previous work mostly collected label information from bug-fixing logs, and extracted crash features from stack traces and source code to train classification models for the Identification of Crashing Fault Residence (ICFR) of newly-submitted crashes. However, labeled data are not always fully available in real applications. Hence the classifier training is not always feasible. In this work, we make the first attempt to develop a cross project ICFR model to address the data scarcity problem. This is achieved by transferring the knowledge from external projects to the current project via utilizing a state-of-the-art Balanced Distribution Adaptation (BDA) based transfer learning method. BDA not only combines both marginal distribution and conditional distribution across projects but also assigns adaptive weights to the two distributions for better adjusting specific cross project pair. The experiments on 7 software projects show that BDA is superior to 9 baseline methods in terms of 6 indicators overall.","2332-6549","978-1-7281-4982-0","10.1109/ISSRE.2019.00027","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8987488","crashing fault;stack trace;transfer learning;cross project model","","feature extraction;learning (artificial intelligence);pattern classification;program debugging;project management;software fault tolerance;software quality;source code (software)","crash reports;software crashes;critical activity;software quality assurance;stack traces;program debugging process;label information;bug-fixing logs;classification models;newly-submitted crashes;external projects;software projects;cross project pair;cross project ICFR model;identification of crashing fault residence;balanced distribution adaptation based transfer learning method;crash feature extraction;source code;data scarcity problem;marginal distribution;conditional distribution","","","","54","","10 Feb 2020","","","IEEE","IEEE Conferences"
"Identifying faults in large-scale distributed systems by filtering noisy error logs","X. Rao; H. Wang; D. Shi; Z. Chen; H. Cai; Q. Zhou; T. Sun","National Laboratory for Parallel and Distributed Processing, National University of Defense Technology, Changsha, P.R. China 410073; National Laboratory for Parallel and Distributed Processing, National University of Defense Technology, Changsha, P.R. China 410073; National Laboratory for Parallel and Distributed Processing, National University of Defense Technology, Changsha, P.R. China 410073; National Laboratory for Parallel and Distributed Processing, National University of Defense Technology, Changsha, P.R. China 410073; Computing Platform, Alibaba Cloud Computing Corporation, Hangzhou, P.R. China; Computing Platform, Alibaba Cloud Computing Corporation, Hangzhou, P.R. China; Computing Platform, Alibaba Cloud Computing Corporation, Hangzhou, P.R. China","2011 IEEE/IFIP 41st International Conference on Dependable Systems and Networks Workshops (DSN-W)","21 Jul 2011","2011","","","140","145","Extracting fault features with the error logs of fault injection tests has been widely studied in the area of large scale distributed systems for decades. However, the process of extracting features is severely affected by a large amount of noisy logs. While the existing work tries to solve the problem by compressing logs in temporal and spatial views or removing the semantic redundancy between logs, they fail to consider the co-existence of other noisy faults that generate error logs instead of injected faults, for example, random hardware faults, unexpected bugs of softwares, system configuration faults or the error rank of a log severity. During a fault feature extraction process, those noisy faults generate error logs that are not related to a target fault, and will strongly mislead the resulted fault features. We call an error log that is not related to a target fault a noisy error log. To filter out noisy error logs, we present a similarity-based error log filtering method SBF, which consists of three integrated steps: (1) model error logs into time series and use haar wavelet transform to get the approximate time series; (2) divide the approximate time series into sub time series by valleys; (3) identify noisy error logs by comparing the similarity between the sub time series of target error logs and the template of noisy error logs. We apply our log filtering method in an enterprise cloud system and show its effectiveness. Compared with the existing work, we successfully filter out noisy error logs and increase the precision and the recall rate of fault feature extraction.","2325-6664","978-1-4577-0375-1","10.1109/DSNW.2011.5958800","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5958800","error log;event filtering;fault injection;large scale distributed system","Time series analysis;Noise measurement;Feature extraction;Wavelet transforms;Computer crashes;Complexity theory;Approximation methods","cloud computing;data compression;distributed processing;fault tolerant computing;feature extraction;system monitoring;time series;wavelet transforms","fault identification;noisy error log filtering;fault injection tests;large scale distributed systems;data compression;semantic redundancy;fault feature extraction;time series;wavelet transform;enterprise cloud system","","7","","20","","21 Jul 2011","","","IEEE","IEEE Conferences"
"A User Evaluation of Process Discovery Algorithms in a Software Engineering Company","S. Agostinelli; F. M. Maggi; A. Marrella; F. Milani",Sapienza University of Rome; University of Tartu; Sapienza University of Rome; University of Tartu,"2019 IEEE 23rd International Enterprise Distributed Object Computing Conference (EDOC)","30 Dec 2019","2019","","","142","150","Process mining methods allow analysts to use logs of historical executions of business processes in order to gain knowledge about the actual behavior of these processes. One of the most widely studied process mining operations is automated process discovery. An event log is taken as input by an automated process discovery method and produces a business process model as output that captures the control-flow relations between tasks that are described by the event log. In this setting, this paper provides a systematic comparative evaluation of existing implementations of automated process discovery methods with domain experts by using a real life event log extracted from an international software engineering company and four quality metrics: understandability, correctness, precision, and usefulness. The evaluation results highlight gaps and unexplored trade-offs in the field and allow researchers to improve the lacks in the automated process discovery methods in terms of usability of process discovery techniques in industry.","2325-6362","978-1-7281-2702-6","10.1109/EDOC.2019.00026","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8944956","User Evaluation;Process Mining;Process Discovery;BPMN","Companies;Systematics;Tools;Complexity theory;Software algorithms;Data mining","business data processing;data mining;software engineering","event log;process discovery method;process discovery techniques;process discovery algorithms;process mining methods;business processes;process mining operations;business process model;user evaluation;software engineering company;quality metrics","","","","34","","30 Dec 2019","","","IEEE","IEEE Conferences"
"Comparison of echo-power estimation using linearized video data and raw data for Dynamic Contrast-Enhanced Ultrasound","T. Payen; A. Coron; M. Lamuraglia; O. Lucidarme; D. Le Guillou-Buffello; S. L. Bridal","Laboratoire d'Imagerie Paramétrique UMR 7623 CNRS UPMC, Paris, France; Laboratoire d'Imagerie Paramétrique UMR 7623 CNRS UPMC, Paris, France; Laboratoire d'Imagerie Paramétrique UMR 7623 CNRS UPMC, Paris, France; Functional Imaging Laboratory, INSERM UPMC 678, Paris, France; Laboratoire d'Imagerie Paramétrique UMR 7623 CNRS UPMC, Paris, France; Laboratoire d'Imagerie Paramétrique UMR 7623 CNRS UPMC, Paris, France","2012 IEEE International Ultrasonics Symposium","18 Jul 2013","2012","","","1335","1338","To estimate perfusion using Dynamic Contrast-Enhanced Ultrasound (DCE-US), some manufacturers provide software that analyses DICOM Raw Data formatted files. In those files, most of the information cannot be correctly interpreted except by the manufacturer software. This approach when available is often recommended over use of more accessible DICOM JPEG files which encapsulate ultrasound system images as compressed JPEG images. However, the manufacturers' dedicated software is machine specific with limited functionalities which hinders multicentric studies using different ultrasound machines. The objective of this work is to develop techniques and criteria defining conditions under which compressed video images obtained with the Aplio 50 from Toshiba can be used to accurately estimate contrast concentration. Dedicated CHIQ software from Toshiba was compared to two approaches based on compressed images: commercially available VueBox™ software from Bracco and in-laboratory software PixPower. Proportionality between echo-power estimates and dose of contrast agents in suspension was validated in vitro for the three approaches. VueBox™ and PixPower echo-power estimations from DCE-US sequences acquired in an in vivo tumor model were in agreement with those obtained with the CHIQ reference. This work shows that, for the dynamic ranges considered in this study, DICOM JPEG files can be used to accurately estimate the echo power from video compressed images using linearizing equations based on rigorous calibration steps.","1051-0117","978-1-4673-4562-0","10.1109/ULTSYM.2012.0333","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6562082","linearization;perfusion quantification;log-compression","DICOM;Transform coding;Dynamic range;Ultrasonic imaging;Software;Estimation;Image coding","biomedical ultrasonics;echo;haemodynamics;image enhancement;medical image processing;tumours;ultrasonic imaging;video coding","echo power estimation;linear video data;raw data formatted file;dynamic contrast enhanced ultrasound;perfusion estimation;manufacturer software;DICOM JPEG file;ultrasound system image encapsulation;JPEG image compression;ultrasound machine;video image compression;contrast concentration estimation;CHIQ software;VueBoxTM software;Bracco;in-laboratory software PixPower;contrast agent;PixPower;DCE-US sequence;in vivo tumor model;linear equation","","1","","9","","18 Jul 2013","","","IEEE","IEEE Conferences"
"Automated Performance Model Construction through Event Log Analysis","A. Mizan; G. Franks","Carleton Univ., Ottawa, ON, Canada; Carleton Univ., Ottawa, ON, Canada","2012 IEEE Fifth International Conference on Software Testing, Verification and Validation","17 May 2012","2012","","","636","641","Locating performance bottlenecks in modern distributed systems can be difficult because direct measurement is often not available. Performance models are often very beneficial in these situations because far more information can be extracted as a result. This work demonstrates the generation of a Layered Queueing Network performance model through the analysis of trace information from a live system. The model can then be analyzed to locate bottlenecks in both the hardware and software.","2159-4848","978-0-7695-4670-4","10.1109/ICST.2012.152","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6200164","","Servers;Connectors;Instruments;Program processors;Unified modeling language;Java","distributed processing;performance evaluation;queueing theory","automated performance model construction;event log analysis;distributed systems;direct measurement;layered queueing network performance model;trace information analysis;live system;performance bottleneck location","","4","","10","","17 May 2012","","","IEEE","IEEE Conferences"
"Blockchain Based Log System","J. Huang; H. Li; J. Zhang","Shenzhen Graduate School, Peking University, Shenzhen, China; Shenzhen Graduate School, Peking University, Shenzhen, China; Shenzhen Graduate School, Peking University, Shenzhen, China","2018 IEEE International Conference on Big Data (Big Data)","24 Jan 2019","2018","","","3033","3038","The logging system records the logs generated by the software so that the administrator can handle the problems that occur. However, the traditional log system is not secure enough and the stored logs are easily falsified. As a decentralized distributed storage technology, the blockchain can ensure that the blockchain network works normally in the presence of a few malicious nodes or failed nodes. So we use the blockchain to store the logs, which improves the security of the log system. In order to improve the performance of the blockchain, we use a voting-based consensus algorithm as a blockchain consistency protocol. This article introduces the architecture and implementation of the log system and verifies the feasibility of the system through experiments.","","978-1-5386-5035-6","10.1109/BigData.2018.8622204","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8622204","Log system;the blockchain;block synchronization;nodes discovery","Blockchain;Protocols;Peer-to-peer computing;Consensus algorithm;Security;Synchronization;Reliability","cryptography;distributed databases;protocols;storage management;system monitoring","stored logs;decentralized distributed storage technology;blockchain network;voting-based consensus algorithm;blockchain consistency protocol;blockchain based log system;logging system","","1","","10","","24 Jan 2019","","","IEEE","IEEE Conferences"
"Risk assessment quantification in life log service","S. Tanimoto; K. Takahashi; T. Yabuki; K. Kato; M. Iwashita; H. Sato; A. Kanai","Chiba Institute of Technology, Japan; Chiba Institute of Technology, Japan; Chiba Institute of Technology, Japan; Chiba Institute of Technology, Japan; Chiba Institute of Technology, Japan; The University of Tokyo, Japan; Hosei University, Japan","15th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)","1 Sep 2014","2014","","","1","6","A Life Log Service that handles action records, such as an individual search logs and data on shoppers' browsing and buying habits, have attracted attention with the spread of the Internet. Life Log Service is thought to present various risks to private information, such as personal information. For this reason, countermeasures to these risks need to be investigated. We have already qualitatively analyzed the risks of life log services. To extract the specific risks of using a life log service comprehensively, we used a Risk Breakdown Structure (RBS), which is the typical risk-analysis method. Furthermore, after risks were analyzed, concrete countermeasures were proposed. However, these results need to be quantitatively evaluated from a more practical viewpoint. In this paper, the validity is visualized by performing quantitive risk assessment on the proposed countermeasures for risks in the life log obtained in our previous research. Specifically, the risk value based on a risk formula is computed to a risk factor and its proposed countermeasures. Thereby, the effects of the proposed countermeasures on risks of the life log service found in previous research are evaluated quantitatively, and this will contribute to spreading and promoting the life log service.","","978-1-4799-5604-3","10.1109/SNPD.2014.6888723","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6888723","Life Log;Risk Assessment;Risk Breakdown Structure;Risk Quantification","Companies;Security;History;Risk management;Internet;Approximation methods;Certification","Internet;personal information systems;risk management","risk assessment quantification;life log service;Internet;personal information;risk breakdown structure;risk-analysis method","","","","20","","1 Sep 2014","","","IEEE","IEEE Conferences"
"Developing Software-Based Plug&Play Capabilities for Analog Sensors over a Network Using a Microcontroller Development Board","B. S. M. Croitoru; A. Tulbure; A. I. Filip","""1 Decembrie 1918"" University,Department of Precise and Engineering Sciences,Alba Iulia,Romania; ""1 Decembrie 1918"" University,Department of Precise and Engineering Sciences,Alba Iulia,Romania; ""Valahia"" University,Doctoral School,Târgoviște,Romania","2019 IEEE 25th International Symposium for Design and Technology in Electronic Packaging (SIITME)","13 Feb 2020","2019","","","90","93","The present paper describes a standard C algorithm (mathematical equations + software implementation) for making analog sensors Plug & Play over TCP/IP - Ethernet protocol. This algorithm represents a beginning step for creating a prototype of smart transducer system according to IEEE 1451 family of standards. The sensors are connected to the Analog Input Ports of a microcontroller (MCU) development board. The standard C program running on the microcontroller will automatically detect the connected sensors and will send a list of sensor information over TCP/IP - Ethernet to a remote location. The MCU algorithm is able to detect when a sensor is connected to any of its I/O Analog Ports by monitoring some external and internal parameters like: the default I/O port voltage value, the voltage value of a port when a sensor is connected to that port, the influence between analog ports when connecting a sensor to a port. The microcontroller development board is partially programmed to be an HTML Web Server. In a Web Browser will be displayed the status of all Analog Input Ports. When a sensor is detected, a text file containing sensor data will be created on a local micro-SD card (data logging capabilities). The sensor data can be accessed through a Web Browser by using the IP address of the Web Server.","2642-7036","978-1-7281-3330-0","10.1109/SIITME47687.2019.8990885","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8990885","plug&play;sensors;network;algorithm;microcontroller","","C listings;computerised instrumentation;file servers;hypermedia markup languages;intelligent sensors;Internet;local area networks;mathematical analysis;microcontrollers;online front-ends;transducers;transport protocols","Web browser;IP address;HTML Web server;I-O analog input ports;MCU development board;microcontroller;IEEE 1451 standards;Ethernet protocol;TCP-IP;software-based plug & play capabilities;standard C program;smart transducer system;analog sensors;software implementation;mathematical equations;microcontroller development board","","","","9","","13 Feb 2020","","","IEEE","IEEE Conferences"
"Making real time data analytics available as a service","D. Xu; D. Wu; X. Xu; L. Zhu; L. Bass","Software Systems Research Group, NICTA, Sydney, Australia; Software Systems Research Group, NICTA, Sydney, Australia; Software Systems Research Group, NICTA, Sydney, Australia; Software Systems Research Group, NICTA, Sydney, Australia; Software Systems Research Group, NICTA, Sydney, Australia","2015 11th International ACM SIGSOFT Conference on Quality of Software Architectures (QoSA)","14 Apr 2016","2015","","","73","82","Conducting (big) data analytics in an organization is not just about using a processing framework (e.g. Hadoop/Spark) to learn a model from data currently in a single file system (e.g. HDFS). We frequently need to pipeline real time data from other systems into the processing framework, and continually update the learned model. The processing frameworks need to be easily invokable for different purposes to produce different models. The model and the subsequent model updates need to be integrated with a product that may require a real time prediction using the latest trained model. All these need to be shared among different teams in the organization for different data analytics purposes. In this paper, we propose a real time data-analytics-as-service architecture that uses RESTful web services to wrap and integrate data services, dynamic model training services (supported by big data processing framework), prediction services and the product that uses the models. We discuss the challenges in wrapping big data processing frameworks as services and other architecturally significant factors that affect system reliability, real time performance and prediction accuracy. We evaluate our architecture using a log-driven system operation anomaly detection system where staleness of data used in model training, speed of model update and prediction are critical requirements.","","978-1-4503-3470-9","10.1145/2737182.2737186","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7450804","Software Architecture;Big Data Processing;Data Analytics Service","Real-time systems;Data models;Big data;Predictive models;Training;Data analysis;Computer architecture","Big Data;data analysis;learning (artificial intelligence);service-oriented architecture;Web services","real time data analytics-as-service architecture;RESTful Web services;data services;dynamic model training services;Big Data processing framework;prediction services;log-driven system operation anomaly detection system","","3","","20","","14 Apr 2016","","","IEEE","IEEE Conferences"
"A Geiger-Mode APD Photon Counting System With Adjustable Dead-Time and Interchangeable Detector","S. Deng; D. Gordon; A. P. Morrison","Centre for Advanced Photonics and Process Analysis, Cork Institute of Technology, Cork, Ireland; Department of Electrical and Electronic Engineering, University College Cork, Cork, Ireland; Department of Electrical and Electronic Engineering, University College Cork, Cork, Ireland","IEEE Photonics Technology Letters","26 Nov 2015","2016","28","1","99","102","A Geiger-mode avalanche photodiode (GM-APD) photon counting system is presented in this letter. The system provides a maximum counting rate of 35 Mcounts/s and is capable of directly displaying the counting rate and data logging to a PC. In this system, the detector can be easily changed to enhance its usefulness in different applications. A novel active quench and reset integrated circuit (AQR-IC) is designed for the system with adjustable hold-off time from several nanoseconds up to 1.6 μs with a setting resolution of ~6.5 ns. This facilitates optimal performance when using different types of APDs. The AQR-IC also registers each avalanche event as a TTL pulse that is processed by a microcontroller to calculate the photon-counting rate. The microcontroller can be interfaced with a PC over USB to record the measured data and to allow further processing. Software was also written to calculate the photon-counting rate, display the results and save the data to files.","1941-0174","","10.1109/LPT.2015.2487342","Science Foundation Ireland; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7289373","Geiger-mode avalanche photodiode;Single photon counting;adjustable dead-time;photon-counting system;Geiger-mode avalanche photodiode;single photon counting;adjustable dead-time;photon-counting system","Photonics;Cathodes;Radiation detectors;Integrated circuits;Graphical user interfaces;Detectors","avalanche photodiodes;integrated optoelectronics;photodetectors;photon counting","Geiger-mode APD photon counting system;adjustable dead-time;interchangeable detector;Geiger-mode avalanche photodiode photon counting system;data logging;PC;active quench and reset integrated circuit;hold-off time;TTL pulse;microcontroller;photon-counting rate;USB","","8","","13","","5 Oct 2015","","","IEEE","IEEE Journals"
"Detecting ""0-Day"" Vulnerability: An Empirical Study of Secret Security Patch in OSS","X. Wang; K. Sun; A. Batcheller; S. Jajodia",George Mason University; George Mason University; Northrop Grumman; George Mason University,"2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)","22 Aug 2019","2019","","","485","492","Security patches in open source software (OSS) not only provide security fixes to identified vulnerabilities, but also make the vulnerable code public to the attackers. Therefore, armored attackers may misuse this information to launch N-day attacks on unpatched OSS versions. The best practice for preventing this type of N-day attacks is to keep upgrading the software to the latest version in no time. However, due to the concerns on reputation and easy software development management, software vendors may choose to secretly patch their vulnerabilities in a new version without reporting them to CVE or even providing any explicit description in their change logs. When those secretly patched vulnerabilities are being identified by armored attackers, they can be turned into powerful ""0-day"" attacks, which can be exploited to compromise not only unpatched version of the same software, but also similar types of OSS (e.g., SSL libraries) that may contain the same vulnerability due to code clone or similar design/implementation logic. Therefore, it is critical to identify secret security patches and downgrade the risk of those ""0-day"" attacks to at least ""n-day"" attacks. In this paper, we develop a defense system and implement a toolset to automatically identify secret security patches in open source software. To distinguish security patches from other patches, we first build a security patch database that contains more than 4700 security patches mapping to the records in CVE list. Next, we identify a set of features to help distinguish security patches from non-security ones using machine learning approaches. Finally, we use code clone identification mechanisms to discover similar patches or vulnerabilities in similar types of OSS. The experimental results show our approach can achieve good detection performance. A case study on OpenSSL, LibreSSL, and BoringSSL discovers 12 secret security patches.","1530-0889","978-1-7281-0057-9","10.1109/DSN.2019.00056","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8809499","security patch;vulnerability detection;open source software","Security;Databases;Machine learning;Open source software;Training;Computer bugs","learning (artificial intelligence);public domain software;security of data;software development management;software maintenance","0-day vulnerability;open source software;security fixes;vulnerable code public;armored attackers;n-day attacks;unpatched OSS versions;easy software development management;software vendors;secretly patched vulnerabilities;0-day attacks;security patch database;BoringSSL;secret security patches;security patches mapping","","4","","30","","22 Aug 2019","","","IEEE","IEEE Conferences"
"The Design and Implementation of Host-Based Intrusion Detection System","Y. -j. Ou; Y. Lin; Y. Zhang; Y. -j. Ou","NA; Sch. of Software, Yunnan Univ., Kunming, China; Comput. Sci. Dept., Southwest Forestry Univ., Kunming, China; Sch. of Inf. Sci. & Eng., Central South Univ., Changsha, China","2010 Third International Symposium on Intelligent Information Technology and Security Informatics","22 Apr 2010","2010","","","595","598","Intrusion detection is the process of identifying and responding to suspicious activities targeted at computing and communication resources, and it has become the mainstream of information assurance as the dramatic increase in the number of attacks. Intrusion detection system (IDS) monitors and collects data from a target system that should be protected, processes and correlates the gathered information, and initiates responses when evidence of an intrusion is detected. In this paper, we designed and implemented a host-based intrusion detection system, which combines two detection technologies, one is log file analysis technology and the other is BP neural network technology. Log file analysis is an approach of misuse detection, and BP neural network is an approach of anomaly detection. By combination of these two kinds of detection technologies, the HIDS that we have implemented can effectively improve the efficiency and accuracy of intrusion detection.","","978-1-4244-6743-3","10.1109/IITSI.2010.127","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5453694","intrusion detection;intrusion detection system;HIDS;Log analysis;BP neural network;OSSEC","Intrusion detection;Neural networks;Protection;Information analysis;Telecommunication traffic;Computer displays;Pattern matching;Decoding;Information technology;Computer security","backpropagation;data analysis;neural nets;security of data","host-based intrusion detection system;log file analysis;BP neural network;backpropagation;anomaly detection","","21","","9","","22 Apr 2010","","","IEEE","IEEE Conferences"
"Profiling the Usage of an Extreme-Scale Archival Storage System","H. Sim; S. S. Vazhkudai",Oak Ridge National Laboratory; Oak Ridge National Laboratory,"2019 IEEE 27th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)","25 Sep 2019","2019","","","410","422","Profiling the archival storage system in scientific computing environments has received much less attention compared to the parallel file system, but is equally important since it stores the final data products safely, for a long duration. In this paper, we analyze eight years worth of data transfer logs for accessing the archival file system (HPSS) in the Oak Ridge Leadership Computing Facility (OLCF), which has been hosting the world's largest supercomputers and file systems. Our analysis encompasses about 135 million data transfer activities to the 80 PB High Performance Storage System (HPSS), between 2010 and 2017. We analyze the logs from several dimensions, including studying the workload characteristics (e.g., access patterns, frequency of accesses and temporal behavior), file system characteristics (e.g., directory depth, file system scaling trends, file types), and scientific user behavior (e.g., domain-specific usage and organization). Based on the analysis, we derive insights into the future evolution of the archive in terms of provisioning, desired features and functionality from the archive software, role and right sizing of the archive tiers, quota management, and the importance of smart and efficient metadata and storage management. We believe our study will prove useful for both operating current archival storage and better provisioning future systems.","2375-0227","978-1-7281-4950-9","10.1109/MASCOTS.2019.00050","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8842850","supercomputers;file systems;large scale systems","Data transfer;Supercomputers;Computational modeling;Data models;Databases;Analytical models;Systems architecture","parallel processing;storage management","data transfer logs;archival file system;HPSS;Oak Ridge Leadership Computing Facility;high performance storage system;access patterns;file system characteristics;file system scaling trends;file types;scientific user behavior;domain-specific usage;archive software;storage management;parallel file system;extreme-scale archival storage system","","","","55","","25 Sep 2019","","","IEEE","IEEE Conferences"
"Privacy preservation and enhanced utility in search log publishing using improved zealous algorithm","S. Belinsha; A. P. V. Raghavendra","Department of Computer Science and Engineering, VSB Engineering College, Karur, India; Department of Computer Science and Engineering, VSB Engineering College, Karur, India","2013 International Conference on Green High Performance Computing (ICGHPC)","17 Jun 2013","2013","","","1","5","Search log records can enhance the quality and delivery of internet information services to the end user. Analysing and exploring the search log can explore the user's behaviour. When these search logs are published it must ensure privacy of the users at the same time it should exhibit better utility. The existing ZEALOUS algorithm uses a two threshold framework to provide probabilistic differential privacy. In the course of providing this level of privacy the search log looses it's utility, as it publishes only frequent items. So an algorithm is proposed to enhance the utility of search log by qualifying the infrequent items while publishing at the same time preserving the stronger level of privacy.","","978-1-4673-2594-3","10.1109/ICGHPC.2013.6533936","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6533936","Information services;search queries;differential privacy;utility;search behaviours","Privacy;Computers;Probabilistic logic;Search engines;Noise;Software;Publishing","data privacy;information retrieval;information services;Internet;probability","privacy preservation;search log publishing;improved zealous algorithm;internet information services;probabilistic differential privacy","","","","5","","17 Jun 2013","","","IEEE","IEEE Conferences"
"Improving Web Navigation Usability by Comparing Actual and Anticipated Usage","R. Geng; J. Tian","Department Computer Science and Engineering, Southern Methodist University, Dallas, TX, USA; Department of Computer Science and Engineering, Southern Methodist University, Dallas, TX, USA","IEEE Transactions on Human-Machine Systems","20 May 2017","2015","45","1","84","94","We present a new method to identify navigation-related Web usability problems based on comparing actual and anticipated usage patterns. The actual usage patterns can be extracted from Web server logs routinely recorded for operational websites by first processing the log data to identify users, user sessions, and user task-oriented transactions, and then applying an usage mining algorithm to discover patterns among actual usage paths. The anticipated usage, including information about both the path and time required for user-oriented tasks, is captured by our ideal user interactive path models constructed by cognitive experts based on their cognition of user behavior. The comparison is performed via the mechanism of test oracle for checking results and identifying user navigation difficulties. The deviation data produced from this comparison can help us discover usability issues and suggest corrective actions to improve usability. A software tool was developed to automate a significant part of the activities involved. With an experiment on a small service-oriented website, we identified usability problems, which were cross-validated by domain experts, and quantified usability improvement by the higher task success rate and lower time and effort for given tasks after suggested corrections were implemented. This case study provides an initial validation of the applicability and effectiveness of our method.","2168-2305","","10.1109/THMS.2014.2363125","National Science Foundation; Raytheon; NSF Net-Centric I/UCRC; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6939718","Cognitive user model;sessionization;software tool;test oracle;usability;usage pattern;Web server log;Cognitive user model;sessionization;software tool;test oracle;usability;usage pattern;Web server log","Usability;Navigation;Unified modeling language;Web servers;Computational modeling;Cognition","data mining;Internet","Web navigation usability;actual usage pattern;anticipated usage pattern;navigation-related Web usability problems;log data processing;user identification;user sessions;user task-oriented transaction;usage mining algorithm;software tool;service-oriented Web site;usability improvement","","25","","42","","29 Oct 2014","","","IEEE","IEEE Journals"
"A semantic model of events for integrating photovoltaic monitoring data","P. Dagnely; E. Tsiporkova; T. Tourwé; T. Ruette; K. De Brabandere; F. Assiandi","Sirris - Software Engineering & ICT Group, A. Reyerslaan 80 - 1030 Brussels - Belgium; Sirris - Software Engineering & ICT Group, A. Reyerslaan 80 - 1030 Brussels - Belgium; Sirris - Software Engineering & ICT Group, A. Reyerslaan 80 - 1030 Brussels - Belgium; Sirris - Software Engineering & ICT Group, A. Reyerslaan 80 - 1030 Brussels - Belgium; 3E - iLab Kalkkaai 6 - 1000 Brussels - Belgium; 3E - iLab Kalkkaai 6 - 1000 Brussels - Belgium","2015 IEEE 13th International Conference on Industrial Informatics (INDIN)","1 Oct 2015","2015","","","24","30","Solar plants typically consist of several thousands of passive photovoltaic modules that are connected via thousands of string boxes to hundreds of inverters. In addition, a solar plant has meteo-sensors, power meters and control switches. All these components continuously generate data that is collected by monitoring systems or SCADA systems on-site. From there onwards, this data is pushed to remote analysis servers. The optimal exploitation of this data is hampered by a lack of harmonisation and standardisation in the photovoltaic domain. The data-generating components originate from several different manufacturers, models and versions, and their output is thus not easily commensurable. Crucial for this paper is the fact that conceptually identical failure events are not logged with the same identifying labels. Therefore, every analysis of monitoring system data coming from photovoltaic plants needs an initial integration step to resolve this labeling issue. Our proposal is to facilitate the integration with semantic modelling by means of creating a photovoltaic event ontology with an SWRL reasoning layer.","2378-363X","978-1-4799-6649-3","10.1109/INDIN.2015.7281705","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7281705","Semantic integration;ontology model;reasoning;SWRL;solar plants;Internet of Things","Inverters;Ontologies;Monitoring;Semantics;Circuit faults;XML;Standards","invertors;ontologies (artificial intelligence);photovoltaic power systems;power engineering computing;power meters;SCADA systems;solar cells","event semantic model;photovoltaic monitoring data integration;solar plant;passive photovoltaic module;inverter;meteo-sensors;power meter;control switch;SCADA system;remote analysis;photovoltaic event ontology;SWRL reasoning layer","","5","","22","","1 Oct 2015","","","IEEE","IEEE Conferences"
"A Privacy-Preserving and Fully Decentralized Storage and Sharing System on Blockchain","G. Li; H. Sato",The University of Tokyo; The University of Tokyo,"2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC)","9 Jul 2019","2019","2","","694","699","With the blockchain receiving extensive attention in recent years, many storage schemes based on the blockchain have been proposed as alternative means of cloud storage for data outsourcing. However, the conventional access control methods in the current sharing schemes require either individual permission granting via symmetric keys or a trusted central attribute authority for ciphertext-policy attribute-based encryption (CP-ABE). Moreover, due to the transparency of the blockchain, the data query logs are recorded in public ledgers. Nevertheless, the problem of privacy protection for data consumers has not been properly addressed. In this paper, we propose a fully decentralized data storage and sharing system on a blockchain by using multi-authority CP-ABE and decentralized multi-authority attribute-based signatures (DMA-ABSs). In our system, every party can securely store and share its data with a set of individuals satisfying a policy with no need to grant separate permissions individually. Additionally, the data owners can fully control their data, know how their data are accessed due to the nature of the blockchain and have the ability to opt-out at any time. The public ledger of the blockchain provides immutable logs of data address pointers, access policies, attribute public keys and data queries. In addition, data consumers' attributes are publicly verifiable through the DMA-ABS scheme without revealing more private information. Finally, the combination of the multi-authority CP-ABE with the blockchain guarantees the integrity, confidentiality, and accessibility of the data without the need for trusted third parties, such as a central authority or a data center.","0730-3157","978-1-7281-2607-4","10.1109/COMPSAC.2019.10289","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8754300","privacy, blockchain, access control, file sharing, CP ABE, attribute-based signature","Blockchain;Public key;Ciphers;Authorization;Cloud computing","authorisation;cloud computing;cryptography;data privacy;outsourcing;query processing","decentralized storage;blockchain;storage schemes;cloud storage;data outsourcing;current sharing schemes;trusted central attribute authority;ciphertext-policy attribute-based encryption;data query logs;public ledger;data consumers;fully decentralized data storage;multiauthority CP-ABE;multiauthority attribute-based signatures;data owners;data address pointers;data center;access control methods","","3","","9","","9 Jul 2019","","","IEEE","IEEE Conferences"
"Process mining in software systems: Discovering real-life business transactions and process models from distributed systems","M. Leemans; W. M. P. van der Aalst","Eindhoven University of Technology, P.O. Box 513, 5600 MB, The Netherlands; Eindhoven University of Technology, P.O. Box 513, 5600 MB, The Netherlands","2015 ACM/IEEE 18th International Conference on Model Driven Engineering Languages and Systems (MODELS)","30 Nov 2015","2015","","","44","53","This paper presents a novel reverse engineering technique for obtaining real-life event logs from distributed systems. This allows us to analyze the operational processes of software systems under real-life conditions, and use process mining techniques to obtain precise and formal models. Hence, the work can be positioned in-between reverse engineering and process mining. We present a formal definition, implementation and an instrumentation strategy based the joinpoint-pointcut model. Two case studies are used to evaluate our approach. These concrete examples demonstrate the feasibility and usefulness of our approach.","","978-1-4673-6908-4","10.1109/MODELS.2015.7338234","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7338234","Reverse Engineering;Process Mining;Event Log;Distributed Systems;Performance Analysis;Process Discovery;Joinpoint-Pointcut Model;Aspect-Oriented Programming","Unified modeling language;Analytical models;Instruments;Testing;Java;Data mining;Reverse engineering","business data processing;data mining;reverse engineering;transaction processing","process mining;software systems;real-life business transactions;process models;reverse engineering technique;real-life event logs;distributed systems;operational process analysis;real-life conditions;formal definition;formal implementation;instrumentation strategy;join-point-point-cut model","","25","","41","","30 Nov 2015","","","IEEE","IEEE Conferences"
"Developing an Error Logging Framework for Ruby on Rails Application Using AOP","M. Gomathy; V. K. Devi; D. Meenakshi","Dept. of Comput. Sci., Shrimati Indira Gandhi Coll., Trichirapalli, India; Dept. of Inf. Technol. & Applic., Shrimati Indira Gandhi Coll., Trichirapalli, India; Dept. of Inf. Technol. & Applic., Shrimati Indira Gandhi Coll., Trichirapalli, India","2014 World Congress on Computing and Communication Technologies","3 Apr 2014","2014","","","44","49","A framework for detecting and recording the flaws that happen during the usage of web applications is designed and a library functionality to perform this is discussed in this paper. The recorded information can be stored at different levels of detail, commonly called the logging levels. For some modules more than others, it may be required to store more detailed information about any error that arises during its usage according to its importance. A Web Application also needs to print the stack trace containing the error information on the web page when an error occurs for the user to understand the nature of the error. When dealing with legacy web applications, it is difficult to insert code. The proposed and designed framework is tested with a web application called Kic Kart.","","978-1-4799-2877-4","10.1109/WCCCT.2014.19","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6755103","Error Recording;Web applciations;Aspect Oriented Programming;Ruby on Rails","Servers;XML;Rails;Sockets;Weaving;Programming;Libraries","aspect-oriented programming;Internet;public domain software;system monitoring","Kic Kart;Web page error information;stack trace;logging levels;library functionality;Web applications;flaws detection;aspect-oriented programming;AOP;Ruby on Rails application;error logging framework development","","","1","9","","3 Apr 2014","","","IEEE","IEEE Conferences"
"A Set-Aware Key-Value Store on Shingled Magnetic Recording Drives with Dynamic Band","T. Yao; Z. Tan; J. Wan; P. Huang; Y. Zhang; C. Xie; X. He","Wuhan Nat. Lab. for Optoelectron., HUST, Wuhan, China; Sch. of Comput. Sci. & Technol., HUST, Wuhan, China; Wuhan Nat. Lab. for Optoelectron., HUST, Wuhan, China; Dept. of Comput. & Inf. Sci., Temple Univ., Philadelphia, PA, USA; Wuhan Nat. Lab. for Optoelectron., HUST, Wuhan, China; Wuhan Nat. Lab. for Optoelectron., HUST, Wuhan, China; Dept. of Comput. & Inf. Sci., Temple Univ., Philadelphia, PA, USA","2018 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","6 Aug 2018","2018","","","306","315","Key-value (KY) stores play an increasingly critical role in supporting diverse large-scale applications in modern data centers hosting terabytes of KY items which even might reside on a single server due to virtualization purpose. The combination of ever growing volume of KY items and storage/application consolidation is driving a trend of high storage density for KY stores. Shingled Magnetic Recording (SMR) represents a promising technology for increasing disk capacity, but it comes at a cost of poor random write performance and severe I/O amplification. Applications/software working with SMR devices need to be designed and optimized in an SMR-friendly manner. In this work, we present SEALDB, a Log-Structured Merge tree (LSM-tree) based key-value store that is specifically optimized for and works well with SMR drives via adequately addressing the poor random writes and severe I/O amplification issues. First, for LSM-trees, SEALDB concatenates SSTables of each compaction, and groups them into sets. Taking sets as the basic unit for compactions, SEALDB improves compaction efficiency by mitigating random I/Os. Second, SEALDB creates varying size bands on HM-SMR drives, named dynamic bands. Dynamic bands not only accommodate the storage of sets, but also eliminate the auxiliary write amplification from SMR drives. We demonstrate the advantages of SEALDB via extensive experiments in various workloads. Overall, SEALDB delivers impressive performance improvement. Compared with LevelDB, SEALDB is 3.42× faster on random load due to improved compaction efficiency and eliminated auxiliary write amplification on SMR drives.","1530-2075","978-1-5386-4368-6","10.1109/IPDPS.2018.00040","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8425184","LSM-tree;SMR;KV store;Set;Dynamic band","Compaction;Drives;Magnetic recording;Performance evaluation;Data centers;Servers;Electronic mail","computer centres;disc drives;magnetic recording;merging;storage management;virtualisation","set-aware key-value store;shingled Magnetic Recording drives;large-scale applications;KY items;virtualization purpose;KY stores;SMR devices;SMR-friendly manner;SEALDB;LSM-tree;HM-SMR drives;data centers;random write performance;dynamic bands;compaction efficiency;log-structured merge tree","","1","","34","","6 Aug 2018","","","IEEE","IEEE Conferences"
"Managing technical debt in practice: An industrial report","C. A. Siebra; G. S. Tonin; F. Q. B. da Silva; R. G. Oliveira; L. C. Antonio; R. C. G. Miranda; A. L. M. Santos","Center of Informatics, Federal University of Paraiba, Joao Pessoa - PB - Brazil; CIn/Samsung Project - Federal University of Pernambuco; CIn/Samsung Project - Federal University of Pernambuco; CIn/Samsung Project - Federal University of Pernambuco; CIn/Samsung Project - Federal University of Pernambuco; CIn/Samsung Project - Federal University of Pernambuco; CIn/Samsung Project - Federal University of Pernambuco","Proceedings of the 2012 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement","7 Mar 2013","2012","","","247","250","The Technical Debt (TD) metaphor has been used as a way to manage and communicate long-term consequences that some decisions may cause. However the state of the art in TD has not culminated yet in rigorous analysis models for large-scale projects. This work analyses an industrial project, from the perspective of its decisions and related events, so that we can better characterize the existence of TD and show the evolution of its parameters. The project in study had a life cycle of six years (2005-2011) and its data for analysis was collected from emails, documents, CVS logs, code files and interviews with developers and project managers. From this analysis, we identified the factors that had influence on the project decisions and their impact on the system along the time. Furthermore, we were able to extract a set of lessons associated with the characterization of TD in projects of this port.","1949-3789","978-1-4503-1056-7","10.1145/2372251.2372297","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6475425","Management;Measurement;Documentation","Protocols;Mobile communication;Electronic mail;Monitoring;Automation;Manuals;Interviews","project management;software development management","technical debt management;industrial project management;TD characterization;TD parameter;software development process","","4","","5","","7 Mar 2013","","","IEEE","IEEE Conferences"
"An Adaptive Performance Modeling Approach to Performance Profiling of Multi-service Web Applications","X. Huang; W. Wang; W. Zhang; J. Wei; T. Huang","Technol. Center of Software Eng., Chinese Acad. of Sci., Beijing, China; Technol. Center of Software Eng., Chinese Acad. of Sci., Beijing, China; Technol. Center of Software Eng., Chinese Acad. of Sci., Beijing, China; Technol. Center of Software Eng., Chinese Acad. of Sci., Beijing, China; Technol. Center of Software Eng., Chinese Acad. of Sci., Beijing, China","2011 IEEE 35th Annual Computer Software and Applications Conference","3 Oct 2011","2011","","","4","13","The performance of multi-service applications are known to be determined mainly by the interactions between workload and behaviors of the application. The change of workload can lead to dynamic service demands on system resources, and even cause dynamic bottleneck switches between services inside the application. In this paper, to profiling large-applications' behaviors, and help to locate the bottleneck and optimize their capacities, we focus on modeling their behavior according to the workload. Although this topic has been well studied at testing stage, building such a model under live workload remains a challenge, because the workload and application behaviors are time-varying. To tackle this problem, we propose an adaptive approach to build and rebuild performance model according to log files. Both the user behaviors and their corresponding internal service relations are modeled, and the CPU time consumed by each service is also obtained through Kalman filter, which can ""absorb"" some level of noise in real-world data. Our model can explain the behaviors of both the whole application and the individual services, and provide valuable information for capacity planning and bottleneck detection. At last, our work is evaluated with TPC-W bench mark, whose results can demonstrate the effectiveness of our approach.","0730-3157","978-0-7695-4439-7","10.1109/COMPSAC.2011.10","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6032318","web applications;capacity planning;bottleneck detection;performance modeling;Kalman filter","Servers;Monitoring;Kalman filters;Adaptation models;Capacity planning;Analytical models;Databases","Kalman filters;Web services","adaptive performance modeling approach;performance profiling;multiservice Web application;dynamic service demands;system resources;dynamic bottleneck switches;log files;internal service relation;Kalman filter;capacity planning;bottleneck detection","","3","","33","","3 Oct 2011","","","IEEE","IEEE Conferences"
"An Event-based Local Action Model for Queriable Wireless Sensor Actuator Networks","R. Bergelt; W. Hardt","Chemnitz University of Technology,Professorship for Computer Engineering,Chemnitz,Germany; Chemnitz University of Technology,Professorship for Computer Engineering,Chemnitz,Germany","2019 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)","21 Nov 2019","2019","","","1","6","Wireless sensor networks (WSN) are in use in a wide range of areas. Historically, they have been mostly used for passive surveillance of physical phenomena and data logging activities where measurements are transferred and stored outside the actual network. In recent years, with the emerge of Internet of Things (IoT) technologies and Vehicle2X communication scenarios novel challenges for wireless sensor network platforms - regarding hardware and software - arose. Queriable wireless sensor networks enable users to specify measurement tasks in a declarative way and thus allow also non-technicians to define network applications. Most current WSN applications feature active parts with which nodes can directly influence their environment, hence they are called wireless sensor actuator networks (WSAN). However, these actuator actions are often defined from outside the network in response to aggregated measurements or detected events, i.e. in a global fashion. In this paper, we propose a way to extend existing queriable wireless sensor network implementations in order to allow them to respond to events and to control actuators in an energy-efficient, timely and local manner which in turn improves the adaptivity and energy-efficiency of the network and its application. Additionally, we will show that this event system is viable to support distinct types of the sub query problem in WSN.","1847-358X","978-953-290-088-0","10.23919/SOFTCOM.2019.8903666","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8903666","wireless sensor actuator network;event system;queriable networks","","actuators;energy conservation;Internet of Things;query processing;telecommunication power management;vehicular ad hoc networks;wireless sensor networks","event-based local action model;wireless sensor actuator networks;data logging activities;WSN applications;Vehicle2X communication;Internet of Things technologies","","1","","23","","21 Nov 2019","","","IEEE","IEEE Conferences"
"Debug Support for Model-Based GUI Testing","H. Heiskanen; A. Jääskeläinen; M. Katara","Dept. of Software Syst., Tampere Univ. of Technol., Tampere, Finland; Dept. of Software Syst., Tampere Univ. of Technol., Tampere, Finland; Dept. of Software Syst., Tampere Univ. of Technol., Tampere, Finland","2010 Third International Conference on Software Testing, Verification and Validation","3 Jun 2010","2010","","","25","34","The fact that model-based testing has not yet attained a high rate of adoption in industry can in part be attributed to the perceived difficulty of debugging long error traces often produced by the online version of this technology. Given the extensive manual labor commonly involved in the debugging phase, automating parts of this process could yield considerable productivity benefits. This paper presents viable debugging strategies applicable in model-based graphical user interface testing, from which two methods were refined and experimented with. The first is based on superimposing log-derived, synchronized subtitles on recorded test run footage, while the second addresses error trace shortening. The results obtained from applying these methods in real-life case studies demonstrate the practical utility of these methods.","2159-4848","978-1-4244-6436-4","10.1109/ICST.2010.36","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5477102","","Graphical user interfaces;Software testing;Automatic testing;System testing;Productivity;Software debugging;Automation;Software systems;Computer industry;Production","graphical user interfaces;program debugging;program testing","debug support;model-based GUI testing;debugging phase;graphical user interface;recorded test run footage;error trace shortening","","4","2","16","","3 Jun 2010","","","IEEE","IEEE Conferences"
"Design and Evaluation of a Smartphone Based Wearable Life-Logging and Social Interaction System","W. Burns; C. Nugent; P. McCullagh; H. Zheng","Comput. Sci. Res. Inst., Univ. of Ulster, Newtownabbey, UK; Comput. Sci. Res. Inst., Univ. of Ulster, Newtownabbey, UK; Comput. Sci. Res. Inst., Univ. of Ulster, Newtownabbey, UK; Comput. Sci. Res. Inst., Univ. of Ulster, Newtownabbey, UK","2014 IEEE 27th International Symposium on Computer-Based Medical Systems","25 Aug 2014","2014","","","435","440","In this paper we outline the design, development and evaluation of a smartphone based life-logging and social interaction reminder system intended for use by persons with dementia. By using a smartphone, the wearer's daily activities can be recorded in picture format, along with meta data providing activity levels and location data. In addition to this data, social interactions can also be logged and subsequently identified, using Quick Response (QR) codes. The intervention was evaluated on six healthy participants aged between 24 - 46 years of age who wore the system for 2.5 hours. The qualitative feedback received was that the technology was easy to use and was responsive and accurate at identifying, recording and displaying social interaction data.","2372-9198","978-1-4799-4435-4","10.1109/CBMS.2014.42","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6881921","Smartphone;Life-logging;Dementia;Social Interactions;User Interaction","Software;Dementia;Real-time systems;Biomedical monitoring;Cameras;Sensors","medical computing;patient care;smart phones;social sciences computing","smartphone;wearable life-logging system;social interaction reminder system;dementia;daily activity recording;meta data;quick response codes;QR codes;time 2.5 hour","","5","","20","","25 Aug 2014","","","IEEE","IEEE Conferences"
"Defect Analysis and Reliability Assessment for Transactional Web Applications","W. Jaffal; J. Tian","Dept. of Comput. Sci. & Eng., Southern Methodist Univ., Dallas, TX, USA; Northwestern Polytech. Univ., Xi'an, China","2014 IEEE International Symposium on Software Reliability Engineering Workshops","15 Dec 2014","2014","","","245","250","In this research, we analyze defects collected from different failure sources and use it for reliability assessment. Failure is the inability of a system or component to perform its required functions within specified performance requirements. Reliability is the probability of failure-free operations, and it is one of the most important quality attribute for the users of web applications. Reliability analysis and modeling can help Web service providers to assess current reliability, predict future reliability, and quantify potential improvement to reach reliability target. The failure information and the related workload measurements are required input for various reliability models under variable work load such as modeling transactional web applications. In this paper, we used session tracking data in addition to web server logs to characterize and measure the variable application workload. Session data provides accurate measurements of web applications' usage time such as the number of transactions that are required for reliability analysis. In addition to web failures recorded in web logs and defect tracking system, we also considered failures from the server side application error logs to quantify unique failures that occur on the web server. We applied different reliability assessment approaches covering time domain and input domain on data from our case study application and demonstrated the applicability and effectiveness of our approach.","","978-1-4799-7377-4","10.1109/ISSREW.2014.12","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6983847","Defect Analysis;Workload Measurement;Software Reliability Analysis;Reliability Modeling","Web servers;Software reliability;Time measurement;Data models","software quality;software reliability;Web services","defect analysis;reliability assessment;transactional Web application;failure-free operation;quality attribute;Web service;session tracking data;Web server logs;Web failure;defect tracking system","","1","","19","","15 Dec 2014","","","IEEE","IEEE Conferences"
"Workflow Model Mining Based On Educational Management Data Logs","N. Cheng; L. Wang; R. Fei; W. Li; B. Wang","School of Computer Science and Engineering, Xi’an University of Technology, Xi’an, 710048; School of Computer Science and Engineering, Xi’an University of Technology, Xi’an, 710048; School of Computer Science and Engineering, Xi’an University of Technology, Xi’an, 710048; School of Computer Science and Engineering, Xi’an University of Technology, Xi’an, 710048; School of Computer Science and Engineering, Xi’an University of Technology, Xi’an, 710048","2019 Chinese Control And Decision Conference (CCDC)","12 Sep 2019","2019","","","5450","5455","Process model plays a guiding role in the programming and configuration phases, while data plays a leading role in the implementation, monitoring, and diagnostic phases. The paper selects the logs data in Xi'an Education Statistics Basic Database Platform as the data source and the purpose is to mine the real data service flow. During the mining process, we need to solve the two-loop problem. At the end of the paper, the improved algorithm results are measured by four quality dimensions (fitness, simplicity, precision and generalization), and finally the complete process model from the event logs is discovered. The results indicate that the improvements have an important influence for process model optimization.","1948-9447","978-1-7281-0106-4","10.1109/CCDC.2019.8832988","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8832988","Process Mining;Two-loop Problem;Compliance Analysis","Petri nets;Data mining;Computational modeling;Education;Task analysis;Machine learning algorithms","data mining;educational administrative data processing;workflow management software","educational management data logs;event logs;process model optimization;complete process model;quality dimensions;two-loop problem;mining process;data service flow;data source;Xian Education Statistics Basic Database Platform;logs data;diagnostic phases;configuration phases;programming;workflow model mining","","","","18","","12 Sep 2019","","","IEEE","IEEE Conferences"
"A Comprehensive Solution for Deterministic Replay Debugging of SoftPLC Applications","H. Prahofer; R. Schatz; C. Wirth; H. Mossenbock","Christian Doppler Laboratory for Automated Software Engineering, Institute for Systems Software, Johannes Kepler University, Linz, Austria; Christian Doppler Laboratory for Automated Software Engineering, Institute for Systems Software, Johannes Kepler University, Linz, Austria; Christian Doppler Laboratory for Automated Software Engineering, Institute for Systems Software, Johannes Kepler University, Linz, Austria; Christian Doppler Laboratory for Automated Software Engineering, Institute for Systems Software, Johannes Kepler University, Linz, Austria","IEEE Transactions on Industrial Informatics","7 Nov 2011","2011","7","4","641","651","Deterministic replay debugging is an approach to finding bugs in deployed software. It records an application run in the field so that it can deterministically be replayed offline in a development system for debugging purposes. To enable deterministic replay debugging, it is necessary to record all external influences and sources of nondeterminism in the original program run. From that trace log and from a known initial state, the program can be replayed deterministically without requiring any connection to the original environment. In this paper, we present a solution for deterministic replay debugging of hard real-time multitasking SoftPLC applications written in the IEC 61131-3 languages. By taking advantage of the special properties of these programs and by careful engineering, our technique allows recording a SoftPLC application run in the field with minimal overhead and obeying real-time constraints. In later phases, which are offline, the original program run is reconstructed from the minimal information recorded so it can be replayed for debugging. In comparison to previous work, our solution has several advantages: Instead of recording task scheduling information, it reconstructs the task interleaving based on data dependencies, thereby significantly simplifying the recording phase. Additionally, it incorporates a technique for periodically capturing the complete internal state of the system, which can later be used as a starting point for replay. We present the conceptual basis of our approach, a tool chain which provides deterministic replay debugging to the user as a set of fully automated tools, and an evaluation as well as an industrial case study for validating the approach.","1941-0050","","10.1109/TII.2011.2166768","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6009197","Software tracing;real-time debugging;PLC applications","Real time systems;Debugging;Software debugging","control engineering computing;high level languages;IEC standards;multiprogramming;program debugging;programmable controllers;real-time systems","deterministic replay debugging;SoftPLC application;real-time multitasking;IEC 61131-3 language;task interleaving reconstruction;data dependency","","25","","34","","5 Sep 2011","","","IEEE","IEEE Journals"
"Evaluation of authorship attribution software on a Chat bot corpus","N. Ali; M. Hindi; R. V. Yampolskiy","Computer Engineering and Computer Science, J. B. Speed School of Engineering, University of Louisville, Louisville, KY. USA; Computer Engineering and Computer Science, J. B. Speed School of Engineering, University of Louisville, Louisville, KY. USA; Computer Engineering and Computer Science, J. B. Speed School of Engineering, University of Louisville, Louisville, KY. USA","2011 XXIII International Symposium on Information, Communication and Automation Technologies","15 Dec 2011","2011","","","1","6","Authorship recognition is a technique used to identify the author of an unclaimed document, or in case when more than one author claims a document. Authorship recognition has great potential for applications in Computer forensics. The intended goal of this research is to identify a Chat bot by analyzing conversation log files. This is a novel area of investigation, as artificially intelligent authors have not been profiled based on their linguistic behavior. The collected data comes from chat logs between different Chat Bots and between Chat Bots and Human users. The initial experiments utilizing collected data demonstrate the feasibility of our approach.","","978-1-4577-0746-9","10.1109/ICAT.2011.6102123","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6102123","Authorship attribution;Authorship recognition;Chat bot;JGAAP;Stylometry","Accuracy;Feature extraction;Humans;Educational institutions;Entropy;Computers;Computer science","artificial intelligence;computer forensics;text analysis","authorship attribution software;chat bot corpus;authorship recognition;computer forensics;conversation log files","","8","","24","","15 Dec 2011","","","IEEE","IEEE Conferences"
"Challenges and Directions in Security Information and Event Management (SIEM)","M. Cinque; D. Cotroneo; A. Pecchia","Dipt. di Ing. Elettr. e delle Tecnol. dell'Inf., Univ. degli Studi di Napoli Federico II, Naples, Italy; Dipt. di Ing. Elettr. e delle Tecnol. dell'Inf., Univ. degli Studi di Napoli Federico II, Naples, Italy; Dipt. di Ing. Elettr. e delle Tecnol. dell'Inf., Univ. degli Studi di Napoli Federico II, Naples, Italy","2018 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)","18 Nov 2018","2018","","","95","99","Security Information and Event Management (SIEM) is the state-of-the-practice in handling heterogeneous data sources for security analysis. This paper presents challenges and directions in SIEM in the context of a real-life mission critical system by a top leading company in the Air Traffic Control domain. The system emits massive volumes of highly-unstructured text logs. We present the challenges in addressing such logs, ongoing work on the integration of an open source SIEM, and directions in modeling system behavioral baselines for inferring compromise indicators. Our explorative analysis paves the way for data discovery approaches aiming to complement the current SIEM practice.","","978-1-5386-9443-5","10.1109/ISSREW.2018.00-24","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8539170","SIEM;security;log analysis;information retrieval;Latent Dirichlet Allocation","Security;XML;Monitoring;Correlation;Runtime;Industries;Standards","air traffic control;security of data;text analysis","heterogeneous data sources;security analysis;real-life mission critical system;highly-unstructured text logs;open source SIEM;system behavioral baselines;data discovery;air traffic control domain;security information and event management;SIEM practice","","2","","10","","18 Nov 2018","","","IEEE","IEEE Conferences"
"DReAM: Deep Recursive Attentive Model for Anomaly Detection in Kernel Events","O. M. Ezeme; Q. H. Mahmoud; A. Azim","Department of Electrical, Computer and Software Engineering, University of Ontario Institute of Technology, Oshawa, Canada; Department of Electrical, Computer and Software Engineering, University of Ontario Institute of Technology, Oshawa, Canada; Department of Electrical, Computer and Software Engineering, University of Ontario Institute of Technology, Oshawa, Canada","IEEE Access","17 Feb 2019","2019","7","","18860","18870","System logs and traces contain information that reflects the state of the system and serves as a rich source of knowledge for system monitoring from the application to the kernel layer. Moreover, logging of traces as a tool for monitoring the operation of a cyber-physical system is recommended by most safety standard organizations. However, because the data can be overwhelmingly huge within a short space of time, the use of models that do not rely only on known signatures for online anomaly detection becomes difficult to use due to the challenge of processing such an enormous amount of data at runtime. Hence, most practitioners resort to the use of signature-based tools. In this paper, we introduce an anomaly detection model that uses intra-trace and inter-trace context vectors with long short-term memory networks to overcome the challenge of online anomaly detection in cyber-physical systems. We test the performance of the model with publicly available datasets that reflect the internal and external control flow of an embedded application and our model demonstrates both the effectiveness and robustness in detecting an anomalous sequence in a system call stream.","2169-3536","","10.1109/ACCESS.2019.2897122","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8633836","Anomaly detection;kernel events;embedded system;machine learning","Anomaly detection;Kernel;Context modeling;Computational modeling;Standards;Embedded systems;Unsupervised learning","cyber-physical systems;digital signatures;system monitoring","anomaly detection model;inter-trace context vectors;short-term memory networks;online anomaly detection;cyber-physical system;system logs;system monitoring;kernel layer;safety standard organizations;signature-based tools;deep recursive attentive model;intra-trace context vectors;system call stream;DReAM","","3","","26","","3 Feb 2019","","","IEEE","IEEE Journals"
"On event-driven knowledge graph completion in digital factories","M. Ringsquandl; E. Kharlamov; D. Stepanova; S. Lamparter; R. Lepratti; I. Horrocks; P. Kröger","Ludwig-Maximilians Universität, Munich, Germany; Oxford University, Oxford, United Kingdom; Max-Planck-Institut für Informatik, Saarbrücken, Germany; Siemens AG CT, Munich, Germany; Siemens PLM Software, Genoa, Italy; Oxford University, Oxford, United Kingdom; Ludwig-Maximilians Universität, Munich, Germany","2017 IEEE International Conference on Big Data (Big Data)","15 Jan 2018","2017","","","1676","1681","Smart factories are equipped with machines that can sense their manufacturing environments, interact with each other, and control production processes. Smooth operation of such factories requires that the machines and engineering personnel that conduct their monitoring and diagnostics share a detailed common industrial knowledge about the factory, e.g., in the form of knowledge graphs. Creation and maintenance of such knowledge is expensive and requires automation. In this work we show how machine learning that is specifically tailored towards industrial applications can help in knowledge graph completion. In particular, we show how knowledge completion can benefit from event logs that are common in smart factories. We evaluate this on the knowledge graph from a real world-inspired smart factory with encouraging results.","","978-1-5386-2715-0","10.1109/BigData.2017.8258105","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8258105","Industrial Applications;Machine Leaning;Knowledge Graphs;Events;Manufacturing","Production facilities;Manufacturing;Robots;Maintenance engineering;Virtual manufacturing","factory automation;graph theory;learning (artificial intelligence);production engineering computing;production facilities;virtual manufacturing","digital factories;machine learning;event logs;manufacturing environment;production process control;industrial event-driven knowledge graph;factory automation","","3","","17","","15 Jan 2018","","","IEEE","IEEE Conferences"
"Information Security in Virtualized Data Center Network","M. Klymash; O. Shpur; O. Lavriv; N. Peleh","Department of Telecommunication, Lviv Polytechnic National University, Lviv, Ukraine; Department of Telecommunication, Lviv Polytechnic National University, Lviv, Ukraine; Department of Telecommunication, Lviv Polytechnic National University, Lviv, Ukraine; Department of Telecommunication, Lviv Polytechnic National University, Lviv, Ukraine","2019 3rd International Conference on Advanced Information and Communications Technologies (AICT)","26 Sep 2019","2019","","","419","422","It is a matter of common knowledge that internet is not secure. Many instances have shown that there are people in this huge interconnection of networks that want to, with various intentions, steal others information, disrupt service of a general service provider and attack into systems to gain access or to bring them down. Network security has turned out to be a fundamental element of every organization to ensure secure internet connectivity and protection against data breach. While many organizations have turned towards Data Center service providers to save their time and effort on the acquisition, installation, management and security of hardware, servers and other devices, Data Centers themselves are not secure from attackers on the internet. This is high time for Data Center to prove their trustworthiness to customers by not only securing their data but also by providing them isolation from other customers that share the same infrastructure and by providing uninterrupted service with a minimum amount of downtime. To secure Data Centers networks and prevent data breaches, different vendors and Data Center professionals have suggested various solutions out of which some have been discussed in this paper. Moreover, as Data Center technology has been developing to fully adapt to automation through software abstraction, virtualization has become an inseparable part of it. In this publication, we offer our concept of data security in virtualized datacenters, based on an improved authentication process. Its essence is to generate an authentication message code that is transmitted along with the encrypted data in the cloud. This block will have a fixed data size and will be based on an encrypted 128-bit key file and log file of the user. It will act as a cryptographic checksum and will be used to validate the hypervisor of the TDV masters when accessing data.","","978-1-7281-2399-8","10.1109/AIACT.2019.8847764","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8847764","data center;information security;data categorization;log management (key words)","Data centers;Cloud computing;Encryption;Servers;Databases","computer centres;computer network security;cryptography;data protection;Internet","secure Internet connectivity;virtualized data center network;network interconnection;data breach protection;data center service providers;software abstraction;improved authentication process;log file;cryptographic checksum;TDV masters;encrypted data;data security;network security;general service provider;information security","","2","","16","","26 Sep 2019","","","IEEE","IEEE Conferences"
"A Runtime Verification Tool for Detecting Concurrency Bugs in FreeRTOS Embedded Software","S. A. Asadollah; D. Sundmark; S. Eldh; H. Hansson","Malardalen Univ., Vasteras, Sweden; Malardalen Univ., Vasteras, Sweden; Malardalen Univ., Vasteras, Sweden; Malardalen Univ., Vasteras, Sweden","2018 17th International Symposium on Parallel and Distributed Computing (ISPDC)","30 Aug 2018","2018","","","172","179","This article presents a runtime verification tool for embedded software executing under the open source real-time operating system FreeRTOS. The tool detects and diagnoses concurrency bugs such as deadlock, starvation, and suspension based-locking. The tool finds concurrency bugs at runtime without debugging and tracing the source code. The tool uses the Tracealyzer tool for logging relevant events. Analysing the logs, our tool can detect the concurrency bugs by applying algorithms for diagnosing each concurrency bug type individually. In this paper, we present the implementation of the tool, as well as its functional architecture, together with illustration of its use. The tool can be used during program testing to gain interesting information about embedded software executions. We present initial results of running the tool on some classical bug examples running on an AVR 32-bit board SAM4S.","","978-1-5386-5330-2","10.1109/ISPDC2018.2018.00032","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8452035","Concurrency Bugs;Runtime Verification Tool;Bug Detector;Embedded Software;FreeRTOS","Computer bugs;Tools;Task analysis;Concurrent computing;Runtime;Embedded software;Monitoring","concurrency control;embedded systems;microcontrollers;operating systems (computers);program debugging;program diagnostics;program testing;program verification;public domain software","runtime verification tool;FreeRTOS embedded software;open source real-time operating system FreeRTOS;source code;Tracealyzer tool;concurrency bug type;embedded software executions;program testing;AVR 32-bit board SAM4S","","1","","19","","30 Aug 2018","","","IEEE","IEEE Conferences"
"Design of a Concealed File System Adapted for Mobile Devices Based on GPS Information","Y. Jin; M. Tomoishi; S. Matsuura","Tokyo Inst. of Technol., Tokyo, Japan; Tokyo Inst. of Technol., Tokyo, Japan; Tokyo Inst. of Technol., Tokyo, Japan","2016 IEEE 40th Annual Computer Software and Applications Conference (COMPSAC)","25 Aug 2016","2016","1","","459","464","The Internet Security Threat Report by Symantec announced that the number of data breaches increased 23 percent in 2014 and the causes by theft or loss of devices reached to 21 percent. Carrying mobile devices with business data and private information is indispensable for human's social activities nowadays and unexpected data breach is one of the severe ongoing issues in cyber security. In this paper, we propose a concealed file system adapted for mobile devices based on GPS (Global Positioning System) information which is only mountable in the designated area. Differs from conventional encryption technologies, the proposed file system can be completely isolated from the viruses and attacks outside the designated area. Moreover, instead of the GPS information of the designated area, the encrypted hash value will be stored in mobile devices for the privacy concerns. We statistically analyzed the GPS information logged in our lab and defined an algorithm for deciding the designated area without leaking the GPS information. Based on the algorithm, we evaluated the proposed file system using Veracrypt by adding the hash of GPS information indicating the designated area as one of the attributes for mounting authentication. As a result, we confirmed that the proposed file system was mounted with about 91% success rate within average in the designated area even with noise interference.","0730-3157","978-1-4673-8845-0","10.1109/COMPSAC.2016.93","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7552049","Security;Mobile security;Location based security;Concealed file system","Global Positioning System;Mobile handsets;Encryption;Business;Authentication;Real-time systems","computer viruses;cryptography;data privacy;Global Positioning System;mobile computing","concealed file system;mobile devices;GPS information;Internet security threat report;Symantec;data breaches;cyber security;Global Positioning System information;viruses;encrypted hash value;privacy concerns;Veracrypt;authentication","","3","","13","","25 Aug 2016","","","IEEE","IEEE Conferences"
"Efficient lossless compression of CAN traffic logs","A. Gazdag; L. Buttyan; Z. Szalay","Laboratory of Cryptography and System Security, Department of Networked Systems and Services, Budapest University of Technology and Economics; Laboratory of Cryptography and System Security, Department of Networked Systems and Services, Budapest University of Technology and Economics; Department of Automotive Technologies, Faculty of Transportation Engineering and Vehicle Engineering, Budapest University of Technology and Economics","2017 25th International Conference on Software, Telecommunications and Computer Networks (SoftCOM)","23 Nov 2017","2017","","","1","6","In this paper, we propose a compression method that allows for the efficient storage of large amounts of CAN traffic data, which is needed for the forensic investigations of accidents caused by cyber attacks on vehicles. Compression of recorded CAN traffic also reduces the time (or bandwidth) needed to off-load that data from the vehicle. In addition, our compression method allows analysts to perform log analysis on the compressed data, therefore, it contributes to reduced analysis time and effort. We achieve this by performing semantic compression on the CAN traffic logs, rather than simple syntactic compression. Our compression method is lossless, thus preserving all information for later analysis. Besides all the above advantages, the compression ratio that we achieve is better than the compression ratio of state-of-the-art syntactic compression methods, such as gzip.","1847-358X","978-953-290-078-1","10.23919/SOFTCOM.2017.8115527","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8115527","","Semantics;Accidents;Forensics;Protocols;Compression algorithms;Syntactics","data compression;security of data","efficient lossless compression;traffic logs;traffic data;recorded CAN traffic;log analysis;compressed data;syntactic compression methods;cyber attacks","","1","","11","","23 Nov 2017","","","IEEE","IEEE Conferences"
"Deriving Process-Related RBAC Models from Process Execution Histories","A. Baumgrass; S. Schefer-Wenzl; M. Strembeck","Inst. for Inf. Syst. & New Media, Vienna Univ. of Econ. & Bus. (WU Vienna), Vienna, Austria; Inst. for Inf. Syst. & New Media, Vienna Univ. of Econ. & Bus. (WU Vienna), Vienna, Austria; Inst. for Inf. Syst. & New Media, Vienna Univ. of Econ. & Bus. (WU Vienna), Vienna, Austria","2012 IEEE 36th Annual Computer Software and Applications Conference Workshops","10 Nov 2012","2012","","","421","426","In a business process context, access permissions grant the rights to perform certain tasks. In particular, process-related role-based access control (RBAC) models define RBAC policies for process-aware information systems (PAIS). In addition, process-related RBAC models allow for the definition of entailment constraints on tasks, such as mutual exclusion or binding constraints, for example. This paper presents an approach to derive process-related RBAC models from process execution histories recorded by a PAIS. In particular, we present algorithms to derive corresponding RBAC artifacts and entailment constraints from standardized XML-based log files. All algorithms presented in this paper have been implemented and were tested via process logs created with CPN Tools.","","978-0-7695-4758-9","10.1109/COMPSACW.2012.80","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6341612","PAIS;RBAC;Constraints;Process Execution History","History;Contracts;Process control;Biological system modeling;Access control;Standards","authorisation;business data processing;information systems;XML","process-related RBAC models;process execution histories;business process context;access permissions;rights granting;process-related role-based access control models;RBAC policies;process-aware information systems;PAIS;entailment constraints;mutual exclusion;binding constraints;RBAC artifacts;standardized XML-based process log files;CPN tools","","6","","17","","10 Nov 2012","","","IEEE","IEEE Conferences"
"Controller-Oblivious Dynamic Access Control in Software-Defined Networks","S. R. Gomez; S. Jero; R. Skowyra; J. Martin; P. Sullivan; D. Bigelow; Z. Ellenbogen; B. C. Ward; H. Okhravi; J. W. Landry",MIT Lincoln Laboratory; MIT Lincoln Laboratory; MIT Lincoln Laboratory; MIT Lincoln Laboratory; MIT Lincoln Laboratory; MIT Lincoln Laboratory; MIT Lincoln Laboratory; MIT Lincoln Laboratory; MIT Lincoln Laboratory; MIT Lincoln Laboratory,"2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)","22 Aug 2019","2019","","","447","459","Conventional network access control approaches are static (e.g., user roles in Active Directory), coarse-grained (e.g., 802.1x), or both (e.g., VLANs). Such systems are unable to meaningfully stop or hinder motivated attackers seeking to spread throughout an enterprise network. To address this threat, we present Dynamic Flow Isolation (DFI), a novel architecture for supporting dynamic, fine-grained access control policies enforced in a Software-Defined Network (SDN). These policies can emit and revoke specific access control rules automatically in response to network events like users logging off, letting the network adaptively reduce unnecessary reachability that could be potentially leveraged by attackers. DFI is oblivious to the SDN controller implementation and processes new packets prior to the controller, making DFI's access control resilient to a malicious or faulty controller or its applications. We implemented DFI for OpenFlow networks and demonstrated it on an enterprise SDN testbed with around 100 end hosts and servers. Finally, we evaluated the performance of DFI and how it enables a novel policy, which is otherwise difficult to enforce, that protects against a surrogate of the recent NotPetya malware in an infection scenario. We found that the threat was most limited in its ability to spread using our policy, which automatically restricted network flows over the course of the attack, compared to no access control or a static role-based policy.","1530-0889","978-1-7281-0057-9","10.1109/DSN.2019.00053","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8809519","network security;software defined networking;SDN;access control","Access control;IP networks;Ethernet;Software;Authentication;Computer architecture","authorisation;invasive software;software defined networking","fine-grained access control policies;SDN controller implementation;malicious controller;faulty controller;OpenFlow networks;enterprise SDN;static role-based policy;controller-oblivious dynamic access control;dynamic flow isolation;software-defined network;DFIs access control;network access control approaches;NotPetya malware","","","","55","","22 Aug 2019","","","IEEE","IEEE Conferences"
"Anti-Aging LFS: Self-Defragmentation With Fragmentation-Aware Cleaning","J. Park; Y. I. Eom","Department of Computer Science and Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Electrical and Computer Engineering, College of Computing, Sungkyunkwan University, Suwon, South Korea","IEEE Access","26 Aug 2020","2020","8","","151474","151486","For several decades, filesystem aging has been widely studied, but nonetheless, it still remains an unsolved problem. Among various filesystems, log-structured filesystems have been reported to be vulnerable to fragmentation due to their append-only write policy. Fragmentation hinders various I/O activities such as sequential read and trim operations, regardless of the underlying storage types. This paper extensively analyzes the rationale behind performance degradation incurred by fragmentation on various types of storage devices. To eliminate fragmentation without additional I/O overhead, we propose an anti-aging log-structured filesystem, called AALFS. During segment cleaning, AALFS re-arranges the order of valid blocks based on inode number and file offset to eliminate existing fragmentation. To enhance the efficacy of the re-ordering process, the new victim selection policy of AALFS reflects the fragmentation degree of each segment in the selection of victim segments. Our experimental results show that AALFS effectively eliminates fragmentation by up to 99.8% and significantly improves sequential read performance on various types of storage devices. Particularly, AALFS improves the sequential read throughput of IOzone on hard disk drives by up to 22.8 times.","2169-3536","","10.1109/ACCESS.2020.3017240","Institute of Information & communications Technology Planning & Evaluation (IITP) grant; Korea government (MSIT) (IITP-2015-0-00284, (SW Starlab) Development of UX Platform Software for Supporting Concurrent Multi-users on Large Displays) and Basic Science Research Program through the National Research Foundation of Korea (NRF); Ministry of Science and ICT; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9169877","Fragmentation;log-structured filesystems;operating systems;storage","Performance evaluation;Cleaning;Aging;Kernel;Degradation;Licenses;Hard disks","ageing;disc drives;hard discs;storage management","storage devices;segment cleaning;anti-aging LFS;fragmentation-aware cleaning;log-structured file system;sequential read operations;file system aging;AALFS;anti-aging log-structured file system;hard disk drive;append-only write policy","","","","29","CCBY","17 Aug 2020","","","IEEE","IEEE Journals"
"Big Data Meets HPC Log Analytics: Scalable Approach to Understanding Systems at Extreme Scale","B. H. Park; S. Hukerikar; R. Adamson; C. Engelmann","Comput. Sci. & Math. Div., Oak Ridge Nat. Lab., Oak Ridge, TN, USA; Comput. Sci. & Math. Div., Oak Ridge Nat. Lab., Oak Ridge, TN, USA; Nat. Center for Comput. Sci., Oak Ridge Nat. Lab., Oak Ridge, TN, USA; Comput. Sci. & Math. Div., Oak Ridge Nat. Lab., Oak Ridge, TN, USA","2017 IEEE International Conference on Cluster Computing (CLUSTER)","25 Sep 2017","2017","","","758","765","Today's high-performance computing (HPC) systems are heavily instrumented, generating logs containing information about abnormal events, such as critical conditions, faults, errors and failures, system resource utilization, and about the resource usage of user applications. These logs, once fully analyzed and correlated, can produce detailed information about the system health, root causes of failures, and analyze an application's interactions with the system, providing valuable insights to domain scientists and system administrators. However, processing HPC logs requires a deep understanding of hardware and software components at multiple layers of the system stack. Moreover, most log data is unstructured and voluminous, making it more difficult for system users and administrators to manually inspect the data. With rapid increases in the scale and complexity of HPC systems, log data processing is becoming a big data challenge. This paper introduces a HPC log data analytics framework that is based on a distributed NoSQL database technology, which provides scalability and high availability, and the Apache Spark framework for rapid in-memory processing of the log data. The analytics framework enables the extraction of a range of information about the system so that system administrators and end users alike can obtain necessary insights for their specific needs. We describe our experience with using this framework to glean insights from the log data about system behavior from the Titan supercomputer at the Oak Ridge National Laboratory.","2168-9253","978-1-5386-2326-8","10.1109/CLUSTER.2017.113","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8049013","log data analytics;big data processing;system monitoring","Distributed databases;Data models;Monitoring;Correlation;Graphics processing units;Sensors;Data mining","Big Data;data analysis;parallel processing;relational databases;SQL","HPC systems;data processing;big data challenge;HPC log data analytics framework;high-performance computing systems;system resource utilization;user applications;HPC logs;system stack;Titan supercomputer;Oak Ridge National Laboratory;distributed NoSQL database technology;Apache Spark framework","","7","","23","","25 Sep 2017","","","IEEE","IEEE Conferences"
"Event Based Robot Prognostics Using Principal Component Analysis","V. Sathish; S. D. Sudarsan; S. Ramaswamy","Dev. Center, ABB, India; Dev. Center, ABB, India; Dev. Center, ABB, India","2014 IEEE International Symposium on Software Reliability Engineering Workshops","15 Dec 2014","2014","","","14","17","As industrial systems are getting complicated, challenges in coming up with efficient maintenance strategies which include predicting failures in the system become important industry specific research topic. Traditionally, research focuses on developing failure prediction models based on physical understanding of the system. But, development of such models are often time consuming and labour intensive for complex systems. In recent past, due to advent of cheaper data collection mechanisms and efficient algorithms, data driven approaches for predicting failures are gaining significant interest in industrial research community. In this paper, we provide a Principal component Analysis (PCA) based approach of failure prediction in industrial robots using event log information. The event logs are collected through remote service set-up from a robot controller. The proposed method will reduce the dimensionality of the original data which consist of interrelated events while retaining the variation present in the data. Using PCA and multivariate statistics such as Hotelling T2, Q Residuals and Q contributions charts, we are able to detect abnormal behavior of event pattern within 30 days before failure.","","978-1-4799-7377-4","10.1109/ISSREW.2014.52","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6983790","Robot Fault Prognosis;Principal Component Analysis","Principal component analysis;Maintenance engineering;Service robots;Mathematical model;Covariance matrices;Process control","failure analysis;industrial control;industrial robots;maintenance engineering;principal component analysis","Q contribution charts;Q residuals;Hotelling T2;multivariate statistics;data dimensionality reduction;event log information;industrial robots;data collection mechanisms;complex systems;failure prediction models;maintenance strategies;industrial systems;principal component analysis;event based robot prognostics","","1","","15","","15 Dec 2014","","","IEEE","IEEE Conferences"
"Process mining-based approach for investigating malicious login events","S. Lagraa; R. State","University of Luxembourg,Interdisciplinary Centre for Security; University of Luxembourg,Interdisciplinary Centre for Security","NOMS 2020 - 2020 IEEE/IFIP Network Operations and Management Symposium","8 Jun 2020","2020","","","1","5","A large body of research has been accomplished on prevention and detection of malicious events, attacks, threats, or botnets. However, there is a lack of automatic and sophisticated methods for investigating malicious events/users, understanding the root cause of attacks, and discovering what is really happening before an attack. In this paper, we propose an attack model discovery approach for investigating and mining malicious authentication events across user accounts. The approach is based on process mining techniques on event logs reaching attacks in order to extract the behavior of malicious users. The evaluation is performed on a publicly large dataset, where we extract models of the behavior of malicious users via authentication events. The results are useful for security experts in order to improve defense tools by making them robust and develop attack simulations.","2374-9709","978-1-7281-4973-8","10.1109/NOMS47738.2020.9110301","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9110301","","","data mining;invasive software","malicious authentication events;user accounts;event logs;malicious users;attack simulations;process mining-based approach;malicious login events;attack model discovery approach;botnets;publicly large dataset","","","","27","","8 Jun 2020","","","IEEE","IEEE Conferences"
"EagleEye: A logging framework for accountable distributed and networked systems","N. Kathiresshan; Z. Xiao; Y. Xiao","Department of Computer Science, The University of Alabama, Tuscaloosa, 35487-0290 USA; Department of Computer Science, The University of Alabama, Tuscaloosa, 35487-0290 USA; Department of Computer Science, The University of Alabama, Tuscaloosa, 35487-0290 USA","2011 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)","23 Jun 2011","2011","","","958","963","We propose EagleEye, an accountable logging framework as a middleware for distributed and networked systems. EagleEye offloads the logging function from the distributed application program so that applications can focus on the logic handling without worrying about when and how to do logging. By capturing and analyzing network packets, EagleEye is able to reproduce the entire networking event history in the application layer, which is the basis of implementing an accountable system. We provide a case study by replacing the logging component of PeerReview [1] with EagleEye. The evaluation result shows that EagleEye can achieve equivalent accountability without modifying the host application program, which can save numerous workloads of modifying, republishing, and redeploying the host software.","","978-1-4577-0248-8","10.1109/INFCOMW.2011.5928951","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5928951","","Peer to peer computing;Protocols;IP networks;Distributed databases;Software;Computers;Servers","middleware;system monitoring","EagleEye;middleware;distributed application program;PeerReview;logging framework;accountable logging framework;distributed systems;networked systems;network packets;networking event history","","1","","35","","23 Jun 2011","","","IEEE","IEEE Conferences"
"Partial discharge localization in transformers using monopole and log-spiral UHF sensors","H. H. Sinaga; B. T. Phung; T. R. Blackburn","University of New South Wales, Sydney 2052, Australia; University of New South Wales, Sydney 2052, Australia; University of New South Wales, Sydney 2052, Australia","2012 IEEE 10th International Conference on the Properties and Applications of Dielectric Materials","4 Oct 2012","2012","","","1","4","The location of a partial discharge (PD) source inside a transformer can be determined from the time differences of arrival (TDOA) between signals that are captured by an array of UHF sensors. From these, the PD location can be found by geometric triangulation which involves solving a set of non-linear equations. This can be achieved using an efficient software realization of the maximum-likelihood estimator. The recorded PD waveforms are affected by the type of sensor used to capture PD signals. In this paper, the accuracy of the PD localization using different sensors is investigated. Two types of sensor, i.e. short monopole and log-spiral are used to capture electromagnetic waves emitted from a PD source. To calculate the TDOA, two methods: first peak and cross-correlation were applied. The localization result shows the monopole sensor produces higher accuracy than the log-spiral and the first peak method achieves better result than the cross-correlation method.","2160-9241","978-1-4673-2851-7","10.1109/ICPADM.2012.6319006","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6319006","partial discharge location;ultra-high frequency (UHF) sensors","Sensors","array signal processing;correlation methods;direction-of-arrival estimation;nonlinear equations;partial discharge measurement;power transformers;sensor arrays;time-of-arrival estimation;UHF detectors","partial discharge localization;power transformers;log-spiral UHF sensor array;monopole UHF sensor array;PD source;time-difference-of-arrival estimation;TDOA;geometric triangulation;nonlinear equations;maximum-likelihood estimator;software realization;PD signals;electromagnetic waves;cross-correlation method;first peak method","","3","","8","","4 Oct 2012","","","IEEE","IEEE Conferences"
"Virtual Machine Introspection for Anomaly-Based Keylogger Detection","H. Huseynov; K. Kourai; T. Saadawi; O. Igbe","City University of New York,Department of Electrical Engineering,City College,New York,United States; Kyushu Institute of Technology,Department of Computer Science and Networks,Fukuoka,Japan; City University of New York,Department of Electrical Engineering,City College,New York,United States; City University of New York,Department of Electrical Engineering,City College,New York,United States","2020 IEEE 21st International Conference on High Performance Switching and Routing (HPSR)","22 May 2020","2020","","","1","6","Software Keyloggers are dominant class of malicious applications that surreptitiously logs all the user activity to gather confidential information. Among many other types of keyloggers, API-based keyloggers can pretend as unprivileged program running in a user-space to eavesdrop and record all the keystrokes typed by the user. In a Linux environment, defending against these types of malware means defending the kernel against being compromised and it is still an open and difficult problem. Considering how recent trend of edge computing extends cloud computing and the Internet of Things (IoT) to the edge of the network, a new types of intrusion-detection system (IDS) has been used to mitigate cybersecurity threats in edge computing. Proposed work aims to provide secure environment by constantly checking virtual machines for the presence of keyloggers using cutting edge artificial immune system (AIS) based technology. The algorithms that exist in the field of AIS exploit the immune system's characteristics of learning and memory to solve diverse problems. We further present our approach by employing an architecture where host OS and a virtual machine (VM) layer actively collaborate to guarantee kernel integrity. This collaborative approach allows us to introspect VM by tracking events (interrupts, system calls, memory writes, network activities, etc.) and to detect anomalies by employing negative selection algorithm (NSA).","2325-5609","978-1-7281-4846-5","10.1109/HPSR48589.2020.9098980","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9098980","Artificial Immune System;Edge Computing;Virtual Machine Introspection;Keylogger;Spyware;Invasive Software;Genetic Algorithm","Immune system;Linux;Keyboards;Artificial intelligence;Virtual machining;Detectors;Malware","application program interfaces;artificial immune systems;cloud computing;data privacy;Internet of Things;invasive software;Linux;operating system kernels;virtual machines","API-based keyloggers;user-space;Linux environment;open problem;edge computing;cloud computing;intrusion-detection system;cybersecurity threat mitigation;secure environment;virtual machine layer;kernel integrity;network activities;virtual machine introspection;anomaly-based keylogger detection;Software Keyloggers;confidential information;malware;user activity;Internet of Things;IoT;artificial immune system based technology;host OS;collaborative approach;negative selection algorithm","","","","15","","22 May 2020","","","IEEE","IEEE Conferences"
"Understanding Architectural Knowledge Sharing in AGSD Teams: An Empirical Study","G. Borrego; A. L. Morán; R. Palacio; O. M. Rodríguez","Fac. de Cienc., Univ. Autonoma de Baja California, Ensenada, CA, USA; Fac. de Cienc., Univ. Autonoma de Baja California, Ensenada, CA, USA; Dept. de Comput. y Diseno, Inst. Tecnol. de Sonora, Navojoa, Mexico; Div. de Investig. y Posgrado, Inst. Tecnol. de Hermosillo, Hermosillo, Mexico","2016 IEEE 11th International Conference on Global Software Engineering (ICGSE)","29 Sep 2016","2016","","","109","118","Nowadays, the use of agile methodologies (AM) in Global Software Development (GSD) -- known as AGSD -- is increasingly common. However, AM and GSD are not completely compatible. On the one hand, in AM people interactions (face-to-face) are preferred over document-based communications to share knowledge. On the other hand, in GSD knowledge sharing is conducted through documents to minimize the effect of the inherent four distances (physical, temporal, language and cultural). This means that tacit knowledge is preferred in AM and explicit knowledge is preferred in GSD. These differences between AM and GSD affect many aspects of software development, for instance: Architectural Knowledge Management. According to the literature, in AGSD it is preferred to convey Architectural Knowledge (AK) by frequent interactions across sites through unstructured and textual electronic media (UTEM) (chats, emails, forums, etc.), that is, AK is articulated in these media. UTEM leave a textual record of the transmitted information, thus leaving an unstructured log of the shared AK of the project. In this paper we present an empirical study to understand AK articulation in UTEM in AGSD teams. Our results consist of an ontology that represents the involved aspects in AK articulation in UTEM in AGSD teams. Additionally, we identified eleven categories of interactions across sites through UTEM, where requirements and coding themes are prominent. Finally, we found that AK in UTEM is perceived as important, regardless the interaction frequency. These results lead us to think that a tool to structure and exploit AK in UTEM is needed in AGSD, in order to bridge the gap between AM and GSD.","2329-6313","978-1-5090-2680-7","10.1109/ICGSE.2016.29","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7577427","agile global software development;virtual teams;architectural knowledge;unstructured and textual electronic media","Companies;Software;Media;Electronic mail;Documentation;Knowledge engineering;Monitoring","knowledge management;ontologies (artificial intelligence);software architecture;software prototyping","architectural knowledge sharing;AGSD teams;agile methodologies;global software development;face-to-face communications;document-based communications;tacit knowledge;explicit knowledge;architectural knowledge management;unstructured and textual electronic media;UTEM;AK articulation;ontology","","8","","40","","29 Sep 2016","","","IEEE","IEEE Conferences"
"Triaging incoming change requests: Bug or commit history, or code authorship?","M. Linares-Vásquez; K. Hossen; H. Dang; H. Kagdi; M. Gethers; D. Poshyvanyk","Computer Science Department, The College of William and Mary, Williamsburg, VA 23185; Department of Computer Science Wichita State University Wichita, KS 67260-0083; Department of Computer Science Wichita State University Wichita, KS 67260-0083; Department of Computer Science Wichita State University Wichita, KS 67260-0083; Computer Science Department, The College of William and Mary, Williamsburg, VA 23185; Computer Science Department, The College of William and Mary, Williamsburg, VA 23185","2012 28th IEEE International Conference on Software Maintenance (ICSM)","10 Jan 2013","2012","","","451","460","There is a tremendous wealth of code authorship information available in source code. Motivated with the presence of this information, in a number of open source projects, an approach to recommend expert developers to assist with a software change request (e.g., a bug fixes or feature) is presented. It employs a combination of an information retrieval technique and processing of the source code authorship information. The relevant source code files to the textual description of a change request are first located. The authors listed in the header comments in these files are then analyzed to arrive at a ranked list of the most suitable developers. The approach fundamentally differs from its previously reported counterparts, as it does not require software repository mining. Neither does it require training from past bugs/issues, which is often done with sophisticated techniques such as machine learning, nor mining of source code repositories, i.e., commits. An empirical study to evaluate the effectiveness of the approach on three open source systems, ArgoUML, JEdit, and MuCommander, is reported. Our approach is compared with two representative approaches: (1) using machine learning on past bug reports, and (2) based on commit logs. The presented approach is found to provide recommendation accuracies that are equivalent or better than the two compared approaches. These findings are encouraging, as it opens up a promising and orthogonal possibility of recommending developers without the need of any historical change information.","1063-6773","978-1-4673-2312-3","10.1109/ICSM.2012.6405306","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6405306","code authorship;information retrieval;change request;triaging;expert developer recommendations","Data mining;Accuracy;Software maintenance;Unified modeling language;Large scale integration;Support vector machines","information retrieval;learning (artificial intelligence);program debugging;public domain software;recommender systems","open source projects;bug fixing;information retrieval technique;source code authorship information;source code files;software change request textual description;header comments;ArgoUML;JEdit;MuCommander;machine learning;commit log history;recommendation accuracies","","55","2","41","","10 Jan 2013","","","IEEE","IEEE Conferences"
"Cloud-Based Secure Logger for Medical Devices","H. Nguyen; B. Acharya; R. Ivanov; A. Haeberlen; L. T. X. Phan; O. Sokolsky; J. Walker; J. Weimer; W. Hanson; I. Lee","Dept. of Comput. & Inf. Sci., Univ. of Pennsylvania, Philadelphia, PA, USA; Dept. of Comput. & Inf. Sci., Univ. of Pennsylvania, Philadelphia, PA, USA; Dept. of Comput. & Inf. Sci., Univ. of Pennsylvania, Philadelphia, PA, USA; Dept. of Comput. & Inf. Sci., Univ. of Pennsylvania, Philadelphia, PA, USA; Dept. of Comput. & Inf. Sci., Univ. of Pennsylvania, Philadelphia, PA, USA; Dept. of Comput. & Inf. Sci., Univ. of Pennsylvania, Philadelphia, PA, USA; NA; Dept. of Comput. & Inf. Sci., Univ. of Pennsylvania, Philadelphia, PA, USA; Hosp. of the Univ. of Pennsylvania, PA, USA; Dept. of Comput. & Inf. Sci., Univ. of Pennsylvania, Philadelphia, PA, USA","2016 IEEE First International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)","18 Aug 2016","2016","","","89","94","A logger in the cloud capable of keeping a secure, time-synchronized and tamper-evident log of medical device and patient information allows efficient forensic analysis in cases of adverse events or attacks on interoperable medical devices. A secure logger as such must meet requirements of confidentiality and integrity of message logs and provide tamper-detection and tamper-evidence. In this paper, we propose a design for such a cloud-based secure logger using the Intel Software Guard Extensions (SGX) and the Trusted Platform Module (TPM). The proposed logger receives medical device information from a dongle attached to a medical device. The logger relies on SGX, TPM and standard encryption to maintain a secure communication channel even on an untrusted network and operating system. We also show that the logger is resilient against different kinds of attacks such as Replay attacks, Injection attacks and Eavesdropping attacks.","","978-1-5090-0943-5","10.1109/CHASE.2016.48","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7545819","intel software guard extensions;secure logger","Medical diagnostic imaging;Software;Radiation detectors;Safety;Cryptography;Communication channels","cloud computing;cryptography;data integrity;digital forensics;medical information systems;trusted computing","time-synchronized log;tamper-evident log;patient information;forensic analysis;adverse events;interoperable medical device attacks;message log confidentiality;message log integrity;tamper-detection;cloud-based secure logger design;Intel Software Guard Extensions;trusted platform module;medical device information;dongle;TPM;Intel SGX;standard encryption;secure communication channel;untrusted network;operating system;replay attacks;injection attacks;eavesdropping attacks","","10","","16","","18 Aug 2016","","","IEEE","IEEE Conferences"
"Variadic Parameter Command Pattern and the Utilization in MVC","L. Yingda; L. Jianzhuang; C. Xiaowen","School of Computer, National University of Defense Technology Kaifu, Changsha, China; School of Computer, National University of Defense Technology Kaifu, Changsha, China; School of Computer, National University of Defense Technology Kaifu, Changsha, China","2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS)","10 Mar 2019","2018","","","374","377","As software design pattern, command pattern encapsulates operation requests as command objects, which is benefit for the realization of queuing requests, recording logs and undo operations. Aiming at the poor universality of classical command pattern and combining with the new characteristic of variadic template in C++ 11, the variadic parameter command pattern that is of great universality is proposed. MVC architecture separates the GUI and model logic of application, which can facilitate the development and maintenance. The design that inserting command layer into MVC is introduced and the MCVC architecture is proposed.","2327-0594","978-1-5386-6565-7","10.1109/ICSESS.2018.8663736","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8663736","command pattern;variadic parameter;MVC;command layer","Receivers;Software;C++ languages;Computer architecture;Compounds;Graphical user interfaces;Maintenance engineering","C++ language;graphical user interfaces;object-oriented methods;object-oriented programming;software architecture","command objects;queuing requests;variadic parameter command pattern;software design pattern;operation requests;command pattern;variadic template;C++ 11;GUI;MCVC architecture","","","","10","","10 Mar 2019","","","IEEE","IEEE Conferences"
"Security system and actual operation benefit of data transmission on heterogeneous network","C. -W. Huang","Information Management Center, National Chung-Shan Institute of Science & Technology, 481 Jai'an Sec., Zhongzhen Rd., Lungtan, Taoyuan 32599, R.O.C.","2015 International Carnahan Conference on Security Technology (ICCST)","25 Jan 2016","2015","","","165","168","According to statistics, there are currently over a million website which provides knowledge of how to code a computer virus and how to be a cyber cracker. Coding malware and computer virus is different from traditional weapon system research and development, the high investment is not needed and there is also no policy-related constrain, people can launch cyber attack all the time. Due to hacking techniques renovates constantly, since from Distribute Denial of Service (DDoS), Session Hijacking, to Advance Persistent Threat (APT) that lead to paralysis of six corporations. In addition to protection of firewall, anti-virus software, and packet-filtering devices, it is more effective to isolate internal network from web to form several heterogeneous networks in a corporation. To satisfy data exchange or transmission requirements between heterogeneous networks, it is described in this paper on how to design and construct ""Heterogeneous Network System of Data transmission and Control"" based on requirements. The security of data transmission is designed based on cyber security requirements; data transmitted from network A to network B should be inspected through multiple specific areas which are equipped with different anti-virus software and information security policy, abnormal data will be blocked and logged. In order to isolate heterogeneous network A from network B, the Enable/Disable is utilized on switches of the first and the last inspection area to control data transmission. The switches are also ruled by tens of policies to assure one and only control system (ex. Access Control List). The whole process of data transmission is conducted automatically and single event and transmission result will be logged in the supervisory control apparatus for administrators. The security system is developed based on ""labor-cost effective"", ""high-security assurance"", ""highly hardware compatible"", and ""data transmission inspected"".","2153-0742","978-1-4799-8691-0","10.1109/CCST.2015.7389676","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7389676","heterogeneous network;cyber attack;cyber protection","Heterogeneous networks;Data communication;Information security;Process control;File servers;Control systems;Ports (Computers)","computer viruses;firewalls","security system;operation benefit;Web site;cyber cracker;malware coding;computer virus coding;distribute denial of service;DDoS;session hijacking;advance persistent threat;APT;firewall protection;antivirus software;packet-filtering devices;internal network;data exchange;transmission requirements;heterogeneous networks;heterogeneous network system;data transmission security;cyber security requirements;anti-virus software;information security policy;heterogeneous network isolation;supervisory control apparatus","","","","3","","25 Jan 2016","","","IEEE","IEEE Conferences"
"Applying high-performance bioinformatics tools for outlier detection in log data","M. Wurzenberger; F. Skopik; R. Fiedler; W. Kastner","Center for Digital Safety & Security, Austrian Inst. of Technol., Vienna, Austria; Center for Digital Safety & Security, Austrian Inst. of Technol., Vienna, Austria; Center for Digital Safety & Security, Austrian Inst. of Technol., Vienna, Austria; Vienna Univ. of Technol., Vienna, Austria","2017 3rd IEEE International Conference on Cybernetics (CYBCONF)","20 Jul 2017","2017","","","1","8","Most of today's security solutions, such as security information and event management (SIEM) and signature based IDS, require the operator to evaluate potential attack vectors and update detection signatures and rules in a timely manner. However, today's sophisticated and tailored advanced persistent threats (APT), malware, ransomware and rootkits, can be so complex and diverse, and often use zero day exploits, that a pure signature-based blacklisting approach would not be sufficient to detect them. Therefore, we could observe a major paradigm shift towards anomaly-based detection mechanisms, which try to establish a system behavior baseline - either based on netflow data or system logging data - and report any deviations from this baseline. While these approaches look promising, they usually suffer from scalability issues. As the amount of log data generated during IT operations is exponentially growing, high-performance analysis methods are required that can handle this huge amount of data in real-time. In this paper, we demonstrate how high-performance bioinformatics tools can be applied to tackle this issue. We investigate their application to log data for outlier detection to timely reveal anomalous system behavior that points to cyber attacks. Finally, we assess the detection capability and run-time performance of the proposed approach.","","978-1-5386-2201-8","10.1109/CYBConf.2017.7985760","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7985760","","Tools;Amino acids;Bioinformatics;Biological system modeling;Clustering algorithms;Security;Biological information theory","bioinformatics;invasive software;system monitoring","high-performance bioinformatics tools;outlier detection;log data;security information and event management;SIEM;signature based IDS;attack vectors;update detection signatures;advanced persistent threats;APT;malware;ransomware;rootkits;zero day exploits;anomaly-based detection mechanisms;system behavior baseline;netflow data;system logging data;IT operations;high-performance analysis methods;cyber attacks","","","","17","","20 Jul 2017","","","IEEE","IEEE Conferences"
"CADMANT: Context Anomaly Detection for MAintenance and Network Troubleshooting","E. Martinez; E. Fallon; S. Fallon; M. Wang","Software Research Institute, Athlone Institute of Technology, Ireland; Software Research Institute, Athlone Institute of Technology, Ireland; Software Research Institute, Athlone Institute of Technology, Ireland; Network Management Laboratory, Ericsson, Athlone Ireland","2015 International Wireless Communications and Mobile Computing Conference (IWCMC)","5 Oct 2015","2015","","","1017","1022","In telecommunications network troubleshooting, analytical applications are widely used. Such applications typically use CEP (Complex Event Processing) and SQL queries for data processing and network analysis. Performance engineers need in-depth knowledge of both the telecommunications domain and telecommunications data structures in order to create the required queries. Moreover valuable information contained in free form text data fields such as “additional_info”, “user_text” or “problem_text” can also be ignored. This work proposes CADMANT: Context Anomaly Detection for MAintenance and Network Troubleshooting. Traditional approaches focus on a specific record type and create specific cause and effect rules. With the CADMANT approach all free form text fields of alarms, logs, etc. are treated as text documents similar to Twitter feeds. CADMANT uses distance based outlier detection within sliding windows to detect abnormal terms at configurable time intervals. The CADMANT approach provides automated analysis without the requirement for SQL/CEP queries and provides distinct network insights in comparison to traditional approaches.","2376-6506","978-1-4799-5344-8","10.1109/IWCMC.2015.7289222","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7289222","outlier;text anomaly;distance based;sliding windows;search engine;telecommunications","Context;Telecommunications;Search engines;Indexes;Big data;Knowledge engineering;Data structures","data mining;data structures;query processing;SQL","CADMANT;context anomaly detection for maintenance and network troubleshooting;telecommunications network troubleshooting;complex event processing;CEP;SQL queries;data processing;network analysis;performance engineers;telecommunication data structures;telecommunication domain;CADMANT approach;text documents;Twitter feeds;outlier detection;sliding windows;abnormal term detection;automated analysis;SQL/CEP queries;data mining","","","","21","","5 Oct 2015","","","IEEE","IEEE Conferences"
"Taming of the Shrew: Modeling the Normal and Faulty Behaviour of Large-scale HPC Systems","A. Gainaru; F. Cappello; W. Kramer","Comput. Sci. Dept., UIUC, Urbana, IL, USA; INRIA, France; NCSA, UIUC, Urbana, IL, USA","2012 IEEE 26th International Parallel and Distributed Processing Symposium","16 Aug 2012","2012","","","1168","1179","HPC systems are complex machines that generate a huge volume of system state data called ""events"". Events are generated without following a general consistent rule and different hardware and software components of such systems have different failure rates. Distinguishing between normal system behaviour and faulty situation relies on event analysis. Being able to detect quickly deviations from normality is essential for system administration and is the foundation of fault prediction. As HPC systems continue to grow in size and complexity, mining event flows become more challenging and with the upcoming 10 Pet flop systems, there is a lot of interest in this topic. Current event mining approaches do not take into consideration the specific behaviour of each type of events and as a consequence, fail to analyze them according to their characteristics. In this paper we propose a novel way of characterizing the normal and faulty behaviour of the system by using signal analysis concepts. All analysis modules create ELSA (Event Log Signal Analyzer), a toolkit that has the purpose of modelling the normal flow of each state event during a HPC system lifetime, and how it is affected when a failure hits the system. We show that these extracted models provide an accurate view of the system output, which improves the effectiveness of proactive fault tolerance algorithms. Specifically, we implemented a filtering algorithm and short-term fault prediction methodology based on the extracted model and test it against real failure traces from a large-scale system. We show that by analyzing each event according to its specific behaviour, we get a more realistic overview of the entire system.","1530-2075","978-1-4673-0975-2","10.1109/IPDPS.2012.107","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6267920","fault tolerance;large-scale HPC systems;signal analysis;fault detection","Data mining;Correlation;Analytical models;Predictive models;Signal analysis;Prediction algorithms;Large-scale systems","data mining;failure analysis;large-scale systems;parallel processing;prediction theory;program diagnostics;signal processing;software fault tolerance","faulty behaviour;large-scale HPC systems;complex machines;system state data generation;events generation;software components;hardware components;failure rates;normal system behaviour;event analysis;system administration;event flows mining;10 PetaHop systems;event mining approaches;signal analysis;ELSA;event log signal analyzer;fault tolerance algorithms;short-term fault prediction methodology;filtering algorithm;real failure tracing","","44","1","29","","16 Aug 2012","","","IEEE","IEEE Conferences"
"Mining Invariants from SaaS Application Logs (Practical Experience Report)","S. Sarkar; R. Ganesan; M. Cinque; F. Frattini; S. Russo; A. Savignano","Infosys Labs., Bangalore, India; Infosys Labs., Bangalore, India; Univ. degli Studi di Napoli Federico II, Naples, Italy; Univ. degli Studi di Napoli Federico II, Naples, Italy; Univ. degli Studi di Napoli Federico II, Naples, Italy; Univ. degli Studi di Napoli Federico II, Naples, Italy","2014 Tenth European Dependable Computing Conference","26 May 2014","2014","","","50","57","The increasing popularity of Software as a Service (SaaS) stresses the need of solutions to predict failures and avoid service interruptions, which invariably result in SLA violations and severe loss of revenue. A promising approach to continuously monitor the correct functioning of the system is to check the execution conformance to a set of invariants, i.e., properties that must hold when the system is deemed to run correctly. In this paper we propose a framework and a tool to automatically discover invariants from application logs and to online detect their violation. The framework has been applied on 9 months of log events from a real-world SaaS application. Results show that the proposed tool is able to automatically select 12 invariants with a stringent goodness of fit criteria out of more than 500 potential relationships. We also show the usefulness of our approach to detect runtime issues from logs in the form of violations of selected invariants, corresponding to silent errors that usually go unnoticed by the system maintenance personnel, even if they could represent symptoms of upcoming service failures.","","978-1-4799-3804-9","10.1109/EDCC.2014.18","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6821088","invariant mining;SaaS;time coalescence;failure detection","Software as a service;Time series analysis;Monitoring;Data mining;Transient analysis;Personnel","cloud computing;data mining;software reliability","SaaS application logs;invariant mining;software as a service;SLA violations;system maintenance personnel;service failures;failure detection","","4","","19","","26 May 2014","","","IEEE","IEEE Conferences"
"Network events in a large commercial network: What can we learn?","A. Messager; G. Parisis; R. Harper; P. Tee; I. Z. Kiss; L. Berthouze","School of Engineering and Informatics, University of Sussex, Brighton, UK; School of Engineering and Informatics, University of Sussex, Brighton, UK; Moogsoft Ltd, River Reach, 31-35 High Street, Kingston-Upon-Thames, UK; Moogsoft Inc, 1265 Battery St, San Francisco, CA 94111; School of Engineering and Informatics, University of Sussex, Brighton, UK; School of Engineering and Informatics, University of Sussex, Brighton, UK","NOMS 2018 - 2018 IEEE/IFIP Network Operations and Management Symposium","9 Jul 2018","2018","","","1","6","ISP and commercial networks are complex and thus difficult to characterise and manage. Network operators rely on a continuous flow of event log messages to identify and handle service outages. However, there is little published information about such events and how they are typically exploited. In this paper, we describe in as much detail as possible the event logs and network topology of a major commercial network. Through analysing the network topology, textual information of events and time of events, we highlight opportunities and challenges brought by such data. In particular, we suggest that the development of methods for inferring functional connectivity could unlock more of the informational value of event log messages and assist network management operators.","2374-9709","978-1-5386-3416-5","10.1109/NOMS.2018.8406289","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8406289","","Network topology;Topology;Servers;Knowledge engineering;Databases;Electronic mail;Software","telecommunication network management","ISP;network management operators;textual information;network topology;event logs;commercial networks;network events","","2","","20","","9 Jul 2018","","","IEEE","IEEE Conferences"
"Hierarchical Abstraction of Execution Traces for Program Comprehension","Y. Feng; K. Dreef; J. Jones; A. van Deursen",University of California; University of California; University of California; Delft University of Technology,"2018 IEEE/ACM 26th International Conference on Program Comprehension (ICPC)","30 Jan 2020","2018","","","86","8610","Understanding the dynamic behavior of a software system is one of the most important and time-consuming tasks for today's software maintainers. In practice, understanding the inner workings of software requires studying the source code and documentation and inserting logging code in order to map high-level descriptions of the program behavior with low-level implementation, i.e., the source code. Unfortunately, for large codebases and large log files, such cognitive mapping can be quite challenging. To bridge the cognitive gap between the source code and detailed models of program behavior, we propose a fully automatic approach to present a semantic abstraction with different levels of functional granularity from full execution traces. Our approach builds multi-level abstractions and identifies frequent behaviors at each level based on a number of execution traces, and then, it labels phases within individual execution traces according to the identified major functional behaviors of the system. To validate our approach, we conducted a case study on a large-scale subject program, Javac, to demonstrate the effectiveness of the mining result. Furthermore, the results of a user study demonstrate that our approach is capable of presenting users a high-level comprehensible abstraction of execution behavior. Based on a real world subject program the participants in our user study were able to achieve a mean accuracy of 70%.","2643-7171","978-1-4503-5714-2","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8973063","Dynamic Analysis;Execution trace understanding;Program comprehension;multi level abstraction","","software maintenance;source code (software);system monitoring","source code;program behavior;semantic abstraction;execution traces;multilevel abstractions;large-scale subject program;high-level comprehensible abstraction;execution behavior;hierarchical abstraction;program comprehension;dynamic behavior;software maintainers;logging code;log files;cognitive mapping;Javac","","","","42","","30 Jan 2020","","","IEEE","IEEE Conferences"
"The CodeCompass Comprehension Framework","Z. Porkoláb; T. Brunner",Eötvös Loránd University; Eötvös Loránd University,"2018 IEEE/ACM 26th International Conference on Program Comprehension (ICPC)","30 Jan 2020","2018","","","393","3933","CodeCompass is an open source LLVM/Clang based tool developed by Ericsson Ltd. and the Eötvös Loránd University, Budapest to help understanding large legacy software systems. Based on the LLVM/Clang compiler infrastructure, CodeCompass gives exact information on complex C/C++ language elements like overloading, inheritance, the usage of variables and types, possible uses of function pointers and the virtual functions - features that various existing tools support only partially. Steensgaard's and Andersen's pointer analysis algorithm are used to compute and visualize the use of pointers/references. The wide range of interactive visualizations extends further than the usual class and function call diagrams; architectural, component and interface diagrams are a few of the implemented graphs. To make comprehension more extensive, CodeCompass is not restricted to the source code. It also utilizes build information to explore the system architecture as well as version control information e.g. git commit history and blame view. Clang based static analysis results are also integrated to CodeCompass. Although the tool focuses mainly on C and C++, it also supports Java and Python languages. In this demo we will simulate a typical bug-fixing work flow in a C++ system. First, we show, how to use the combined text and definition based search for a fast feature location. Here we also demonstrate our log search, which can be used to locate the code source of an emitted message. When we have an approximate location of the issue, we can start a detailed investigation understanding the class relationships, function call chains (including virtual calls, and calls via function pointers), and the read/write events on individual variables. We also visualize the pointer relationships. To make the comprehension complete, we check the version control information who committed the code, when and why. This Tool demo submission is complementing our Industry track submission with the similar title. A live demo is also available at the homepage of the tool https://github.com/ericsson/codecompass.","2643-7171","978-1-4503-5714-2","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8973042","code comprehension;C/C++ programming language;software visualization","","C++ language;configuration management;program compilers;program debugging;program visualisation;public domain software;software architecture;software maintenance;source code (software)","CodeCompass comprehension framework;legacy software systems;function pointers;virtual functions;pointer analysis algorithm;interactive visualizations;function call diagrams;interface diagrams;source code;system architecture;version control information;C++ system;code source;function call chains;bug-fixing work flow;Clang based static analysis;open source LLVM/Clang;LLVM/Clang compiler infrastructure;C/C++ language elements","","","","0","","30 Jan 2020","","","IEEE","IEEE Conferences"
"Effective Team Formation in Workflow Process Context","S. Lin; Z. Luo; Y. Yu; M. Pan","Sch. of Inf. Sci. & Technol., Sun Yat-Sen Univ., Guanzhou, China; Sch. of Inf. Sci. & Technol., Sun Yat-Sen Univ., Guanzhou, China; Sch. of Inf. Sci. & Technol., Sun Yat-Sen Univ., Guanzhou, China; Sch. of Software, Sun Yat-Sen Univ., Guanzhou, China","2013 International Conference on Cloud and Green Computing","19 Dec 2013","2013","","","508","513","With the purpose of improving the efficiency to accomplish a business process, how to form a multi-role collaborative team is a valuable research in the field of workflow. In this paper, based on the information about personal expertise and handover relations mined from event-log, we model the problem with Hidden Markov Models (HMM), then we use Viterbi algorithm to solve the problem. Finally we carry out experiments on actual data-set released by BPIC 2012 to evaluate the performance of our algorithm. The result shows we achieve at least 12% improve in the average efficiency of actual situation. Compared with the strategy only consider communication cost, our algorithm achieve a significant increase in efficiency (at most 408%), with a slight increase in communication cost (6.3%).","","978-0-7695-5114-2","10.1109/CGC.2013.85","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6686077","team formation;event-log;handover;HMM;business process","Hidden Markov models;Handover;Business;Viterbi algorithm;Context;Educational institutions","business data processing;data mining;groupware;hidden Markov models;team working;workflow management software","effective team formation;workflow process context;business process;multirole collaborative team;personal expertise;handover relations;event-log mining;hidden Markov models;HMM;Viterbi algorithm;BPIC 2012;communication cost","","4","","14","","19 Dec 2013","","","IEEE","IEEE Conferences"
"Contextual Linking between Workflow Provenance and System Performance Logs","E. e. K. Ahanach; S. Koulouzis; Z. Zhao",University of Amsterdam; University of Amsterdam; University of Amsterdam,"2019 15th International Conference on eScience (eScience)","19 Mar 2020","2019","","","634","635","When executing scientific workflows, anomalies of the workflow behavior are often caused by different issues such as resource failures at the underlying infrastructure. The provenance information collected by workflow management systems only captures the transformation of data at the workflow level. Analyzing provenance information and apposite system metrics requires expertise and manual effort. Moreover, it is often time-consuming to aggregate this information and correlate events occurring at different levels of the infrastructure. In this paper, we propose an architecture to automate the integration among workflow provenance information and performance information from the infrastructure level. Our architecture enables workflow developers or domain scientists to effectively browse workflow execution information together with the system metrics, and analyze contextual information for possible anomalies.","","978-1-7281-2451-3","10.1109/eScience.2019.00093","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9041690","Scientific workflow;provenance","Task analysis;Measurement;Monitoring;Cloud computing;Web services;Tools","natural sciences computing;workflow management software","system metrics;workflow execution information;contextual information;domain scientists;workflow developers;infrastructure level;performance information;workflow provenance information;workflow level;workflow management systems;resource failures;workflow behavior;executing scientific workflows;system performance logs;contextual linking","","","","10","","19 Mar 2020","","","IEEE","IEEE Conferences"
"Enabling a Platform for Habitat and Marine Assessment with Real Time Monitoring and Synchronous Databasing","A. Silverman; S. Butcher; C. Lembke; M. Lindemuth; S. Murawski","College of Marine Science Center for Ocean Technology, University of South Florida, St. Petersburg, Florida, 33701; College of Marine Science Center for Ocean Technology, University of South Florida, St. Petersburg, Florida, 33701; College of Marine Science Center for Ocean Technology, University of South Florida, St. Petersburg, Florida, 33701; Stratus Video Clearwater, Florida, 33755; College of Marine Science, University of South Florida, St. Petersburg, Florida, 33701","OCEANS 2018 MTS/IEEE Charleston","10 Jan 2019","2018","","","1","6","Quality data is important in making estimates of the current state of marine environments. At the Center for Ocean Technology at the University of South Florida, we have developed the Camera Based Assessment Survey System (C-BASS) towbody by combining the ease of off the shelf components with a custom software framework. C-BASS entails the integration of custom software and hardware components, off the shelf instrumentation, and web servers to distribute this data real-time to the user while storing all the parameters vital to the research endeavor.Key choices made during the design phase encompassed three main areas of focus; first and foremost was the ease of use for the operators of C-BASS while ensuring the robustness of the data collected. The topside client interface is based on a RESTful web api interface, creating a familiar environment for users entailing a modern web-browser as the only topside software for operators. The data collection entails a database synchronized to a GPS ship clock, including all towbody video, sensors, and sonar, as well as ship-sourced data such as sonars, positioning, and attitude. Therefore, in analyzing the data complex operations and custom data tables can be created from the relational SQL database. Second, there was a focus on the use of commercial off the shelf hardware components (COTS), software libraries, and applications to speed development. The base communications link was realized with digital subscriber line (DSL) modems over cables as basic as three internal conductors and a conductive outer shielding as a fourth conductor. While limiting the data sent through the uplink, the DSL modems remove the need for fiber optic cables and the operational needs associated them. Thus, the paradigm of a high bandwidth local data collection on the towbody and a low bandwidth real time link for real time monitoring of data quality during deployments, led to the development of a client-server model. Control and monitoring are done on the client side interface, while a small form factor COTS computer running Ubuntu Linux acts as the C-BASS main processor handling the server side backend, requests, and data recording.Last, prioritization was given to the ability for streamlined future enhancements. These enhancements can include additional or better cameras, new sensors, or changes to the user interface (UI). While the use of COTS components eases initial development, it also serves to keep the system in a modular design which assists later modifications. Further, the ability to utilize a wide variety of sensor interfaces allows for these components, and for a client side code display tailored to the user. However, it is in keeping an eye on being easily expandable that led to the design of sensor readers, with much of the code inherited from a generic parent function, along with the separation of such in the software framework from the tasks of data logging and data display in the UI. This paper will discuss the design choices that were made as well as the reasons for our methodology.","0197-7385","978-1-5386-4814-8","10.1109/OCEANS.2018.8604912","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8604912","towed vehicles;underwater video;software design","Software;Cameras;DSL;Cable shielding;Sensors;HTML;Communication cables","application program interfaces;cameras;client-server systems;digital subscriber lines;Linux;marine engineering;modems;online front-ends;public domain software;relational databases;software libraries;SQL;underwater vehicles;user interfaces;video signal processing;Web services","real time monitoring;Web servers;Web-browser;off the shelf hardware components;COTS computer;habitat assessment;marine assessment;Ubuntu Linux;camera based assessment survey system;underwater video;data logging;sensor interfaces;user interface;C-BASS main processor;data quality;digital subscriber line modems;software libraries;relational SQL database;GPS ship clock;RESTful web;topside client interface;South Florida;synchronous databasing","","","","4","","10 Jan 2019","","","IEEE","IEEE Conferences"
"On the statistical distribution of object-oriented system properties","I. Herraiz; D. Rodriguez; R. Harrison","Technical University of Madrid, Madrid, Spain; University of Alcala, Alcala de Henares, Spain; Oxford Brookes University, Oxford, United Kingdom","2012 3rd International Workshop on Emerging Trends in Software Metrics (WETSoM)","28 Jun 2012","2012","","","56","62","The statistical distributions of different software properties have been thoroughly studied in the past, including software size, complexity and the number of defects. In the case of object-oriented systems, these distributions have been found to obey a power law, a common statistical distribution also found in many other fields. However, we have found that for some statistical properties, the behavior does not entirely follow a power law, but a mixture between a lognormal and a power law distribution. Our study is based on the Qualitas Corpus, a large compendium of diverse Java-based software projects. We have measured the Chidamber and Kemerer metrics suite for every file of every Java project in the corpus. Our results show that the range of high values for the different metrics follows a power law distribution, whereas the rest of the range follows a lognormal distribution. This is a pattern typical of so-called double Pareto distributions, also found in empirical studies for other software properties.","2327-0969","978-1-4673-1762-7","10.1109/WETSoM.2012.6226994","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6226994","object-oriented properties;statistical distribution;Chidamber & Kememer;double Pareto;lognormal;power law;Qualitas Corpus","Measurement;Testing;Object oriented modeling;Java;Statistical distributions;Middleware","log normal distribution;object-oriented programming;software metrics","statistical distribution;object-oriented system properties;software properties;software size;lognormal distribution;power law distribution;Java-based software projects;Chidamber metrics suite;Kemerer metrics suite","","8","","14","","28 Jun 2012","","","IEEE","IEEE Conferences"
"Clustering on the Stream of Crowdsourced Testing","S. Shen; H. Lian; T. He; Z. Chen","Stake Key Lab. of Novel Software Technol., Nanjing Univ., Nanjing, China; Stake Key Lab. of Novel Software Technol., Nanjing Univ., Nanjing, China; Stake Key Lab. of Novel Software Technol., Nanjing Univ., Nanjing, China; Stake Key Lab. of Novel Software Technol., Nanjing Univ., Nanjing, China","2017 14th Web Information Systems and Applications Conference (WISA)","9 Apr 2018","2017","","","317","322","In this paper, we propose a clustering framework to analyze the log files generated along crowdsourcing mobile application testing. Our object is to automatically identify the type of testing work that the worker is performing as to reduce the work of developers clustering the test reports. By taking full data information of the log files, we establish the hierarchy of the testing data. Through the application of data processing and stream clustering methods, we accomplish the static mining and dynamic division of the test stream data. Experiments on a crowdsourcing mobile application testing dataset the efficacy of our approach.","","978-1-5386-4806-3","10.1109/WISA.2017.47","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8332638","stream clustering;crowdsourced testing;frequent mining;mobile application","Testing;Data mining;Clustering algorithms;Mobile applications;Crowdsourcing;Software;Task analysis","crowdsourcing;data mining;mobile computing;pattern clustering;program testing","crowdsourced testing;clustering framework;log files;crowdsourcing mobile application testing;data processing;stream clustering methods;test stream data","","1","","11","","9 Apr 2018","","","IEEE","IEEE Conferences"
"Reverse engineering of production processes based on Markov chains","A. Mazak; M. Wimmer; P. Patsuk-Boesch","CDL-MINT and TU Wien at the Business Informatics Group (BIG), a research group of the Institute of Software Technology and Interactive Systems; CDL-MINT and TU Wien at the Business Informatics Group (BIG), a research group of the Institute of Software Technology and Interactive Systems; CDL-MINT and TU Wien at the Business Informatics Group (BIG), a research group of the Institute of Software Technology and Interactive Systems","2017 13th IEEE Conference on Automation Science and Engineering (CASE)","15 Jan 2018","2017","","","680","686","Understanding and providing knowledge of production processes is crucial for flexible production systems as many decisions are postponed to the operation time. Furthermore, dealing with process improvements requires to have a clear picture about the status of the currently employed process. This becomes even more challenging with the emergence of Cyber-Physical Production Systems (CPPS). However, CPPS also provide the opportunity to observe the running processes by using concepts from IoT to producing logs for reflecting the events happening in the system during its execution. Therefore, we propose in this paper a fully automated approach for representing operational logs as models which additionally allows analytical means. In particular, we provide a transformation chain which allows the reverse engineering of Markov chains from event logs. The reverse engineered Markov chains allow to abstract the complexity of run-time information as well as to enable what-if analysis whenever improvements are needed by employing current model-based as well as measurement-based technologies. We demonstrate the approach based on a lab-sized transportation line system.","2161-8089","978-1-5090-6781-7","10.1109/COASE.2017.8256182","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8256182","","Adaptation models;Computational modeling;Reverse engineering;Automation;Production systems;Data models","flexible manufacturing systems;knowledge based systems;manufacturing systems;Markov processes;production engineering computing;production planning;reverse engineering","operational logs;transformation chain;reverse engineering;Markov chains;event logs;run-time information;transportation line system;production processes;flexible production systems;operation time;process improvements;CPPS;running processes;producing logs;cyber-physical production systems;IoT","","","","36","","15 Jan 2018","","","IEEE","IEEE Conferences"
"An automated approach for creating workload models from server log data","F. Abbors; D. Truscan; T. Ahmad","Department of Information Technologies, Åbo Akademi University, Joukahaisenkatu 3-5 A, Turku, Finland; Department of Information Technologies, Åbo Akademi University, Joukahaisenkatu 3-5 A, Turku, Finland; Department of Information Technologies, Åbo Akademi University, Joukahaisenkatu 3-5 A, Turku, Finland","2014 9th International Conference on Software Engineering and Applications (ICSOFT-EA)","8 Oct 2015","2014","","","14","25","We present a tool-supported approach for creating workload models from historical web access log data. The resulting workload models are stochastic, represented as Probabilistic Timed Automata (PTA), and describe how users interact with the system. Such models allow one to analyze different user profiles and to mimic real user behavior as closely as possible when generating workload. We provide an experiment to validate the approach.","","978-9-8975-8124-3","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7293833","Workload Model Generation;Log File Analysis;Performance Testing;Probabilistic Timed Automata","Load modeling;Data models;Testing;Probabilistic logic;Clustering algorithms;Stochastic processes;Analytical models","","","","","","21","","8 Oct 2015","","","IEEE","IEEE Conferences"
"The Challenge of Helping the Programmer during Debugging","S. P. Reiss","Dept. of Comput. Sci., Brown Univ. Providence, Providence, RI, USA","2014 Second IEEE Working Conference on Software Visualization","11 Dec 2014","2014","","","112","116","Programmers spend considerable time debugging their systems. They add logging statements and use debuggers to run their systems in a controlled environment all in an attempt to understand what is happening as their program executes. Our hypothesis is that visualization tools can significantly improve the debugging process. A wide variety of tools have been developed for visualizing and understanding the dynamics of program execution. These tools can provide lots of information about executions. However, most tools are not designed to be used with a debugger. What is needed are tools that can work while the programmer is debugging a system and that provide the information the programmer needs to understand and assist the debugging process. We have started to develop such tools within the context of the Code Bubbles development environment. However, there is much room for improvement and we call upon the software visualization community to think about and develop practical tools that will improve the debugging process.","","978-1-4799-6150-4","10.1109/VISSOFT.2014.27","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6980222","Software visualization;dynamic visualization;debugging","Debugging;Software;Data visualization;Visualization;History;Context;Data structures","program debugging;program visualisation","system debugging process;logging statements;debuggers;visualization tools;program execution dynamics;code bubble development environment;software visualization community","","4","","14","","11 Dec 2014","","","IEEE","IEEE Conferences"
"Website clustering from query graph using social network analysis","Weiduo Wang; Bin Wu; Zhonghui Zhang","Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia, Beijing University of Posts and Telecommunications, China; Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia, Beijing University of Posts and Telecommunications, China; Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia, Beijing University of Posts and Telecommunications, China","2010 IEEE International Conference on Emergency Management and Management Sciences","7 Sep 2010","2010","","","439","442","Along with informationization advancement thorough and Internet rapid development, there exists millions of websites on the Internet. Search engines become a mediator to connect web users and websites. The query logs in which recorded daily contains a wealth of knowledge about the actions of the users of search engines, and as such they contain valuable information about the interests, the preferences, and the behavior of the users, as well as their implicit feedback to search-engine results. By constructing a novel query graph, considering for the classification of queries, which is utilized to build multi-dimensional vector, we adopt social network analysis method to detect communities in the graph to implement website clustering. Website clustering can contribute to spam website, pornographic website and political sensitive website detection. So it can be applied to websites supervision.","","978-1-4244-6065-6","10.1109/ICEMMS.2010.5563409","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5563409","component;Website Clustering;Social Network Analysis;Query Logs;Websites supervision","Communities;Image edge detection;Economics;Motion pictures;Games;History;Education","pattern clustering;query processing;search engines;social networking (online);Web sites","query graph;Web site clustering;social network analysis method;Internet;search engines;query logs;query classification;multidimensional vector;spam Web site;pornographic Web site;political sensitive Web site detection","","3","","15","","7 Sep 2010","","","IEEE","IEEE Conferences"
"User Profiling in Elderly Healthcare Services in China: Scalper Detection","C. Xie; H. Cai; Y. Yang; L. Jiang; P. Yang","School of Software, Yunnan University, Kunming, China; Software School, Shanghai Jiaotong University, Shanghai, China; School of Software, Yunnan University, Kunming, China; Software School, Shanghai Jiaotong University, Shanghai, China; School of Computing and Mathematics, Liverpool John Moores University, Liverpool, U.K.","IEEE Journal of Biomedical and Health Informatics","17 Oct 2018","2018","22","6","1796","1806","Driven by the automation technologies and health informatics of Industry 4.0, hospitals in China have deployed a complete automation system/platform for healthcare services accessing. Without much more Internet knowledge, elderlies usually seek the third-party to assist them to get healthcare services from Web or APPs, it consequently results in an unexpected situation that scalpers could grab all healthcare services booking by unrighteous means in order to resell to elderlies for a much higher price. Moreover, it is hard for physicians to identify the scalpers due to the complexity, ad-hoc, and multiscenario nature of healthcare processes. In this paper, a novel method is proposed for the identification and creation of user groups of scalpers in mobile healthcare services. The approach utilizes and extends state of the art data analysis approaches in the event-logs of the mobile system to identify user groups. Based on the user groups, user profiles are extracted by identifying representative eventcases from hierarchical user-event clusters. A comprehensive evaluation is conducted in a selected test-set from the event-logs of a mobile healthcare APP. The result shows its accuracy and effectiveness in scalper detection in mobile healthcare APP. Further, a complete case study is deployed in a real word hospital to ensure its utility, efficacy, and reliability.","2168-2208","","10.1109/JBHI.2018.2852495","National Natural Science Foundation of China; Yunnan Applied Fundamental Research Project; Yunnan Provincial Young academic and technical leaders reserve talents; Yunnan Provincial Science Research Project of the Department of Education; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8401867","User Profiling;Mobile Healthcare;Scalper Detection;Elderly Services;Clustering;Process Mining","Hospitals;Senior citizens;Medical treatment;Data models;Automation;Internet;Clustering methods;Mobile applications","data analysis;geriatrics;health care;hospitals;Internet;mobile computing","healthcare processes;user groups;mobile healthcare services;event-logs;mobile system;hierarchical user-event clusters;mobile healthcare APP;scalper detection;user profiling;elderly healthcare services;automation technologies;health informatics;healthcare services accessing;Internet knowledge;healthcare services booking;data analysis approaches;China;complete automation system-platform;hospitals;Web;physicians;ad-hoc nature;multiscenario nature","Appointments and Schedules;China;Data Mining;Humans;Medical Informatics;Mobile Applications;Telemedicine","4","","41","","2 Jul 2018","","","IEEE","IEEE Journals"
"Using Double-Check Mechanism in IoT Software Environments","C. Damian; C. Ursache; S. Alboaie; R. Maştaleru; A. Panu; L. Alboaie","Faculty of Electrical Engineering, Technical University “Gheorghe Asachi” of Iasi, Iasi, Romania; Faculty of Computer Science, Alexandru Ioan Cuza University of Iasi, Iasi, Romania; Faculty of Computer Science, Alexandru Ioan Cuza University of Iasi, Iasi, Romania; RomSoft SRL, Iasi, Romania; Faculty of Computer Science, Alexandru Ioan Cuza University of Iasi, Iasi, Romania; Faculty of Computer Science, Alexandru Ioan Cuza University of Iasi, Iasi, Romania","2018 International Conference and Exposition on Electrical And Power Engineering (EPE)","6 Dec 2018","2018","","","0816","0821","This paper presents the possibility to add asserts, validation and security checks in Internet of Things (IoT) software environments based on executable choreographies. These techniques can be applied to any software system that uses Node.JS (javascript on servers) as programming platform. Double-check is a Node.JS module that puts together mechanisms like logging, exception handling, asserts, checks, and other type of semantic checks. The main functions of the module and few examples are presented in the paper and for more examples and an updated version the PrivateSKY github page can be accessed.","","978-1-5386-5062-2","10.1109/ICEPE.2018.8559800","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8559800","IoT;executable coreographies;node.js;asserts;logging;testing","","file servers;Internet of Things;Java;security of data","double-check mechanism;IoT software environments;security checks;software system;javascript on servers;Node.JS module;exception handling;semantic checks;Internet of Things;PrivateSKY github page","","3","","14","","6 Dec 2018","","","IEEE","IEEE Conferences"
"MICE: Monitoring high-level events in cloud environments","G. Apostol; F. Pop","Department of Computer Science and Engineering, University Politehnica of Bucharest, Hungary; Department of Computer Science and Engineering, University Politehnica of Bucharest, Hungary","2016 IEEE 11th International Symposium on Applied Computational Intelligence and Informatics (SACI)","9 Jul 2016","2016","","","377","380","The concept of Cloud Computing illustrates a convenient model employed by many companies in order to facilitate quick access to a group of easy-to-setup computing resources. With businesses depending on the availability of the acquired resources, Service Level Agreements (SLA) guarantees a certain degree of quality in terms of availability, performance and cost. To design a reliable monitoring framework from scratch, a certain amount of knowledge is required on various system parameters. Our solution, the MICE Project, was designed to offer a performance overview of the high-level events occurring in a regular browsing session and also providing a logging framework regarding browsing behaviors, interests, habits and time expenditures. MICE was primarily targeted for Google Chrome users, and can be deployed on any operating system, offering an interface for displaying stored events to administrators and clients, with respect towards privacy and confidentiality. This software implementation can be used in different areas of interest, in order to further enhance available data, or to provide the building blocks of the next-generation of distributed search engines.","","978-1-5090-2380-6","10.1109/SACI.2016.7507405","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7507405","Cloud Computing;performance monitoring;Chrome extension","Monitoring;Cloud computing;Browsers;Mice;Throughput;Error analysis;Measurement","cloud computing;contracts;data privacy;search engines;system monitoring","monitoring high-level events;cloud environments;cloud computing;service level agreements;SLA;MICE project;regular browsing session;logging framework;browsing behaviors;Google Chrome;operating system;confidentiality;data privacy;distributed search engines","","2","","15","","9 Jul 2016","","","IEEE","IEEE Conferences"
"Fraud detection on event logs using fuzzy association rule learning","K. D. Febriyanti; R. Sarno; Y. A. Effendi","Department of Information Technology Management, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Department of Informatics, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Department of Informatics, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia","2017 11th International Conference on Information & Communication Technology and System (ICTS)","22 Jan 2018","2017","","","149","154","Enterprise Resource Planning (ERP) is an enterprise information system designed to coordinate resources, information, and all activities required for a complete business process within a company. The most important requirement of an ERP system is integration. Integration in ERP system is used to combine the needs of a single software in a logical database, so that all departments in a company can share information and communicate with each other well. However, in its application, business process always changes dynamically. The changes will cause many variations to the original business process. This research discusses some frauds occurred due to variations in business processes. Some frauds can be detected by using a method of process mining with Association Rule Learning approach. Process mining technique mine the fraud in the business process by checking the mismatch between event logs of business process with business process model of a company, which is better known as Standard Operating Procedure (SOP).","2338-185X","978-1-5386-2827-0","10.1109/ICTS.2017.8265661","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8265661","association rule learning;fraud detection;fuzzy multi attribute decision making;fraud detection","Companies;PROM;Tools;Analytical models;Decision making;Standards","business data processing;data mining;enterprise resource planning;fraud;fuzzy reasoning;information systems;learning (artificial intelligence)","fraud detection;event logs;fuzzy association rule learning;Enterprise Resource Planning;enterprise information system;ERP system;business process model;process mining technique;business process fraud","","5","","11","","22 Jan 2018","","","IEEE","IEEE Conferences"
"Catena: Efficient Non-equivocation via Bitcoin","A. Tomescu; S. Devadas","CSAIL, MIT, Cambridge, MA, USA; CSAIL, MIT, Cambridge, MA, USA","2017 IEEE Symposium on Security and Privacy (SP)","26 Jun 2017","2017","","","393","409","We present Catena, an efficiently-verifiable Bitcoin witnessing scheme. Catena enables any number of thin clients, such as mobile phones, to efficiently agree on a log of application-specific statements managed by an adversarial server. Catena implements a log as an OP_RETURN transaction chain andprevents forks in the log by leveraging Bitcoin's security againstdouble spends. Specifically, if a log server wants to equivocate ithas to double spend a Bitcoin transaction output. Thus, Catena logs are as hard to fork as the Bitcoin blockchain: an adversarywithout a large fraction of the network's computational power cannot fork Bitcoin and thus cannot fork a Catena log either. However, different from previous Bitcoin-based work, Catena decreases the bandwidth requirements of log auditors from 90GB to only tens of megabytes. More precisely, our clients only need to download all Bitcoin block headers (currently less than35 MB) and a small, 600-byte proof for each statement in a block. We implement Catena in Java using the bitcoinj library and use itto extend CONIKS, a recent key transparency scheme, to witnessits public-key directory in the Bitcoin blockchain where it can beefficiently verified by auditors. We show that Catena can secure many systems today, such as public-key directories, Tor directory servers and software transparency schemes.","2375-1207","978-1-5090-5533-3","10.1109/SP.2017.19","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7958589","","Bitcoin;Servers;Software;Public key;Prototypes;Bandwidth","file servers;Java;public key cryptography","Catena;nonequivocation;efficiently-verifiable Bitcoin witnessing scheme;adversarial server;OP_RETURN transaction chain;Bitcoin blockchain;Java;bitcoinj library;CONIKS;key transparency scheme;public-key directory","","28","","99","","26 Jun 2017","","","IEEE","IEEE Conferences"
"Distributed Data Collection, Correlation and Data Reduction for Iterative Streaming Analytics in Wireless Mobile Networks","C. W. Boyle; S. K. Kovvali","Movik Networks, Inc., Westford, MA, USA; Movik Networks, Inc., Westford, MA, USA","2015 IEEE First International Conference on Big Data Computing Service and Applications","13 Aug 2015","2015","","","384","391","In this paper, we propose a policy-based, real-time packet analytics framework developed to evaluate 3G and 4G mobile communication technologies, and the associated signaling, tunneling, & user application protocols with the ultimate goal of providing context-enriched correlated information to streaming analytics platforms based on business needs. The framework allows user-defined dimensioning, telemetry generation and Key Performance Indicators (KPI) calculations with a focus on real-time network event measurements on the order of milliseconds to seconds. Unlike traditional streaming analytics platforms which rely on raw data logs and call detail records (CDRs), our framework allows augmenting data streams, such as binding TCP, HTTP, or application-level transactions with pre-correlated network state, subscriber patterns, location changes, service indicators, and computed KPIs, for continuous export into business intelligence architectures. By way of example, we present in detail, a closed-loop network optimization use case constituting a stream of KPIs including cell utilization indicators, subscriber mobility behavior, and application usage patterns towards a Business Intelligence (BI) policy enforcement point. This facilitates real-time actions such as congestion mitigation, and QoE management across heterogeneous networks and application behaviors.","","978-1-4799-8128-1","10.1109/BigDataService.2015.65","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7184906","mobile communication;streaming analytics;radio area network;telemetry","Protocols;Mobile communication;Mobile computing;Real-time systems;Wireless networks;Mobile handsets;IP networks","3G mobile communication;4G mobile communication;data analysis;protocols;software defined networking;telecommunication computing","distributed data collection;data correlation;data reduction;iterative streaming analytics;wireless mobile networks;policy-based realtime packet analytics framework;3G mobile communication;4G mobile communication;signaling protocol;tunneling protocol;user application protocol;context-enriched correlated information;business needs;user-defined dimensioning;telemetry generation;KPI calculations;key performance indicators;network event measurements;data logs;call detail records;BI policy enforcement;business intelligence","","","","16","","13 Aug 2015","","","IEEE","IEEE Conferences"
"Study on processing hierarchical structures in relational databases within a distributed application","A. Andreica; D. Stuparu; C. Miu","IT Department, Babes-Bolyai University, Cluj-Napoca, Romania; IT Department, Babes-Bolyai University, Cluj-Napoca, Romania; IT Department, Babes-Bolyai University, Cluj-Napoca, Romania","9th RoEduNet IEEE International Conference","5 Aug 2010","2010","","","187","191","The paper focuses on presenting systematic design techniques for processing hierarchical structures: we deal with representation and data selection techniques, as well as with processing ones. We comparatively present these methods and discuss their run-time efficiency based on a dedicated benchmark. The paper also addresses methods for optimizing database access; in this respect, we briefly present some database optimization actions that we have performed consequent to log processing. Case studies are performed on AcademicInfo information system that we have designed and implemented. We reveal the advantages of parameterized stored procedures and of using MS SQL WITH statement in processing hierarchic structures.","2247-5443","978-9-7373-9951-9","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5541576","software design;hierarchical structures;database selection & processing;run-time efficiency;database access optimization;WITH statement","Relational databases;Computer architecture;Financial management;Software design;Software systems;Process design;Optimization methods;Information systems;Application software;Technology management","data structures;optimisation;relational databases;SQL","hierarchical structures;relational databases;distributed application;systematic design techniques;data selection techniques;run-time efficiency;database optimization;AcademicInfo information system;MS SQL WITH statement;hierarchic structures","","","","22","","5 Aug 2010","","","IEEE","IEEE Conferences"
"Enabling comprehensive data-driven system management for large computational facilities","J. C. Browne; R. L. DeLeon; C. -D. Lu; M. D. Jones; S. M. Gallo; A. Ghadersohi; A. K. Patra; W. L. Barth; J. Hammond; T. R. Furlani; R. T. McLay","Center for Computational Research, SUNY at Buffalo, NY, USA; Center for Computational Research, SUNY at Buffalo, NY, USA; Center for Computational Research, SUNY at Buffalo, NY, USA; Center for Computational Research, SUNY at Buffalo, NY, USA; Center for Computational Research, SUNY at Buffalo, NY, USA; Center for Computational Research, SUNY at Buffalo, NY, USA; Center for Computational Research, SUNY at Buffalo, NY, USA; Texas Advanced Computing Center, University of Texas, Austin, USA; Texas Advanced Computing Center, University of Texas, Austin, USA; Center for Computational Research, SUNY at Buffalo, NY, USA; Texas Advanced Computing Center, University of Texas, Austin, USA","SC '13: Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis","14 Aug 2014","2013","","","1","11","This paper presents a tool chain, based on the open source tool TACC_Stats, for systematic and comprehensive job level resource use measurement for large cluster computers, and its incorporation into XDMoD, a reporting and analytics framework for resource management that targets meeting the information needs of users, application developers, systems administrators, systems management and funding managers. Accounting, scheduler and event logs are integrated with system performance data from TACC_Stats. TACC_Stats periodically records resource use including many hardware counters for each job running on each node. Furthermore, system level metrics are obtained through aggregation of the node (job) level data. Analysis of this data generates many types of standard and custom reports and even a limited predictive capability that has not previously been available for open-source, Linux-based software systems. This paper presents case studies of information that can be applied for effective resource management. We believe this system to be the first fully comprehensive system for supporting the information needs of all stakeholders in open-source software based HPC systems.","2167-4337","978-1-4503-2378-9","10.1145/2503210.2503230","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6877519","","Standards;Performance evaluation;Abstracts;Market research;Servers;Sockets;Bandwidth","Linux;public domain software;resource allocation;workstation clusters","data-driven system management;computational facilities;tool chain;open source tool;TACC_Stats;job level resource;systematic resource;cluster computers;XDMoD;resource management;system administrators;system management;funding managers;system level metrics;Linux-based software systems;open-source software based HPC systems","","8","","28","","14 Aug 2014","","","IEEE","IEEE Conferences"
"Rinascimento: using event-value functions for playing Splendor","I. Bravi; S. M. Lucas","Queen Mary University of London,School of Electronic Engineering and Computer Science,London,United Kingdom; Queen Mary University of London,School of Electronic Engineering and Computer Science,London,United Kingdom","2020 IEEE Conference on Games (CoG)","20 Oct 2020","2020","","","283","290","In the realm of games research, Artificial General Intelligence algorithms often use score as main reward signal for learning or playing actions. However this has shown its severe limitations when the point rewards are very rare or absent until the end of the game. This paper proposes a new approach based on event logging: the game state triggers an event every time one of its features changes. These events are processed by an Event-value Function (EF) that assigns a value to a single action or a sequence. The experiments have shown that such approach can mitigate the problem of scarce point rewards and improve the AI performance. Furthermore this represents a step forward in controlling the strategy adopted by the artificial agent, by describing a much richer and controllable behavioural space through the EF. Tuned EF are able to neatly synthesise the relevance of the events in the game. Agents using an EF show more robust when playing games with several opponents.","2325-4289","978-1-7281-4533-4","10.1109/CoG47356.2020.9231691","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9231691","artificial general intelligence;benchmark;game-playing;hyper-parameter optimisation","Games;Artificial intelligence;Planning;Heuristic algorithms;Tuning;Reinforcement learning;Optimization","computer games;learning (artificial intelligence);software agents","event logging;artificial agent;event value function;playing action learning;artificial general intelligence algorithms;Splendor;Rinascimento","","","","20","","20 Oct 2020","","","IEEE","IEEE Conferences"
"Portable system to monitor vital signs with a Java based computer display and data logging","O. S. Dhande; R. R. Nambiar; Y. R. Deshpande; P. Dhage","Dept. of Electronics and Telecommunication, MIT College of Engineering, Pune, India; Dept. of Electronics and Telecommunication, MIT College of Engineering, Pune, India; Dept. of Electronics and Telecommunication, MIT College of Engineering, Pune, India; Dept. of Electronics and Telecommunication, MIT College of Engineering, Pune, India","2016 Conference on Advances in Signal Processing (CASP)","17 Nov 2016","2016","","","144","149","The first step to improving public health is by the assessment of vital signs, which are valuable indicators of overall health. In the present day there are a number of complex, albeit expensive instruments that can measure various parameters accurately. However, these instruments often tend to be bulky, expensive and difficult to operate. Hence it is difficult to use these instruments in isolated locations such as rural or disaster stricken areas, for bedside monitoring or even in military camps. The objective of this paper is to introduce the concept of a user friendly modular system which can be used to monitor various physiological parameters efficiently. In the proposed system different sensing circuits are attached to a common MicroController Unit (MCU) which can be updated with the associated firmware. Any general computing system such as a laptop is connected to the MCU via a USB-UART bridge. After the connection is established the controller starts sending data to the computer. A JavaScript is run on the laptop using open-source software called Processing 2.0, which initiates a customized display of the various parameters using the incoming data. For data logging we store the serial data in text files. As proof of principle we have measured two parameters, namely the body temperature using the LM35 sensor and the heart rate using photoplethysmography. Such a switchable system can be used as a tool to measure various parameters like oxygen saturation, blood pressure, ECG etc. on the computer screen.","","978-1-5090-0849-0","10.1109/CASP.2016.7746154","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7746154","Photoplethysmography;Operational amplifier;Low pass filter;High Pass Filter;JavaScript;modular;temperature measurement;heart rate measurement","Temperature measurement;Temperature sensors;Heart rate;Biomedical monitoring;Monitoring;Portable computers","blood pressure measurement;computer displays;electrocardiography;firmware;human computer interaction;Java;medical signal processing;microcontrollers;oximetry;patient monitoring;photoplethysmography;portable instruments;public domain software","portable system;vital sign monitoring;Java based computer display;data logging;photoplethysmography;oxygen saturation;blood pressure;ECG;computer screen;heart rate;LM35 sensor;body temperature;text files;serial data;customized display;Processing 2.0;open-source software;JavaScript;data sending;USB-UART bridge;laptop;general computing system;associated firmware;microcontroller unit;sensing circuits;physiological parameters;user friendly modular system;military camps;bedside monitoring;disaster stricken areas;rural stricken areas;complex albeit expensive instruments;public health","","","","16","","17 Nov 2016","","","IEEE","IEEE Conferences"
"Dynamic Innate Immune System Model for Malware Detection","M. A. M. Ali; M. A. Maarof","Fac. of Math. Sci., Univ. of Khartoum, Khartoum, Sudan; Fac. of Comput., Univ. Teknol. Malaysia, Skudai, Malaysia","2013 International Conference on IT Convergence and Security (ICITCS)","23 Jan 2014","2013","","","1","4","Malware stand for Malicious Software became a major threat facing the massive amount of data transmitted through the internet and the systems holding that data. Malware detection is the process of identifying the malicious behavior or object as malware. Many methods used to do the detection process, these methods are varied depending on the process used by the detector -anti virus or anti malware is a commercial name of detectors. Signature base, behavior base and specification base. Increasing the detection accuracy is the main goal of researchers in the last decade. In this paper we introduce a dynamic malware detection model by applying the innate immune system to improve the detection accuracy. The proposed model applied to the portable executable file representation by extracting the API call logs from new installed windows environment due to the wide spread of this type of files in different platforms. The results of the experiments show a better detection accuracy of the proposed model for known malware and promising improvement on the new unknown malware and polymorphic malware.","","978-1-4799-2845-3","10.1109/ICITCS.2013.6717828","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6717828","","Immune system;Accuracy;Computers;Computational modeling;Trojan horses;Grippers","application program interfaces;artificial immune systems;digital signatures;Internet;invasive software;text analysis","dynamic innate immune system model;malicious software;data transmission;Internet;malicious behavior;antivirus;antimalware;signature base;behavior base;specification base;dynamic malware detection model;portable executable file representation;API call log extraction;windows environment;polymorphic malware;unknown malware","","1","1","20","","23 Jan 2014","","","IEEE","IEEE Conferences"
"A Low Disk-Bound Transaction Logging System for In-memory Distributed Data Stores","D. Dilli; K. B. Kent; Y. Wang; C. Xu","Fac. of Comput. Sci., Univ. of New Brunswick, Fredericton, NB, Canada; Fac. of Comput. Sci., Univ. of New Brunswick, Fredericton, NB, Canada; Shenzhen Inst. of Adv. Technol., Shenzhen, China; Shenzhen Inst. of Adv. Technol., Shenzhen, China","2016 IEEE International Conference on Cluster Computing (CLUSTER)","8 Dec 2016","2016","","","11","20","Transaction logging and snapshotting are techniques used to deliver durability to the data in in-memory data stores. Absolute durability guarantees are delivered to a system by sequentially recording the transaction logs and snapshots to a non-volatile disk. Recent advancements in database restoration techniques have given rise to lock-free fuzzy snapshots. Still the transaction log that completes the fuzzy snapshots is not lock-free. In addition to locking, the major overhead behind the transaction logging technique is the bottleneck involved in storing the logs to a persistent but slower disk. This paper concentrates on implementing an in-memory transaction logging system with a lesser disk dependency. This logging system mainly targets the distributed in-memory data stores that are transaction replicated, eventually consistent and fault tolerant to crash failures. By making logging in-memory, the performance will be improved, but during the crash fails, the state may be lost. On recovery, we restore the current state partially from the locally available fuzzy snapshot and the remaining from the non-failed nodes in the distributed replica. ZooKeeper, a distributed data store that offers distributed coordination as its major service is used to implement and test our research. On average, a 30 times write performance improvement has been achieved with this approach guaranteeing sufficient durability in replicated mode.","2168-9253","978-1-5090-3653-0","10.1109/CLUSTER.2016.35","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7776474","","Distributed databases;Fault tolerance;Fault tolerant systems;Computer crashes;Servers;Protocols","distributed processing;durability;random-access storage;software performance evaluation;storage management;system monitoring;system recovery;transaction processing","low disk-bound transaction logging system;in-memory distributed data stores;snapshotting;performance improvement;distributed coordination;ZooKeeper;nonfailed nodes;crash failures;fault tolerant;disk dependency;in-memory transaction logging system;lock-free fuzzy snapshots;database restoration;nonvolatile disk;absolute durability guarantees","","","","37","","8 Dec 2016","","","IEEE","IEEE Conferences"
"Rainbow: Efficient memory dependence recording with high replay parallelism for relaxed memory model","X. Qian; H. Huang; B. Sahelices; D. Qian","University of Illinois Urbana-Champaign, USA; AMD Products (China) Co., Ltd., China; Universidad de Valladolid, Spain; Beihang University, China","2013 IEEE 19th International Symposium on High Performance Computer Architecture (HPCA)","3 Jun 2013","2013","","","554","565","Architectures for record-and-replay (R&R) of multithreaded applications ease program debugging, intrusion analysis and fault-tolerance. Among the large body of previous works, Strata enables efficient memory dependence recording with little hardware overhead and can be applied smoothly to snoopy protocols. However, Strata records imprecise happens-before relations and assumes Sequential Consistency (SC) machines that execute memory operations in order. This paper proposes Rainbow, which is based on Strata but records near-precise happens-before relations, reducing the number of logs and increasing the replay parallelism. More importantly, it is the first R&R scheme that supports any relaxed memory consistency model. These improvements are achieved by two key techniques: (1) To compact logs, we propose expandable spectrum (the region between two logs). It allows younger non-conflict memory operations to be moved into older spectrum, increasing the chance of reusing existing logs. (2) To identify the overlapped and incompatible spectra due to reordered memory operations, we propose an SC violation detection mechanism based on the existing logs and the extra information can be recorded to reproduce the violations when they occur. Our simulation results with 10 SPLASH-2 benchmarks show that Rainbow reduces the log size by 26.6% and improves replay speed by 26.8% compared to Strata. The SC violations are few but do exist in the applications evaluated.","1530-0897","978-1-4673-5587-2","10.1109/HPCA.2013.6522349","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6522349","","Europe;Abstracts","benchmark testing;multi-threading;parallel architectures;program debugging;software fault tolerance","SPLASH-2 benchmarks;nonconfiict memory operations;expandable spectrum;replay parallelism;near-precise happens-before relations;memory operations;SC;sequential consistency machines;snoopy protocols;Strata;fault-tolerance;intrusion analysis;program debugging;multithreaded applications;R&R;record-and-replay architectures;relaxed memory model;high replay parallelism;memory dependence recording;Rainbow","","7","","29","","3 Jun 2013","","","IEEE","IEEE Conferences"
"POMP++: Facilitating Postmortem Program Diagnosis with Value-set Analysis","D. Mu; Y. Du; J. Xu; J. Xu; X. Xing; B. Mao; P. Liu","Computer Science and Technology, Nanjing University, 12581 Nanjing, jiangsu China (e-mail: dzm77@ist.psu.edu); Department of Computer Science and Technology, Nanjing University, Nanjing, Jiangsu China (e-mail: duyunlan@smail.nju.edu.cn); Department of Computer Science and Technology, Nanjing University, Nanjing, Jiangsu China (e-mail: jianhao_xu@smail.nju.edu.cn); Department of Computer Science, Stevens Institute of Technology Charles V Schaefer Jr School of Engineering and Science, 200807 Hoboken, New Jersey United States (e-mail: jxu69@stevens.edu); College of Information Sciences and Technology, Pennsylvania State University, 8082 University Park, Pennsylvania United States (e-mail: xxing@ist.psu.edu); Computer Science, Nanjing University, Nanjing, Jiangsu China (e-mail: maobing@nju.edu.cn); College of IST, Penn State, University Park, Pennsylvania United States 16802 (e-mail: pliu@ist.psu.edu)","IEEE Transactions on Software Engineering","","2019","PP","99","1","1","With the emergence of hardware-assisted processor tracing, execution traces can be logged with lower runtime overhead and integrated into the core dump. In comparison with an ordinary core dump, such a new post-crash artifact provides software developers and security analysts with more clues to a program crash. However, existing works only rely on the resolved runtime information, which leads to limitation in data flow recovery within long execution traces. In this work, we propose POMP++, an automated tool to facilitate the analysis of post-crash artifacts. More specifically, POMP++ introduces a reverse execution mechanism to construct the data flow that a program followed prior to its crash. Furthermore, POMP++ utilizes Value-set Analysis, which helps to verify memory alias relation, to improve the ability of data flow recovery. With the restored data flow, POMP++ then performs backward taint analysis and highlights program statements that actually contribute to the crash. We have implemented POMP++ for Linux system on x86-32 platform, and tested it against various crashes resulting from 31 distinct real-world security vulnerabilities. The evaluation shows that, our work can pinpoint the root causes in 29 cases, increase the amount of recovered memory addresses by 12% and reduce the execution time by 60% compared with existing reverse execution. In short, POMP++ can accurately and efficiently pinpoint program statements that truly contribute to the crashes, making failure diagnosis significantly convenient.","1939-3520","","10.1109/TSE.2019.2939528","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8823943","Postmortem Program Diagnosis;Failure Diagnosis;Reverse Execution;Value-set Analysis","Computer crashes;Software;Security;Core dumps;Registers;Runtime;Tools","","","","","","","","4 Sep 2019","","","IEEE","IEEE Early Access Articles"
"Solar Probe Plus Spacecraft Flight Software requirements verification test framework","S. Jacobs; K. A. Wortman","Johns Hopkins Applied Physics Laboratory, 11100 Johns Hopkins Road, Laurel, MD 20723; Johns Hopkins Applied Physics Laboratory, 11100 Johns Hopkins Road Laurel, MD 20723","2016 IEEE Aerospace Conference","30 Jun 2016","2016","","","1","8","Comprehensive Spacecraft Flight Software requirements verification is essential to the success of deep space missions. NASA's Solar Probe Plus (SPP) Spacecraft Flight Software and requirement verification activities are being implemented by Johns Hopkins University Applied Physics Laboratory (JHU/APL) located in Laurel, MD. JHU/APL's software development process for a critical mission requires an independent verification of all Spacecraft Flight Software requirements. The complexity of SPP's Spacecraft Flight Software and the number of critical requirements mandates an efficient process to build, maintain and execute a comprehensive test suite. In addition, detailed reports must contain relevant information on test execution steps for post-execution analysis verification and for test artifact dissemination to external reviewers (NASA Independent Verification and Validation). The Van Allen Probes mission (launched in 2012) implemented the initial Test Framework currently being used to conduct post-launch regression verification of Spacecraft Flight Software releases. The simplistic Test Framework design consists of following a set of conventions for procedure development and the use of specific routines from a procedure library. The procedures are used to create and write relevant information to report files and to verify different types of spacecraft data points. The basic philosophy of the Test Framework design is each test must continue to execute even when a test execution failure is encountered. The SPP Mission Operations ground system and supported test procedure development language are the same as used on Van Allen Probes. Therefore, SPP can reuse the core of the Test Framework infrastructure with minor modifications. A major enhancement in the SPP Test Framework is the feature to allow the test engineer to tailor the test suite to target a specific subset of tests. Scheduling selected tests to launch during non-peak work hours provides an efficient method to maximize test environment usage. Easy access to the test execution reporting system via an Intranet web interface offers another benefit to the cognizant test engineer for post-execution debugging and analysis. Additionally, the SPP Test Framework provides an efficient method to automate the configuration control system check-out process of test procedures and the check-in process of test execution artifacts. Supplemental procedures have also been incorporated into the SPP Test Framework to perform routine error analysis of ground system logs to reduce manual verification and to assist in troubleshooting problems encountered during test execution. The paper will detail the SPP Test Framework and benefits to critical Spacecraft Flight Software requirements verification. The discussion in the paper will include the enhancements in the SPP Test Framework to address the lessons learned from the Van Allen Probes Spacecraft Flight Software requirements verification program. Atlassian Bamboo tool configuration was used as the test harness for the SPP Test Framework to provide the test selection, schedule and automatic launch features to support execution of the SPP Spacecraft Flight Software test suite.","","978-1-4673-7676-1","10.1109/AERO.2016.7500544","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7500544","","Software;Probes;Space vehicles;Space missions;Physics;NASA;Libraries","aerospace computing;formal verification;Internet;program debugging;program diagnostics;program testing;space vehicles","Solar Probe Plus Spacecraft Flight software;software requirements verification test framework;comprehensive spacecraft flight software;SPP spacecraft flight software;Johns Hopkins University Applied Physics Laboratory;JHU/APL;software development process;comprehensive test suite;post-execution analysis verification;test artifact dissemination;NASA independent verification and validation;Van Allen Probes mission;spacecraft flight software release;procedure development;procedure library;spacecraft data points;test framework design;test environment usage;test scheduling;test execution reporting system;Intranet web interface;post-execution debugging;configuration control system","","","","8","","30 Jun 2016","","","IEEE","IEEE Conferences"
"On the Effectiveness of Extracting Important Words from Proxy Logs","M. Mimura","Nat. Defense Acad., Yokosuka, Japan","2018 Sixth International Symposium on Computing and Networking Workshops (CANDARW)","27 Dec 2018","2018","","","424","430","Modern http-based malware imitates benign traffic to evade detection. To detect unseen malicious traffic, many methods using machine learning techniques have been proposed. These methods took advantage of the characteristic of malicious traffic, and usually require additional parameters which are not obtained from essential security devices such as a proxy server or IDS (Intrusion Detection System). Thus, most previous methods are not applicable to actual information systems. To tackle a realistic threat, a linguistic-based detection method for proxy logs has been proposed. This method extracts words as feature vectors automatically with natural language techniques, and discriminates between benign traffic and malicious traffic. The previous method generates a corpus from the whole extracted words which contain trivial words. To generate discriminative feature representation, a corpus has to be effectively summarized. This paper extracts important words from proxy logs to summarize the corpus. To define the word importance score, this paper uses term frequency and document frequency. Our method summarizes the corpus and improves the detection rate. We conducted cross-validation and timeline analysis with captured pcap files from Exploit Kit (EK) between 2014 and 2016. The experimental result shows that our method improves the accuracy. The best F-measure achieves 1.00 in the cross-validation and timeline analysis.","","978-1-5386-9184-7","10.1109/CANDARW.2018.00084","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8590938","Proxy Log, Exploit Kit, Paragraph Vector, Doc2vec, TFIDF","Feature extraction;Servers;Information systems;Natural language processing;Malware;Machine learning;Security","feature extraction;invasive software;learning (artificial intelligence);natural language processing","proxy logs;benign traffic;machine learning techniques;essential security devices;proxy server;linguistic-based detection method;feature vectors;natural language techniques;discriminative feature representation;word importance score;intrusion detection system;information systems;http-based malware;malicious traffic;F-measure;cross-validation analysis;timeline analysis;exploit kit;term frequency;document frequency","","","","30","","27 Dec 2018","","","IEEE","IEEE Conferences"
"Remote Sensing Kit for Contamination Event Detection in Water","M. Siam; J. I. Munna; M. Hasan; T. Rahman","North South University,Department of Electrical and Computer Engineering,Dhaka,Bangladesh; North South University,Department of Electrical and Computer Engineering,Dhaka,Bangladesh; North South University,Department of Electrical and Computer Engineering,Dhaka,Bangladesh; North South University,Department of Electrical and Computer Engineering,Dhaka,Bangladesh","2019 IEEE R10 Humanitarian Technology Conference (R10-HTC)(47129)","19 Mar 2020","2019","","","175","179","Here, we propose an Internet of Things (IoT) enabled remote sensing kit for multipara-meter based water quality monitoring and contamination event detection. The proposed kit is able to collect real-time data from household reservoirs, analyze and display them on an easy to use platform for monitoring purpose. It can measure Temperature (T), pH, Electrical Conductivity (EC), and Turbidity (Tb), as vital indicators of water quality. It can also preprocess acquired data by the onboard processor (NodeMCU) and transfer to the cloud (Firebase) for determining the water quality. Users can monitor water quality regularly as a graph and other means by logging in their web account. The kit has been tested thoroughly to ensure its accuracy and usefulness. The proposed remote sensing kit can be seamlessly integrated into the water management system of the metropolitan area and can be considered as an important part for the future smart city concept.","2572-7621","978-1-7281-0834-6","10.1109/R10-HTC47129.2019.9042459","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9042459","contamination detection;IoT;real-time monitoring;smart city;water quality monitoring","","computerised monitoring;contamination;Internet;Internet of Things;public domain software;remote sensing;water quality","contamination event detection;monitoring purpose;remote sensing kit;water management system;multipara-meter based water quality monitoring","","","","16","","19 Mar 2020","","","IEEE","IEEE Conferences"
"Fuzzy and Cross-App Replay for Smartphone Apps","Y. Hu; I. Neamtiu","Univ. of California, Riverside, Riverside, CA, USA; New Jersey Inst. of Technol., Newark, NJ, USA","2016 IEEE/ACM 11th International Workshop in Automation of Software Test (AST)","9 Jan 2017","2016","","","50","56","The behavior of smartphone apps is driven by input from sensors such as GPS, microphone, or camera. Hence the ability to construct test inputs, and send these inputs to the app is essential for testing. Leveraging our prior results in recording and replaying sensor inputs in Android apps we have constructed a new approach that helps automate smartphone app testing by capturing the input log (sensor stream) and using this log in two ways. First, we fuzz (alter) the log in a semantically-meaningful way: by applying principled transformations (e.g., changing GPS coordinates or navigation speed), a new input log is constructed, which represents a new test case. Second, we use the log captured in app A to test an app B which offers similar functionality, e.g., GPS navigation or image recognition. We have applied our approach to several widely-used Android apps and found that the approach is effective: it has revealed new bugs in four popular apps; has produced new test cases that increase coverage; and has produced test cases from logs originating in other apps.","","978-1-4503-4151-6","10.1109/AST.2016.016","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7809822","Mobile applications;Google Android;Record-and-replay;App testing;Physical sensors","Sensors;Global Positioning System;Cameras;Smart phones;Testing;Microphones;Semantics","Android (operating system);program testing;recording;smart phones","fuzzy replay;cross-app replay;smartphone apps;sensor input record;GPS;microphone;camera;sensor input replay;Android apps;automated smartphone app testing;sensor stream","","","","27","","9 Jan 2017","","","IEEE","IEEE Conferences"
"Evaluating the Effect of 3D World Integration within a Social Software Environment","H. Bani-Salameh; C. Jeffery","Dept. of Software Eng., Hashemite Univ., Zarqa, Jordan; Dept. of Comput. Sci., Univ. of Idaho, Moscow, ID, USA","2015 12th International Conference on Information Technology - New Generations","1 Jun 2015","2015","","","255","260","Several virtual communities have spread during the last decade where hundreds of people interact and socialize. Users use these communities in many fields including education, health, business, and entertainment. They use them to communicate, share information, and collaborate to achieve common goals and finish their tasks. To our knowledge, no existing literature or research studies show the benefit of using virtual world in 1) improving software engineering education, 2) enhancing the collaboration in distributed software development environment, and 3) increasing their effectiveness in the distributed developers' progress. For this purpose we conducted a case study to test effect of integrating a virtual environment called CVE in software development environments. This study presents both qualitative and quantitative analysis of the data collected from the case study surveys and log files. It conducted a survey on the users' preferences a. Also, it collected data about the developer's interactions with the 3D objects, and analyzed the collected results.","","978-1-4799-8828-0","10.1109/ITNG.2015.47","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7113482","3D World;CVEs;IDE (Integrated Development Environment);Virtual Environments;Social Interaction;Social Networking","Three-dimensional displays;Collaboration;Virtual environments;Avatars;Usability;Measurement","computer aided instruction;computer science education;data analysis;distributed processing;groupware;human computer interaction;social aspects of automation;software engineering;virtual reality","3D world integration effect evaluation;social software environment;virtual communities;people socialization;people interaction;virtual world;software engineering education improvement;collaboration enhancement;distributed software development environment;increase distributed developers progress effectiveness;CVE;qualitative data analysis;quantitative data analysis","","1","","19","","1 Jun 2015","","","IEEE","IEEE Conferences"
"Semantic hedgehog for log analysis","J. J. Wiley; F. P. Coyle","Lyle School of Engineering, Southern Methodist University, Dallas, Texas, USA; Lyle School of Engineering, Southern Methodist University, Dallas, Texas, USA","2012 International Conference for Internet Technology and Secured Transactions","11 Mar 2013","2012","","","748","752","Computer system log analysis should proactively support information security decisions of all types. These security decisions will likely include whether to update configurations, close ports, block access, patch systems, maneuver the system elements, or to do nothing because the risk is acceptable. In a world with Big Data, and a heterogeneous, distributed enterprise, log analysis can be difficult at best. There is so much data from a multitude of logs (e.g. event, application, and security) within the enterprise. On top of that, enterprises have varying configurations based on hardware, software, current patch level, and operating systems. Logs must track all of this data on all of these devices. The authors suggest that semantic technologies hold one key to providing a capability for proactive, and more meaningful, log analysis.","","978-1-908320-08-7","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6470918","proactive defense;semantic web;syslog;triples","Security;Correlation;Engines","business data processing;data analysis;security of data","semantic hedgehog;computer system log analysis;information security decision;update configuration decision;close port decision;block access decision;patch system decision;Big Data;distributed enterprise;semantic technology","","","","9","","11 Mar 2013","","","IEEE","IEEE Conferences"
"Research on grey forecast the results of men's 100ms of the Olympic Games by GM (1, 1) model combining C language programming","Z. Zheng-min; Z. Xiang","P.E Department, China West Normal University, Nanchong, China; P.E Department, China West Normal University, Nanchong, China","2011 IEEE 3rd International Conference on Communication Software and Networks","8 Sep 2011","2011","","","698","701","The authors consult a great deal of reference to collect data, compare results of different sports prediction methods and the limitations of various forecasting methods. From the perspective of special training school and taking men's 100ms of the Olympic Games as an example, the sample data is analyzed. This paper chooses the data fit for the gray prediction model to forecast the results by GM (1,1) based on log transformation. Handle, analyze and verify the data with written C language programming. Research shows that the model can predict accurately and apply to predicting the results of measuring sports events, with log transformation being used to forecast the results of measuring sports events for the first time.","","978-1-61284-486-2","10.1109/ICCSN.2011.6013930","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6013930","log transformation;performance forecasting;grey prediction","Predictive models;Training;Data models;Analytical models","C language;forecasting theory;grey systems;sport","grey forecast;Olympic Games;C language programming;sports prediction method;forecasting method;special training school;gray prediction model;log transformation","","","","9","","8 Sep 2011","","","IEEE","IEEE Conferences"
"Differentiating malware from cleanware using behavioural analysis","R. Tian; R. Islam; L. Batten; S. Versteeg","School of IT, Deakin University, Melbourne, Australia; School of IT, Deakin University, Melbourne, Australia; School of IT, Deakin University, Melbourne, Australia; CA Labs, Melbourne, Australia","2010 5th International Conference on Malicious and Unwanted Software","13 Dec 2010","2010","","","23","30","This paper proposes a scalable approach for distinguishing malicious files from clean files by investigating the behavioural features using logs of various API calls. We also propose, as an alternative to the traditional method of manually identifying malware files, an automated classification system using runtime features of malware files. For both projects, we use an automated tool running in a virtual environment to extract API call features from executables and apply pattern recognition algorithms and statistical methods to differentiate between files. Our experimental results, based on a dataset of 1368 malware and 456 cleanware files, provide an accuracy of over 97% in distinguishing malware from cleanware. Our techniques provide a similar accuracy for classifying malware into families. In both cases, our results outperform comparable previously published techniques.","","978-1-4244-9356-2","10.1109/MALWARE.2010.5665796","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5665796","Malware;strings;API;dynamic","Malware;Feature extraction;Accuracy;Classification algorithms;Software;Cloud computing;Monitoring","application program interfaces;invasive software;pattern classification;statistical analysis","malware files;cleanware;behavioural analysis;malicious files distinguishing;API call feature;automated classification system;virtual environment;pattern recognition algorithm;statistical method","","68","1","18","","13 Dec 2010","","","IEEE","IEEE Conferences"
"Multi-level logs based web performance evaluation and analysis","Xiaokai Xia; Qiuhong Pei; Yongpo Liu; Ji Wu; Chao Liu","School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China","2010 International Conference on Computer Application and System Modeling (ICCASM 2010)","4 Nov 2010","2010","4","","V4-37","V4-41","The execution of web applications always involves many levels (Operating system, Java virtual machine, Web server, Database server, application itself). Each level can produce its corresponding logs, which record the information of the level's running state and history. Based on the multi-level logs generated by the running web systems, this paper proposes a comprehensive method to evaluate and analyze the online web applications' performance. The advantages of this method are that we can not only use it in the testing stage, but also we can discover the online web applications' performance problems not found in the performance testing stage, and can analyze the reasons of the problems effectively. Case study shows that it is an effective method for performance evaluation and analysis.","2161-9077","978-1-4244-7237-6","10.1109/ICCASM.2010.5620512","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5620512","Multi-level Logs;Web Performance Evaluation;Web Performance Analysis;Indicators","Levee;Artificial neural networks;Servers","Internet;software performance evaluation","multi-level logs;Web performance evaluation;Web performance analysis;running Web systems","","","1","11","","4 Nov 2010","","","IEEE","IEEE Conferences"
"Tiler: An Autonomous Region-Based Scheme for SMR Storage","C. Ma; Z. Shen; J. Wang; Y. Wang; R. Chen; Y. Guan; Z. Shao","College of Big Data and Internet, Shenzhen Technology University, Shenzhen, Guangdong, China; School of Computer Science and Technology, Shandong University, Jinan, Shandong, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, Shaanxi, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China; College of Intelligence and Computing, Shenzhen Research Institute of Tianjin University, Tianjin University, China; College of Computer and Information Management, Capital Normal University, Beijing, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Transactions on Computers","13 Jan 2021","2021","70","2","291","304","Shingled Magnetic Recording (SMR) Disks are adopted as a high-density, non-volatile media that significantly precedes conventional disks in both the storage capacity and cost. However, inefficient read-modify-writes (RMWs) greatly challenge the management of SMR disks. This article for the first time presents an approach called Tiler to manage SMR disks by dividing the physical space into small autonomous regions (ARs). Each AR can manage its space allocation, address mapping, and cleaning independently. By managing these ARs in a log-structured way, RMWs can be avoided; besides, ARs can also help update data when the adjacent tracks contain no valid data. Tiler is capable of partitioning a large-scale cleaning into self-contained-small-scale cleaning and thus, the data that need to be relocated are limited inside independent ARs, which further minimizes the performance overhead. Our experimental results show that Tiler can shorten the overall system response time by 50.21 percent and reduce the cleaning time by 90.24 percent on average.","1557-9956","","10.1109/TC.2020.2988004","National Natural Science Foundation of China; Shenzhen Science and Technology Foundation; Natural Science Foundation of Tianjin City; Guangdong Basic and Applied Basic Research Foundation; Fundamental Research Funds of Shandong University; Natural Science Basic Research Program of Shaanxi; Research Grants Council of the Hong Kong Special Administrative Region, China; Chinese University of Hong Kong; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9069446","Shingled magnetic recording;cache management;garbage collection","Cleaning;Writing;Computer science;Magnetic recording;Memory management;Resource management;Time factors","magnetic disc storage;magnetic recording;minimisation;performance evaluation;storage management","performance overhead minimization;address mapping;high-density nonvolatile media;independent AR;cleaning time reduction;self-contained-small-scale cleaning;large-scale cleaning;space allocation;autonomous regions;physical space;SMR disks;RMW;read-modify-writes;storage capacity;shingled magnetic recording disks;SMR storage;autonomous region-based scheme;Tiler","","","","30","IEEE","16 Apr 2020","","","IEEE","IEEE Journals"
"A Cross-Organizational Process Mining Framework for Obtaining Insights from Software Products: Accurate Comparison Challenges","Ü. Aksu; D. M. M. Schunselaar; H. A. Reijers","Vrije Univ. Amsterdam, Amsterdam, Netherlands; Vrije Univ. Amsterdam, Amsterdam, Netherlands; Vrije Univ. Amsterdam, Amsterdam, Netherlands","2016 IEEE 18th Conference on Business Informatics (CBI)","12 Dec 2016","2016","01","","153","162","Software vendors offer various software products to large numbers of enterprises to support their organization, in particular Enterprise Resource Planning (ERP) software. Each of these enterprises use the same product for similar goals, albeit with different processes and configurations. Therefore, software vendors want to obtain insights into how the enterprises use the software product, what the differences are in usage between enterprises, and the reasons behind these differences. Cross-organizational process mining is a possible solution to address these needs, as it aims at comparing enterprises based on their usage. In this paper, we present a novel Cross-Organizational Process Mining Framework which takes as input, besides event log, semantics (meaning of terms in an enterprise) and organizational context (characteristics of an enterprise). The framework provides reasoning capabilities to determine what to compare and how. Besides, the framework enables one to create a catalog of metrics by deducing diagnostics from the usage. By using this catalog, the framework can monitor the (positive) effects of changes on processes. An enterprise operating in a similar context might also benefit from the same changes. To accommodate these improvement suggestions, the framework creates an improvement catalog of observed changes. Later, we provide a set of challenges which have to be met in order to obtain the inputs from current products to show the feasibility of the framework. Next to this, we provide preliminary results showing they can be met and illustrate an example application of the framework in cooperation with an ERP software vendor.","2378-1971","978-1-5090-3231-0","10.1109/CBI.2016.25","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7780310","Cross-Organizational Process Mining;Framework;ERP","Organizations;Measurement;Semantics;Context;Software;Data mining","data mining;DP industry;enterprise resource planning;organisational aspects;reasoning about programs;software product lines","cross-organizational process mining;software products;enterprise resource planning software;reasoning capabilities;ERP software vendor;metrics catalog","","3","","16","","12 Dec 2016","","","IEEE","IEEE Conferences"
"Event segmentation using MapReduce based big data clustering","M. O. Shafiq","School of Information Technology, Carleton University, Ottawa, ON, Canada","2016 IEEE International Conference on Big Data (Big Data)","6 Feb 2017","2016","","","1857","1866","Event segmentation is an important step in monitoring and management applications that categorizes different events into different segments. This is important especially when applications, to be monitored and managed, are large-scale, comprehensive and data-intensive in nature. The process of segmentation is based on data clustering which is one of the key data mining methods used these days. There are several decent algorithms and techniques that exist to perform clustering on small to medium scale data. In the era of Big Data and with applications being large-scale and data-intensive in nature, there is a significant increment in volume, variety and velocity of data in the form of log events produced by such applications. This makes the task of clustering of huge amounts of data more challenging and limited. This paper presents a proposed an effective and efficient approach of event segmentation in logs. It is based on parallel k-means clustering, inherited from MapReduce paradigm, to be used for event segmentation. The proposed approach has been tested and evaluated on large-scale log data derived from real-life case-study. Evaluation includes measuring efficiency and effectiveness of the proposed solution for its usability on log data with large volume, variety and velocity, as well as its applicability on large-scale applications.","","978-1-4673-9005-7","10.1109/BigData.2016.7840804","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7840804","Event segmentation;Clustering;Parallel;MapReduce","Monitoring;Clustering algorithms;Software;Semantics;Big data;Algorithm design and analysis;Clustering methods","Big Data;data handling;data mining;pattern clustering","event segmentation;MapReduce based big data clustering;data mining methods;parallel k-means clustering","","3","","26","","6 Feb 2017","","","IEEE","IEEE Conferences"
"Research of personalized recommendation system based on client interest model in the smart mobile phone","F. Changhong; Y. Fan; W. Shunxiang; Z. Shunzhi","Department of Automation, Xiamen University, Xiamen, China; Department of Automation, Xiamen University, Xiamen, China; Department of Automation, Xiamen University, Xiamen, China; Department of Computer and Technology, Xiamen University of Technology, Xiamen, China","2010 5th International Conference on Computer Science & Education","30 Sep 2010","2010","","","371","374","The problem of information overload and resource disorientation in smart mobile phone made it hard for mobile client to find the interesting products and services, by referring to the useful information stored in log files, the paper proposes a personalized recommendation system based on client interest model to solve the above problem and protect the client's privacy better. Firstly, the procedure of system is introduced in detail. Secondly, the paper presents the concepts of interest degree to application, then the client interest model is designed and the recommendation algorithm is analyzed. The experiments and questionnaire to Symbian client have shown that the recommendation algorithm proposed in the paper can exploit the true client interest, and proved the effectiveness and feasibility of algorithm, which is important to design the personalized recommendation software in future.","","978-1-4244-6005-2","10.1109/ICCSE.2010.5593609","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5593609","smart mobile phone;client interest degree;recommendation algorithm;personalized recommendation system","Mobile handsets;Algorithm design and analysis;Software algorithms;Classification algorithms;Software;Filtering;Computational modeling","client-server systems;data privacy;information resources;information storage;mobile handsets;personal computing;recommender systems","personalized recommendation system;client interest model;smart mobile phone;information overload;information resource;mobile client;log file;client privacy;Symbian client;personalized recommendation software","","1","","10","","30 Sep 2010","","","IEEE","IEEE Conferences"
"Workload Dependent Fault Analysis","M. N. Vora; M. K. Nambiar","TCS Res., Tata Consultancy Services (TCS) Ltd., Mumbai, India; TCS Res., Tata Consultancy Services (TCS) Ltd., Mumbai, India","2017 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)","16 Nov 2017","2017","","","222","227","Online transaction failures often show up as ""500: Internal Server Error"" in web server access logs. In many instances determining the root cause of the failure is a difficult task. It could either be a bug associated with a specific HTTP request alone or the result of an undesirable state created by previous HTTP transactions. The latter case, which we call workload dependent faults, are relatively difficult to diagnose or even reproduce. In this paper we present a methodology to detect and identify such disruptive workload pattern for any web based IT system - in other words determine a specific set of HTTP transactions (or URL's) causing an internal error on the access of a specific URL. Only web server logs are used as input.","","978-1-5386-2387-9","10.1109/ISSREW.2017.31","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8109287","software faults;workload pattern;mandlebugs","Uniform resource locators;Biological cells;Computer bugs;Computational modeling;Servers;Genetic algorithms;Data models","file servers;hypermedia;Internet;transaction processing;transport protocols","Internal Server Error;web server access logs;internal error;web server logs;online transaction failures;URL;workload dependent fault analysis;HTTP request;HTTP transactions;web based IT system","","","","20","","16 Nov 2017","","","IEEE","IEEE Conferences"
"Design and implementation of an ASP.NET-based digital archives management system","Liang Gao; Dong Zhao; Fei Pei","Software College, Zhongyuan University of Technology, Zhengzhou, China; Software College, Zhongyuan University of Technology, Zhengzhou, China; School of Computer Science, Zhongyuan University of Technology, Zhengzhou, China","2012 IEEE International Conference on Computer Science and Automation Engineering","16 Aug 2012","2012","","","587","589","This paper introduces an ASP.NET_based digital archives management system, which satisfies the requirement that the Archives needs to handle all kinds of documents. The system discussed in the paper is powerful and easy to control and spread. This paper details the implementation of the system functions.","2327-0594","978-1-4673-2008-5","10.1109/ICSESS.2012.6269535","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6269535","Digital Archives;Composite Control;Log;Privilege","","document handling;information retrieval systems;Internet;records management","ASP.NET-based digital archives management system;documents;system function implementation","","","","2","","16 Aug 2012","","","IEEE","IEEE Conferences"
"Business activity monitoring solution to detect deviations in business process execution","M. L. Sebu; H. Ciocârlie","Computer and Software Engineering Department, Politehnica University of Timisoara, Timisoara, Romania; Computer and Software Engineering Department, Politehnica University of Timisoara, Timisoara, Romania","2015 IEEE 10th Jubilee International Symposium on Applied Computational Intelligence and Informatics","20 Aug 2015","2015","","","437","442","Current paper proposes a custom solution for detecting deviations in process execution by analyzing event logs of running cases. The monitored business processes are extracted with process mining techniques. The behavior of the current alert system is illustrated on a case study, the event process logs resulted from the execution of a software development process used in the IT department of a large automotive company. Enhancing the level of control in project management activities, improving the business process execution by detecting deviations in real time and helping organizations to perform better by providing guidance to the people during task execution are the main reasons for designing the current solution. The statistical information about detected deviations represents the input for an analysis phase having as major scope business process improvements. The alert system was designed as a Web based application and is presented as a Business Activity Monitoring solution operating with process mining concepts.","","978-1-4799-9911-8","10.1109/SACI.2015.7208243","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7208243","process mining;operational support;business activity monitoring","Data mining;Monitoring;Project management;Organizations;Software;Computational intelligence","automobile industry;business data processing;data mining;Internet;statistical analysis","business activity monitoring solution;business process execution;event logs;process mining techniques;current alert system;software development process;IT department;automotive company;project management activities;task execution;statistical information;business process improvements;Web based application","","1","","7","","20 Aug 2015","","","IEEE","IEEE Conferences"
"Rootkit (malicious code) prediction through data mining methods and techniques","R. G. Ramani; S. S. Kumar; S. G. Jacob","Department of Information Science and Technology, College of Engineering, Anna University, Chennai, India; Department of Computer Science and Engineering, Rajalakshmi Engineering College, Chennai, India; Department of Computer Science and Engineering, SSN College of Engineering, Chennai, India","2013 IEEE International Conference on Computational Intelligence and Computing Research","27 Jan 2014","2013","","","1","5","Rootkits refer to software that is used to hide the presence and activity of malware and permit an attacker to take control of a computer system by affecting the kernel. This paper explores the application of data mining methods to predict rootkits based on the attributes extracted from the information contained in the log files. The rootkit records were categorized as Inline and Others based on the attribute values. Nine classification algorithm were investigated to identify the most accurate and efficient classifier for rootkit prediction. The Correlation Bayes algorithm was found to attain the maximum level of prediction accuracy (87.4%) through 10-fold cross-validation. Moreover, inorder to affirm the performance of the algorithm on unbalanced data, the Mathews Correlation Coefficient was also calculated. The Correlation Bayes algorithm yielded the highest MCC of 0.679 on the Rootkit dataset.","","978-1-4799-1597-2","10.1109/ICCIC.2013.6724243","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6724243","computer security;rootkits;data mining;prediction","Data mining;Classification algorithms;Malware;Prediction algorithms;Jacobian matrices;Accuracy;Correlation","data mining;invasive software;pattern classification","Rootkit prediction;malicious code prediction;data mining methods;malware activity;computer system;log files;information extraction;classification algorithm;correlation Bayes algorithm;Mathews correlation coefficient;Rootkit dataset","","2","","42","","27 Jan 2014","","","IEEE","IEEE Conferences"
"Digital forensic evidence collection of cloud storage data for investigation","S. Easwaramoorthy; S. Thamburasa; G. Samy; S. B. Bhushan; K. Aravind","SITE, VIT University, Vellore, India; Synophic System Private limited, Bangalore, India; Department of Information Technology, PSG College of Technology, Coimbatore, India; SITE, VIT University, Vellore, India; SITE, VIT University, Vellore, India","2016 International Conference on Recent Trends in Information Technology (ICRTIT)","19 Sep 2016","2016","","","1","6","In recent days Cloud services such as storage is more familiar to business and Individuals. This storage services are found as a problem to examiners and researchers in the field of forensics. There are many kind of storage services available in cloud and every service face a diverse issues in illegitimate action. The evidence identification, preservation, and collection are hard when dissimilar services are utilized by offenders. Lack of knowledge regarding location of evidence data can also affect investigation and it take more time to meet every cloud storage providers to decide where the evidence is saved within their infrastructure. In this study two popular public cloud service providers (Microsoft One Drive and Amazon cloud drive) are used to perform forensics evidence collection procedure through browser and service providers software on a Windows 7 computer. By identifying the evidence data on a client device, provide a clear idea about type of evidences are exist in machine for forensics practitioners. Possible evidence determined throughout this study include file timestamps, file hashes, client software log files, memory captures, link files and other evidences are also obtainable to different cloud service providers.","","978-1-4673-9802-2","10.1109/ICRTIT.2016.7569516","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7569516","Digital forensics;web browser;client software;Microsoft One Drive;Amazon cloud Drive;Windows 7","Cloud computing;Browsers;Computers;Companies;Digital forensics","cloud computing;digital forensics;storage allocation","digital forensic evidence collection;cloud storage data;storage services;public cloud service;Windows 7 computer;client device;file hashes;file timestamps;client software log files","","4","","10","","19 Sep 2016","","","IEEE","IEEE Conferences"
"Recursive Segmentation Procedure Based on the Akaike Information Criterion Test","A. Sato","Dept. of Appl. Math. & Phys., Kyoto Univ., Kyoto, Japan","2013 IEEE 37th Annual Computer Software and Applications Conference","31 Oct 2013","2013","","","226","233","This study proposes a recursive segmentation procedure for multivariate time series based on Akaike information criterion. The Akaike information criterion, between independently identically distributed multivariate Gaussian samples and two successive segments drawn from different multivariate Gaussian distributions, is used as a discriminator to segment multivariate time series. The bootstrap method is employed in order to evaluate the statistical significance level. The proposed method is performed for an artificial multi-dimensional time series consisting of two segments with different statistics. The log-return time series of currency exchange rates for 30 currency pairs for the period from January 4, 2001 to December 30, 2011 are also divided into 11 segments with the proposed method. This method confirms that some segments correspond to historical events recorded as critical situations.","0730-3157","978-0-7695-4986-6","10.1109/COMPSAC.2013.38","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6649825","Akaike information criterion;Bootstrap distribution","Time series analysis;Gaussian distribution;Covariance matrices;Standards;Exchange rates;Eigenvalues and eigenfunctions;Estimation error","Gaussian distribution;statistical testing;time series","historical events;currency exchange rates;log-return time series;statistical significance level;bootstrap method;multivariate Gaussian distributions;independently identically distributed multivariate Gaussian samples;multivariate time series;Akaike information criterion test;recursive segmentation procedure","","2","","32","","31 Oct 2013","","","IEEE","IEEE Conferences"
"Efficient NFV-Enabled Multicasting in SDNs","Z. Xu; W. Liang; M. Huang; M. Jia; S. Guo; A. Galis","Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, School of Software, Dalian University of Technology, Dalian, China; Research School of Computer Science, Australian National University, Canberra, ACT, Australia; Research School of Computer Science, Australian National University, Canberra, ACT, Australia; Research School of Computer Science, Australian National University, Canberra, ACT, Australia; Department of ComputingThe Hong Kong Polytechnic University Shenzhen Research InstituteThe Hong Kong Polytechnic University; Department of Electronic and Electrical Engineering, University College London, London, U.K.","IEEE Transactions on Communications","15 Mar 2019","2019","67","3","2052","2070","Multicasting is a fundamental functionality of many network applications, including online conferencing, event monitoring, video streaming, and so on. To ensure reliable, secure, and scalable multicasting, a service chain that consists of network functions (e.g., firewalls, intrusion detection systems, and transcoders) usually is associated with each multicast request. We refer to such a multicast request with service chain requirement as an network function virtualization (NFV)-enabled multicast request. In this paper, we study NFV-enabled multicasting in a software-defined network (SDN) with an aim to maximize network throughput while minimizing the implementation cost of admitted NFV-enabled multicast requests, subject to network resource capacity, where the implementation cost of a request consists of its computing resource consumption cost in servers and its network bandwidth consumption cost when routing and processing its data packets in the network. To this end, we first formulate two NFV-enabled multicasting problems with and without resource capacity constraints and one online NFV-enabled multicasting problem. We then devise two approximation algorithms with an approximation ratio of $2M$ for the NFV-enabled multicasting problems with and without resource capacity constraints, if the number of servers for implementing the service chain of each request is no greater than a constant $M$ (≥1). We also study dynamic admissions of NFV-enabled multicast requests without the knowledge of future request arrivals with the objective to maximize the network throughput, for which we propose an efficient heuristic, and for the special case of dynamic request admissions, we devise an online algorithm with a competitive ratio of $O(\log n)$ for it when $M=1$ , where $n$ is the number of nodes in the network. We finally evaluate the performance of the proposed algorithms through experimental simulations. Experimental results demonstrate that the proposed algorithms are promising and outperform existing heuristics.","1558-0857","","10.1109/TCOMM.2018.2881438","National Natural Science Foundation of China; fundamental research funds for the central universities in China; Dalian University of Technology; EU NECOS projects; National Natural Science Foundation of China; Shenzhen Basic Research Funding Scheme; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8536407","Network function virtualization;software-defined networks;multicasting;NFV-enabled multicasting;service chains;virtualized network functions;routing;approximation;online algorithms","Multicast communication;Multicast algorithms;Servers;Heuristic algorithms;Approximation algorithms;Bandwidth;Software","multicast communication;software defined networking;telecommunication network routing;virtualisation","scalable multicasting;network functions;service chain requirement;network function virtualization-enabled multicast request;software-defined network;network throughput;admitted NFV-enabled multicast requests;network resource capacity;computing resource consumption cost;network bandwidth consumption cost;NFV-enabled multicasting problems;resource capacity constraints;online NFV-enabled multicasting problem;dynamic request admissions;NFV-enabled multicasting","","20","","43","","15 Nov 2018","","","IEEE","IEEE Journals"
"Complete Runtime Tracing for Device Drivers Based on LLVM","J. Bai; H. Liu; Y. Wang; S. Hu","Nat. Lab. for Inf. Sci. & Technol., Tsinghua Univ., Beijing, China; Nat. Lab. for Inf. Sci. & Technol., Tsinghua Univ., Beijing, China; Nat. Lab. for Inf. Sci. & Technol., Tsinghua Univ., Beijing, China; Nat. Lab. for Inf. Sci. & Technol., Tsinghua Univ., Beijing, China","2015 IEEE 39th Annual Computer Software and Applications Conference","24 Sep 2015","2015","2","","200","209","Device drivers often suffer from much more bugs than the kernel, so testing device drivers becomes more and more important and necessary. In software testing, runtime tracing is an important technique to monitor real executing procedures of the program. Meanwhile, runtime information can also assist the programmer to make more accurate analysis of the program, like verifying the correctness of code execution and detecting bugs. However, due to kernel-mode execution and high complexity of kernel code, completely tracing drivers is hard, which causes real execution paths can not be clearly identified. In order to provide more powerful support for software testing of device drivers, we propose a method named Driver Trace, to do complete runtime tracing at the function level. Driver Trace utilizes instrumentation technique for runtime tracing, which is implemented based on LLVM compiler infrastructure. When the target driver works, Driver Trace records complete runtime information of function calls, like function names, return values and parameter pointers, and the information is recorded in a log file for future analysis. We have successfully implemented Driver Trace on 10 real device drivers in Linux 3.16.4 and made the evaluation as well. The experimental results show that Driver Trace provides an effective method of runtime tracing for device drivers with the modest overhead. Moreover, using an automated analysis of the runtime information recorded by Driver Trace, we also find 6 violations about resource usages in these 10 device drivers.","0730-3157","978-1-4673-6564-2","10.1109/COMPSAC.2015.24","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7273619","testing;device drivers;runtime tracing;LLVM;instrumentation","Runtime;Kernel;Instruments;Linux;Joints;Testing;Monitoring","device drivers;Linux;operating system kernels;program compilers;program debugging;program diagnostics;program testing","complete runtime tracing;device driver software testing;program analysis;bug detection;kernel-mode execution;Driver Trace;LLVM compiler infrastructure;function calls;Linux 3.16.4","","1","","35","","24 Sep 2015","","","IEEE","IEEE Conferences"
"System reliability models with stress covariates for changing load profiles","A. Hada; D. Coit; M. Agnello; K. Megow","Department of Industrial Engineering, Rutgers University, 96 Frelinghuysen Road, Piscataway, NJ 08854 USA; Department of Industrial Engineering, Rutgers University, 96 Frelinghuysen Road, Piscataway, NJ 08854 USA; Naval Air Engineering Station Lakehurst, Lakehurst, New Jersey 08733; Naval Air Engineering Station Lakehurst, Lakehurst, New Jersey 08733","2011 Proceedings - Annual Reliability and Maintainability Symposium","21 Apr 2011","2011","","","1","7","This paper presents various models to estimate reliability for a future profile with increased stress using the current observations to develop a model for future reliability. For the foreseeable future the equipment itself will not change, and thus the increase of the load or stress, that the system is exposed to, may decrease reliability and increase maintenance. The model is required to determine the impact of these future pro files. This paper is motivated by an actual application modeling the current and anticipated future reliability of naval aircraft launch and recovery equipment (ALRE). The cycles-to-failure data (CTF) was generated by simulation and serves as the basis in explaining four methods that will be evaluated during 2011-2012. System reliability models are presented and demonstrated to anticipate system reliability and availability for changing and increasing applied loading distributions. These models are int ended for systems whose components are exposed to predictable and quantifiable, but changing loading patterns. In the intended application, the models will be used to estimate the future reliability of naval aircraft catapult and arresting gear when subjected to different air wing compositions. Each air wing usage profile is potentially different, forming a distribution of usage stresses, and this distribution is shifting with time, as aircraft weight and missions change. The current ALRE systems will continue to be used for the foreseeable fut ure and the model is required to predict future performance and to identify the most unreliable components or problems within the existing design. Weibull distribution models are used in the typical fashion to model component failure times, but the initial Weibull distribution parameters are mathematical functions of the current, known applied stress distributions. These component models are then used within a discrete event simulation model to predict system reliability and availability. After this initial evaluation, models and software normally applied for accelerated life testing applications are used to develop models which have Weibull distribution parameters that are both mathematical functions with usage stress covariates and also mathematical functions of the distribution or variations in the changing applied loading conditions. Four specific modeling approaches are presented, and compared and contrasted based on data requirements, practicality and other criteria. The four modeling approaches involved models and data analysis at the component-level based on (i) linear stress adjusted usage measures in place of time, (ii) nonlinear stress adjusted usage measures in place of time, (iii) Weibull shape parameters modeled using a general log-linear model based on the mean and standard deviation of critical stress measures in a changing environment, and (iv) Weibull shape parameters modeled using a general log-linear model based on the distributional form of critical stress measures in a changing environment. The component models are then assembled into a system model. This methodology is demonstrated and compared on an example system using simulated data. The models collectively provide a practical methodology to use existing failure and usage stress data to predict future reliability based on a changing and increasing loading pattern.","0149-144X","978-1-4244-8856-8","10.1109/RAMS.2011.5754528","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5754528","Simulation Models;Failure Rate Model;Accelerated Life","Stress;Load modeling;Mathematical model;Reliability;Data models;Maximum likelihood estimation;Loading","aerospace components;aircraft testing;data analysis;discrete event simulation;failure analysis;gears;life testing;naval engineering;reliability;Weibull distribution","system reliability model;stress covariate;load profile change;naval aircraft launch and recovery equipment;cycles-to-failure data;air wing;arresting gear;air wing usage profile;ALRE system;Weibull distribution model;component failure time;discrete event simulation model;accelerated life testing application;data analysis;nonlinear stress;linear stress;log-linear model","","3","1","5","","21 Apr 2011","","","IEEE","IEEE Conferences"
"Improving system health monitoring with better error processing","B. Kain; N. Ozarin","The Omnicon Group Inc., Hauppauge New York USA; The Omnicon Group Inc., Hauppauge New York USA","2011 IEEE Conference on Prognostics and Health Management","22 Sep 2011","2011","","","1","8","To help identify unexpected software events and impending hardware failures, developers typically incorporate error-checking code in their software to detect and report them. Unfortunately, implementing checks with reporting capabilities that give the most useful results comes at a price. Such capabilities should report the exact nature of impending failures and additionally limit reporting to only the first occurrence of an error to prevent flooding the error log with the same message. They must report when an existing error or fault is replaced by another error of a different nature or value. They must recognize what makes occasional faults allowable and they must reset themselves upon recovery from a reported failure so the checking process can begin anew. They must also report recovery from previously reported failures that appear to have healed themselves. Since the price associated with providing all these features is limited by budget and schedule, system reliability and health monitoring often suffer. However, there are practical techniques that can simplify the effort associated with incorporating such error detection and reporting. When done properly, they can greatly improve system reliability and health monitoring by finding potentially hidden problems during development and can also greatly improve system maintainability by providing concise running descriptions of problems when things go wrong particularly when minor errors might otherwise go unnoticed. In addition, preventative maintenance can be greatly aided by applying error detection techniques to performance monitoring in the absence of errors. Many of the techniques described in this paper take advantage of simple classes to do bookkeeping tasks such as updating and tracking statistical analysis of errors and error reporting. The paper highlights several of these classes and gives examples from actual applications.","","978-1-4244-9827-7","10.1109/ICPHM.2011.6024322","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6024322","software health;BIT;software monitoring","Monitoring;Temperature sensors;Software;Temperature measurement;Maintenance engineering;Hardware;Reliability","error analysis;software fault tolerance;statistical analysis;system recovery","system health monitoring;error processing;unexpected software events;error checking code;hardware failures;allowable faults;system reliability;error detection;error reporting;statistical error analysis","","","","3","","22 Sep 2011","","","IEEE","IEEE Conferences"
"AptStore: Dynamic Storage Management for Hadoop","K. R. Krish; A. Khasymski; A. R. Butt; S. Tiwari; M. Bhandarkar",NA; NA; NA; NA; NA,"2013 IEEE 5th International Conference on Cloud Computing Technology and Science","6 Mar 2014","2013","1","","33","41","Typical Hadoop setups employ Direct Attached Storage (DAS) with compute nodes and uniform replication of data to sustain high I/O throughput and fault tolerance. However, not all data is accessed at the same time or rate. Thus, if a large replication factor is used to support higher throughput for popular data, it wastes storage by unnecessarily replicating unpopular data as well. Conversely, if less replication is used to conserve storage for the unpopular data, it means fewer replicas for even popular data and thus lower I/O throughput. We present Apt Store, a dynamic data management system for Hadoop, which aims to improve overall I/O throughput while reducing storage cost. We design a tiered storage that uses the standard DAS for popular data to sustain high I/O throughput, and network-attached enterprise filers for cost-effective, fault-tolerant, but lower-throughput storage for unpopular data. We design a file Popularity Prediction Algorithm (PPA) that analyzes file system audit logs and predicts the appropriate storage policy of each file, as well as use the information for transparent data movement between tiers. Our evaluation of Apt Store on a real cluster shows 21.3% improvement in application execution time over standard Hadoop, while trace driven simulations show 23.7% increase in read throughput and 43.4% reduction in the storage capacity requirement of the system.","","978-0-7695-5095-4","10.1109/CloudCom.2013.12","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6753775","","Throughput;Bandwidth;Prediction algorithms;Standards;Algorithm design and analysis;Fault tolerance;Fault tolerant systems","distributed processing;public domain software;software fault tolerance;storage management","AptStore;dynamic storage management;direct attached storage;DAS;Hadoop;I/O throughput;fault tolerance;dynamic data management system;storage cost reduction;tiered storage design;network-attached enterprise filers;file popularity prediction algorithm;PPA;file system audit log analysis;transparent data movement;application execution time;trace driven simulations;Hadoop distributed file system","","3","","28","","6 Mar 2014","","","IEEE","IEEE Conferences"
"OPIA: A Tool for On-Device Testing of Vulnerabilities in Android Applications","L. Bello-Jiménez; A. Mazuera-Rozo; M. Linares-Vásquez; G. Bavota",Universidad de Los Andes; Università della Svizzera italiana; Universidad de Los Andes; Università della Svizzera italiana,"2019 IEEE International Conference on Software Maintenance and Evolution (ICSME)","5 Dec 2019","2019","","","418","421","Mobile developers constantly have to deal with users pressure for continuous delivery of apps while keeping quality attributes such as confidentiality and data integrity. To better support developers in testing security vulnerabilities during evolution and maintenance of mobile apps, in this demo we present a novel tool, OPIA, for on-device security testing. OPIA allows developers/testers to (i) conduct SQL-injection attacks and collect logs to identify leaks of sensitive information through record-and-replay testing, and (ii) extract data stored in local databases and shared preferences to identify sensitive information that is not properly encrypted, anonymized. OPIA is publicly available at GitHub.","2576-3148","978-1-7281-3094-1","10.1109/ICSME.2019.00073","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8918944","security;Android;testing;confidentiality","Data mining;Tools;Testing;NoSQL databases;Androids;Humanoid robots","Android (operating system);cryptography;data integrity;data privacy;mobile computing;program testing;software maintenance;software quality;SQL","sensitive information;OPIA;android applications;mobile developers;users pressure;quality attributes;testing security vulnerabilities;mobile apps;on-device security testing;conduct SQL-injection attacks;record-and-replay testing","","","","13","","5 Dec 2019","","","IEEE","IEEE Conferences"
"Automated software testing for high-end touch screen automotive displays","S. Patil; U. Parmar; R. Karegaonkar",NA; NA; NA,"2015 IEEE International Transportation Electrification Conference (ITEC)","21 Jan 2016","2015","","","1","4","The Current Methodologies of virtual and Automated Testing on touch screen displays are limited, since, minor screen changes and display software updates will cause the automated tests to fail and hence there will be a need to fix them. Addition of new items to a drop-down list, movement of buttons, reusability and conversion of Test scripts for similar devices will be a cumbersome job. The general approach of testing Display manually, is not feasible due to sluggishness, monotony and repetitiveness of work, which can cause manual errors in different volumes at different times, it will also reduce the option of Batch executions of tests 24*7. The automated approach for testing such Displays through XY coordinates pointing on screen, taking Screen captures and using optical character recognition for verification methods are non-reliable and affects the performance of testing. This makes automating a test difficult, causes a high amount of risk as well as high maintenance costs. This paper offers recent ideas and its implementation of software testing, on touch screen Displays, which are widely used in sophisticated passenger cars as well as in off-highway equipment like tractors, construction and Forestry etc. In this method of testing the Display's User interface, we use `Test Events' which are linked with different objects/Icons on displays. Every user action like selecting menu item from dropdown, pressing button, swiping, turning pages are considered as unique actions, and these actions are then bundled with unique Object Id data to create the events on the Display. These user events are simulated automatically and the response is monitored and logged for identifying and analyzing Software defects. The architecture is based on ""Data Driven Model"" where test data is separated from script in order to take care of any changes in quick and agile way. The Data such as unique references to objects and Pools on the screen are placed in SQL Database, this enables to access data from different locations by multiple users. Though, the approach can be widely applied to majority of Touch screen applications, we restrict the scope of this paper to automotive domain due to the Standards and protocol used during implementation.","","978-1-5090-1911-3","10.1109/ITEC-India.2015.7386880","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7386880","","Testing;Software;Automation;Servers;Computer architecture;Manuals;Databases","automotive electronics;power engineering computing;program testing;SQL;user interfaces","automated software testing;high-end touch screen automotive displays;automated tests;reusability;batch executions;optical character recognition;maintenance costs;off-highway equipment;user interface;data driven model;SQL database;automotive domain;protocol","","1","","3","","21 Jan 2016","","","IEEE","IEEE Conferences"
"Behavior checking of web applications after testing","M. Jiang; Z. Ding; Q. Ge","Center of Math Computing and Software Engineering, Zhejiang Sci-Tech University, Hangzhou, China; Center of Math Computing and Software Engineering, Zhejiang Sci-Tech University, Hangzhou, China; Faculty of Education, Yamaguchi University, 1677-1 Yashida, Yamaguchi 753-8513, Japan","2010 International Conference on Electronics and Information Engineering","2 Sep 2010","2010","2","","V2-163","V2-167","Testing is the last step to check the correctness of a software system. However, due to its incompleteness, we still do not know if the implementation behavior matches the design behavior. In this paper, we provide a new solution for web applications. Since web application has special navigation character, we can get rich information in the log file after testing. From such log file, we extract the Petri net based behavior of the web application. The behavior is then checked by SPIN. Market Information System has been adopted as the case study.","","978-1-4244-7681-7","10.1109/ICEIE.2010.5559739","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5559739","Web navigation;Petri net;Behavior checking;SPIN","Navigation;Unified modeling language;Mice;Analytical models;Testing;Web pages;Browsers","information retrieval;Internet;Petri nets","behavior checking;Web application;log file;Petri net;SPIN;market information system","","","","7","","2 Sep 2010","","","IEEE","IEEE Conferences"
"What Requirements Engineering can Learn from Process Mining","M. Ghasemi","Sch. of Electr. Eng. & Comput. Sci, Univ. of Ottawa, Ottawa, ON, Canada","2018 1st International Workshop on Learning from other Disciplines for Requirements Engineering (D4RE)","30 Dec 2018","2018","","","8","11","Process Mining is an approach that uses event logs of systems or processes and turns them into valuable insights. The main characteristic of process mining techniques is that they focus on and exploit ""real behavior"" of a large number of stakeholders of a system or of a process. On the other hand, requirements engineering is concerned with requirements elicitation and analysis not only in terms of software specifications but also in terms of activities carried out within an organizational and social context. Furthermore, involving a large number of users/stakeholders has always been a challenge with traditional requirements engineering methods. Although both requirements engineering and process mining have gained increasing research attention, the synergy between these two domains is yet to be exploited. Such a synergy can help both domains benefit from their capabilities and mitigate their own challenges. The ability of process mining to exploit huge data logs can help requirements engineers cope with the above challenge. This paper aims to highlight how requirements engineering can benefit from process mining's components such as execution logs, process discovery and conformance techniques for requirements elicitation, prioritization and validation.","","978-1-5386-8418-4","10.1109/D4RE.2018.00008","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8595126","Process Mining, Goal Modeling, Requirements Engineering, Event Logs, Data-Driven, Goal Mining","Conferences;Requirements engineering;Economic indicators","data mining;formal specification;systems analysis","process mining techniques;requirements elicitation;traditional requirements engineering methods;requirements engineers;process discovery;conformance techniques;social context;software specifications","","2","","25","","30 Dec 2018","","","IEEE","IEEE Conferences"
"How Developers' Collaborations Identified from Different Sources Tell Us about Code Changes","S. Panichella; G. Bavota; M. D. Penta; G. Canfora; G. Antoniol","Dept. of Eng., Univ. of Sannio, Benevento, Italy; Dept. of Eng., Univ. of Sannio, Benevento, Italy; Dept. of Eng., Univ. of Sannio, Benevento, Italy; Dept. of Eng., Univ. of Sannio, Benevento, Italy; Ecole Polytech. de Montreal, Montreal, QC, Canada","2014 IEEE International Conference on Software Maintenance and Evolution","6 Dec 2014","2014","","","251","260","Written communications recorded through channels such as mailing lists or issue trackers, but also code co-changes, have been used to identify emerging collaborations in software projects. Also, such data has been used to identify the relation between developers' roles in communication networks and source code changes, or to identify mentors aiding newcomers to evolve the software project. However, results of such analyses may be different depending on the communication channel being mined. This paper investigates how collaboration links vary and complement each other when they are identified through data from three different kinds of communication channels, i.e., mailing lists, issue trackers, and IRC chat logs. Also, the study investigates how such links overlap with links mined from code changes, and how the use of different sources would influence (i) the identification of project mentors, and (ii) the presence of a correlation between the social role of a developer and her changes. Results of a study conducted on seven open source projects indicate that the overlap of communication links between the various sources is relatively low, and that the application of networks obtained from different sources may lead to different results.","1063-6773","978-1-4799-6146-7","10.1109/ICSME.2014.47","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6976091","Developers;Developer Social Network;Empirical Study","Electronic mail;Communication channels;Welding;Measurement;Software;Birds","program diagnostics;public domain software;software development management","developer collaborations;written communications;mailing lists;issue trackers;source code changes;communication networks;IRC chat logs;project mentors;open source projects","","16","","37","","6 Dec 2014","","","IEEE","IEEE Conferences"
"Radio Diagnostic tool: IDoctor","W. H. Chong","Motorola Solutions Sdn. Bhd., Penang, Malaysia","2013 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)","19 Dec 2013","2013","","","47","48","Reproducing issues to verify if the issues, especially reset issues, reported by customer are valid issues are tedious and time consuming. The rationale is not all issues can be reproduced and it is not easy to reproduce as some issues may be caused by hardware failure, environmental factor such as air pressure and temperature, and etc. Although the existing reset capture implemented in the radio is used to capturing this kind of intermittent defects, these dependencies are not recorded into the radio when issue happened. The reset capture is able to capture the last state of the radio before reset happens. So, the information such as, how many calls had been made before the radio hangs and resets? What is the battery level before resets? It remains unknown. This information is crucial for the issue as we need to know whether the issue happens after the first call made or after many calls made; during battery level is low or full. Hence, a Diagnostic tool is introduced in order to track radios' condition and to serve as an additional information for developer in bug fixing. iDoctor aims to reduce time in reproducing issues and helps developers to resolve issues faster if the comprehensive data logging is performed. At the same time, the health of the radio can be monitored as well. As a result, this will increase customer's satisfaction and their confidence level to the company that may directly impact to the company's market share.","","978-1-4799-2552-0","10.1109/ISSREW.2013.6688864","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6688864","","Batteries;Software reliability;Companies;Software;Customer satisfaction;Maintenance engineering;Real-time systems","data loggers","radio diagnostic tool;hardware failure;radios condition;bug fixing;iDoctor;data logging;customer satisfaction;company market share","","","","4","","19 Dec 2013","","","IEEE","IEEE Conferences"
"LCDD: Detecting Business Process Drifts Based on Local Completeness","L. Lin; L. Wen; L. Lin; J. Pei; H. Yang","School Of Software, Tsinghua University, 12442 Beijing, Beijing, China, 100084 (e-mail: leilei_lin@126.com); Tsinghua University, School of Software, Beijing, Beijing, China, 100084 (e-mail: wenlj@tsinghua.edu.cn); Tsinghua University, 12442 Beijing, Beijing, China, (e-mail: lin-l16@mails.tsinghua.edu.cn); School of Software, Tsinghua University, Beijing, Beijing, China, (e-mail: 12283776@qq.com); Tsinghua University, 12442 Beijing, Beijing, China, (e-mail: hedong_yang@126.com)","IEEE Transactions on Services Computing","","2020","PP","99","1","1","Flexibility and evolution have been a hot topic in the context of business process management. Business process drift detection is a family of methods to detect changes by analyzing the event log, but existing methods have some disadvantages in dealing with concept drifts. First, most of these methods detect changes depending on an exploration of a potentially large feature space and are time consuming. Second, with the size of lag period becoming small, the accuracies of some methods will drop rapidly. In this paper, we address these problems by proposing a novel drift detection technique called LCDD that significantly differs from existing methods. The core idea is to find out new features and disappeared features from traces after the log fragment meets local completeness. To begin with, we use the relations of direct succession as the lightweight feature, which are compared between complete window and detection window. Then, we find new direct successions and stable disappeared direct successions by just moving detection window. Finally, the forgetting mechanism is used to abandon some direct successions after finding a change point. An extensive empirical evaluation shows that LCDD is faster and more accurate.","1939-1374","","10.1109/TSC.2020.3032787","National Natural Science Foundation of China; the National Key Research and Development Program of China; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9234652","Process mining;concept drift;direct succession;change detection;log completeness","Task analysis;Data mining;Feature extraction;Microsoft Windows;Steady-state;Process modeling","","","","","","","","21 Oct 2020","","","IEEE","IEEE Early Access Articles"
"Lessons Learned from the Analysis of System Failures at Petascale: The Case of Blue Waters","C. Di Martino; Z. Kalbarczyk; R. K. Iyer; F. Baccanico; J. Fullop; W. Kramer","Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Federico II Univ. of Naples, Naples, Italy; Nat. Center for Supercomput. Applic., USA; Nat. Center for Supercomput. Applic., USA","2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks","22 Sep 2014","2014","","","610","621","This paper provides an analysis of failures and their impact for Blue Waters, the Cray hybrid (CPU/GPU) supercomputer at the University of Illinois at Urbana-Champaign. The analysis is based on both manual failure reports and automatically generated event logs collected over 261 days. Results include i) a characterization of the root causes of single-node failures, ii) a direct assessment of the effectiveness of system-level fail over as well as memory, processor, network, GPU accelerator, and file system error resiliency, and iii) an analysis of system-wide outages. The major findings of this study are as follows. Hardware is not the main cause of system downtime. This is notwithstanding the fact that hardware-related failures are 42% of all failures. Failures caused by hardware were responsible for only 23% of the total repair time. These results are partially due to the fact that processor and memory protection mechanisms (x8 and x4 Chip kill, ECC, and parity) are able to handle a sustained rate of errors as high as 250 errors/h while providing a coverage of 99.997% out of a set of more than 1.5 million of analyzed errors. Only 28 multiple-bit errors bypassed the employed protection mechanisms. Software, on the other hand, was the largest contributor to the node repair hours (53%), despite being the cause of only 20% of the total number of failures. A total of 29 out of 39 system-wide outages involved the Lustre file system with 42% of them caused by the inadequacy of the automated fail over procedures.","2158-3927","978-1-4799-2233-8","10.1109/DSN.2014.62","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6903615","Failure Analysis;Failure Reports;Cray XE6;Cray XK7;Supercomputer;Machine Check;Nvidia GPU errors","Random access memory;Hardware;Graphics processing units;Blades;Maintenance engineering;Error correction codes","Cray computers;failure analysis;mainframes;parallel machines;system recovery","system failure analysis;Blue Waters;Cray hybrid supercomputer;CPU-GPU supercomputer;University of Illinois;Urbana-Champaign;manual failure reports;automatically generated event logs;single-node failures;GPU accelerator;file system error resiliency;system-wide outage analysis;hardware-related failures;memory protection mechanisms;x8 Chip kill;x4 Chip kill;ECC;parity;Lustre file system","","98","","24","","22 Sep 2014","","","IEEE","IEEE Conferences"
"Using high-powered long-range zigbee devices for communication during amateur car racing events","J. Wakemen; M. Hodson; P. Shafer; V. Hnatyshin","Department of Computer Science, Rowan University, Glassboro, NJ 08028, USA; Department of Computer Science, Rowan University, Glassboro, NJ 08028, USA; Department of Computer Science, Rowan University, Glassboro, NJ 08028, USA; Department of Computer Science, Rowan University, Glassboro, NJ 08028, USA","Wireless VITAE 2013","3 Oct 2013","2013","","","1","5","In the world of amateur motorsports the racers are always looking to improve their skills. This has led to the development of various monitoring and data logging systems, often coupled with “action sports” cameras which capture the car performance in enough detail to allow the driver to study the race after completion. In this paper we investigate the possibility of the using data logging systems together with IEEE 802.15.4 high power devices for race car - pit crew communication during the race. This will allow the pit crew to monitor the car and get the driver to stop before a catastrophic failure occurs. We used OPNET IT Guru ver. 17.0 network software package to conduct our simulation study. Specifically, we focus on such aspects of 802.15.4 protocol as communication range, reliable data delivery, achievable throughput, and end-to-end delay experiences by the application during the car race.","","978-1-4799-0239-2","10.1109/VITAE.2013.6617066","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6617066","802.15.4;ZigBee;XBee;race car communciation;vehicular communication","IEEE 802.11 Standards;Receivers;Radio frequency","access protocols;cameras;data loggers;delays;software packages;Zigbee","high-powered long-range zigbee devices;amateur car racing events;amateur motorsports;monitoring systems;data logging systems;action sports cameras;IEEE 802.15.4;pit crew communication;catastrophic failure;OPNET IT Guru ver.17.0 network;software package;802.15.4 protocol;data delivery;end-to-end delay","","1","","7","","3 Oct 2013","","","IEEE","IEEE Conferences"
"TRACERJD: Generic trace-based dynamic dependence analysis with fine-grained logging","H. Cai; R. Santelices","University of Notre Dame, Indiana, USA; University of Notre Dame, Indiana, USA","2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)","9 Apr 2015","2015","","","489","493","We present the design and implementation of TRACERJD, a toolkit devoted to dynamic dependence analysis via fine-grained whole-program dependence tracing. TRACERJD features a generic framework for efficient offline analysis of dynamic dependencies, including those due to exception-driven control flows. Underlying the framework is a hierarchical trace indexing scheme by which TRACERJD maintains the relationships among execution events at multiple levels of granularity while capturing those events at runtime. Built on this framework, several application tools are provided as well, including a dynamic slicer and a performance profiler. These example applications also demonstrate the flexibility and ease with which a variety of client analyses can be built based on the framework. We tested our toolkit on four Java subjects, for which the results suggest promising efficiency of TRACERJD for its practical use in various dependence-based tasks.","1534-5351","978-1-4799-8469-5","10.1109/SANER.2015.7081862","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7081862","Dependence analysis;tracing;slicing;profiling","Runtime;Probes;Indexes;Java;Algorithms;Monitoring","Java;program debugging;program slicing;software tools","TracerJD toolkit;generic trace-based dynamic dependence analysis;fine-grained logging;fine-grained whole-program dependence tracing;exception-driven control flows;hierarchical trace indexing scheme;dynamic slicer;performance profiler;client analyses;Java subjects;dependence-based tasks","","6","","22","","9 Apr 2015","","","IEEE","IEEE Conferences"
"Monitoring and Predicting Hardware Failures in HPC Clusters with FTB-IPMI","R. Rajachandrasekar; X. Besseron; D. K. Panda","Network-Based Comput. Lab., Ohio State Univ., Columbus, OH, USA; Comput. Sci. & Commun. Res. Unit, Univ. of Luxembourg, Luxembourg, Luxembourg; Network-Based Comput. Lab., Ohio State Univ., Columbus, OH, USA","2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum","20 Aug 2012","2012","","","1136","1143","Fault-detection and prediction in HPC clusters and Cloud-computing systems are increasingly challenging issues. Several system middleware such as job schedulers and MPI implementations provide support for both reactive and proactive mechanisms to tolerate faults. These techniques rely on external components such as system logs and infrastructure monitors to provide information about hardware/software failure either through detection, or as a prediction. However, these middleware work in isolation, without disseminating the knowledge of faults encountered. In this context, we propose a light-weight multi-threaded service, namely FTB-IPMI, which provides distributed fault-monitoring using the Intelligent Platform Management Interface (IPMI) and coordinated propagation of fault information using the Fault-Tolerance Backplane (FTB). In essence, it serves as a middleman between system hardware and the software stack by translating raw hardware events to structured software events and delivering it to any interested component using a publish-subscribe framework. Fault-predictors and other decision-making engines that rely on distributed failure information can benefit from FTB-IPMI to facilitate proactive fault-tolerance mechanisms such as preemptive job migration. We have developed a fault-prediction engine within MVAPICH2, an RDMA-based MPI implementation, to demonstrate this capability. Failure predictions made by this engine are used to trigger migration of processes from failing nodes to healthy spare nodes, thereby providing resilience to the MPI application. Experimental evaluation clearly indicates that a single instance of FTB-IPMI can scale to several hundreds of nodes with a remarkably low resource-utilization footprint. A deployment of FTB-IPMI that services a cluster with 128 compute-nodes, sweeps the entire cluster and collects IPMI sensor information on CPU temperature, system voltages and fan speeds in about 0.75 seconds. The average CPU utilization of this service running on a single node is 0.35%.","","978-1-4673-0974-5","10.1109/IPDPSW.2012.139","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6270765","Fault detection;coordinated fault propogation;IPMI;FTB. HPC Clusters","Libraries;Monitoring;Temperature sensors;Fault tolerant systems;Fault tolerance;Software;Hardware","computerised monitoring;decision making;fault diagnosis;fault tolerant computing;middleware;multi-threading;online front-ends;pattern clustering;resource allocation;user interfaces","hardware failure prediction;hardware failure monitoring;HPC clusters;FTB-IPMI;cloud computing systems;system middleware;job schedulers;reactive mechanisms;proactive mechanisms;external components;system logs;infrastructure monitors;hardware-software failure;light-weight multithreaded service;distributed fault monitoring;Intelligent Platform Management Interface;fault tolerance backplane;software stack;hardware stack;raw hardware events;structured software events;publish-subscribe framework;decision making engines;distributed failure information;proactive fault tolerance mechanisms;preemptive job migration;fault prediction engine;MVAPICH2;RDMA-based MPI implementation;failing nodes;healthy spare nodes;resource utilization;IPMI sensor information;CPU temperature;system voltages;fan speeds;high-performance computing clusters","","13","","31","","20 Aug 2012","","","IEEE","IEEE Conferences"
"Decisions as a Service for Application Centric Real Time Analytics","P. Tendick; A. Mockus","Avaya Labs. Res., Basking Ridge, NJ, USA; Avaya Labs. Res., Basking Ridge, NJ, USA","2016 IEEE/ACM 2nd International Workshop on Big Data Software Engineering (BIGDSE)","16 Jan 2017","2016","","","1","7","The need for application-level intelligence cannot be easily satisfied with existing architectures or methodologies that separate methods and tools for application developers and data scientists. We aim, therefore, to develop a framework (an architecture and a methodology) to make it possible to add intelligence capabilities to existing applications (decision-enablement) and to facilitate building new decision-enabled applications. The proposed approach starts by instrumenting the existing code with logging and instrumented decision points. The execution of the application produces information that is initially used to reengineer its behavior and then the decision points are used to conduct search-based experimentation to optimize its behavior. This lightweight instrumentation allows the application developer and data scientist to fully exploit their capabilities, with the framework providing the glue needed to put their work together easily and transparently. In particular, the analytic capabilities, such as analysis of operational data, are better dealt with in the decision-making part, without complicating the mechanics of how the application functions. We plan to apply this framework to decision enable existing systems and to build new systems from scratch and measure the effectiveness of the approach and of the resulting products.","","978-1-4503-4152-3","10.1109/BIGDSE.2016.009","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7811379","Decisions as a service;decision injection;instrumentation;search-based experimentation;logging;scoring;reverse engineering;evidence based design;next best action;event driven application;data scientist;application developer","Engines;Real-time systems;Servers;Data models;Decision making;Software","artificial intelligence;software engineering","decisions-as-a-service;application-level intelligence;application developers;data scientists;instrumented decision points","","","","14","","16 Jan 2017","","","IEEE","IEEE Conferences"
"Abnormal Process Instances Identification Method in Healthcare Environment","B. Han; L. Jiang; H. Cai","Sch. of Software, Shanghai Jiao Tong Univ., Shanghai, China; Sch. of Software, Shanghai Jiao Tong Univ., Shanghai, China; Sch. of Software, Shanghai Jiao Tong Univ., Shanghai, China","2011IEEE 10th International Conference on Trust, Security and Privacy in Computing and Communications","2 Jan 2012","2011","","","1387","1392","In order to gain the competitive advantage, more and more hospitals put their attention on determining and optimizing the standard clinical pathway. However, there are many abnormal instances in the event logs which disturb the effect of the process mining and the process analysis. Also, the prescription and the medical test items may have a large difference in the specific disease, where has the similar clinical pathways due to the multi-factor in the clinical pathways. In this paper, an abnormal process instances identification method (APIIM) is proposed. Given the event logs and the standard clinical pathway, the method classifies the instances based on the classification attributes and identifies the abnormal instances by the outlier detection technology. Moreover, a case study using the real data in one hospital is implemented and the result shows that the method is effective and efficient in discovering the abnormal process instances in the healthcare environment.","2324-9013","978-1-4577-2135-9","10.1109/TrustCom.2011.189","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6120985","business process mining;clinical pathway;outlier detection;process analysis","Process control;Data mining;Diabetes;Detection algorithms;Hospitals;Educational institutions","data mining;diseases;health care;hospitals;pattern classification;security of data","healthcare environment;hospitals;standard clinical pathway optimization;event logs;process mining;process analysis;medical test items;disease;abnormal process instances identification method;APIIM;classification attributes;outlier detection technology","","5","","10","","2 Jan 2012","","","IEEE","IEEE Conferences"
"Intruder detector: A continuous authentication tool to model user behavior","L. C. Milton; A. Memon","Information Technology Laboratory, U.S. Army ERDC, Vicksburg, MS, USA; Department of Computer Science, University of Maryland, College Park, USA","2016 IEEE Conference on Intelligence and Security Informatics (ISI)","17 Nov 2016","2016","","","286","291","This paper presents techniques to continuously authenticate users as they interact with web-based software. Unique behavioral footprints, indicating patterns of use for groups of users, are captured from web server log files and integrated into an n-gram model. These statistical language models provide sequences and sub-sequences of user interaction, ordering, and temporal relationships. When users interact with web-based software, their stored usage profile is compared to their current interactions. Deviations may indicate malicious activity. We use our innovative tool, Intruder Detector (ID) to generate the profiles. Afterwards, we use various measures-of-effectiveness techniques to understand the feasibility of our approach. Our empirical study shows that session length and the prevalence of user data significantly affect the model's ability to correctly classify users.","","978-1-5090-3865-7","10.1109/ISI.2016.7745492","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7745492","behavioral modeling;continuous authentication;software security;n-grams;statistical language models","Authentication;Computational modeling;Biometrics (access control);Training;Context modeling;Web servers;Measurement","file servers;human factors;Internet;natural language processing;security of data;user interfaces;user modelling","intruder detector;continuous authentication tool;user behavior modelling;user authentication;Web-based software;Web server log files;n-gram model;statistical language models;user interaction;usage profile;malicious activity;measures-of-effectiveness techniques;user classification","","1","","28","","17 Nov 2016","","","IEEE","IEEE Conferences"
"Hierarchical Business Process Discovery: Identifying Sub-processes Using Lifecycle Information","C. Liu","School of Computer Science and Technology, Shandong University of Technology,Zibo,China","2020 IEEE International Conference on Web Services (ICWS)","22 Dec 2020","2020","","","423","427","This paper aims to introduce a novel approach to discover hierarchical business process models from event logs with lifecycle information. To handle noise and infrequent behavior, we introduce the notion of nesting ratio to quantify the probability of nesting. All proposed approaches have been implemented in the open-source process mining toolkit ProM. The proposed approach is compared to existing process discovery techniques using both synthetic and real-life lifecycle event logs, and finally we show that our approach outperforms exiting approaches to discover hierarchical processes.","","978-1-7281-8786-0","10.1109/ICWS49710.2020.00062","National Natural Science Foundation of China; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9283976","Process discovery;Sub-process Detection;Hierarchical Petri nets;Event Log with Lifecycle Information","Web services;Scalability;Conferences;PROM;Petri nets;Open source software;Business","business data processing;data mining;probability","hierarchical business process discovery;identifying sub-processes;lifecycle information;hierarchical business process models;infrequent behavior;nesting ratio;open-source process mining toolkit ProM;existing process discovery techniques;real-life lifecycle event logs;hierarchical processes","","","","18","","22 Dec 2020","","","IEEE","IEEE Conferences"
"Interactive and Automated Debugging for Big Data Analytics","M. A. Gulzar","Univ. of California, Los Angeles, Los Angeles, CA, USA","2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)","30 Aug 2018","2018","","","509","511","An abundance of data in many disciplines of science, engineering, national security, health care, and business has led to the emerging field of Big Data Analytics that run in a cloud computing environment. To process massive quantities of data in the cloud, developers leverage Data-Intensive Scalable Computing (DISC) systems such as Google's MapReduce, Hadoop, and Spark. Currently, developers do not have easy means to debug DISC applications. The use of cloud computing makes application development feel more like batch jobs and the nature of debugging is therefore post-mortem. Developers of big data applications write code that implements a data processing pipeline and test it on their local workstation with a small sample data, downloaded from a TB-scale data warehouse. They cross fingers and hope that the program works in the expensive production cloud. When a job fails or they get a suspicious result, data scientists spend hours guessing at the source of the error, digging through post-mortem logs. In such cases, the data scientists may want to pinpoint the root cause of errors by investigating a subset of corresponding input records. The vision of my work is to provide interactive, real-time and automated debugging services for big data processing programs in modern DISC systems with minimum performance impact. My work investigates the following research questions in the context of big data analytics: (1) What are the necessary debugging primitives for interactive big data processing? (2) What scalable fault localization algorithms are needed to help the user to localize and characterize the root causes of errors? (3) How can we improve testing efficiency during iterative development of DISC applications by reasoning the semantics of dataflow operators and user-defined functions used inside dataflow operators in tandem? To answer these questions, we synthesize and innovate ideas from software engineering, big data systems, and program analysis, and coordinate innovations across the software stack from the user-facing API all the way down to the systems infrastructure.","2574-1934","978-1-4503-5663-3","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8449640","Debugging;Testing;Fault Localization;Big Data Systems;Data Intensive Scalable Computing Systems;Data Provenance;Data Cleaning","Debugging;Big Data;Testing;Cloud computing;Sparks;Semantics;Computer crashes","application program interfaces;Big Data;cloud computing;data analysis;data structures;data warehouses;program debugging;program verification;software engineering;system monitoring","cloud computing environment;DISC applications;application development;data processing pipeline;TB-scale data warehouse;data scientists;automated debugging services;big data processing programs;Big Data analytics;data-intensive scalable computing;Big Data applications;interactive Big Data processing;software stack;dataflow operators;user-defined functions;fault localization algorithms","","","","","","30 Aug 2018","","","IEEE","IEEE Conferences"
"Discovering cross-organizational business rules from the cloud","M. L. Bernardi; M. Cimitile; F. M. Maggi","University of Sannio, Benevento, Italy; Unitelma Sapienza, Italy; University of Tartu, Estonia","2014 IEEE Symposium on Computational Intelligence and Data Mining (CIDM)","15 Jan 2015","2014","","","389","396","Cloud computing is rapidly emerging as a new information technology that aims at providing improved efficiency in the private and public sectors, as well as promoting growth, competition, and business dynamism. Cloud computing represents, today, an opportunity also from the perspective of business process analytics since data recorded by process-centered cloud systems can be used to extract information about the underlying processes. Cloud computing architectures can be used in cross-organizational environments in which different organizations execute the same process in different variants and share information about how each variant is executed. If the process is characterized by low predictability and high variability, business rules become the best way to represent the process variants. The contribution of this paper consists in providing: (i) a cloud computing multi-tenancy architecture to support cross-organizational process executions; (ii) an approach for the systematic extraction/composition of distributed data into coherent event logs carrying process-related information of each variant; (iii) the integration of online process mining techniques for the runtime extraction of business rules from event logs representing the process variants running on the infrastructure. The proposed architecture has been implemented and applied for the execution of a real-life process for acknowledging an unborn child performed in four different Dutch municipalities.","","978-1-4799-4518-4","10.1109/CIDM.2014.7008694","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7008694","","Cloud computing;Computer architecture;Software as a service;Organizations;Monitoring","business data processing;cloud computing;data mining","cross-organizational business rules;process-centered cloud systems;business rules;cloud computing multitenancy architecture;cross-organizational process executions;systematic extraction;online process mining techniques;runtime extraction;event logs","","3","","22","","15 Jan 2015","","","IEEE","IEEE Conferences"
"Semantic Process Mining Towards Discovery and Enhancement of Learning Model Analysis","K. Okoye; A. R. H. Tawil; U. Naeem; E. Lamine","Sch. of Archit. Comput. & Eng., Univ. of East London, London, UK; Sch. of Archit. Comput. & Eng., Univ. of East London, London, UK; Sch. of Archit. Comput. & Eng., Univ. of East London, London, UK; Mines-Albi, Univ. de Toulouse, Albi, France","2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems","30 Nov 2015","2015","","","363","370","Process mining algorithms use event logs to learn and reason about processes by technically coupling event history data and process models. During the execution of a learning process, several events occur which are of interest and/or necessary for completing and achieving a learning goal. The work in this paper describes a Semantic Process Mining approach directed towards automated learning. The proposed approach involves the extraction of process history data from learning execution environments, which is then followed by submitting the resulting eXtensible Event Streams (XES) and Mining eXtensible Markup Language (MXML) format to the process analytics environment for mining and further analysis. The XES and MXML data logs are enriched by using Semantic Annotations that references concepts in an Ontology specifically designed for representing learning processes. This involves the identification and modelling of data about different users. The approach focuses on augmenting information values of the resulting model based on individual learner profiles. A series of validation experiments were conducted in order to prove how Semantic Process Mining can be utilized to address the problem of analyzing concepts and relationships amongst learning objects, which also aid in discovering new and enhancement of existing learning processes. To this end, we demonstrate how data from learning processes can be extracted, semantically prepared, and transformed into mining executable formats for improved analysis.","","978-1-4799-8937-9","10.1109/HPCC-CSS-ICESS.2015.164","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7336189","process model;learning process;semantic annotation;ontology;process mining;event logs","Data mining;Semantics;Cognition;Ontologies;Data models;Context;History","data mining;learning (artificial intelligence);ontologies (artificial intelligence);XML","semantic process mining algorithm;learning model analysis;automated learning;resulting extensible event streams;XES;mining extensible markup language;MXML;semantic annotations;ontology","","1","","22","","30 Nov 2015","","","IEEE","IEEE Conferences"
"Exception beyond Exception: Crashing Android System by Trapping in ""Uncaught Exception""","J. Wu; S. Liu; S. Ji; M. Yang; T. Luo; Y. Wu; Y. Wang",NA; NA; NA; NA; NA; NA; NA,"2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track (ICSE-SEIP)","3 Jul 2017","2017","","","283","292","Android is characterized as a complicated open source software stack created for a wide array of devices with different form of factors, whose latest release has over one hundred million lines of code. Such code is mainly developed with the Java language, which builds complicated logic and brings implicit information flows among components and the inner framework. By studying the source code of system service interfaces, we discovered an unknown type of code flaw, which is named uncaughtException flaw, caused by un-well implemented exceptions that could crash the system and be further vulnerable to system level Denial-of-Service (DoS) attacks. We found that exceptions are used to handle the errors and other exceptional events but sometimes they would kill some critical system services exceptionally. We designed and implemented ExHunter, a new tool for automatic detection of this uncaughtException flaw by dynamically reflecting service interfaces, continuously fuzzing parameters and verifying the running logs. On 11 new popular Android devices, ExHunter extracted 1045 system services, reflected 758 suspicious functions, discovered 132 uncaughtException flaws which are 0-day vulnerabilities that have never been known before and generated 275 system DoS attack exploitations. The results showed that: (1) almost every type of Android phone suffers from this flaw, (2) the flaws are different from phone by phone, and (3) all the vulnerabilities can be exploited by direct/indirect trapping. To mitigate uncaughtException flaws, we further developed ExCatcher to re-catch the exceptions. Finally, we informed four internationally renowned manufacturers and provided secure improvements in their commercial phones.","","978-1-5386-2717-4","10.1109/ICSE-SEIP.2017.12","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7965452","Exception;Android System Service;Vulnerability;DoS Attack","Smart phones;Computer crashes;Computer crime;Message systems;Software;Java","mobile computing;public domain software;security of data","Android system crashing;open source software stack;source code;uncaughtException flaw;denial-of-service attacks;ExHunter;DoS attack;ExCatcher","","3","","21","","3 Jul 2017","","","IEEE","IEEE Conferences"
"Unique features of electronic device testing using NI-technologies","V. Butin; A. Butina; F. Chubrukov","All-Russia Research Institute of Automatics, FSUE VNIIA, Moscow, Russia; All-Russia Research Institute of Automatics, FSUE VNIIA, Moscow, Russia; All-Russia Research Institute of Automatics, FSUE VNIIA, Moscow, Russia","2015 International Siberian Conference on Control and Communications (SIBCON)","2 Jul 2015","2015","","","1","4","The paper describes methodological problems of functional check and parametric testing of electronic devices by using modular NI PXI-devices in various environmental conditions.The operation algorithm of developed automated test system can analyze the time measurement of various parameters; it synchronizes time between measurements, creates experimental data back-ups and minimizes measurement time. The log-file allows detecting the sequence of errors during the functional check of electronic device. The interface module controls the data exchange by multiplex channel and it is integrated in automated test system. The application-dependent software provides the remote parameters measurement and research control. The control program realizes the switching of the device power supply to external power source in case of the control parameters exceeding the limits. The developed test system has been approbated at modeling and simulation facilities of ROSATOM companies such as FSUE VNIIA (Moscow, Russia), FSUE RISI (Lytkarino, Russia) and SSC RF - IPPE (Obninsk, Russia).","","978-1-4799-7103-9","10.1109/SIBCON.2015.7147314","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7147314","PXI;testing algorithm;electronic device;multiplex channel","Testing;Power supplies;Couplings;Nickel;Software;Microcontrollers;Resistance","automatic testing;electron device testing;error detection","national instruments PXI;SSC RF-IPPE;FSUE RISI;FSUE VNIIA;ROSATOM company;external power source;device power supply;control program;research control;remote parameter measurement;application-dependent software;multiplex channel;interface module;functional check;error sequence detection;log-file;experimental data back-up;time measurement;automated test system;modular NI PXI-device;NI-technology;electronic device testing","","","","8","","2 Jul 2015","","","IEEE","IEEE Conferences"
"Work in progress: Management of online assessments as a replacement for exams","A. Maxwell","Faculty of Engineering and Surveying, University of Southern Queensland, Toowoomba, Australia","2012 Frontiers in Education Conference Proceedings","18 Feb 2013","2012","","","1","2","Whilst online Learning Management Systems (LMS) environments are rapidly becoming commonplace and a justified means for supporting learning and teaching, courses are often unable to be entirely taught online due to the necessity for traditional paper-based examinations, usually of significant percentage weighting of the total course grade. In this paper, the author describes the preliminary development of procedures and workflows for managing online assessments as a replacement for these traditional examination conditions, seeking to reduce examination stress and increase assessment validity. An indevelopment software suite, based on a LMS log-file pre-filter mechanism and behavior identification heuristic engine, is introduced to assist with identifying online compliance and collusion.","2377-634X","978-1-4673-1352-0","10.1109/FIE.2012.6462488","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6462488","online assessment;examinations;examiniation stress;instruction compliance","Least squares approximation;Stress;Software;Australia;Abstracts;Measurement;Monitoring","courseware;educational administrative data processing;educational courses;teaching","online assessment management;online learning management systems;teaching;paper-based examination;percentage weighting;examination stress reduction;in-development software suite;LMS log-file prefilter mechanism;behavior identification heuristic engine;online compliance identification;collusion identification","","","","6","","18 Feb 2013","","","IEEE","IEEE Conferences"
"CS-ACELP Speech Coding Simulink Modeling, Verification, and Optimized DSP Implementation on DSK 6713","H. Ahmed Elsayed; A. M. Abotaleb; E. M. Mahmoud; A. Zekry",Ain Shams University; Cairo University; Modern Academy of Engineering and Technology; Ain Shams University,"2019 International Conference on Promising Electronic Technologies (ICPET)","9 Dec 2019","2019","","","80","85","The emerging of real-time telecommunication systems over low bandwidth channels enforces constraints on the transmitted data rate and it also needs an optimum speech quality at received destination, so many standards of hybrid speech coding techniques are being developed. Conjugate Structure Algebraic Code Excited Linear Prediction (CS-ACELP) hybrid speech coding was being utilized and optimized in the current work, Firstly, a CS-ACELP frame analysis with MATLAB SIMULINK model was implemented and its corresponding DSP based C++ code was generated too. Then additional optimizations were done, then, a CS-ACELP encoder/decoder with International telecommunication union (ITU) MATLAB CS-ACELP code was developed, and then MEX tool was used to generate C++ files that could compile and run it into desktop PCs. This desktop application is helpful in chat applications that require low bandwidth over the Internet and low connection speed. The performance of both DSP and Desktop implementations are evaluated using different quality evaluation tests. These tests are segmented signal-to-noise ratio (seg-SNR), log-likelihood ratio (LLR), mean opinion score (MOS) and praat software, The results showed that the optimizations done in the MATLAB Simulink work decreased the processing time by 8.564 us and reduced the used memory by 8% with acceptable speech quality.","","978-1-7281-2337-0","10.1109/ICPET.2019.00022","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8925319","CSACELP, Simulink, MATLAB, Visual Studio, DSK 6713, segSNR, LLR, MOS, PRAAT","Speech coding;Software packages;Matlab;Mathematical model;C++ languages;Microsoft Windows;Optimization","algebraic codes;digital signal processing chips;linear predictive coding;Matlab;speech coding","segmented signal-to-noise ratio;log-likelihood ratio;mean opinion score;praat software;chat applications;desktop application;C++ files;MEX tool;acceptable speech quality;MATLAB Simulink work;low connection speed;International telecommunication union MATLAB CS-ACELP code;MATLAB SIMULINK model;CS-ACELP frame analysis;Conjugate Structure Algebraic Code Excited Linear Prediction hybrid speech coding;optimum speech quality;transmitted data rate;low bandwidth channels;real-time telecommunication systems;optimized DSP implementation","","","","21","","9 Dec 2019","","","IEEE","IEEE Conferences"
"Middleware Adaptation through Process Mining","N. Rosa","Centro de Inf., Univ. Fed. de Pernambuco, Recife, Brazil","2017 IEEE 31st International Conference on Advanced Information Networking and Applications (AINA)","8 May 2017","2017","","","244","251","The development of adaptive middleware systems is a complex task due to the difficulty of dealing with adaptation issues, such as how to implement the adaptation mechanism, where to insert the adaptive code into the middleware, and when the adaptive code is composed with the middleware logic. Existing solutions to build adaptive middleware usually concentrate on the use of software technologies like aspect oriented programming and computational reflection to face with the how issue. In this paper, we propose a solution to build middleware that is adapted at runtime (when), whose adaptation decisions and actions are moved from the middleware to an external component (where) and whose adaptation makes use of process mining techniques and software architecture (how). The adaptation process is triggered based on the verification of the middleware event log. In order to evaluate the proposed approach, we carried an experimental evaluation to check the quality of the mined middleware model and the verification overhead.","1550-445X","978-1-5090-6029-0","10.1109/AINA.2017.25","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7920915","adaptive middleware;process mining;software architecture","Middleware;Data mining;Adaptation models;Connectors;Ports (Computers)","data mining;formal verification;middleware;software architecture","adaptive middleware systems;adaptive code;middleware logic;software technologies;aspect oriented programming;computational reflection;external component;process mining;software architecture;middleware event log verification;verification overhead","","","","21","","8 May 2017","","","IEEE","IEEE Conferences"
"Signal-to-noise ratio measurements and statistical characterization in Gen2 RFID","Z. Blažević; P. Šolić; M. Škiljo; M. Stella; Č. Stefanović; P. Popovski; G. F. Pedersen","University of Split, Faculty of Electrical Engineering, Mechanical Engineering and Naval Architecture, Split, Croatia; University of Split, Faculty of Electrical Engineering, Mechanical Engineering and Naval Architecture, Split, Croatia; University of Split, Faculty of Electrical Engineering, Mechanical Engineering and Naval Architecture, Split, Croatia; University of Split, Faculty of Electrical Engineering, Mechanical Engineering and Naval Architecture, Split, Croatia; Department of Electronic Systems, Aalborg University, Aalborg, Denmark; Department of Electronic Systems, Aalborg University, Aalborg, Denmark; Department of Electronic Systems, Aalborg University, Aalborg, Denmark","2017 2nd International Multidisciplinary Conference on Computer and Energy Science (SpliTech)","31 Aug 2017","2017","","","1","4","In this paper, we present measurements of the optimum signal-to-noise ratio distribution recorded by RFID reader based on software radio. The measurements are conducted in an indoor environment while maintaining an RFID setup that does not deteriorate the tag responsiveness. The analysis shows that, although non-linear in nature, the RFID system signal-to-noise can be characterized through the scale parameter of lognormal distribution, independent of the transmitting power and propagation environment. This approach can be exploited, for example, to analyze the capture effect in real environments, as well as an input to a localization technique.","","978-953-290-071-2","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8019281","radio-frequency identification;optimum signal-to-noise ratio;capturing effect","","indoor radio;log normal distribution;radiofrequency identification;software radio","signal-to-noise ratio measurements;statistical characterization;radiofrequency identification;Gen2 RFID;optimum signal-to-noise ratio distribution;RFID reader;software radio;indoor environment;tag responsiveness;lognormal distribution","","1","","13","","31 Aug 2017","","","IEEE","IEEE Conferences"
"FloTracker: Log-Free and Instantaneous Host-Based Intrusion Root-Cause Analysis","S. Zonouz; A. Seyfi; A. Mesa; G. Salles-Loustau","Electr. & Comput. Eng, Univ. of Miami, Miami, FL, USA; Electr. & Comput. Eng, Univ. of Miami, Miami, FL, USA; Electr. & Comput. Eng, Univ. of Miami, Miami, FL, USA; Electr. & Comput. Eng, Univ. of Miami, Miami, FL, USA","2013 IEEE 19th Pacific Rim International Symposium on Dependable Computing","26 May 2014","2013","","","246","255","Preserving the availability and integrity of security-critical computer systems in a fast-spreading sophisticated intrusions environment, requires advance algorithms, accurate and efficient intrusion diagnosis, along side with root-cause analysis techniques. In this paper we introduce FloTracker that is an online log-free host-based root-cause analysis detection engine, with instantaneous forensics capabilities. FloTracker presents security administrators as well as automated response systems, with immediate forensics information. For instance, it will identify a system's entry point of intrusion as soon as a critical security incident occurs, e.g., a sensitive system file modification is detected within the target system. To this end, FloTracker automatically defines an access control policy set (possibly with no access restriction) for the target system that facilitates real-time backtracking of an intrusion, given a detection point. Our experimental results on a real-world SE-Linux test-bed showed that the FloTracker could efficiently update the system's configuration thus modifications will not affect the functionalities of the system, yet providing a log-free and instantaneous root-cause analysis capability.","","978-0-7695-5130-2","10.1109/PRDC.2013.46","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6820872","","Access control;Intrusion detection;Algorithm design and analysis;Generators;Databases;Software","authorisation;safety-critical software","access control policy set;security incident;forensics information;automated response systems;security administrators;instantaneous forensics capabilities;root-cause analysis detection engine;root-cause analysis techniques;intrusion diagnosis;system integrity;system availability;security-critical computer systems;log-free instantaneous host-based intrusion root-cause analysis;FloTracker","","","","19","","26 May 2014","","","IEEE","IEEE Conferences"
"An Open-System Transportation Security Sensor Network: Field-Trial Experiences","D. T. Fokum; V. S. Frost; M. Kuehnhausen; D. DePardo; A. N. Oguna; L. S. Searl; E. Komp; M. Zeets; D. D. Deavours; J. B. Evans; G. J. Minden","Information and Telecommunication Technology Center, The University of Kansas (KU), Lawrence, KS, USA; Information and Telecommunication Technology Center, The University of Kansas (KU), Lawrence, KS, USA; Information and Telecommunication Technology Center, The University of Kansas (KU) , Lawrence, KS, USA; Information and Telecommunication Technology Center, The University of Kansas (KU), Lawrence, KS, USA; Information and Telecommunication Technology Center, The University of Kansas (KU), Lawrence, KS, USA; Information and Telecommunication Technology Center, The University of Kansas (KU), Lawrence, KS, USA; Information and Telecommunication Technology Center, The University of Kansas (KU), Lawrence, KS, USA; Information and Telecommunication Technology Center, The University of Kansas (KU), Lawrence, KS, USA; Information and Telecommunication Technology Center, The University of Kansas (KU), Lawrence, KS, USA; Information and Telecommunication Technology Center, The University of Kansas (KU), Lawrence, KS, USA; Information and Telecommunication Technology Center, The University of Kansas (KU), Lawrence, KS, USA","IEEE Transactions on Vehicular Technology","14 Oct 2010","2010","59","8","3942","3955","Cargo shipments are subject to hijack, theft, or tampering. Furthermore, cargo shipments are at risk of being used to transport contraband, potentially resulting in fines to shippers. The Transportation Security Sensor Network (TSSN), which is based on open software systems and service-oriented architecture principles, has been developed to mitigate these risks. Using commercial off-the-shelf hardware, the TSSN can detect and report events that are relevant to appropriate decision makers. However, field testing is required to validate the system architecture and to determine if the system can provide timely event notification. Field experiments were conducted to assess the TSSN's suitability to monitor rail-borne cargo. Log files were collected from these experiments and were postprocessed. We present the TSSN architecture and results of field experiments, including the time taken to report events using the TSSN and the interaction between various components of the TSSN. These results show that the TSSN architecture can be used to monitor rail-borne cargo.","1939-9359","","10.1109/TVT.2010.2060504","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5518450","Cargo security;mobile rail network (MRN);service-oriented architecture (SOA);trade data exchange (TDE);transportation security;virtual network operations center (VNOC)","Open systems;Transportation;Sensor systems;Service oriented architecture;Monitoring;Software systems;Hardware;Event detection;System testing;Computer architecture","freight handling;goods distribution;security;transportation;wireless sensor networks","open-system transportation security sensor network;cargo shipment;TSSN;open software system;service-oriented architecture;commercial off-the-shelf hardware;rail-borne cargo","","5","","33","","23 Jul 2010","","","IEEE","IEEE Journals"
"SCMiner: Localizing System-Level Concurrency Faults from Large System Call Traces","T. S. Zaman; X. Han; T. Yu",University of Kentucky; University of Kentucky; University of Kentucky,"2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)","9 Jan 2020","2019","","","515","526","Localizing concurrency faults that occur in production is hard because, (1) detailed field data, such as user input, file content and interleaving schedule, may not be available to developers to reproduce the failure; (2) it is often impractical to assume the availability of multiple failing executions to localize the faults using existing techniques; (3) it is challenging to search for buggy locations in an application given limited runtime data; and, (4) concurrency failures at the system level often involve multiple processes or event handlers (e.g., software signals), which can not be handled by existing tools for diagnosing intra-process(thread-level) failures. To address these problems, we present SCMiner, a practical online bug diagnosis tool to help developers understand how a system-level concurrency fault happens based on the logs collected by the default system audit tools. SCMiner achieves online bug diagnosis to obviate the need for offline bug reproduction. SCMiner does not require code instrumentation on the production system or rely on the assumption of the availability of multiple failing executions. Specifically, after the system call traces are collected, SCMiner uses data mining and statistical anomaly detection techniques to identify the failure-inducing system call sequences. It then maps each abnormal sequence to specific application functions. We have conducted an empirical study on 19 real-world benchmarks. The results show that SCMiner is both effective and efficient at localizing system-level concurrency faults.","2643-1572","978-1-7281-2508-4","10.1109/ASE.2019.00055","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8952396","Multi Process Applications, Concurrency Failures, Fault Localization","Concurrent computing;Production;Principal component analysis;Computer bugs;Tools;Debugging;Instruments","concurrency (computers);data mining;fault diagnosis;program debugging;program diagnostics;program testing;scheduling;security of data;statistical analysis","file content;interleaving schedule;multiple failing executions;system level;multiple processes;event handlers;SCMiner;practical online bug diagnosis tool;system-level concurrency fault;default system audit tools;production system;system call traces;data mining;statistical anomaly detection techniques;failure-inducing system call sequences;localizing system-level concurrency faults","","","","56","","9 Jan 2020","","","IEEE","IEEE Conferences"
"Formal Verification of Temporal Constraints for Mobile Service-Based Business Process Models","D. Zhao; W. Gaaloul; W. Zhang; C. Zhu; Z. Zhou","School of Information Engineering, China University of Geosciences, Beijing, China; Computer Science Department, TELECOM SudParis, Évry, France; Technology Center of Software Engineering, Institute of Software, Chinese Academy of Sciences, Beijing, China; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada; School of Information Engineering, China University of Geosciences, Beijing, China","IEEE Access","6 Nov 2018","2018","6","","59843","59852","Service-based business processes are typically used for achieving business goals through the execution of a set of activities. These activities are usually implemented upon mobile devices in terms of mobile services in a dynamic and pervasive environment. The execution of business processes is recorded in event logs in contemporary enterprise information systems and is discovered, monitored, and improved leveraging process mining techniques. However, process mining from time perspective which has been evolving as an emerging principle for supporting the temporal analysis of these mobile service-based business processes has not been explored extensively. To address this challenge, we propose to extend the mobile service-based business process model on the basis of the specification and verification of temporal constraints. A timed business process model is proposed where the relative and absolute temporal constraints correspond to each individual activity, sets of activities with control relations, and edges between activities or gateways are estimated. Conformance checking is implemented for the verification and improvement of the proposed model with respect to the accuracy, precision, and recall. Extensive evaluations and comparison of our technique with the state of the art are conducted based on publicly available real-life event logs, and the results demonstrate the effectiveness and verification of the proposed model.","2169-3536","","10.1109/ACCESS.2018.2874937","National Natural Science Foundation of China; China University of Geosciences, Beijing; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8488601","Mobile service-based business process model;temporal constraint;conformance checking","Business;Logic gates;Collaboration;Computational modeling;Mobile handsets;Information systems;Monitoring","business data processing;data mining;formal verification;information systems","relative constraints;timed business process model;mobile service-based business processes;process mining;leveraging process;mobile services;business goals;mobile service-based business process model;absolute temporal constraints","","1","","31","","10 Oct 2018","","","IEEE","IEEE Journals"
"An extensible Hadoop framework for monitoring performance metrics and events of OpenStack cloud","J. Yang; C. Shen; Y. Chi; P. Xu; W. Sun","Department of communication engineering, Beijing Electronic Science and Technology Institute & Key Laboratory of Network Assessment Technology, Institute of Information Engineering, CAS Beijing, China; Department of communication engineering, Beijing Electronic Science and Technology Institute, Beijing, China; Department of communication engineering, Beijing Electronic Science and Technology Institute, Beijing, China; Department of communication engineering, Beijing Electronic Science and Technology Institute, Beijing, China; Department of communication engineering, Beijing Electronic Science and Technology Institute, Beijing, China","2018 IEEE 3rd International Conference on Big Data Analysis (ICBDA)","28 May 2018","2018","","","222","226","In recent years, many hot topics for research are manipulation of big data, cloud computing and a combination of them. OpenStack is a complex and evolving system that continuously generates vast amounts of metrics and log data. The more complex your system gets, the harder it is to understand its performance and troubleshoot problems, making monitoring a critical piece of the OpenStack control system. In order to effectively create, test and deploy new algorithms or frameworks one need suitable monitoring solutions. However, the conventional monitoring tools lack some performances to keep data freshness from a variety of sources in OpenStack. In the paper we design a critical monitoring solutions based on existing Hadoop architecture integrated with OpenStack clouds. Hundreds of different metrics are gathered form Hadoop metrics subsystem and all data is stored in HBase database to make it ready for processing or displaying it with MapReduce paradigm. With alarms/events generated, we also present a comprehensive view of the status of the infrastructure resources as well as the network services running on OpenStack.","","978-1-5386-4794-3","10.1109/ICBDA.2018.8367681","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8367681","OpenStack;monitoring;Hadoop;distributed","Monitoring;Cloud computing;Telecommunication traffic;Virtual machining;Computer architecture;Real-time systems;Distributed databases","Big Data;cloud computing;data analysis;data handling;parallel processing;software metrics;software performance evaluation","data freshness;OpenStack cloud;extensible Hadoop framework;cloud computing;OpenStack control system;Hadoop architecture;Big Data;performance metrics monitoring;Hadoop metrics subsystem;HBase database;MapReduce paradigm","","1","","9","","28 May 2018","","","IEEE","IEEE Conferences"
"A MapReduce Approach for Processing Student Data Activity in a Peer-to-Peer Networked Setting","J. Miguel; S. Caballé; F. Xhafa","Dept. of Comput. Sci., Open Univ. of Catalonia Barcelona, Barcelona, Spain; Dept. of Comput. Sci., Open Univ. of Catalonia Barcelona, Barcelona, Spain; Dept. of Comput. Sci., Open Univ. of Catalonia Barcelona, Barcelona, Spain","2015 10th International Conference on P2P, Parallel, Grid, Cloud and Internet Computing (3PGCIC)","3 Mar 2016","2015","","","9","16","Collaborative and peer-to-peer networked based models generate a large amount of data from students' learning tasks. We have proposed the analysis of these data to tackle information security in e-Learning breaches with trustworthiness models as a functional requirement. In this context, the computational complexity of extracting and structuring students' activity data is a computationally costly process as the amount of data tends to be very large and needs computational power beyond of a single processor. For this reason, in this paper, we propose a complete MapReduce and Hadoop application for processing learning management systems log file data.","","978-1-4673-9473-4","10.1109/3PGCIC.2015.27","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7424535","Hadoop;MapReduce;log files;parallel processing;student activity data","Parallel processing;Computational modeling;Peer-to-peer computing;Programming;Data models;Software;Computer architecture","data handling;learning management systems;parallel programming;trusted computing","MapReduce approach;student data activity;peer-to-peer networked based models;e-learning breaches;information security;computational complexity;Hadoop application;learning management systems log file data;trustworthiness models","","1","","37","","3 Mar 2016","","","IEEE","IEEE Conferences"
"VoIP Intrusion Detection System with Snort","P. Číž; O. Lábaj; P. Podhradský; J. Londák","Faculty of Electrical Engineering and Information Technology of the Slovak University of Technology in Bratislava, Bratislava, Slovakia; Faculty of Electrical Engineering and Information Technology of the Slovak University of Technology in Bratislava, Bratislava, Slovakia; Faculty of Electrical Engineering and Information Technology of the Slovak University of Technology in Bratislava, Bratislava, Slovakia; Faculty of Electrical Engineering and Information Technology of the Slovak University of Technology in Bratislava, Bratislava, Slovakia","Proceedings ELMAR-2012","25 Oct 2012","2012","","","137","140","In this paper we introduce some attack types, which can be led against VoIP traffic and we present protection forms against them. We have performed an experiment on the proposed protection model, which was focused on signaling DoS attack with aim to cause malfunction of the software exchange Asterisk. For attack we have used software tool SIPp as messages generator to flood the exchange with a huge amount of INVITE messages. Software tool Snort, used as IDS system, logged an alert in case of the running attack to notify administrator of malicious activity. Subsequently the administrator has to analyze logged event.","1334-2630","978-953-7044-14-5","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6338490","VoIP;IDS;DoS attack;Asterisk;Snort;SIPp","Computer crime;Software;Protocols;Context;IP networks;Servers","computer network security;Internet telephony;software tools;telecommunication traffic","VoIP intrusion detection system;VoIP traffic;protection model;signaling DoS attack;Asterisk software exchange;SIPp software tool;Snort software tool;IDS system","","","","16","","25 Oct 2012","","","IEEE","IEEE Conferences"
"Exploring Tools and Strategies Used During Regular Expression Composition Tasks","G. R. Bai; B. Clee; N. Shrestha; C. Chapman; C. Wright; K. T. Stolee",North Carolina State University; North Carolina State University; North Carolina State University; Iowa State University; Iowa State University; North Carolina State University,"2019 IEEE/ACM 27th International Conference on Program Comprehension (ICPC)","29 Aug 2019","2019","","","197","208","Regular expressions are frequently found in programming projects. Studies have found that developers can accurately determine whether a string matches a regular expression. However, we still do not know the challenges associated with composing regular expressions. We conduct an exploratory case study to reveal the tools and strategies developers use during regular expression composition. In this study, 29 students are tasked with composing regular expressions that pass unit tests illustrating the intended behavior. The tasks are in Java and the Eclipse IDE was set up with JUnit tests. Participants had one hour to work and could use any Eclipse tools, web search, or web-based tools they desired. Screen-capture software recorded all interactions with browsers and the IDE. We analyzed the videos quantitatively by transcribing logs and extracting personas. Our results show that participants were 30% successful (28 of 94 attempts) at achieving a 100% pass rate on the unit tests. When participants used tools frequently, as in the case of the novice tester and the knowledgeable tester personas, or when they guess at a solution prior to searching, they are more likely to pass all the unit tests. We also found that compile errors often arise when participants searched for a result and copy/pasted the regular expression from another language into their Java files. These results point to future research into making regular expression composition easier for programmers, such as integrating visualization into the IDE to reduce context switching or providing language migration support when reusing regular expressions written in another language to reduce compile errors.","2643-7171","978-1-7281-1519-1","10.1109/ICPC.2019.00039","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8813286","Exploratory study;regular expressions;problem solving strategies;personas","","computer aided instruction;formal specification;Internet;Java;program compilers;program testing;software tools;string matching","regular expression composition tasks;unit tests;programming projects;Java;Eclipse IDE;JUnit tests;Eclipse tools;Web search;Web-based tools;screen-capture software;browsers;novice tester;knowledgeable tester personas;compile errors;Java files","","1","","56","","29 Aug 2019","","","IEEE","IEEE Conferences"
"Blockchain as an Audit-Able Communication Channel","S. Suzuki; J. Murai","Grad. Sch. of Media & Governance, Keio Univ., Yokohama, Japan; Fac. of Environ. & Inf. Studies, Keio Univ., Yokohama, Japan","2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC)","11 Sep 2017","2017","2","","516","522","Applications requiring strict access control, such as medical record query, often require auditing of the query. The current typical design relies on server side logging. However, logging on server-side do not provide strict means of auditing, since the server can be tampered with attackers, and also anybody who has permission to write can modify the log. We propose a scheme using blockchain technology, as a request-response channel for a client-server system, to record both client request and server reply in an audi-table manner. We have implemented a proof-of-concept system on top of a publicly available blockchain testbed. By using a blockchain as a client-server request-response channel, the request-response sequence can be verified by anybody who has access to the blockchain, providing a way to implement audit log for strictly controlled resources.","0730-3157","978-1-5386-0367-3","10.1109/COMPSAC.2017.72","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8029983","blockchain;secure logging;audit","Servers;Peer-to-peer computing;Receivers;Bitcoin;Resistance","auditing;authorisation;client-server systems;query processing","strict access control;server side logging;auditing;blockchain technology;client-server system;client request;server reply;audi-table manner;proof-of-concept system;publicly available blockchain testbed;client-server request-response channel;request-response sequence;audit log","","5","","30","","11 Sep 2017","","","IEEE","IEEE Conferences"
"A Lifelog System Specialized to Record Timestamp of Daily Works","N. Sato","Fac. of Software & Inf. Sci., Iwate Prefectural Univ., Iwate, Japan","2014 17th International Conference on Network-Based Information Systems","29 Jan 2015","2014","","","75","82","It has been long time passed from when research works that generate life log based on sensor data, information on blogs and SNSs. On the other hand, daily behavior of a particular students and employees are almost the same. Especially, it is better that recordings of which timestamps are important such as records of working time are based on these daily behaviors rather than based on data mining of blog and SNS data. Therefore, in this paper, life log system that is specialized to record timestamps, based on login, logout, and manipulation of equipment on building, such as to open and close of particular doors and locks is proposed. This system mainly focuses such an environment like university, where working time is not strictly recorded. As a prototype of the proposing system, a timestamp recorder is implemented based on only logins and logouts on particular PCs. In this paper, design of the proposing system and implementation of the prototype are described, and evaluation of the prototype will be shown.","2157-0426","978-1-4799-4224-4","10.1109/NBiS.2014.64","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7023937","accurate timetamp oriented lifelog system;lifelog system based on building installations;working-time recorder","Estimation;Smart phones;Prototypes;Educational institutions;Buildings;Accuracy;Contracts","data recording;mobile computing","lifelog system;timestamp recorder;daily works;blogs;SNS","","","","10","","29 Jan 2015","","","IEEE","IEEE Conferences"
"LLR-Based Sentiment Analysis for Kernel Event Sequences","R. Luh; S. Schrittwieser; S. Marschalek","Josef Ressel Center TARGET, St. Polten, Austria; Josef Ressel Center TARGET, St. Pulten, Austria; Josef Ressel Center TARGET, St. Pulten, Austria","2017 IEEE 31st International Conference on Advanced Information Networking and Applications (AINA)","8 May 2017","2017","","","764","771","Behavior-based analysis of dynamically executed binaries has become a widely used technique for the identification of suspected malware. Most solutions rely on function call patterns to determine whether a sample is exhibiting malicious behavior. These system and API calls are usually regarded individually and do not consider contextual information or process inter-dependencies. In addition, the patterns are often fixed in nature and do not adapt to changing circumstances on the system environment level. To address these shortcomings, this paper proposes a sentiment extraction and scoring system capable of learning the maliciousness inherent to n-grams of kernel events captured by a real-time monitoring agent. The approach is based on calculating the log likelihood ratio (LLR) of all identified n-grams, effectively determining neighboring sequences as well as assessing whether certain event combinations incline towards the benign or malicious. The extraction component automatically compiles a WordNet-like sentiment dictionary of events, which is subsequently used to score unknown traces of either individual processes, or a session in its entirety. The system was evaluated using a large set of real-world event traces collected on live corporate workstations as well as raw API call traces created in a dedicated malware analysis environment. While applicable to both scenarios, the introduced solution performed best for our abstracted kernel events, generating both new insight into malware- system interaction and assisting with the scoring of hitherto unknown application behavior.","1550-445X","978-1-5090-6029-0","10.1109/AINA.2017.47","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7920985","malware;system behavior;sentiment analysis","Malware;Monitoring;Kernel;Data mining;Dictionaries;Context","application program interfaces;invasive software","LLR;sentiment analysis;kernel event sequences;suspected malware identification;malicious behavior;API;contextual information;process interdependencies;environment level;log likelihood ratio;identified n-grams;extraction component;WordNet-like sentiment dictionary","","1","","30","","8 May 2017","","","IEEE","IEEE Conferences"
"Analyzing Boundary Device Logs on the In-memory Platform","F. Cheng; A. Sapegin; M. Gawron; C. Meinel","Univ. of Potsdam, Potsdam, Germany; Univ. of Potsdam, Potsdam, Germany; Univ. of Potsdam, Potsdam, Germany; Univ. of Potsdam, Potsdam, Germany","2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems","30 Nov 2015","2015","","","1367","1372","The boundary devices, such as routers, firewalls, proxies, and domain controllers, etc., are continuously generating logs showing the behaviors of the internal and external users, the working state of the network as well as the devices themselves. To rapidly and efficiently analyze these logs makes great sense in terms of security and reliability. However, it is a challenging task due to the fact that a huge amount of data might be generated for being analyzed in very short time. In this paper, we address this challenge by applying complex analytics and modern in-memory database technology on the large amount of log data. Logs from different kinds of devices are collected, normalized, and stored in the In-Memory database. Machine learning approaches are then implemented to analyze the centralized big data to identify attacks and anomalies which are not easy to be detected from the individual log event. The proposed method is implemented on the In-Memory platform, i.e., SAP HANA Platform, and the experimental results show that it has the expected capabilities as well as the high performance.","","978-1-4799-8937-9","10.1109/HPCC-CSS-ICESS.2015.284","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7336358","SIEM;Active Directory;Brute Force;Machine Learning;K-Means","Databases;Security;Libraries;Real-time systems;Servers;Data mining;Testing","Big Data;learning (artificial intelligence);storage management","boundary device logs;complex analytics;in-memory database technology;machine learning approaches;centralized Big Data","","","","9","","30 Nov 2015","","","IEEE","IEEE Conferences"
"Business process mining algorithms","D. Chinces; I. Salomie","Technical University of Cluj-Napoca Cluj-Napoca, Romania; Technical University of Cluj-Napoca Cluj-Napoca, Romania","2013 IEEE 9th International Conference on Intelligent Computer Communication and Processing (ICCP)","24 Oct 2013","2013","","","271","277","This paper presents our work in developing three business process mining algorithms, followed by a comparison between GLS Miner, ILS Miner and ACO Miner. All of these algorithms have been proved as generating better solutions compared to the state of the art and can discover process models that correctly map to the event log. The algorithms and a comparison between them is presented in the current paper, as well as the mapping of each algorithm to the common business process structures.","","978-1-4799-1494-4","10.1109/ICCP.2013.6646120","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6646120","Guided Local Search;GLS Miner;Iterative Local Search;ILS Miner;Ant Colony Optimization;ACO BP Miner;business process mining;event log;BPMN","Organizations;Search problems;PROM;Heuristic algorithms;Optimization;Computational modeling","ant colony optimisation;business data processing;data mining;workflow management software","business process mining algorithms;process models;event log;business process structures;ant colony optimization;workflow management systems","","3","","19","","24 Oct 2013","","","IEEE","IEEE Conferences"
"Process Modeling, Behavior Analytics and Group Performance Assessment of e-Learning Logs Via Fuzzy Miner Algorithm","W. Premchaiswadi; P. Porouhan; N. Premchaiswadi","Grad. Sch. of Inf. Technol., Siam Univ., Bangkok, Thailand; Grad. Sch. of Inf. Technol., Siam Univ., Bangkok, Thailand; Coll. of Creative Design & Entertainment Technol., Dhurakij Pundit Univ., Bangkok, Thailand","2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC)","22 Jun 2018","2018","02","","304","309","The major objective of this study is to bring together a variety of topics such as process mining, e-learning, and educational data mining to discuss the opportunities of applying event modeling and process management techniques to e-learning systems. Accordingly, this work aims to identify and analyze behavioral patterns that affect and influence the quality of students' academic performance within/during an undergraduate computer programming course. Process mining Fuzzy Miner techniques enabled us to discover a set of patterns describing how the performance of groups following a particular e-learning program, differs from the performance of another group, as well as their time-related performance differences. The results showed that groups with high grades spent much more time (i.e., 47.4 days in average) to accomplish the e-learning activity, while groups with low grades spent less time to accomplish the same e-learning activity. Moreover, groups with high grades performed more actions creating more events throughout the course, almost double, compared with the groups with low grades. Accordingly, the students with high grades put much more effort by retaking and retrying the course quiz multiple times.","0730-3157","978-1-5386-2667-2","10.1109/COMPSAC.2018.10247","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8377875","Process Mining, e-Learning, Fuzzy Mining, Group Performance Analysis, Group Behavior Analytics, Learning Technologies","Electronic learning;Data mining;Navigation;Tools;Programming profession;Learning management systems","computer aided instruction;data mining;educational courses","process modeling;group performance assessment;educational data mining;event modeling;process management techniques;e-learning systems;behavioral patterns;undergraduate computer programming course;particular e-learning program;time-related performance differences;e-learning activity;fuzzy miner algorithm;e-learning logs;process mining fuzzy miner techniques","","","","19","","22 Jun 2018","","","IEEE","IEEE Conferences"
"Application of a Knowledge Dissemination Model based on Forgetting Mechanism in Interactive Collaborative Learning","W. Zheng; F. Liu; Q. You","School of Software, Nanchang Hangkong University,Nanchang,China; School of Software, Nanchang Hangkong University,Nanchang,China; School of Software, Nanchang Hangkong University,Nanchang,China","2020 IEEE 2nd International Conference on Computer Science and Educational Informatization (CSEI)","17 Jul 2020","2020","","","54","57","Based on the fact that knowledge diffusion needs to be digested and absorbed in real life and that communication between friends is more efficient. This paper proposes a knowledge diffusion model with forgetting mechanism (SEIH model). In this model, the states of individuals in the process of diffusion are summarized into four. In order to study the process of knowledge diffusion in social networks, log events of Bluetooth users on campus were used to construct interactive events. The dynamic equations of propagation in the propagation network are given; further we discussed the diffusion conditions and diffusion efficiency of knowledge in the model. The interaction network between individuals is constructed, and the changes in individual state density under different propagation conditions are simulated. The results indicate that the interaction rate and the forgetting or depreciation of knowledge have the most significant effect on the efficiency of knowledge diffusion. Finally, an empirical analysis of six classes in three courses shows the validity of the conclusions.","","978-1-7281-7008-4","10.1109/CSEI50228.2020.9142537","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9142537","Knowledge diffusion;Social network;Collaborative learning;Forgetting mechanism","Knowledge engineering;Mathematical model;Standards;Software;Bluetooth;Conferences;Collaborative work","Bluetooth;groupware;knowledge management;learning (artificial intelligence);social networking (online)","knowledge diffusion model;interactive collaborative learning;knowledge dissemination model;individual state density;interaction network;diffusion efficiency;diffusion conditions;propagation network;interactive events;SEIH model;forgetting mechanism","","","","20","","17 Jul 2020","","","IEEE","IEEE Conferences"
"ReaderTrack: Reader-Book Interaction Reasoning Using RFID and Smartwatch","Y. Deng; D. Wang; Q. Zhang; R. Zhao; B. Chen","Sch. of Software, Shanghai Jiao Tong Univ., Shanghai, China; Sch. of Software, Shanghai Jiao Tong Univ., Shanghai, China; Sch. of Software, Shanghai Jiao Tong Univ., Shanghai, China; Sch. of Software, Shanghai Jiao Tong Univ., Shanghai, China; Sch. of Software, Shanghai Jiao Tong Univ., Shanghai, China","2018 27th International Conference on Computer Communication and Networks (ICCCN)","11 Oct 2018","2018","","","1","9","Online bookstores are capable of capturing readers preferences by analyzing click logs and transaction records, while physical bookstores or libraries still lack effective methods to gather reader behavioral data. Fortunately, the widespread use of mobile wearable devices and RFID technology opens up new possibilities for uncovering in-store experience. In this paper, we propose ReaderTrack, a system that integrates smartwatch and RFID to excavate interactions between readers and books. We first leverage inertial sensors of smartwatch and backscatter signals of RFID tags to infer reader behaviors and book motions, respectively. Then we associate readers with their corresponding books according to previously inferred behaviors and motions. We implement ReaderTrack with COTS devices and evaluate it extensively in our lab environment which mimics a typical reading room. Experimental results show the effectiveness and robustness of ReaderTrack in reader-book interaction reasoning.","1095-2055","978-1-5386-5156-8","10.1109/ICCCN.2018.8487346","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8487346","","Legged locomotion;Gravity;Acceleration;Radiofrequency identification;Feature extraction;Gesture recognition","electronic publishing;radiofrequency identification;retail data processing;ubiquitous computing;wearable computers","book motions;reader-book interaction reasoning;smartwatch;online bookstores;transaction records;physical bookstores;reader behavioral data;mobile wearable devices;RFID technology;backscatter signals;RFID tags;click logs analysis;readertrack;COTS devices","","1","","33","","11 Oct 2018","","","IEEE","IEEE Conferences"
"Research on Incremental Heterogeneous Database Synchronization Update Based on Web Service","Y. Wang; J. Wen; W. Fang; X. Rao","Sch. of Comput. Sci., Chongqing Univ., Chongqing, China; Sch. of Comput. Sci., Chongqing Univ., Chongqing, China; Sch. of Software Eng., Chongqing Univ., Chongqing, China; Sch. of Software Eng., Chongqing Univ., Chongqing, China","2015 International Conference on Computational Intelligence and Communication Networks (CICN)","18 Aug 2016","2015","","","1415","1419","This paper analyzes the problem of heterogeneous databases exist changes in data capture and data synchronization in enterprise data integration, a variety of ways to change data capture for comparison, A method combining triggers with log tables for change data capture is put forward based on the researches above, and an incremental synchronization mechanism based on web service is designed for distributed heterogeneous databases. The establishment of a central database, it takes use of the event-driven data transportation mechanism to synchronize the incremental data captured by each distributed databases and the center database. This paper then implemented a prototype system of incremental data synchronization for distributed heterogeneous database and finally tested it using an example to verify the feasibility and rationality of the design.","2472-7555","978-1-5090-0076-0","10.1109/CICN.2015.273","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7546331","web service;heterogeneous database;changes capturing;incremental synchronization update;Event-driven","Synchronization;Distributed databases;Simple object access protocol;Data models;Database systems","business data processing;distributed databases;synchronisation;Web services","incremental heterogeneous database synchronization update;Web service;data capture;enterprise data integration;event-driven data transportation;distributed heterogeneous database","","1","","10","","18 Aug 2016","","","IEEE","IEEE Conferences"
"SEALDB: An Efficient LSM-tree Based KV Store on SMR Drives with Sets and Dynamic Bands","T. Yao; Z. Tan; J. Wan; P. Huang; Y. Zhang; C. Xie; X. He","Wuhan National Laboratory for Optoelectronics, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Computer and Information Sciences, Temple University, PA, USA; Wuhan National Laboratory for Optoelectronics, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Computer and Information Sciences, Temple University, PA, USA","IEEE Transactions on Parallel and Distributed Systems","9 Oct 2019","2019","30","11","2595","2607","Key-value (KV) stores play an increasingly critical role in supporting diverse large-scale applications in modern data centers hosting terabytes of KV items which even might reside on a single server due to virtualization purposes. The combination of the ever-growing volume of KV items and storage/application consolidation is driving a trend of high storage density for KV stores. Shingled Magnetic Recording (SMR) represents a promising technology for increasing disk capacity, which however comes with the increased complexity of handling random writes. To take the best advantages of SMR drives, applications are expected to work in an SMR-friendly way. In this work, we present SEALDB, a Log-Structured Merge tree (LSM-tree) based key-value store that is specifically optimized for SMR drives via avoiding random writes and the corresponding write amplification on SMR drives. First, for LSM-trees, SEALDB collects and groups participating data of each compaction into sets. Using a set as the basic unit for compactions, SEALDB improves compaction efficiency by reducing random I/Os. Second, SEALDB creates variable sized bands on original HM-SMR drives, named dynamic bands. Dynamic bands store sets in an SMR-friendly way to eliminate the auxiliary write amplification from SMR drives. Third, SEALDB employs two light-weight garbage collection (GC) policies to further improve the space efficiency. We demonstrate the advantages of SEALDB via extensive experiments with various workloads. Overall, SEALDB delivers impressive performance compared with LevelDB, e.g., 3.42×/2.65× faster for random writes (without or with GCs), and 3.96× faster for sequential reads.","1558-2183","","10.1109/TPDS.2019.2918219","National Natural Science Foundation of China; NSFC; National Natural Science Foundation of China; National Science Foundation; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8721159","LSM-tree;SMR;key-value store;set;dynamic band","Compaction;Drives;Computer architecture;Magnetic recording;Software;Data centers;Databases","computer centres;magnetic recording;merging;storage management;tree data structures","SEALDB;LSM-tree based KV store;dynamic bands;modern data centers;HM-SMR drives;key-value stores;shingled magnetic recording;log-structured merge tree","","1","","48","","23 May 2019","","","IEEE","IEEE Journals"
"Rethinking HBase: Design and Implementation of an Elastic Key-Value Store over Log-Structured Local Volumes","G. Saloustros; K. Magoutis","Inst. of Comput. Sci. (ICS), Found. for Res. & Technol. (FORTH), Heraklion, Greece; Inst. of Comput. Sci. (ICS), Found. for Res. & Technol. (FORTH), Heraklion, Greece","2015 14th International Symposium on Parallel and Distributed Computing","23 Jul 2015","2015","","","225","234","HBase is a prominent NoSQL system used widely in the domain of big data storage and analysis. It is structured as two layers: a lower-level distributed file system (HDFS)supporting the higher-level layer responsible for data distribution, indexing, and elasticity. Layered systems have in many occasions proven to suffer from overheads due to the isolation between layers, HBase is increasingly seen as an instance of this. To overcome this problem we designed, implemented, and evaluated HBase-BDB, an alternative to HBase that replaces the HDFS store with a thinner layer of a log-structured B+ tree key value store (Berkeley DB) operating over local volumes. We show that HBase-BDB overcomes HBase's performance bottlenecks (while retaining compatibility with HBase applications) without losing on elasticity features. We evaluate the performance of HBase and HBase-BDB using the Yahoo! Cloud Serving Benchmark (YCSB) and online transaction processing(OLTP) workloads on a commercial public Cloud provider. We find that HBase-BDB outperforms a tuned HBase configuration by up to 85% under a write-intensive workload due to HBase-BDB's reduced background-write activity. HBase-BDB's novel elasticity mechanisms operating over local volumes are shown to be as perform ant as HBase's equivalent features when stress-tested under TPC-C workloads.","2379-5352","978-1-4673-7148-3","10.1109/ISPDC.2015.33","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7165150","NoSQL;key-value stores;HBase;elasticity","Elasticity;Servers;Computer architecture;Compaction;Distributed databases;Big data","Big Data;cloud computing;data analysis;data mining;database indexing;distributed databases;software performance evaluation;storage management;transaction processing","evaluated HBase-BDB;elastic key-value storage;log-structured local volumes;NoSQL system;big data storage;big data analysis;lower-level distributed file system;HDFS;data distribution;data indexing;data elasticity;performance bottlenecks;Yahoo! Cloud Serving Benchmark;YCSB;online transaction processing workloads;OLTP;commercial public cloud provider;write-intensive workload;background-write activity reduction;TPC-C workloads","","2","","25","","23 Jul 2015","","","IEEE","IEEE Conferences"
"A Framework for Inter-Organizational Performance Analysis from EDI Messages","W. Krathu; C. Pichler; R. Engel; M. Zapletal; H. Werthner; C. Huemer","Inst. of Software Technol. & Interactive Syst., Vienna Univ. of Technol., Vienna, Austria; Inst. of Software Technol. & Interactive Syst., Vienna Univ. of Technol., Vienna, Austria; Inst. of Software Technol. & Interactive Syst., Vienna Univ. of Technol., Vienna, Austria; Inst. of Software Technol. & Interactive Syst., Vienna Univ. of Technol., Vienna, Austria; Inst. of Software Technol. & Interactive Syst., Vienna Univ. of Technol., Vienna, Austria; Inst. of Software Technol. & Interactive Syst., Vienna Univ. of Technol., Vienna, Austria","2014 IEEE 16th Conference on Business Informatics","22 Sep 2014","2014","1","","17","24","The evaluation of an organization's performance may also consider the assessment of inter-organizational relationships (IORs). However, the evaluation of IORs is typically based on success factors, such as trust, which are difficult to be measured quantitatively. In this paper, we present a framework supporting inter-organizational performance evaluation which integrates (i) a bottom-up approach supporting the identification of Key Performance Indicators (KPIs) from business information, event logs, as well as process models, and (ii) a top-down approach for measuring business performance on the strategic level based on the Balanced Scorecard (BSC) method. In order to prove the feasibility of the framework, we present an inter-organizational performance analysis case study of a beverage manufacturing company. The case study shows that the framework enables (i) the derivation of quantifiable KPIs from operational data and (ii) the alignment of KPIs with business objectives allowing an evaluation of IORs on the strategic level.","2378-1971","978-1-4799-5779-8","10.1109/CBI.2014.19","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6904132","","Business;Measurement;Ontologies;Bismuth;Performance analysis;Data mining;Aggregates","beverage industry;business data processing;organisational aspects","inter-organizational performance analysis framework;EDI messages;inter-organizational relationship assessment;IOR assessment;success factors;inter-organizational performance evaluation;bottom-up approach;key performance indicator identification;KPI identification;business information;event logs;process models;top-down approach;strategic level business performance measurement;balanced scorecard method;BSC method;beverage manufacturing company;operational data","","3","","41","","22 Sep 2014","","","IEEE","IEEE Conferences"
"Ziffersystem: A novel malware distribution detection system","T. Chuang; S. Huang; C. Mao; A. B. Jeng; H. Lee","Cyber Trust Technology Institute, Institute for Information Industry, Taipei, Taiwan; Cyber Trust Technology Institute, Institute for Information Industry, Taipei, Taiwan; Cyber Trust Technology Institute, Institute for Information Industry, Taipei, Taiwan; Department of Computer Science and Information Engineering, Jinwen University of Science and Technology, Taipei, Taiwan; Department of Computer Science and Information Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan","2017 IEEE Conference on Dependable and Secure Computing","19 Oct 2017","2017","","","509","515","Cyber-criminals use various malware technologies to bypass antivirus software. For example, drive-by downloads happen without a person's knowledge when visiting a website, viewing an email message, or clicking on a deceptive pop-up window. One way to understand drive-by download attacks is to study the connections between different drive-by download behaviors during the installation phase. However, current solutions need a large number of browsing records from ISPs to build up a model. Insufficient historical browsing data may prevent this approach from working. In this study, we propose Ziffersystem, a system that identifies the suspicious connections in a targeted enterprise. We develop a graph-based model of malicious orchestrated behaviors. Ziffersystem does not need large-scale network data (e.g., IPS traffic) to model malicious activity, and therefore the system is useful for an enterprise with few in-house blacklists and highly sensitive data. We apply the proposed system to the analysis of blacklists from public and private sources, and we show its effectiveness for visualizing malicious download behavior that cannot be identified through piecewise event logs.","","978-1-5090-5569-2","10.1109/DESEC.2017.8073834","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8073834","malicious URL;malware distribution;visualization;drive-by download","Malware;Uniform resource locators;Web pages;Servers;Information industry;IP networks;Browsers","invasive software;Web sites","Ziffersystem;novel malware distribution detection system;cyber-criminals;antivirus software;website;email message;deceptive pop-up window;drive-by download attacks;installation phase;browsing records;suspicious connections;targeted enterprise;malicious orchestrated behaviors;large-scale network data;model malicious activity;malicious download behavior","","1","","29","","19 Oct 2017","","","IEEE","IEEE Conferences"
"A case study of program comprehension effort and technical debt estimations","V. Singh; L. L. Pollock; W. Snipes; N. A. Kraft","University of Delaware, Newark, USA; University of Delaware, Newark, USA; ABB Corporate Research, Raleigh, NC, USA; ABB Corporate Research, Raleigh, NC, USA","2016 IEEE 24th International Conference on Program Comprehension (ICPC)","7 Jul 2016","2016","","","1","9","This paper describes a case study of using developer activity logs as indicators of a program comprehension effort by analyzing temporal sequences of developer actions (e.g., navigation and edit actions). We analyze developer activity data spanning 109,065 events and 69 hours of work on a medium-sized industrial application. We examine potential correlations between different measures of developer activity, code change metrics and code smells to gain insight into questions that could direct future technical debt interest estimation. To gain more insights into the data, we follow our analysis with commit message analysis and a developer interview. Our results indicate that developer activity as an estimate of program comprehension effort is correlated with both change proneness and static metrics for code smells.","","978-1-5090-1428-6","10.1109/ICPC.2016.7503710","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7503710","program comprehension effort;technical debt interest;developer activity logging;code smells","Measurement;Couplings;Navigation;Software;Maintenance engineering;Correlation;History","program diagnostics;software metrics;source code (software)","developer activity logs;program comprehension effort;temporal sequence analysis;developer activity data analysis;medium-sized industrial application;code change metrics;code smells;technical debt interest estimation;commit message analysis;change proneness;static metrics","","3","","24","","7 Jul 2016","","","IEEE","IEEE Conferences"
"Implementation of the recommendation model LIME in cognitive and visual interactive tutors from PSLC","A. Corbi; D. Burgos","Univ. Internacional de La Rioja, Logrono, Spain; Univ. Internacional de La Rioja, Logrono, Spain","IEEE Latin America Transactions","6 Mar 2015","2015","13","2","516","522","This paper presents an application scenario of a rule-based recommender model (LIME) to a PSLC cognitive tutor analytics database. LIME is a recommendation model which provides a formative support to students, tutors and lecturers, thanks to a visual interaction between users and the elearning system. On the other side, cognitive computer tutors, kindly available from the Pittsburg Science Learning Centre (PSLC) and the Carnegie Mellon University (CMU) learning analytics repository, provide intelligent and visual interactive activities that help middle-school students improve their daily academic skills. They also offer on-demand step-by-step feedback during problem solving and report on student progress for teachers and students. Every single learner interaction with a tutor is logged and is freely available within specific databases expressed in the PSLC Logging Format (also known as Tutor Message format). This format is well defined in an XML Schema Definition (XSD file) and a Document Type Definition (DTD file). We begin our text by presenting all these technologies (LIME model, tutors and the PSLC Logging Format). We then forge some example configurations of our recommender model, and apply them to a database of over 1250 students, 2250 interaction hours and almost 1 Gb of logged data. The result of this process is a potential recommendation to each of these students after the use period of the interactive tutor software. Finally, we present the results of this practical implementation, and discuss the validity or our model in the related tutoring environments.","1548-0992","","10.1109/TLA.2015.7055573","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7055573","cognitive interactive tutor;informal learning;interaction;learning;PSLC Datashop;Recommender model;simulation","Monitoring;Visualization;Software;Adaptation models;Human computer interaction;XML;Databases","cognition;database management systems;educational administrative data processing;human computer interaction;intelligent tutoring systems;interactive systems;recommender systems;XML","cognitive interactive tutors;visual interactive tutors;rule-based recommender model;LIME;PSLC cognitive tutor analytics database;recommendation model;formative support;visual interaction;elearning system;cognitive computer tutors;Pittsburg Science Learning Centre;Carnegie Mellon University;CMU;learning analytics repository;intelligent interactive activities;visual interactive activities;middle-school students;academic skills;on-demand step-by-step feedback;student progress;learner interaction;PSLC logging format;tutor message format;XML schema definition;XSD file;document type definition;DTD file;interactive tutor software;tutoring environments","","","","","","6 Mar 2015","","","IEEE","IEEE Journals"
"Development of a bottom-up methodology for modelling electrical residential load from event data","K. du Preez; H. J. Vermeulen",NA; NA,"2016 International Conference on the Domestic Use of Energy (DUE)","16 Jun 2016","2016","","","1","7","Electricity utilities are increasingly targeting the residential load sector with energy management interventions, either in response to generation and transmission capacity constraints or with the view to realize energy savings in support of energy sustainability and environmental considerations. This increases the importance of load modelling for the residential sector, particularly with reference to appliance inventories, the associated usage patterns and end-user behaviour. Measurement and verification of the impacts of energy conservation measures introduced in this sector also requires more detailed statistical information on the energy consumption models for the individual load technologies found in typical domestic scenarios. Due to the distributed nature of residential loads and large and diverse end-user population, obtaining this information through measurements is expensive and time-consuming. This paper investigates a methodology and supporting database development for use in deriving statistically significant samples of load data for individual residential load components using inexpensive event loggers, including the implementation of a software application to derive the cumulative load profile using a bottom-up approach. Some results are given for a case study for a typical middle-income residential household.","","978-0-9946-7590-3","10.1109/DUE.2016.7466700","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7466700","Domestic energy consumption;Energy Management;event logging;residential load sector;load profile","Load modeling;Home appliances;Data models;Energy consumption;Water heating;Software;Resistance heating","","","","","","15","","16 Jun 2016","","","IEEE","IEEE Conferences"
"Simulation Study on Noise Control Algorithm in Logging While Drilling","Z. Zhang; L. Fa","Circuit System Theory Research, Xi ’an University of Posts and Telecommunications, Shaanxi Province, 710121, China; Sound and Photoelectric Intersection Theory, Xi ’an University of Posts and Telecommunications, Shaanxi Province, 710121, China","2019 IEEE International Conference on Mechatronics and Automation (ICMA)","29 Aug 2019","2019","","","1133","1138","The logging while drilling technology (LWD) is a method for acquiring and transmitting downhole logging data in real time during bit drilling. Since the drilling channel used in the LWD system is a time-space frequency change, long delay, and extremely limited feedback deletion channel, the conventional LWD data transmission strategy is difficult to ensure the reliability and validity of data transmission. In view of the key problems of real-time transmission of the above-mentioned logging while drilling data, this paper has carried out comprehensive and in-depth research. The main innovations are as follows: (1) Introducing adaptive active denoising algorithm for LWD technology to overcome passive denoising technology Defects improve the reliability of logging data. (2) According to the characteristics of the drilling channel, overcoming the inconsistency between the convergence speed of the filter weight coefficient and the steady-state offset in the traditional algorithm, an improved minimum mean square algorithm suitable for the drilling channel is proposed. The algorithm has the advantages of low linear complexity and strong mutation tracking ability. The simulation results of software experiments show that under the harsh environment of the drilling channel, compared with the traditional algorithm, the improved algorithm converges faster and the steady-state offset is smaller. When the system is abrupt, it has stronger tracking ability. Noise is removed and key information about the signal is well preserved.","2152-744X","978-1-7281-1699-0","10.1109/ICMA.2019.8816610","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8816610","Automatic adaptation;Active noise elimination;Improved LMS algorithm;Simulation calculation","Software algorithms;Filtering algorithms;Steady-state;Correlation;Noise reduction;Mean square error methods;Estimation","data recording;drilling;least mean squares methods;noise (working environment);signal denoising","drilling channel;noise control algorithm;drilling technology;bit drilling;LWD system;time-space frequency change;extremely limited feedback deletion channel;conventional LWD data transmission strategy;real-time transmission;drilling data;adaptive active denoising algorithm;LWD technology;passive denoising technology defects;minimum mean square algorithm","","","","11","","29 Aug 2019","","","IEEE","IEEE Conferences"
"A low-cost embedded web-server for an institutional e-learning strategy","R. O. Ocaya; S. R. Katashaya","Dept. of Physics, Univ. of Free State (Qwaqwa Campus), P. Bag X13 Phuthaditjhaba 9866, South Africa; Dept. of Physics and Electronics, North-West University (Mafikeng Campus), P. Bag X2046 Mmabatho 2745, South Africa","2011 3rd International Conference on Electronics Computer Technology","7 Jul 2011","2011","1","","59","63","A TCP/IP based scientific instrument control and data distribution system is presented as part of a low cost e-learning strategy of a learning institution to offer remote collaborative science experimentation. The system uses an IEEE 802.3 full-duplex Medium Access Controller (MAC) and Physical Layer Device connected to transducers and actuators for control, data acquisition, distribution and logging over a TCP/IP network. The hardware comprises of a PIC18O620 and ENC28J60 server interconnected with PC terminals via a network hub. The software comprises the firmware written in C and Javascript, and a simple client web browser written in the Visual Basic .NET framework. The article addresses some concerns about secure access while emphasizing the figures of performance such as round-trip and inter-sample times. Finally, sample outputs of two networked PCs in a typical application to Newton's law of cooling experiment are presented.","","978-1-4244-8679-3","10.1109/ICECTECH.2011.5941560","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5941560","","Servers;Browsers;IP networks;Collaboration;Hardware;Protocols;Software","access protocols;computer aided instruction;file servers;firmware;groupware;Internet;IP networks;transport protocols","low-cost embedded Web-server;institutional e-learning strategy;TCP/IP based scientific instrument control;data distribution system;remote collaborative science experimentation;IEEE 802.3 full-duplex medium access controller;MAC;physical layer device;transducer;actuator;data acquisition;data logging;PIC18O620 server;ENC28J60 server;firmware;C language;Javascript;client Web browser;Visual Basic .NET framework","","","","21","","7 Jul 2011","","","IEEE","IEEE Conferences"
"Application of Adaptive Particle Swarm Optimization in Computer Forensics","D. Wang; H. Shi; H. Ma","Software Technol. Inst., Dalian Jiaotong Univ., Dalian, China; Software Technol. Inst., Dalian Jiaotong Univ., Dalian, China; Software Technol. Inst., Dalian Jiaotong Univ., Dalian, China","2010 WASE International Conference on Information Engineering","16 Sep 2010","2010","2","","147","149","Computer Forensics is a field that all of the digital devices leave digital crime and these crime are valuable evidence in a wide range of inquiries. Timestamps stored on digital media play a crucial role in evidence analysis, but digital timestamps easily be modified. So this paper will collect some data about timestamp or other properties from different type of files, such as file system, registry and log file. Through an Adaptive Particle Swarm Optimization 〈 APOS〉 algorithm to analyze the data to obtain approximate optimal solution.","","978-1-4244-7507-0","10.1109/ICIE.2010.131","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5571302","computer forensics;timestamp;APSO","Computers;Forensics;Classification algorithms;File systems;Particle swarm optimization;Optimization;Mathematical model","computer forensics;particle swarm optimisation","adaptive particle swarm optimization;computer forensics;digital devices;digital crime;digital media;APOS","","1","","10","","16 Sep 2010","","","IEEE","IEEE Conferences"
"Precise Calling Context Encoding","W. N. Sumner; Y. Zheng; D. Weeratunge; X. Zhang","Purdue University, West Lafayette; Purdue University, West Lafayette; Purdue University, West Lafayette; Purdue University, West Lafayette","IEEE Transactions on Software Engineering","24 Sep 2012","2012","38","5","1160","1177","Calling contexts (CCs) are very important for a wide range of applications such as profiling, debugging, and event logging. Most applications perform expensive stack walking to recover contexts. The resulting contexts are often explicitly represented as a sequence of call sites and hence are bulky. We propose a technique to encode the current calling context of any point during an execution. In particular, an acyclic call path is encoded into one number through only integer additions. Recursive call paths are divided into acyclic subsequences and encoded independently. We leverage stack depth in a safe way to optimize encoding: If a calling context can be safely and uniquely identified by its stack depth, we do not perform encoding. We propose an algorithm to seamlessly fuse encoding and stack depth-based identification. The algorithm is safe because different contexts are guaranteed to have different IDs. It also ensures contexts can be faithfully decoded. Our experiments show that our technique incurs negligible overhead (0-6.4 percent). For most medium-sized programs, it can encode all contexts with just one number. For large programs, we are able to encode most calling contexts to a few numbers. We also present our experience of applying context encoding to debugging crash-based failures.","1939-3520","","10.1109/TSE.2011.70","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5963696","Calling context;context sensitivity;profiling;path encoding;calling context encoding;call graph","Context;Encoding;Instruments;Image edge detection;Runtime;Decoding;Software algorithms","optimisation;program compilers;program debugging","precise calling context encoding;CC;profiling;event logging;stack walking;context recovery;call sites;recursive call paths;acyclic subsequences;encoding optimization;stack depth-based identification;ID;medium-sized programs;crash-based failure debugging","","12","","54","","28 Jul 2011","","","IEEE","IEEE Journals"
"Development of an automatic data collection and communication system for greenhouse in Japan","K. Yasuba; H. Kurosaki; M. Takaichi; K. Suzuki","Advanced Greenhouse Production Research Team, National Institute of Vegetable and Tea, Aichi, Japan; Advanced Greenhouse Production Research Team, National Institute of Vegetable and Tea, Aichi, Japan; Advanced Greenhouse Production Research Team, National Institute of Vegetable and Tea, Aichi, Japan; Advanced Greenhouse Production Research Team, National Institute of Vegetable and Tea, Aichi, Japan","SICE Annual Conference 2011","27 Oct 2011","2011","","","2806","2807","UECS (ubiquitous environment control system) is one of the environmental controlling systems using information communication on LAN. Formatted correspondences on UDP packets were broadcasted from UECS devices for transmitting information and controlling devices. Newly developed software analyzed these correspondences automatically, and analyzed data were logged every one minute. Html file was renewed after received data. If the configuration file concerning making graph, alert mail, daily report, and calculation water status of air must be made, user could use these functions of this software. It was thought that this software was useful for UECS installed greenhouses.","","978-4-907764-39-5","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6060459","ubiquitous environment control system;greenhouse;software;database","Software;Green products;Control systems;Production;Internet;Databases;Local area networks","environmental science computing;greenhouses;hypermedia markup languages;local area networks;transport protocols;ubiquitous computing","automatic data collection;communication system;greenhouse;Japan;ubiquitous environment control system;environmental controlling system;information communication;LAN;UDP packets;information transmission;HTML file","","1","","3","","27 Oct 2011","","","IEEE","IEEE Conferences"
"Online Extracting Sessions of Frequent Users","Y. Bei; Z. Cai","Ningbo Research Institute, Zhejiang University,College of Software Technology,Hangzhou,China; Ningbo Research Institute, Zhejiang University,College of Software Technology,Hangzhou,China","2020 IEEE 5th International Conference on Cloud Computing and Big Data Analytics (ICCCBDA)","19 May 2020","2020","","","440","446","Web usage mining aims to discover interesting user access patterns from the data derived from interactions of users. The primary source data for web usage mining approach is user sessions extracted from web server logs. However, logs are usually very large and contain sessions of users with different behaviors. To expedite mining processes and enhance mining results, we consider extracting sessions of frequent users. In this paper, we investigate the problem of finding all frequent users as well as obtaining their sessions. Instead of discovering frequent users over static web log files, we present an online algorithm named FUSMiner to solve the mining task in a streaming environment. The experimental results show that our proposed algorithm is both efficient and effective.","","978-1-7281-6024-5","10.1109/ICCCBDA49378.2020.9095681","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9095681","web usage mining;frequent user;session;session extracting;data stream","Data mining;Web servers;Task analysis;Web mining;Web sites;Algorithm design and analysis;Data structures","data mining;Internet;Web sites","web server logs;static web log files;user access patterns;web usage mining approach;online extracting sessions","","","","23","","19 May 2020","","","IEEE","IEEE Conferences"
"SaNSA - The Supercomputer and Node State Architecture","N. Agarwal; H. Greenberg; S. Blanchard; N. DeBardeleben","Ultrascale Syst. Res. Center, Los Alamos Nat. Lab., Los Alamos, NM, USA; Ultrascale Syst. Res. Center, Los Alamos Nat. Lab., Los Alamos, NM, USA; Ultrascale Syst. Res. Center, Los Alamos Nat. Lab., Los Alamos, NM, USA; Ultrascale Syst. Res. Center, Los Alamos Nat. Lab., Los Alamos, NM, USA","2018 IEEE/ACM 8th Workshop on Fault Tolerance for HPC at eXtreme Scale (FTXS)","6 Dec 2018","2018","","","69","78","In this work we present SaNSA, the Supercomputer and Node State Architecture, a software infrastructure for historical analysis and anomaly detection. SaNSA consumes data from multiple sources including system logs, the resource manager, scheduler, and job logs. Furthermore, additional context such as scheduled maintenance events or dedicated application run times for specific science teams can be overlaid. We discuss how this contextual information allows for more nuanced analysis. SaNSA allows the user to apply arbitrary attributes, for instance, positional information where nodes are located in a data center. We show how using this information we identify anomalous behavior of one rack of a 1,500 node cluster. We explain the design of SaNSA and then test it on four open compute clusters at LANL. We ingest over 1.1 billion lines of system logs in our study of 190 days in 2018. Using SaNSA, we perform a number of different anomaly detection methods and explain their findings in the context of a production supercomputing data center. For example, we report on instances of misconfigured nodes which receive no scheduled jobs for a period of time as well as examples of correlated rack failures which cause jobs to crash.","","978-1-7281-0222-1","10.1109/FTXS.2018.00011","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8564489","system-state;node-state;health-monitoring;anomaly-detection;software-architecture","Booting;Supercomputers;Computer architecture;Maintenance engineering;Data centers;Distributed databases","computer centres;mainframes;parallel machines;scheduling;software architecture;system monitoring","SaNSA;system logs;job logs;anomaly detection methods;jobs scheduling;supercomputing data center;supercomputer and node state architecture;resource manager;scheduler;software infrastructure","","","","19","","6 Dec 2018","","","IEEE","IEEE Conferences"
"Lifelog collaboration framework for healthcare service on Android platform","K. Kang; Y. Kwon; Y. Kim; J. Lee; C. Bae","Software Research Lab., ETRI, Gajeong-dong Yuseong-gu Daejeon, Korea; Software Research Lab., ETRI, Gajeong-dong Yuseong-gu Daejeon, Korea; Software Research Lab., ETRI, Gajeong-dong Yuseong-gu Daejeon, Korea; Software Research Lab., ETRI, Gajeong-dong Yuseong-gu Daejeon, Korea; Software Research Lab., ETRI, Gajeong-dong Yuseong-gu Daejeon, Korea","International Conference on ICT for Smart Society","2 Sep 2013","2013","","","1","4","This paper proposes lifelog collaboration framework on Android platform for the healthcare service infrastructure. Current many vendors provide network-enabled small gadgets that have ability to gather a variety of life events. However, those devices do not provide collaboration mechanism between different vendors' devices. The lifelog collaboration framework introduced in this paper may help to overcome this disadvantage of those devices. The lifelog collaboration framework consists of three layered architecture such as the data logging layer, the data mining layer, and the data service layer.","","978-1-4799-0145-6","10.1109/ICTSS.2013.6588078","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6588078","lifelog;collaboration;healthcare","Androids;Humanoid robots;Collaboration;Medical services;Computer architecture;Medical diagnostic imaging","data loggers;data mining;health care;mobile computing;smart phones","lifelog collaboration framework;Android platform;healthcare service infrastructure;data logging layer;data mining layer;data service layer","","2","","11","","2 Sep 2013","","","IEEE","IEEE Conferences"
"Hardware Transactional Memory Meets Memory Persistency","D. Castro; P. Romano; J. Barreto","Inst. Super. Tecnico, Univ. of Lisbon, Lisbon, Portugal; Inst. Super. Tecnico, Univ. of Lisbon, Lisbon, Portugal; Inst. Super. Tecnico, Univ. of Lisbon, Lisbon, Portugal","2018 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","6 Aug 2018","2018","","","368","377","Persistent Memory (PM) and Hardware Transactional Memory (HTM) are two recent architectural developments whose joint usage promises to drastically accelerate the performance of concurrent, data-intensive applications. Unfortunately, combining these two mechanisms using existing architectural supports is far from being trivial. This paper presents NV-HTM, a system that allows the execution of transactions over PM using unmodified commodity HTM implementations. NV-HTM relies on a hardware-software co-design technique, which is based on three key ideas: i) relying on software to persist transactional modifications after they have been committed via HTM; ii) postponing the externalization of commit events to applications until it is ensured, via software, that any data version produced and observed by committed transactions is first logged in PM; ii) pruning the commit logs via checkpointing schemes that not only bound the log space and recovery time, but also implement wear levelling techniques to enhance PM's endurance. By means of an extensive experimental evaluation, we show that NV-HTM can achieve up to 10× speed-ups and up to 11.6× reduced flush operations with respect to state of the art solutions, which, unlike NV-HTM, require custom modifications to existing HTM systems.","1530-2075","978-1-5386-4368-6","10.1109/IPDPS.2018.00046","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8425190","transaction;memory;persistent;hardware;system","Hardware;Software;Random access memory;Checkpointing;Proposals;Scalability;Protocols","checkpointing;hardware-software codesign;storage management","memory persistency;Persistent Memory;Hardware Transactional Memory;concurrent data-intensive applications;architectural supports;NV-HTM;unmodified commodity HTM implementations;transactional modifications;commit events;committed transactions;commit logs;HTM systems;hardware-software co-design technique;checkpointing schemes","","3","","42","","6 Aug 2018","","","IEEE","IEEE Conferences"
"A Case for Application-Managed Flash","J. Koo; C. Chung; Arvind; S. Lee","Department of Information and Communication Engineering, Daegu Gyeongbuk Institute of Science and Technology (DGIST), Daegu, Republic of Korea; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Information and Communication Engineering, Daegu Gyeongbuk Institute of Science and Technology (DGIST), Daegu, Republic of Korea","IEEE Transactions on Computers","13 Jan 2021","2021","70","2","240","254","We propose a new I/O architecture for NAND flash-based SSDs, called application-managed flash (AMF) and present two case studies to show its usefulness. In a typical SSD controller, an intermediate software layer, called the flash translation layer (FTL), is employed between NAND flash chips and a host interface. The main responsibility of an FTL is to provide interoperability with conventional HDDs, but this interoperability comes at the cost of extra hardware resources and degraded I/O performance. The proposed AMF refactors the flash storage architecture so that an SSD controller exposes append-only segments, which do not permit overwriting. This refactoring dramatically improves performance of applications and reduces hardware costs by allowing applications to directly manage flash storage with minimal supports from the SSD controller. In order to understand the benefits of AMF, we study two popular applications: a log-structured file system (F2FS) and a key-value store (RocksDB). Our experiments show that the DRAM in the flash controller is reduced by 128X and the performances of the file system and the key-value store improve by 80 and 54 percent, respectively, over conventional SSDs.","1557-9956","","10.1109/TC.2020.2987569","National Research Foundation of Korea; Ministry of Science and ICT; Samsung GRO 2017-2018; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9067076","NAND flash;solid-state disks;file system;key-value store;flash translation layer","Computer architecture;Hardware;Software;Media;Geometry;Interoperability;Random access memory","DRAM chips;flash memories;memory architecture;NAND circuits","DRAM;RocksDB;F2FS;log-structured file system;degraded I/O performance;I/O architecture;application-managed flash;NAND flash-based SSDs;conventional SSDs;flash controller;key-value store;hardware cost reduction;flash storage architecture;AMF refactors;extra hardware resources;host interface;NAND flash chips;FTL;flash translation layer;intermediate software layer;typical SSD controller;SSDs","","","","45","IEEE","14 Apr 2020","","","IEEE","IEEE Journals"
"Formal Passive Testing of Service-Oriented Systems","C. Andrés; M. E. Cambronero; M. Núñez","Dept. de Sist. Informaticos y Comput., Univ. Complutense de Madrid, Madrid, Spain; Dept. de Sist. Informaticos y Comput., Univ. Complutense de Madrid, Madrid, Spain; Dept. de Sist. Informaticos, Univ. de Castilla-La Mancha, Ciudad Real, Spain","2010 IEEE International Conference on Services Computing","26 Aug 2010","2010","","","610","613","This paper presents a formal framework to perform passive testing of service-oriented systems. Our approach uses the historical interaction files between web services to check the absence of faults. It uses a set of properties, that we call invariants, to represent the most relevant expected behaviour of the web services under test. Intuitively, an invariant expresses the fact that each time the system under test performs a given sequence of actions, it must exhibit a behavior reflected in the invariant. Invariants can be defined from a local point of view, that is, to check properties of isolated web services, and from a global point of view, that is, to check web service interaction properties. In order to increase applicability and adaption to a real environment, we assume that we do not have a global log. We show how to use local logs (recorded in each web service) in order to check local properties and how to combine them in order to check global properties.","","978-1-4244-8147-7","10.1109/SCC.2010.62","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5557284","Passive Testing;Service Oriented Systems;Monitoring","Web services;Testing;Monitoring;Semantics;Presses;Software;IEEE Computer Society Press","formal verification;program testing;Web services","formal passive testing;service-oriented systems;formal framework;historical interaction files;Web services;system under test;Web service interaction property","","","","7","","26 Aug 2010","","","IEEE","IEEE Conferences"
"Development of Multitenant SaaS framework at single instance and with zero effort multitenancy","S. S. Kale; R. H. Borhade","Department of Information Technology, Sinhagad Education Society's,SKN COE, Pune, India; Department of Information Technology, Sinhagad Education Society's,SKN COE, Pune, India","2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","21 Oct 2013","2013","","","834","839","A multitenant SaaS model mainly aims to provide the service to customer by providing license. Multitenancy means, only one tenant give service to different customers. Software as a service (SaaS) is used as the service given to the customer. This Proposed framework helps to create, enable a single instance of your application and Software as a service provides software when required to customer. Multi-tenancy is a core concept in SaaS.","","978-1-4673-6217-7","10.1109/ICACCI.2013.6637284","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6637284","multi-tenant;SaaS;SaaS platform;single instance;zero effort multitenancy","Software as a service;Databases;Portals;Computer architecture;Servers;Context","cloud computing;customer services;portals;software engineering","multitenant SaaS framework;zero effort multitenancy;multitenant SaaS model;customer service;software as a service;dot net technology;private cloud;Web portal;software development time reduction;software development cost reduction;tenant encapsulation;bulk insert;event logging;usage metering;service scheduling","","","","12","","21 Oct 2013","","","IEEE","IEEE Conferences"
"PESKEA: Anomaly Detection Framework for Profiling Kernel Event Attributes in Embedded Systems","O. M. Ezeme; A. Azim; Q. Mahmoud","Electrical, Computer and Software Engineering, University of Ontario Institute of Technology, 85458 Oshawa, Ontario Canada L1H 7K4 (e-mail: ezeme.okwudili@gmail.com); Electrical, Computer and Software Engineering, University of Ontario Institute of Technology, 85458 Oshawa, Ontario Canada L1H 7K4 (e-mail: akramul.azim@uoit.ca); Electrical, Computer and Software Engineering, UOIT, Oshawa, Ontario Canada L1H7K4 (e-mail: qusay.mahmoud@uoit.ca)","IEEE Transactions on Emerging Topics in Computing","","2020","PP","99","1","1","In the software development life cycle, we use the execution traces of a given application to examine the behavior of the software when an error occurs or to monitor the software performance and compliance. However, this type of application trace analysis focuses on checking the performance of the software against its design goals. Conversely, the operating system (OS) sits between the application and the hardware, and traces logged from this layer capture the behavior of the embedded system and not just the application. Hence, an analysis of the kernel events captures the system-wide performance of the embedded system. Consequently, we present a feature-based anomaly detection framework called PESKEA, which exploits the statistical variance of the features in the execution traces of an embedded OS to perform trace classification, and subsequently, anomaly detection. We test PESKEA with two public datasets we refer to as Dataset I and Dataset II. On Dataset I, PESKEA results show a 3% to 6% improvement in the true positive rate (TPR) of Dataset I compared to the previous work tested on this dataset, and scores between 88:37% to 100% in Dataset II. We hope to test PESKEA on non-UAV embedded control application datasets in future work.","2168-6750","","10.1109/TETC.2020.2971251","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8979375","Context modeling;anomaly detection framework;embedded operating system;machine learning","Anomaly detection;Embedded systems;Feature extraction;Monitoring;Kernel;Hardware","","","","1","","","","3 Feb 2020","","","IEEE","IEEE Early Access Articles"
"MARBLE: Modernization approach for recovering business processes from legacy information systems","R. Pérez-Castillo","Instituto de Tecnologías y Systemas de Información (ITSI), University of Castilla-La Mancha, Paseo de la Universidad 4 13071, Ciudad Real, Spain","2012 28th IEEE International Conference on Software Maintenance (ICSM)","10 Jan 2013","2012","","","671","676","The volatile IT industry often tempts companies to replace legacy information systems with new ones. However, these systems cannot always be completely discarded because they gradually store a significant amount of valuable business knowledge as a result of progressive maintenance over time. MARBLE semi-automatically rebuilds the hidden business processes embedded in legacy information systems. MARBLE supports a business process archeology method which allows business experts to attain a rapid and meaningful understanding of the organization's business processes. MARBLE-framed techniques are based on static and dynamic analysis by considering different legacy software artifacts (e.g., source code, event logs, etc.). Through the validation of MARBLE with several industrial systems, the proposal proved to be less time-consuming and more exhaustive (since it considers the embedded business knowledge) than a manual process redesigned by experts from scratch. The main implications are that MARBLE provides maintainers with a mechanism with which to modernize legacy information systems in line with the actual business processes of an organization.","1063-6773","978-1-4673-2312-3","10.1109/ICSM.2012.6405351","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6405351","Business Processes;Software Modernization;Static Analisis;Dynamic Analisis;Model Transformation","Information systems;Organizations;Software;Proposals;Reverse engineering;Analytical models","business data processing;information systems;organisational aspects;software maintenance;system recovery","modernization approach;business processes recovery;volatile IT industry;legacy information systems;progressive maintenance;MARBLE;business process archeology method;legacy software artifacts;organization","","8","","24","","10 Jan 2013","","","IEEE","IEEE Conferences"
"Securing Virtual Infrstructure in Cloud Computing using Big Data Analytics","A. Kumar Gupta; A. Choudhary; P. Chauhan","Department of Cyber Security, Sardar Patel University of Police, Jodhpur, India; Department of Cyber Security, Sardar Patel University of Police, Jodhpur, India; Department of Cyber Security, Sardar Patel University of Police, Jodhpur, India","2018 International Conference on Advances in Computing, Communication Control and Networking (ICACCCN)","1 Jul 2019","2018","","","569","573","With the popularity of cloud, attacks on cloud are also increasing. For advanced attacks on cloud, attackers choose Virtual Infrastructures to attack as they provide the platform to develop and deploy the applications through Virtual Machines (VMs). Attacks on virtualized infrastructure are increasing day by day. Attackers try to attack on a VM which sometimes leads them to take the unauthorized access of the cloud. There are different techniques to detect and prevent these attacks. Big Data Analytics field is getting better to perform the analysis of attacks efficiently. This paper focuses on Big Data Analytics approaches to detect the advanced Malware attacks in virtualized infrastructure. In this paper we use Big Data Analytics techniques to analyze log files and network traffic to identify anomalies and suspicious activities in virtualized infrastructure in cloud computing. Network logs and application logs are collected periodically from virtual machines (VMs) and stored in the HadoopDistributed File System. We then use Map Reduce Parser and Graph event based correlation to extract the attack features and with the help of logistic regression and belief propagation we will calculate the attacks conditional probabilities and belief in existence of an attack.","","978-1-5386-4119-4","10.1109/ICACCCN.2018.8748311","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8748311","Big Data Analytics;Cloud Computing;Virtualized Infrastructure;HDFS;Map Reduce;Graph-Event Based Correlation;Malware Detection;Advanced Persistent Threats (APTs);Machine Learning","Malware;Virtual machine monitors;Security;Cloud computing;Virtual machining;Correlation;Feature extraction","Big Data;cloud computing;data analysis;invasive software;regression analysis;security of data;virtual machines","cloud computing;virtual machines;attack features;attacks conditional probabilities;advanced malware attacks;Big Data analytics techniques;virtual infrstructure security;virtual infrstructure;logistic regression","","","","29","","1 Jul 2019","","","IEEE","IEEE Conferences"
"Diagnosys: automatic generation of a debugging interface to the Linux kernel","T. F. Bissyandé; L. Réveillère; J. L. Lawall; G. Muller","University of Bordeaux, France; University of Bordeaux, France; INRIA, France; INRIA, France","2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering","8 Apr 2013","2012","","","60","69","The Linux kernel does not export a stable, well-defined kernel interface, complicating the development of kernel-level services, such as device drivers and file systems. While there does exist a set of functions that are exported to external modules, this set of functions frequently changes, and the functions have implicit, ill-documented preconditions. No specific debugging support is provided. We present Diagnosys, an approach to automatically constructing a debugging interface for the Linux kernel. First, a designated kernel maintainer uses Diagnosys to identify constraints on the use of the exported functions. Based on this information, developers of kernel services can then use Diagnosys to generate a debugging interface specialized to their code. When a service including this interface is tested, it records information about potential problems. This information is preserved following a kernel crash or hang. Our experiments show that the generated debugging interface provides useful log information and incurs a low performance penalty.","","978-1-4503-1204-2","10.1145/2351676.2351686","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6494906","Debugging;Device drivers;Diagnosys;Linux;Wrappers","","Linux;operating system kernels;program debugging;user interfaces","Diagnosys;debugging interface automatic generation;Linux kernel;kernel-level services;external modules;kernel maintainer;kernel crash;log information","","2","1","31","","8 Apr 2013","","","IEEE","IEEE Conferences"
"A Survey of Logstash for Indexing Bioinformatics Data","D. Cho; A. Parsha; S. Yoo; S. E. Pepper; Y. Cui","United States Military Academy; Computational Science Init. Brookhaven National Lab., Upton, NY, USA; Computational Science Init. Brookhaven National Lab., Upton, NY, USA; Security Department Brookhaven National Lab., Nonproliferation and National, Upton, NY, USA; Security Department Brookhaven National Lab., Nonproliferation and National, Upton, NY, USA","2018 New York Scientific Data Summit (NYSDS)","18 Nov 2018","2018","","","1","1","This paper explores the potential for the reuse of Logstash, a popular open source ETL (Extract, Transform, and Load) pipeline. Logstash was initially created to extract, transform, and load single line logs; however, with the growth of popularity, more people have added different sets of plugins to expand its capabilities. Now, Logstash is capable of handling different input types such as connecting to a database and extracting data using SQL statements. This paper investigates the feasibility of Logstash in extracting, transforming, and loading bioinformatics data.","","978-1-5386-7933-3","10.1109/NYSDS.2018.8538952","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8538952","","Scientific computing;National security;Indexing;Bioinformatics;Military computing;Handheld computers;Pipelines","bioinformatics;database indexing;public domain software;SQL","SQL statements;single line logs;extract-transform-and-load pipeline;open source ETL pipeline;bioinformatics data indexing;Logstash","","","","2","","18 Nov 2018","","","IEEE","IEEE Conferences"
"Measuring effort in a corporate repository","M. VanHilst; S. Huang; J. Mulcahy; W. Ballantyne; E. Suarez-Rivero; D. Harwood","College of Computer Science & Engineering, Florida Atlantic University, Boca Raton, United States; College of Computer Science & Engineering, Florida Atlantic University, Boca Raton, United States; College of Computer Science & Engineering, Florida Atlantic University, Boca Raton, United States; Motorola Mobility, Plantation, Florida, United States; Motorola Mobility, Plantation, Florida, United States; Motorola Mobility, Plantation, Florida, United States","2011 IEEE International Conference on Information Reuse & Integration","5 Sep 2011","2011","","","246","252","Project management and process improvement are a critical part of software development in an organization, especially for large scale and long-lived software. Metrics can be used as one of the means to evaluate this process. However, traditional methods of measuring effort, for example, focusing on editing time and lines of code produced, do not reflect the true cost to the organization. The organization pays developers for their time, regardless of whether they are writing code or performing other activities. This paper describes an experimental approach to track and measure effort in developer days, using log files from a corporate repository. The data is fine-grained, empirical, and non-invasively collected. Results have been tested on several projects in a large organization over a period of many years.","","978-1-4577-0966-1","10.1109/IRI.2011.6009554","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6009554","Repository;effort;metrics;project management","Organizations;Measurement;Maintenance engineering;Industries;Data mining;Databases;Project management","software metrics;software process improvement","corporate repository;project management;process improvement;software development;log files","","4","","14","","5 Sep 2011","","","IEEE","IEEE Conferences"
"Business rules handling in relational databases","R. R. Isakh; T. E. Widagdo","School of Electrical Engineering and Informatics, Institut Teknologi Bandung, Indonesia; School of Electrical Engineering and Informatics, Institut Teknologi Bandung, Indonesia","2016 International Conference on Data and Software Engineering (ICoDSE)","1 Jun 2017","2016","","","1","6","Business rules handling is very essential for organization's operational needs. In order to increase efficiency and optimize operational cost, many organizations try to convert their business process into a computer-based system. Therefore, we need a model of business rule in order to maintain the business rules in the system. The aim of this paper is to capture business rules into the computer-based system, particularly on modeling the business rules in the relational model. First-order logic notation will be applied as an approach to model the business rules. This model will be used to store business rules in the database and also to check the data instances in operational databases. The checking results will be reported in the form of log files. As a result of a data checking process, the system can determine whether a data instance comply with the business rules or not.","","978-1-5090-5671-2","10.1109/ICODSE.2016.7936126","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7936126","business rule;rule modeling;instance checking;first-order logic","Computational modeling","business data processing;formal logic;organisational aspects;relational databases","business rules handling;relational databases;organization operational needs;business process;computer-based system;first-order logic notation;business rules;data instances;operational databases;log files;data checking process;data instance","","","","5","","1 Jun 2017","","","IEEE","IEEE Conferences"
"Clustering sequential data with OPTICS","A. Omrani; K. Santhisree; Damodaram","Dept. of Computer science, Jawaharlal Nehru Technology University (JNTUH), Hyderabad, India; Dept. of Computer science, Jawaharlal Nehru Technology University (JNTUH), Hyderabad, India; Dept. of Computer science, Jawaharlal Nehru Technology University (JNTUH), Hyderabad, India","2011 IEEE 3rd International Conference on Communication Software and Networks","8 Sep 2011","2011","","","591","594","The Web has enormous, various and knowledgeable data for data mining research. The web is a biggest knowledgeable database with various types of data to mine. One of interesting type of data is user behaviour that mine from server log files. Many algorithms are for clustering and then discover the knowledge from database. In this paper we use OPTICS (""Ordering Points To Identify the Clustering Structure"") algorithm to find density based clusters on a social music website data (Last.fm website is a free social platform that share listed music with so different music genres). After pre-processing on music dataset and removing unprofitable data from the dataset was ready to clustering. The clusters are generated by OPTICS algorithm and the average of inter cluster and intra cluster are calculated. Then results are visualized and Euclidean distance measure is used to compare results of intra cluster and inter cluster analyses. Finally showed behavior of clusters that made by OPTICS algorithm on a sequential data.","","978-1-61284-486-2","10.1109/ICCSN.2011.6014339","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6014339","Clustering algorithm OPTICS;Sequence mining","Biomedical optical imaging;Adaptation models;Optics;Biology","data mining;database management systems;music;pattern clustering;social networking (online)","sequential data clustering;ordering points to identify the clustering structure algorithm;OPTICS;data mining research;server log files;density based clusters;social music website data;music genres;music dataset;OPTICS algorithm;Euclidean distance measure;visualized distance measure;intracluster analyses;intercluster analyses;database","","2","","7","","8 Sep 2011","","","IEEE","IEEE Conferences"
"Digital forensic analysis of cloud storage data in IDrive and Mega cloud drive","S. Thamburasa; S. Easwaramoorthy; K. Aravind; S. B. Bhushan; U. Moorthy","Information security Analyst, Synophic System Private limited, Bangalore, India; School of Information Technology and Engineering, VIT University, Vellore, India; School of Information Technology and Engineering, VIT University, Vellore, India; School of Information Technology and Engineering, VIT University, Vellore, India; School of Information Technology and Engineering, VIT University, Vellore, India","2016 International Conference on Inventive Computation Technologies (ICICT)","26 Jan 2017","2016","3","","1","6","In this technological world one of the general method for user to save their data is cloud. Most of the cloud storage company provides some storage space as free to its users. Both individuals and corporate are storing their files in the cloud infrastructure so it becomes a problem for a forensics analyst to perform evidence acquisition and examination. One reason that makes evidence acquisition more difficult is user data always saved in remote computer on cloud. Various cloud companies available in the market serving storage as one of their services and everyone delivering different kinds of features and facilities in the storage technology. One area of difficulty is the acquisition of evidential data associated to a cybercrime stored in a different cloud company service. Due to lack of understanding about the location of evidence data regarding which place it is saved could also affect an analytical process and it take a long time to speak with all cloud service companies to find whether data is saved within their cloud. By analyzing two cloud service companies (IDrive and Mega cloud drive) this study elaborates the various steps involved in the activity of obtaining evidence on a user account through a browser and then via cloud software application on a Windows 7 machine. This paper will detail findings for both the Mega cloud drive and IDrive client software, to find the different evidence that IDrive and the mega cloud drive leaves behind on a user computer. By establishing the artifacts on a user machine will give an overall idea regarding kind of evidence residue in user computer for investigators. Key evidences discovered on this investigation comprises of RAM memory captures, registry files application logs, file time and date values and browser artifacts are acquired from these two cloud companies on a user windows machine.","","978-1-5090-1285-5","10.1109/INVENTIVE.2016.7830159","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7830159","Digital forensics;Cloud company services;client software;web browser;IDrive;Mega cloud Drive","Cloud computing;Forensics;Companies;Browsers;Computers","cloud computing;computer crime;data acquisition;digital forensics;storage management","digital forensic analysis;cloud storage data;IDrive cloud drive;Mega cloud drive;evidential data acquisition;cybercrime;Windows 7 machine;RAM memory captures;registry files application logs;file time values;file date values;browser artifacts","","3","","9","","26 Jan 2017","","","IEEE","IEEE Conferences"
"VT-Revolution: Interactive Programming Video Tutorial Authoring and Watching System","L. Bao; Z. Xing; X. Xia; D. Lo","College of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang, China; Research School of Computer Science, Australian National University, Canberra, ACT, Australia; Faculty of Information Technology, Monash University, Clayton, VIC, Australia; School of Information Systems, Singapore Management University, Singapore","IEEE Transactions on Software Engineering","26 Aug 2019","2019","45","8","823","838","Procedural knowledge describes actions and manipulations that are carried out to complete programming tasks. An effective way to document procedural knowledge is programming video tutorials. Unlike text-based software artifacts and tutorials that can be effectively searched and linked using information retrieval techniques, the streaming nature of programming videos limits the ways to explore the captured workflows and interact with files, code and program output in the videos. Existing solutions to adding interactive workflow and elements to programming videos have a dilemma between the level of desired interaction and the efforts required for authoring tutorials. In this work, we tackle this dilemma by designing and building a programming video tutorial authoring system that leverages operating system level instrumentation to log workflow history while tutorial authors are creating programming videos, and the corresponding tutorial watching system that enhances the learning experience of video tutorials by providing programming-specific workflow history and timeline-based browsing interactions. Our tutorial authoring system does not incur any additional burden on tutorial authors to make programming videos interactive. Given a programming video accompanied by synchronously-logged workflow history, our tutorial watching system allows tutorial watchers to freely explore the captured workflows and interact with files, code and program output in the tutorial. We conduct a user study of 135 developers to evaluate the design and effectiveness of our system in helping developers learn programming knowledge in video tutorials.","1939-3520","","10.1109/TSE.2018.2802916","National Natural Science Foundation of China; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8283605","Program comprehension;human-computer interaction;workflow","Tutorials;Programming;Streaming media;Tools;Task analysis;History;Software","authoring systems;computer aided instruction;computer science education;interactive systems;multimedia systems;programming","interactive programming video tutorial authoring;procedural knowledge;video tutorials;text-based software artifacts;authoring tutorials;programming video tutorial authoring system;programming-specific workflow history;programming tasks;tutorial watching system;operating system level instrumentation","","1","","43","","6 Feb 2018","","","IEEE","IEEE Journals"
"Feedback-Directed Instrumentation for Deployed JavaScript Applications","M. Madsen; F. Tip; E. Andreasen; K. Sen; A. Møller","Univ. of Waterloo, Waterloo, ON, Canada; Samsung Res. America, Mountain View, CA, USA; Aarhus Univ., Aarhus, Denmark; EECS Dept., UC Berkeley, Berkeley, CA, USA; Aarhus Univ., Aarhus, Denmark","2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)","3 Apr 2017","2016","","","899","910","Many bugs in JavaScript applications manifest themselves as objects that have incorrect property values when a failure occurs. For this type of error, stack traces and log files are often insufficient for diagnosing problems. In such cases, it is helpful for developers to know the control flow path from the creation of an object to a crashing statement. Such crash paths are useful for understanding where the object originated and whether any properties of the object were corrupted since its creation.We present a feedback-directed instrumentation technique for computing crash paths that allows the instrumentation overhead to be distributed over a crowd of users and to reduce it for users who do not encounter the crash. We implemented our technique in a tool, Crowdie, and evaluated it on 10 real-world issues for which error messages and stack traces are insufficient to isolate the problem. Our results show that feedback-directed instrumentation requires 5% to 25% of the program to be instrumented, that the same crash must be observed 3 to 10 times to discover the crash path, and that feedback-directed instrumentation typically slows down execution by a factor 2x-9x compared to 8x-90x for an approach where applications are fully instrumented.","1558-1225","978-1-4503-3900-1","10.1145/2884781.2884846","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7886966","debugging;dynamic analysis;javascript;crowdsourcing;instrumentation","Instruments;Computer bugs;Reactive power;Debugging;Software;Servers","Java;system recovery","JavaScript applications;feedback-directed instrumentation technique;crash paths;instrumentation overhead;CROWDIE","","4","","32","","3 Apr 2017","","","IEEE","IEEE Conferences"
"Anomalicious: Automated Detection of Anomalous and Potentially Malicious Commits on GitHub","D. Gonzalez; T. Zimmermann; P. Godefroid; M. Schäfer",Rochester Institute of Technology; Microsoft Research; Microsoft Research; GitHub,"2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)","7 May 2021","2021","","","258","267","Security is critical to the adoption of open source software (OSS), yet few automated solutions currently exist to help detect and prevent malicious contributions from infecting open source repositories. On GitHub, a primary host of OSS, repositories contain not only code but also a wealth of commit-related and contextual metadata – what if this metadata could be used to automatically identify malicious OSS contributions?In this work, we show how to use only commit logs and repository metadata to automatically detect anomalous and potentially malicious commits. We identify and evaluate several relevant factors which can be automatically computed from this data, such as the modification of sensitive files, outlier change properties, or a lack of trust in the commit’s author. Our tool, Anomalicious, automatically computes these factors and considers them holistically using a rule-based decision model. In an evaluation on a data set of 15 malware-infected repositories, Anomalicious showed promising results and identified 53.33% of malicious commits, while flagging less than 1% of commits for most repositories. Additionally, the tool found other interesting anomalies that are not related to malicious commits in an analysis of repositories with no known malicious commits.","","978-1-6654-3869-8","10.1109/ICSE-SEIP52600.2021.00035","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9402087","anomaly detection;malicious commits;supply chain attacks","Ecosystems;Tools;Metadata;Security;Open source software;Software engineering;Software development management","","","","","","33","","7 May 2021","","","IEEE","IEEE Conferences"
"Research and Application of gas reservoir modeling","J. Li; Z. Wang; ZhangZhihuan; JiangDan","China University of Petroleum, Beijing, China; China University of Petroleum, Beijing, China; China University of Petroleum, Beijing, China; Geophysical division of China Oilfield Services Limited, Tianjin, China","2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)","15 Sep 2011","2011","4","","2545","2548","In oil and gas field, we have little logging data in the early of exploration and development, but seismic data is abundance, which can be used to response to geological conditions roundly. For more accurate description of the reservoirs, this paper with the thought of combining seismic and well, making full use of different scales and extend of the seismic, logging, geological interpretation, and making variables statistics and variogram analysis with petrel software for building a three-dimensional geological model of reservoirs, which can be closer to the truth of underground geological. The model is under the control of sedimentary microfacies and reservoir properties, which provided the deployment for the gas reservoir development programs.","","978-1-61284-181-6","10.1109/FSKD.2011.6019952","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6019952","three-dimensional model;sedimentary microfacies;seismic attributes","Solid modeling;Geology;Data models;Reservoirs;Analytical models;Three dimensional displays;Materials","geology;hydrocarbon reservoirs;mining;recording;seismology;statistics","gas reservoir modeling;oil field;gas field;logging data;seismic data;geological conditions;geological interpretation;statistics;variogram analysis;petrel software;three-dimensional geological model","","","","6","","15 Sep 2011","","","IEEE","IEEE Conferences"
"From Anticipation to Action: Data Reveal Mobile Shopping Patterns During a Yearly Mega Sale Event in China","M. Guan; M. Cha; Y. Li; Y. Wang; J. Sun","Electronic Engineering, Tsinghua University, 12442 Beijing, Beijing China 100084 (e-mail: guanmz14@mails.tsinghua.edu.cn); School of Computing, Korea Advanced Institute of Science and Technology, 34968 Beijing, Daejeon Korea (the Republic of) (e-mail: meeyoung.cha@gmail.com); Department of Electronic Engineering, Tsinghua University, Beijing, Beijing China (e-mail: liyong07@tsinghua.edu.cn); Department of Electronic Engineering, Tsinghua University, Beijing, Beijing China (e-mail: wangyue@tsinghua.edu.cn); Software, China Telecom Corp Ltd Beijing Research Institute, 127242 Beijing, Beijing China (e-mail: sunjb.bri@chinatelecom.cn)","IEEE Transactions on Knowledge and Data Engineering","","2020","PP","99","1","1","The online retail market shows a sharp increase in traffic during holiday sales. The ability to distinguish customers who will likely purchase is critical for provisioning traffic and for providing cost-effective promotions. This paper uniquely studies the browsing and purchasing behaviors of online shoppers during a yearly sale event in China, the world's largest online marketplace. Based on 31 million action logs gathered from wide residential areas, we characterize the steps leading to purchases and determine their precursors. We investigate the effect of time (e.g., date, time of date), environment (e.g., platform, viewed category), and action (e.g., session time, clicks, sequence) on purchases. Action cues from shopping behaviors can be used for early detection. Within 30 seconds of any browsing session, we are able to predict which sessions will result in purchases with a high accuracy of 92.4%. The findings in this paper provide an understanding of traffic during mega sale events that can help online shops plan and provide a better user experience for upcoming shopping festivals.","1558-2191","","10.1109/TKDE.2020.3001558","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9115271","Online shopping festival;user modeling;purchase prediction","Electronic commerce;Predictive models;Social networking (online);Business;Data models;Sun;Bandwidth","","","","","","","","11 Jun 2020","","","IEEE","IEEE Early Access Articles"
"DTMSim-IoT: A Distributed Trust Management Simulator for IoT Networks","S. W. Abbas Hamdani; A. Waheed Khan; N. Iltaf; W. Iqbal","National University of Sciences and Technology,Department of Computer Software Engineering,Islamabad,Pakistan,44000; National University of Computer and Emerging Sciences,School of Computing,Islamabad,Pakistan,44000; National University of Sciences and Technology,Department of Computer Software Engineering,Islamabad,Pakistan,44000; National University of Sciences and Technology,Department of Information Security,Islamabad,Pakistan,44000","2020 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)","11 Nov 2020","2020","","","491","498","In recent years, several trust management frame-works and models have been proposed for the Internet of Things (IoT). Focusing primarily on distributed trust management schemes; testing and validation of these models is still a challenging task. It requires the implementation of the proposed trust model for verification and validation of expected outcomes. Nevertheless, a stand-alone and standard IoT network simulator for testing of distributed trust management scheme is not yet available. In this paper, a .NET-based Distributed Trust Management Simulator for IoT Networks (DTMSim-IoT) is presented which enables the researcher to implement any static/dynamic trust management model to compute the trust value of a node. The trust computation will be calculated based on the direct-observation and trust value is updated after every transaction. Transaction history and logs of each event are maintained which can be viewed and exported as .csv file for future use. In addition to that, the simulator can also draw a graph based on the .csv file. Moreover, the simulator also offers to incorporate the feature of identification and mitigation of the On-Off Attack (OOA) in the IoT domain. Furthermore, after identifying any malicious activity by any node in the networks, the malevolent node is added to the malicious list and disseminated in the network to prevent potential On-Off attacks.","","978-1-7281-6609-4","10.1109/DASC-PICom-CBDCom-CyberSciTech49142.2020.00091","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9251132","Distributed Trust Management;IoT;On-Off Attack;Weighted-Sum;IoT Security;IoT Simulator","Focusing;Internet of Things;History;Trust management;Task analysis;Standards;Testing","computer network security;Internet of Things;trusted computing;ubiquitous computing","distributed trust management scheme;IoT Networks;DTMSim-IoT;trust value;trust computation;IoT domain;IoT networks;trust management framework;trust model;NET-based distributed trust management simulator;IoT network simulator;on-off attack;malevolent node","","1","","17","","11 Nov 2020","","","IEEE","IEEE Conferences"
"Inferring Directed Static Networks of Influence from Undirected Temporal Networks","T. Takaguchi; N. Sato; K. Yano; N. Masuda","Global Res. Center for Big Data Math., Nat. Inst. of Inf., Tokyo, Japan; Central Res. Lab., Hitachi, Ltd., Kokubunji, Japan; Central Res. Lab., Hitachi, Ltd., Kokubunji, Japan; Dept. of Math. Inf., Univ. of Tokyo, Tokyo, Japan","2013 IEEE 37th Annual Computer Software and Applications Conference","31 Oct 2013","2013","","","155","156","A temporal network consists of a time series of interaction events, each of which is defined by a triplet composed of the indices of two nodes and the time of the event. Mapping a temporal network to a more tractable static network is often useful. A mapping method was recently proposed on the basis of the so-called transfer entropy (G. V. Steeg and A. Galstyan, in Proc. the 21st Int. Conf. WWW, p.509, 2012). In the proposed method, one generates the directed network of influence in which a directed link represents the causal relationship between activity patterns at two nodes. However, the significance of the inferred links and the sensitivity of results to the parameter values are still unclear. We propose a bootstrap sampling method to statistically configure the directed network of influence. We apply our method to the face-to-face interaction logs between office workers in Japanese companies.","0730-3157","978-0-7695-4986-6","10.1109/COMPSAC.2013.24","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6649811","complex networks;temporal networks;transfer entropy;data analysis","Communities;Companies;Entropy;Time series analysis;Informatics;Joints;Probability","complex networks;directed graphs;network theory (graphs);time series","directed static networks;undirected temporal networks;interaction events;time series;bootstrap sampling method;face-to-face interaction logs;office workers;Japanese companies","","2","","8","","31 Oct 2013","","","IEEE","IEEE Conferences"
"Designing a Graphical Domain-Specific Modelling Language Targeting a Filter-Based Data Analysis Framework","C. Köllner; G. Dummer; A. Rentschler; K. D. Müller-Glaser","Dept. of Embedded Syst. & Sensors Eng. (ESS), FZI Res. Center for Inf. Technol. Karlsruhe, Karlsruhe, Germany; Dept. of Embedded Syst. & Sensors Eng. (ESS), FZI Res. Center for Inf. Technol. Karlsruhe, Karlsruhe, Germany; Dept. of Embedded Syst. & Sensors Eng. (ESS), FZI Res. Center for Inf. Technol. Karlsruhe, Karlsruhe, Germany; Dept. of Embedded Syst. & Sensors Eng. (ESS), FZI Res. Center for Inf. Technol. Karlsruhe, Karlsruhe, Germany","2010 13th IEEE International Symposium on Object/Component/Service-Oriented Real-Time Distributed Computing Workshops","7 Jun 2010","2010","","","152","157","We demonstrate the application of a Model-Driven Software Development (MDSD) methodology using the example of an analysis framework designed for a data logging device in the field of vehicle testing. This mobile device is capable of recording the data traffic of automotive-specific bus systems like Controller Area Network (CAN), Local Interconnect Network (LIN), FlexRay and Media Orientied Systems Transport (MOST) in real-time. In order to accelerate the subsequent analysis of the tremendous amount of data, it is advisable to pre-filter the recorded log data on device, during the test-drive. To enable the test engineer of creating data analyses we built a component-based library on top of the languages System{C}/C++. Problematic with this approach is that still substantial programming knowledge is required for implementing filter algorithms, which is usually not the domain of a vehicle test engineer. In a next step we developed a graphical modelling language on top of our library and a graphical editor. The editor is able of verifying a model as well as of generating source code which eliminates the need of manually implementing a filter algorithm. In our contribution we show the design of the graphical language and the editor using the Eclipse platform and the Graphical Modelling Framework (GMF). We describe the automatic extraction of meta-information, such as available components, their interfaces and categorization annotations by parsing the library's C++ implementation with the help of Xtext. The editor will use that information to build a dedicated tool palette providing components that the designer can instantiate and interconnect using drag-and-drop.","","978-1-4244-7219-2","10.1109/ISORCW.2010.33","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5479515","graphical DSL;component-based modelling","Data analysis;Libraries;Vehicles;Automotive engineering;Filters;Application software;Programming;Software testing;Communication system traffic control;Traffic control","automobile industry;automobiles;C++ language;data analysis;field buses;mechanical engineering computing;simulation languages;software engineering;visual languages","graphical domain-specific modelling language design;filter-based data analysis framework;model-driven software development methodology;data logging device;vehicle testing;automotive-specific bus systems;controller area network;local interconnect network;FlexRay;media oriented system transport;component-based library;C++ language parsing;System C language;vehicle test engineer;source code;Eclipse platform;graphical modelling framework;meta-information automatic extraction;categorization annotations","","2","","15","","7 Jun 2010","","","IEEE","IEEE Conferences"
"A Method for Test Cases Reduction in Web Application Testing Based on User Session","S. Wang; W. Wu; J. Sun","Xi'an University of Posts & Telecommunications, School of Computer Science & Technology, Xi'an, China; Xi'an University of Posts & Telecommunications, School of Computer Science & Technology, Xi'an, China; Xi'an University of Posts & Telecommunications, School of Computer Science & Technology, Xi'an, China","2018 International Conference on Networking and Network Applications (NaNA)","24 Feb 2019","2018","","","378","383","With the development of web application, the internal structure of the web application was more complicated. The software testing method based on user session has been used to verify the quality of web application. However, the amount of real user session data were extremely large. In order to reduce the scale of the test case set for web application testing, we proposed a reduced method of test cases based on user session. First, we cleaned the original log file and identified user session and sorted the user sessions according to the Gini index. Then, we made an “OR” operation on the all user sessions coverage to obtain some user sessions that cover all pages of web application. These user sessions were a set of reduced test cases. Finally, the structure dependency graph of web application was generated according to the user sessions. The PageRank algorithm and the Hamming distance are used to optimize the test cases reduced with the same Gini index. By this means, we obtained the final test cases set for reduction. Through experimental verification, we found that the reduction rate of the test cases reached 85.7% and the fault coverage rate reached 100% by our method. Therefore, the method we proposed in this paper improved effectively the reliability of web application and the effectiveness of software testing and reduced the cost of software testing.","","978-1-5386-8303-3","10.1109/NANA.2018.8648767","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8648767","user session;Gini index;PageRank algorithm;Hamming distance;test cases reduction","Indexes;Web pages;Software testing;Software;Software reliability","graph theory;Internet;program testing;software fault tolerance","user session data;software testing;test cases reduction;Web application testing;structure dependency graph;PageRank algorithm;Hamming distance;Gini index;software faults","","","","16","","24 Feb 2019","","","IEEE","IEEE Conferences"
"A Big Data MapReduce Hadoop distribution architecture for processing input splits to solve the small data problem","Manjunath R.; Tejus; Channabasava R.K; Balaji S.","Dept. of CSE, City Engineering College, Kanakapura Road, Doddakallasandra, Bengaluru, Kamataka - 560062, India; Dept. of CSE, City Engineering College, Kanakapura Road, Doddakallasandra, Bengaluru, Kamataka - 560062, India; Dept. of ISE, City Engineering College, Kanakapura Road, Doddakallasandra, Bengaluru, Kamataka - 560062, India; CET, Jain University, India","2016 2nd International Conference on Applied and Theoretical Computing and Communication Technology (iCATccT)","27 Apr 2017","2016","","","480","487","Hadoop deals with big data which is an open source Java framework. There are two core components in it namely: HDFS (Hadoop distributed file system) is the ability of a system to continue normal operation against hardware or software faults using inexpensive hardware and which stocks huge extent of data another one is MapReduce is a processing technique and programming model done in lateral and scattered manner. Hadoop does not perform well for short data because huge amount of short data could be greater task on the NameNode of HDFS which inturn its execution time is prolonged for which MapReduce is encountered. While dealing with great amount of short data as it is particularly designed to handle huge amount of data, hadoop experienced with a performance cost. This analysis permits the indetail description of HDFS, actual ways to deal with the problems along with proposed approach to handle short data files and short data file problems. In proposed approach, small files are merged using programming model on hadoop known as MapReduce. By this approach of Hadoop performance of handling small files which is larger than block size is improved. We also propose a Traffic analyzer with the combination of Hadoop and Map-Reduce paradigm. The joint of Hadoop and MapReduce programming tools makes it possible to provide batch analysis in minimum response time and in memory computing capacity in order to process log in a high available, efficient and stable way.","","978-1-5090-2399-8","10.1109/ICATCCT.2016.7912048","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7912048","Hadoop;MapReduce;input splits","Computer architecture;Robustness;Big Data;Databases;Servers;Java","Big Data;Java;parallel processing;public domain software","Big Data MapReduce Hadoop distribution architecture;processing input splits;open source Java framework;HDFS;Hadoop distributed file system;hardware faults;software faults;inexpensive hardware;programming model;NameNode;short data files;short data file problems;Hadoop performance;traffic analyzer;MapReduce programming tools;batch analysis;minimum response time;memory computing capacity","","4","","21","","27 Apr 2017","","","IEEE","IEEE Conferences"
"End-to-End QoS Prediction of Vertical Service Composition in the Cloud","R. Karim; C. Ding; A. Miri","Dept. of Comput. Sci., Ryerson Univ., Toronto, ON, Canada; Dept. of Comput. Sci., Ryerson Univ., Toronto, ON, Canada; Dept. of Comput. Sci., Ryerson Univ., Toronto, ON, Canada","2015 IEEE 8th International Conference on Cloud Computing","20 Aug 2015","2015","","","229","236","In a cloud-based service selection system, for a given request, there could be a large number of software services matching the functional requirements. The selection should then be done based on their QoS values. Since in a cloud environment, a software service might need collaboration from other types of cloud services (e.g., A software service delivered through an infrastructure service) to offer a complete solution to an end user, the selection system should have a way to measure the QoS values of the whole solution, instead of QoS of software services alone. This kind of end-to-end QoS values of cloud-based software solutions may or may not be available in recorded history logs. In this paper, we propose a model for predicting end-to-end QoS values of cloud-based software solutions composed of services from multiple cloud layers. It relies on the internal features of services and end users such as locations, configurations, functionality, and user profiles to calculate service similarity and then predict QoS values. The experiments demonstrate the accuracy of our approach. We also studied the impact of the proposed internal features on QoS prediction accuracy.","2159-6190","978-1-4673-7287-9","10.1109/CLOUD.2015.39","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7214049","QoS Prediction;Vertical Service Composition;Cloud-based Software Service Selection;Service Similarity","Quality of service;Software as a service;Clustering algorithms;Accuracy;Predictive models;Prediction algorithms","cloud computing;quality of service;software engineering","end-to-end QoS prediction;vertical service composition;cloud-based service selection system;software services matching;cloud-based software solutions;multiple cloud layers","","8","","19","","20 Aug 2015","","","IEEE","IEEE Conferences"
"The development and implementation of eLog technology for the local and international fishing industry","A. Barkail","1Olrac, Silvermine House, Steenberg Office Park, Tokai, Cape Town, 7945, South Africa","2011 IST-Africa Conference Proceedings","19 Dec 2011","2011","","","1","15","Fisheries management is continually frustrated by the lack, or poor quality, of critical data on fishing operations. The only way to accurately monitor at-sea activities and to harness the vast data-gathering and observation potential of fishers, is to introduce electronic data recording and reporting technology to the fishing industry in particular, and to the marine environment in general. In response, OLRAC, a South African company, developed a fisheries data-logging software, Olfish, capable of collecting, analyzing, plotting, mapping, reporting, tracing and transmitting all data related to fishing operations. The aim was to create a tool that would benefit both regulation authorities and commercial fishing companies. The advantages of electronic logbooking include near real time reporting, better stock assessments, management decisions and traceability amongst others. Olfish can be used by skippers, fleet/company managers, offshore mariculture farmers, scientists, observers, compliance inspectors and fisheries management authorities.","","978-1-905824-26-7","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6107384","electronic logbook (eLog);electronic reporting system (ERS);data management;fisheries management;predictive analysis","Aquaculture;Companies;Software;Real time systems;Databases;Monitoring","data acquisition;data analysis;data loggers;data recording;decision making;fishing industry;legislation;marine systems;real-time systems;stock control","eLog technology;local fishing industry;international fishing industry;fishing operations;at-sea activities monitoring;data-gathering potential;observation potential;electronic data recording;reporting technology;marine environment;OLRAC;South African company;fisheries data-logging software;Olfish;regulation authority;commercial fishing company;electronic logbooking;real time reporting;stock assessments;management decisions;traceability;skippers;fleet managers;company managers;offshore mariculture farmers;scientists;observers;compliance inspectors;fisheries management authority","","","","2","","19 Dec 2011","","","IEEE","IEEE Conferences"
"A Dynamic Proxy Based Crawler Strategy for Data Collection on CyberGIS","S. Yu; W. Sun; M. Jia","Sch. of Software, Dalian Univ. of Technol., Dalian, China; Sch. of Software, Dalian Univ. of Technol., Dalian, China; Sch. of Software, Dalian Univ. of Technol., Dalian, China","2018 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery (CyberC)","21 Feb 2019","2018","","","483","4834","With the development of geographic information system, digital earth and digital city play more and more important roles in life. The data generated by sensors or other edge nodes need to be collected by crawlers in the distributed systems in IoT, such as the GIS data in CyberGIS. In some edge networks, network operators have adopted methods to limit crawlers, such as blocking the request IP addresses, requiring logging in verification codes and other measures to avoid disturbance to servers. To collect data from web servers in these types of edge networks, a dynamic IP address based strategy DP-crawler is proposed to solve the anti-crawler strategies in the edge networks. DP-crawler can dynamic get proper IP addresses from a security-aware list and select the best available proxies. The security-aware list is designed to use the block-chain. Security and dynamic storage can be achieved by this method. DP-crawler is used to crawler webs, and the detailed information of Douban movies are obtained in the experiments. The experiment results show that the DP-Crawler can get more information by using the DP-Crawler.","","978-1-7281-0974-9","10.1109/CyberC.2018.00094","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8644687","CyberGIS;Edge-Computing;Block-Chain;Web-Crawler","Conferences;Distributed computing;Knowledge discovery","computer network security;file servers;geographic information systems;Internet of Things;IP networks","digital city;anticrawler strategies;IoT;Web servers;verification codes;dynamic proxy;dynamic storage;security-aware list;strategy DP-crawler;dynamic IP address;request IP addresses;edge networks;GIS data;distributed systems;edge nodes;digital earth;geographic information system;CyberGIS;data collection","","","","11","","21 Feb 2019","","","IEEE","IEEE Conferences"
"Research on timely recovery technology of memory database","Gui-Ming Liao; Jian-Ping Li","School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, 611731, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, 611731, China","2012 International Conference on Wavelet Active Media Technology and Information Processing (ICWAMTIP)","17 Jan 2013","2012","","","268","271","The recovery system of MMDB which based on the traditional recovery techniques cannot meet the real-time requirements of the transaction, and it is not efficiency. To satisfy the real-time requirements of the memory database, and shorten the time of system response, realize quick recovery, this paper studies on the algorithm, designs a kind of system recovery model which based on amending of log drive, mode of dynamic checkpoint and fast recovery algorithms. Finally, the performance analysis of this system was given.","","978-1-4673-4683-2","10.1109/ICWAMTIP.2012.6413491","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6413491","On Time;Record;Detection;Recovery","Databases;Abstracts;Drives","checkpointing;database management systems;software performance evaluation","timely recovery technology;MMDB;memory database;system response;system recovery model;log drive;dynamic checkpoint;fast recovery algorithms;performance analysis","","","","8","","17 Jan 2013","","","IEEE","IEEE Conferences"
"Client Request Analysis Tool for CERN ALICE Grid Services","C. Margineanu; C. Grigoras; M. Carabas; S. Weisz; D. Mihai; M. -E. Mihailescu; N. Tapus","University Politehnica of Bucharest,Faculty of Automatic Control and Computer Science,Bucharest,Romania; Alice Cem,Geneva,Switzerland; University Politehnica of Bucharest,Faculty of Automatic Control and Computer Science,Bucharest,Romania; University Politehnica of Bucharest,Faculty of Automatic Control and Computer Science,Bucharest,Romania; University Politehnica of Bucharest,Faculty of Automatic Control and Computer Science,Bucharest,Romania; University Politehnica of Bucharest,Faculty of Automatic Control and Computer Science,Bucharest,Romania; University Politehnica of Bucharest,Faculty of Automatic Control and Computer Science,Bucharest,Romania","2020 International Conference on Computing and Data Science (CDS)","9 Dec 2020","2020","","","462","467","ALICE is an experiment hosted at the CERN facilities in Geneva. The data collected by the experiment is distributed in multiple geo-locations. ALICE software development team is managing the technologies that offer a unitary view to the distributed resources. AliEn API Services and JAliEn Services are used by the researchers to access and analyze the collected data. The ALICE developers cannot analyze the client-server interactions, while the only information available is gathered in the log files. The main requirement of the project is to provide an analysis tool to extract vital information out the log files. The tool is tested against log files generated by the ALICE production servers and is deployed on a custom ALICE-owned instance. As of June 2020, the project is being used by the developers to extract near real-time viable requests information and have a correct picture of the researcher's interaction with the services.","","978-1-7281-7106-7","10.1109/CDS49703.2020.00097","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9275943","distributed;ALICE experiment;analyse;logs","6G mobile communication;Integrated circuits;Data science","application program interfaces;client-server systems;grid computing;middleware;user interfaces","real-time viable requests information;custom ALICE-owned instance;ALICE production servers;vital information;log files;client-server interactions;ALICE developers;JAliEn Services;AliEn API Services;distributed resources;unitary view;ALICE software development team;multiple geo-locations;CERN facilities;CERN ALICE grid Services;client request analysis tool","","","","7","","9 Dec 2020","","","IEEE","IEEE Conferences"
"A Monitoring System Design Program Based on B/S Mode","L. Yucheng; L. Yubin","Coll. of Electron. Inf. Eng., Chongqing Univ. of Sci. & Technol., Chongqing, China; Comput. Sci. Sch., Panzhihua Univerisity, Panzhihua, China","2010 International Conference on Intelligent Computation Technology and Automation","26 Jul 2010","2010","1","","184","187","Aiming at many monitoring systems focusing on real-time monitoring and so being difficult to more directly and conveniently understand the software performance issues by the historical data analysis and remote monitoring, this paper presents a server performance monitoring system program based on B/S mode. The program is divided into two main functional modules: the monitoring of current use of resources and the log view. The former achieves remote real-time monitoring of system resources during the operation on the server and records monitoring data in the database. The Latter provides reference of debugging and maintenance for managers through Ajax Asynchronous calling the monitoring data in the database and the use of Silverlight to form wave maps by drawing curves in the client-side script. That system uses B/S mode enables administrators to view the server-side situation in the performance testing and network maintenance. Simulation experiment results prove the feasibility of the program.","","978-1-4244-7280-2","10.1109/ICICTA.2010.106","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5523500","historical data analysis;server performance;SQL server;Browser/Server mode;monitoring system","Computerized monitoring;Remote monitoring;Databases;System testing;Software performance;Quality management;Data analysis;Network servers;Computer displays;Paper technology","client-server systems;data analysis;Java;program debugging;software maintenance;software performance evaluation;system monitoring","monitoring system design program;B/S mode;software performance;historical data analysis;remote monitoring;server performance monitoring system program;system resource monitoring;database;Ajax;Silverlight;client-side script;network maintenance;performance testing","","1","","11","","26 Jul 2010","","","IEEE","IEEE Conferences"
"Process-Aware Enterprise Social Network Prediction and Experiment Using LSTM Neural Network Models","D. -L. Pham; H. Ahn; K. -S. Kim; K. P. Kim","Data and Process Engineering Research Laboratory, Division of Computer Science and Engineering, Contents Convergence Software Research Institute, Kyonggi University, Suwon, South Korea; Data and Process Engineering Research Laboratory, Division of Computer Science and Engineering, Contents Convergence Software Research Institute, Kyonggi University, Suwon, South Korea; Data and Process Engineering Research Laboratory, Division of Computer Science and Engineering, Contents Convergence Software Research Institute, Kyonggi University, Suwon, South Korea; Data and Process Engineering Research Laboratory, Division of Computer Science and Engineering, Contents Convergence Software Research Institute, Kyonggi University, Suwon, South Korea","IEEE Access","19 Apr 2021","2021","9","","57922","57940","Process mining that exploits system event logs provides significant information regarding operating events in an organization. By discovering process models and analyzing social network metrics created throughout the operation of the information system, we can better understand the roles of performers and characteristics of activities, and more easily predict what will occur in the next operation of a system. By using accurate and valuable predicted information, we can create effective environments, provide suitable materials to perform activities better, and facilitate more efficient operations. In this study, we apply the long short-term memory, a variant of the recurrent neural network, to predict the enterprise social networks that are formed through information regarding a business system’s operation. More precisely, we apply the multivariate multi-step long short-term memory model to predict not only the next activity and next performer, but also all the variants of a process-aware enterprise social network based on the next performer predictions using a probability threshold. Furthermore, we conduct an experimental evaluation on the real-life event logs and compare our results with some related researches. The results indicate that our approach creates a useful model to predict an enterprise social network and provides metrics to improve the operation of an information system based on the predicted information.","2169-3536","","10.1109/ACCESS.2021.3071789","Basic Science Research Program through the National Research Foundation of Korea (NRF); Ministry of Education; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9399143","Process mining;long short-term memory neural network;process-aware enterprise social network;next event prediction","Predictive models;Mathematical model;Social networking (online);Training;Recurrent neural networks;Modulation;Data models","","","","","","41","CCBY","8 Apr 2021","","","IEEE","IEEE Journals"
"Leveraging Hardware-Assisted Virtualization for Deterministic Replay on Commodity Multi-Core Processors","S. Ren; L. Tan; C. Li; Z. Xiao; W. Song","Department of Computer Science, Peking University, Beijing, China; Department of Computer Science, Peking University, Beijing, China; Department of Computer Science, Peking University, Beijing, China; Department of Computer Science, Peking University, Beijing, China; Department of Computer Science, Cornell University, Ithaca, NY","IEEE Transactions on Computers","11 Dec 2017","2018","67","1","45","58","Deterministic replay, which provides the ability to travel backward in time and reconstruct the past execution flow of a multiprocessor system, has many prominent applications. Prior research in this area can be classified into two categories: hardware-only schemes and software-only schemes. While hardware-only schemes deliver high performance, they require significant modifications to the existing hardware. In contrast, software-only schemes work on commodity hardware, but suffer from excessive performance overhead and huge logs. In this article, we present the design and implementation of a novel system, Samsara, which uses the hardware-assisted virtualization (HAV) extensions to achieve efficient deterministic replay without requiring any hardware modification. Unlike prior software schemes which trace every single memory access to record interleaving, Samsara leverages HAV on commodity processors to track the read-set and write-set for implementing a chunk-based recording scheme in software. By doing so, we avoid all memory access detections, which is a major source of overhead in prior works. Evaluation results show that compared with prior software-only schemes, Samsara significantly reduces the log file size to 1/70th on average, and further reduces the recording overhead from about 10x, reported by state-of-the-art works, to 2.1x on average.","1557-9956","","10.1109/TC.2017.2727492","National Natural Science Foundation of China; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7982675","Deterministic replay;virtualization;multi-core","Program processors;Hardware;Protocols;Virtualization;Computer architecture;Virtual machine monitors","multiprocessing systems;virtualisation","hardware-assisted virtualization;software-only schemes;chunk-based recording scheme;hardware-only schemes;multiprocessor system;commodity multicore processors;Samsara leverages HAV;hardware modification;virtualization extensions","","2","","41","Traditional","17 Jul 2017","","","IEEE","IEEE Journals"
"Using Declarative Specification to Improve the Understanding, Extensibility, and Comparison of Model-Inference Algorithms","I. Beschastnikh; Y. Brun; J. Abrahamson; M. D. Ernst; A. Krishnamurthy","Department of Computer Science, University of British Columbia, Vancouver, BC, Canada; School of Computer Science, University of Massachusetts, Amherst, MA; Facebook Inc., Seattle, WA; Computer Science & Engineering, University of Washington, Seattle, WA; Computer Science & Engineering, University of Washington, Seattle, WA","IEEE Transactions on Software Engineering","14 Apr 2015","2015","41","4","408","428","It is a staple development practice to log system behavior. Numerous powerful model-inference algorithms have been proposed to aid developers in log analysis and system understanding. Unfortunately, existing algorithms are typically declared procedurally, making them difficult to understand, extend, and compare. This paper presents InvariMint, an approach to specify model-inference algorithms declaratively. We applied the InvariMint declarative approach to two model-inference algorithms. The evaluation results illustrate that InvariMint (1) leads to new fundamental insights and better understanding of existing algorithms, (2) simplifies creation of new algorithms, including hybrids that combine or extend existing algorithms, and (3) makes it easy to compare and contrast previously published algorithms. InvariMint's declarative approach can outperform procedural implementations. For example, on a log of 50,000 events, InvariMint's declarative implementation of the kTails algorithm completes in 12 seconds, while a procedural implementation completes in 18 minutes. We also found that InvariMint's declarative version of the Synoptic algorithm can be over 170 times faster than the procedural implementation.","1939-3520","","10.1109/TSE.2014.2369047","NSERC; Google; Microsoft Research via a SEIF; DARPA; NSF; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6951474","Model inference;API mining;specification mining;process mining;declarative specification;inference understanding;inference extensibility;inference comparison;InvariMint;kTails;synoptic;Model inference;API mining;specification mining;process mining;declarative specification;inference understanding;inference extensibility;inference comparison;InvariMint;kTails;synoptic","Inference algorithms;Postal services;Electronic mail;Algorithm design and analysis;Software algorithms;Educational institutions;Approximation algorithms","formal specification;inference mechanisms;system monitoring","log system behavior analysis;model inference algorithm specification;system understanding;InvariMint declarative specification approach;kTails algorithm;synoptic algorithm","","21","","42","","10 Nov 2014","","","IEEE","IEEE Journals"
"Learning Marked Markov Modulated Poisson Processes for Online Predictive Analysis of Attack Scenarios","L. Carnevali; F. Santoni; E. Vicario",University of Florence; University of Florence; University of Florence,"2019 IEEE 30th International Symposium on Software Reliability Engineering (ISSRE)","10 Feb 2020","2019","","","195","205","Runtime predictive analysis of quantitative models can support software reliability in various application scenarios. The spread of logging technologies promotes approaches where such models are learned from observed events. We consider a system visiting transient states of a hidden process until reaching a final state and producing observations with stochastic arrival times and types conditioned by visited states, and we abstract it as a marked Markov modulated Poisson Process (MMMPP) with left-to right structure. We present an Expectation-Maximization (EM) algorithm that learns the MMMPP parameters from observation sequences acquired in repeated execution of the transient behavior, and we use the model at runtime to infer the current state of the process from actual observed events and to dynamically evaluate the remaining time to the final state. The approach is illustrated using synthetic datasets generated from a stochastic attack tree of the literature enriched with an observation model associating each state with an expected statistics of observation types and arrival times. Accuracy of prediction is evaluated under different variability of hidden states sojourn durations and of the observations arrival process, and compared against previous literature that mainly exploits either the timing or the types of observed events.","2332-6549","978-1-7281-4982-0","10.1109/ISSRE.2019.00028","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8987449","Marked Markov Modulated Poisson Processes, Expectation-Maximization algorithm, online failure prediction, state-dependent event types and times, attack trees","","expectation-maximisation algorithm;learning (artificial intelligence);Markov processes;security of data;software reliability","marked Markov modulated Poisson processes;online predictive analysis;attack scenarios;runtime predictive analysis;software reliability;stochastic arrival times;expectation-maximization algorithm;MMMPP parameters;transient behavior;stochastic attack tree;hidden states sojourn durations;observations arrival process","","","","50","","10 Feb 2020","","","IEEE","IEEE Conferences"
"Providing online operational support for distributed, security sensitive electronic business processes","M. Talamo; A. Povilionis; F. Arcieri; C. H. Schunck","University of Rome Tor Vergata, Rome, Italy; University of Rome Tor Vergata, Rome, Italy; Fondazione Inuit, University of Rome Tor Vergata, Rome, Italy; Fondazione Inuit, University of Rome Tor Vergata, Rome, Italy","2015 International Carnahan Conference on Security Technology (ICCST)","25 Jan 2016","2015","","","49","54","Online process mining techniques are increasingly used to provide operational support. In this work we describe tools to support distributed business processes which handle sensitive data and require a high level of security together with real-time validation. The techniques presented here have been specifically developed for real-time compliance checking of distributed processes in choreographies of heterogeneous entities. Challenges include the fast aggregation, analysis and validation of process logs that are collected from the distributed participants. The autonomy of the participating entities has to be respected and no sensitive data pertaining to the content of the individual transactions must be accessed for process support and validation purposes. A validation authority for process monitoring and validation is set up. Together with software agents dispatched to the participating entities the validation authority collects events in a central log and then analyzes these events using a particular representation of the process in form of a validation tree to detect and resolve anomalies. We describe the application of these technologies in a distributed business process with more than 400,000 daily process executions. The business process is supported by a help desk managing and responding to incidents and anomalies. We observe a reduction of 90% of the calls to the help desk and an average reduction of 15% in call length. Further the help desk was enabled to act pro-actively, calling participants to the process even before they became aware of anomalies that affected their organization.","2153-0742","978-1-4799-8691-0","10.1109/CCST.2015.7389656","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7389656","real-time business process validation;compliance checking;process mining;operational support;IT security;IT service desk automation","Business;Security;Monitoring;Automation;Real-time systems;Software agents;Data mining","business data processing;data mining;security of data","online operational support;distributed-security sensitive electronic business processes;online process mining techniques;sensitive data handling;security level;real-time compliance checking;heterogeneous entities;process log aggregation;process log analysis;process log validation;validation authority;process monitoring;software agent dispatch;central log;validation tree;process executions;average call length reduction","","1","","30","","25 Jan 2016","","","IEEE","IEEE Conferences"
"NSTX-U Advances in Real-Time C++11 on Linux","K. G. Erickson","Plasma Physics Lab, Princeton University, Princeton, United States","IEEE Transactions on Nuclear Science","14 Aug 2015","2015","62","4","1758","1765","Programming languages like C and Ada combined with proprietary embedded operating systems have dominated the real-time application space for decades. The new C++11 standard includes native, language-level support for concurrency, a required feature for any nontrivial event-oriented real-time software. Threads, Locks, and Atomics now exist to provide the necessary tools to build the structures that make up the foundation of a complex real-time system. The National Spherical Torus Experiment Upgrade (NSTX-U) at the Princeton Plasma Physics Laboratory (PPPL) is breaking new ground with the language as applied to the needs of fusion devices. A new Digital Coil Protection System (DCPS) will serve as the main protection mechanism for the magnetic coils, and it is written entirely in C++11 running on Concurrent Computer Corporation's real-time operating system, RedHawk Linux. It runs over 600 algorithms in a 5 kHz control loop that determine whether or not to shut down operations before physical damage occurs. To accomplish this, NSTX-U engineers developed software tools that do not currently exist elsewhere, including real-time atomic synchronization, real-time containers, and a real-time logging framework. Together with a recent (and carefully configured) version of the GCC compiler, these tools enable data acquisition, processing, and output using a conventional operating system to meet a hard real-time deadline (that is, missing one periodic is a failure) of 200 microseconds.","1558-1578","","10.1109/TNS.2015.2448106","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7181739","Computer languages;real-time systems;software design","Real-time systems;Message systems;Linux;Concurrent computing;Timing;Kernel","C language;Linux;software engineering","NSTX-U;Linux;real-time C++11;embedded operating system;language-level support;nontrivial event-oriented real-time software;National Spherical Torus Experiment Upgrade;Princeton Plasma Physics Laboratory;PPPL;digital coil protection system;DCPS;Concurrent Computer Corporation;real-time atomic synchronization;real-time containers;real-time logging framework;data acquisition","","1","","15","","6 Aug 2015","","","IEEE","IEEE Journals"
"Automated Traffic Signal Performance Measures: Features and Applications","Y. Zhang; L. Cheng; F. Qiao; A. Patel","College of Technology, University of Houston,Houston,Texas,USA; College of Technology, University of Houston,Houston,Texas,USA; Texas Southern University,Dep. of Transportation Studies,Houston,Texas,USA; University of Houston,Dep. of Computer Science,Houston,Texas,USA","2019 6th International Conference on Systems and Informatics (ICSAI)","27 Feb 2020","2019","","","70","75","Automated Traffic Signal Performance Measures (ATSPMs) are a driving force to furnish traffic signal controllers with high-solution information logging abilities and use this data to produce performance measures. These measures enable traffic engineers to improve activities just as to keep up and work their frameworks in a protected and effective way. Despite the fact that these measures have changed the manner in which that administrators deal with their frameworks, a few weaknesses of the instrument are an absence of information quality control and the degree of assets required to appropriately utilize the apparatus for framework-wide administration. In this paper we will determine the level of In-Service Performance Evaluation (ISPE), which can be performed while using the existing database of roadside features, such as highway and traffic data files with maintenance records, roadside feature inventory data, and crash data with respect to existing roadside hardware categories.","","978-1-7281-5256-1","10.1109/ICSAI48974.2019.9010269","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9010269","ATSPM - Automated Traffic Signal Performance Measures;FHWA - Federal Highway Administration","Detectors;Timing;Software;Databases;Road transportation","intelligent transportation systems;maintenance engineering;quality control;road accidents;road safety;road traffic;roads;traffic engineering computing","traffic signal controllers;high-solution information logging abilities;traffic engineers;traffic data files;automated traffic signal performance measures;in-service performance evaluation","","","","7","","27 Feb 2020","","","IEEE","IEEE Conferences"
"Analysis and Diagnosis of SLA Violations in a Production SaaS Cloud","C. D. Martino; D. Chen; G. Goel; R. Ganesan; Z. Kalbarczyk; R. Iyer","Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Syst. Eng., EdgeVerge, Bangalore, India; Syst. Eng., EdgeVerge, Bangalore, India; Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA","2014 IEEE 25th International Symposium on Software Reliability Engineering","15 Dec 2014","2014","","","178","188","This paper investigates SLA violations of a production SaaS platform by means of joint use of field failure data analysis (FFDA) and fault injection. The objective of this study is to diagnose the causes of SLA violations, pinpoint critical failure modes under realistic error assumptions and identify potential means to increase the user perceived availability of the platform and assurance of SLA requirements. We base our study on 283 days of logs obtained during the production time of the platform, while it was employed to process business data received by 42 customers in 22 countries. In this paper, we develop a set of tools that include i) a FFDA toolset used to analyze the data extracted from the platform and by the operating system event logs and ii) a. NET/C++ injector able to automate the injection of specific runtime errors in the production code and the collection of results. Major findings include i) 93% of all service level agreement (SLA) violations were due to system failures, ii) there were a few cases of bursts of SLA violations that could not be diagnosed from the logs and were revealed from the performed injections, and iii) the error injection revealed several error propagation paths leading to data corruptions that could not be detected from the analysis of failure data.","2332-6549","978-1-4799-6033-0","10.1109/ISSRE.2014.26","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6982625","log analysis;empirical reliability;hazard analysis;fault injection;SLA violations;SaaS","Databases;Production;Servers;Business;Data analysis;Availability;Operating systems","cloud computing;contracts;program diagnostics","SLA violation;production SaaS cloud;field failure data analysis;FFDA;fault injection;critical failure mode;operating system event logs;NET/C++ injector;service level agreement;data corruption","","4","","22","","15 Dec 2014","","","IEEE","IEEE Conferences"
"A virtual shared metadata storage for HDFS","J. Zhou; Y. Chen; Xiaoyan Gu; W. Wang; D. Meng","Texas Tech University, Lubbock, USA; Texas Tech University, Lubbock, USA; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China","2015 IEEE International Conference on Networking, Architecture and Storage (NAS)","14 Sep 2015","2015","","","265","274","Hadoop is a popular open-source framework that allows distributed analysis of large datasets using the MapReduce programming model. A distributed file system HDFS is implemented to provide high-throughput access to datasets. HDFS can achieve high performance metadata service but has two disadvantages. First, when the metadata server stores metadata on persistent devices, it is restricted to read and write operations of local disks. Second, it also lacks effective methods for metadata synchronization and replication, which is critical for metadata availability and reliability. In this research, we introduce a novel Virtual Shared Storage Pool (VSSP) concept and design for storing and sharing metadata in HDFS. The VSSP is a virtual storage device which is built on existing servers and transparent to upper layers. Two strategies, a journal synchronization based on the 2PC protocol and a fine-grained image replication, are introduced in the VSSP according to different metadata access features. The VSSP not only reduces the overhead on metadata modification operations, but also improves the I/O performance for namespace storage. Experimental results show that the VSSP improved the average performance by 40.51% and 23.46% when writing logs compared with the BookKeeper and Hadoop QJM. The average image read and write throughput was nearly 5 times and 2.4 times better than NFS and the original approach. These results confirm that the proposed VSSP solution significantly improves the metadata access performance, scalability, and reliability for HDFS.","","978-1-4673-7891-8","10.1109/NAS.2015.7255195","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7255195","Distributed file systems;cluster file systems;HDFS;shared storage;distributed metadata management","Metadata;Synchronization;Protocols;Servers;Reliability;Streaming media;Algorithm design and analysis","data analysis;meta data;parallel processing;public domain software;synchronisation;virtual storage","virtual shared metadata storage;HDFS;open-source framework;distributed dataset analysis;MapReduce programming model;distributed file system;high performance metadata service;metadata server;persistent devices;read operations;write operations;metadata synchronization;metadata availability;metadata reliability;virtual shared storage pool concept;VSSP concept;virtual storage device;journal synchronization;2PC protocol;fine-grained image replication;metadata access features;metadata modification operations;I/O performance;namespace storage;BookKeeper;Hadoop QJM;average image read and write throughput","","1","","28","","14 Sep 2015","","","IEEE","IEEE Conferences"
"Process Mining Algorithms for Clinical Workflow Analysis","B. Tibeme; H. Shahriar; C. Zhang","Department of Information Systems, Kennesaw State University, USA; Department of Information Technology, Kennesaw State University, USA; Department of Information Technology, Kennesaw State University, USA","SoutheastCon 2018","4 Oct 2018","2018","","","1","6","Process Mining focuses on the extraction of knowledge from data generated and stored by information systems. Log level data contains the signatures of executed processes. Many case studies have used process mining to discover the processes for compliance or identifying anomalies in business workflow. In this paper, we apply workflow analysis for a possible clinical setting by leveraging an open source data mining tool named ProM. We apply four available mining algorithms (Alpha, Heuristic, Inductive and Fuzzy miners) and evaluate the outputs that describe a workflow. The work provides a genesis for clinical practitioners on the advantages and disadvantages of applying various algorithms towards a dataset based on event logs.","1558-058X","978-1-5386-6133-8","10.1109/SECON.2018.8479176","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8479176","Process mining;ProM;Data mining;Event logs;Fuzzy Miner;Heuristic Miner;Workflow analysis","Data mining;PROM;Xenon;Tools;Visualization;Petri nets;Analytical models","data mining;fuzzy set theory;medical information systems;public domain software","process mining algorithms;clinical workflow analysis;information systems;log level data;business workflow;open source data mining tool;clinical practitioners;clinical setting;knowledge extraction;ProM;Fuzzy miners;Alpha miners;Heuristic miners;Inductive miners","","","","15","","4 Oct 2018","","","IEEE","IEEE Conferences"
"The Impact of Measurement Time on Subgroup Detection in Online Communities","S. Zeini; T. Göhnert; U. Hoppe; L. Krempel","Univ. Duisburg-Essen, Duisburg, Germany; Univ. Duisburg-Essen, Duisburg, Germany; Univ. Duisburg-Essen, Duisburg, Germany; Max-Planck-Inst. for the Study of Soc., Cologne, Germany","2012 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining","4 Feb 2013","2012","","","389","394","More and more communities use internet based services and infrastructure for communication and collaboration. All these activities leave digital traces that are of interest for research as real world data sources that can be processed automatically or semi-automatically. Since productive online communities (such as open source developer teams) tend to support the establishment of ties between actors who work on or communicate about the same or similar objects, social network analysis is a frequently used research methodology in this field. A typical application of SNA techniques is the detection of cohesive subgroups of actors (also called ""community detection""). A relatively new method for detecting cohesive subgroups is the Clique Percolation Method (CPM), which allows for detecting overlapping subgroups. We have used CPM to analyze data from some open source developer communities (mailing lists and log files) and have compared the results for varied time windows of measurement. The influence of the time span of data capturing/aggregation can be compared to photography: A certain minimal window size is needed to get a clear image with enough ""light"" (i.e. dense enough interaction data), whereas for very long time spans the image will be blurred because subgroup membership will indeed change during the time span (corresponding to a moving target). In this sense, our target parameter is ""resolution"" of subgroup structures. We have identified several indicators for good resolution. Applying these indicators to the different CPM results shows the best resolution is a time span of around 2-3 months. In general, this value will vary for different types of communities with different communication frequency and behavior. Following our findings, an explicit analysis and comparison of the influence of time window for different communities may be used to better adjust analysis techniques for the communities at hand.","","978-1-4673-2497-7","10.1109/ASONAM.2012.70","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6425734","Time Dimension;Clustering;k-cliques;Clique Percolation Method;Community Detection","Communities;Time measurement;Size measurement;Social network services;Licenses;Software;Internet","data mining;Internet;public domain software;social networking (online);software engineering","Internet based service;Internet based infrastructure;collaboration;digital trace;data source;productive online community;open source developer team;actor ties;social network analysis;community detection;cohesive subgroup detection;clique percolation method;CPM;overlapping subgroup detection;data analysis;mailing lists;log files;measurement time window;data capturing;data aggregation;interaction data;subgroup membership;subgroup structure resolution;time span;communication frequency;communication behavior","","5","","18","","4 Feb 2013","","","IEEE","IEEE Conferences"
"Identifying students' behaviors related to internet usage patterns","Rozita Jamili Oskouei","Computer Science & Engineering Department, Motilal Nehru National Institute Of Technology, Allahabad, Up, India","2010 International Conference on Technology for Education","16 Aug 2010","2010","","","232","233","Our purpose in this investigation is to explore an analogy in students' internet usages patterns and their academic performance and personal behavior and identify factors for usage patterns on internet in terms of average time spent per a day, percentage of users visited academic-websites along with their time spent in these websites, and compare with the results of users percentage of non-academic websites usage and their time spent in these category of websites. Our survey declared more details about, which type of each category of websites, are more favorable for students and majority or minority of visited websites belongs to which categories of our categorized websites, and what is the relation between this usage pattern and students academic results? Furthermore we explored behavior based on each student's undertaken program and also semester. Our paper attempt to briefly explain all analogies of students' usage pattern related to academic issues and make a survey as a guidance for future, for planning new techniques to establishing automatically peer-to-peer connections based on educators similar behaviors, furthermore any other new methods for designing more effectible e-learning websites for academic purposes.","","978-1-4244-7361-8","10.1109/T4E.2010.5550104","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5550104","behavior mining;usage pattern;academic performance;access log file;website classification","Data mining;Computer science;Peer to peer computing;Electronic learning;Time measurement;Web and internet services;Performance analysis;Design methodology;Web page design;Software libraries","computer aided instruction;Internet;Web sites","student behaviors identification;Internet usage patterns;academic performance;users visited academic-websites;non-academic websites usage;academic issues;peer-to-peer connections;educators , similar behaviors;e-learning","","","","7","","16 Aug 2010","","","IEEE","IEEE Conferences"
"Suspicious Traffic Detection in SDN with Collaborative Techniques of Snort and Deep Neural Networks","R. M. A. Ujjan; Z. Pervez; K. Dahal","Sch. of Eng. & Comput., Univ. of the West of Scotland, Paisley, UK; Sch. of Eng. & Comput., Univ. of the West of Scotland, Paisley, UK; Sch. of Eng. & Comput., Univ. of the West of Scotland, Paisley, UK","2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)","24 Jan 2019","2018","","","915","920","Software Defined Networks (SDN) with OpenFlow (OF) based protocol tend to transform traditional network architecture to vendor independent architecture with data-plane and control-plane programmability. This programmability provides a rich functionality in central traffic management, switch configuration, Intrusion Detection System (IDS) integration and global view of entire deployed infrastructure. The SDN network comprises single point failure vulnerability mainly at central controller unit, the deployment of standalone legacy IDS sensors cannot guarantee for safeguard against intruders. Therefore, in the first stage of proposed work, a signature-based Snort IDS is implemented for malicious activity detection and traffic monitoring with traffic mirroring in Open vSwitch (OVS), then store in csv log file of Barnyard 2. In second stage, for the purpose of effective attack detection in the test-bed, a flowbased anomaly detection is deployed with Deep Neural Networks (DNN) to improve the signature-based IDS limitation with higher detection rate with low false-positive triggers. To assess the efficacy of our proposed collaborative detection technique, a testbed is developed to simulate malicious and benign traffic. From the simulation results, our collaborative detection mechanism achieved more than 90% true positive rate with less than 5% of false alarms for all TCP, UDP and ICMP attacks in general, demonstrating effective malicious traffic detection method as compared to conventional signature based methodologies.","","978-1-5386-6614-2","10.1109/HPCC/SmartCity/DSS.2018.00152","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8622890","Software Defined Networks;OpenFlow, Open vSwitch, Barnyard 2, Snort;Deep Neural Networks","Computer crime;Switches;Collaboration;Protocols;Neural networks;Sensors;Data acquisition","computer network management;computer network security;neural nets;software defined networking;telecommunication traffic;transport protocols","entire deployed infrastructure;SDN network;single point failure vulnerability;central controller unit;standalone legacy IDS sensors;signature-based Snort IDS;malicious activity detection;traffic monitoring;traffic mirroring;csv log file;effective attack detection;flowbased anomaly detection;Deep Neural Networks;signature-based IDS limitation;collaborative detection technique;benign traffic;collaborative detection mechanism;effective malicious traffic detection method;suspicious traffic Detection;Software Defined Networks;traditional network architecture;vendor independent architecture;control-plane programmability;central traffic management;switch configuration;data-plane programmability;OpenFlow based protocol;intrusion detection system","","1","","16","","24 Jan 2019","","","IEEE","IEEE Conferences"
"Visualizing Automatically Detected Periodic Network Activity","R. Gove; L. Deason",Two Six Labs; PUNCH Cyber Analytics Group,"2018 IEEE Symposium on Visualization for Cyber Security (VizSec)","9 May 2019","2018","","","1","8","Malware frequently leaves periodic signals in network logs, but these signals are easily drowned out by non-malicious periodic network activity, such as software updates and other polling activity. This paper describes a novel algorithm based on Discrete Fourier Transforms capable of detecting multiple distinct period lengths in a given time series. We pair the output of this algorithm with aggregation summary tables that give users information scent about which detections are worth investigating based on the metadata of the log events rather than the periodic signal. A visualization of selected detections enables users to see all detected period lengths per entity, and compare detections between entities to check for coordinated activity. We evaluate our approach on real-world netflow and DNS data from a large organization, demonstrating how to successfully find malicious periodic activity in a large pool of noise and non-malicious periodic activity.","2639-4332","978-1-5386-8194-7","10.1109/VIZSEC.2018.8709177","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8709177","Human-centered computing—Visualization—Visualization application domains—Information visualization;Human-centered computing—Visualization—Visualization design and evaluation methods;Security and privacy—Intrusion/anomaly detection and malware mitigation—Malware and its mitigation","Time series analysis;Tools;Time-frequency analysis;Microsoft Windows;Data visualization;Malware;Discrete Fourier transforms","data visualisation;discrete Fourier transforms;Internet;invasive software;IP networks;time series","periodic signal;network logs;nonmalicious periodic network activity;software updates;polling activity;multiple distinct period lengths;aggregation summary tables;users information scent;log events;selected detections;coordinated activity;malicious periodic activity;discrete Fourier transforms;time series;malware;DNS data;real-world netflow","","2","","21","","9 May 2019","","","IEEE","IEEE Conferences"
"Behavior-Oriented Time Segmentation for Mining Individualized Rules of Mobile Phone Users","I. H. Sarker; A. Colman; M. A. Kabir; J. Han","Sch. of Software & Electr. Eng., Swinburne Univ. of Technol., Melbourne, VIC, Australia; Sch. of Software & Electr. Eng., Swinburne Univ. of Technol., Melbourne, VIC, Australia; Sch. of Comput. & Math., Charles Sturt Univ., Bathurst, NSW, Australia; Sch. of Software & Electr. Eng., Swinburne Univ. of Technol., Melbourne, VIC, Australia","2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA)","26 Dec 2016","2016","","","488","497","Mobile or cellular phones can record various types of context data related to a user's phone call activities. In this paper, we present an approach to discovering individualized behavior rules for mobile users from their phone call records, based on the temporal context in which a user accepts, rejects or misses a call. One of the determinants of an individual's phone behavior is the various activities undertaken at various times of a day and days of the week. In many cases, such behavior will follow temporal patterns. Currently, researchers modeling user behavior using temporal context statically segment time into arbitrary categories (e.g., morning, evening) or periods (e.g., 1 hour). However, such time categorization does not necessarily map to the patterns of individual user activity and subsequent behavior. Therefore, we propose a behavior-oriented time segmentation (BOTS) technique that dynamically identifies diverse time segments for an individual user's behaviors based on the phone call records. Experiments on real datasets show that our proposed technique better captures the user's dominant call response behavior at various times of the day and week, thereby enabling more appropriate rules to be created for the purpose of automated handling of incoming calls, in an intelligent call interruption management system.","","978-1-5090-5206-6","10.1109/DSAA.2016.60","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7796935","mobile phone;call logs;time segmentation;user behavior modeling;mobile data mining;rule discovery;personalization;call interruptions;intelligent systems","Data mining;Mobile handsets;Mobile communication;Interrupters;Context;Time measurement","behavioural sciences computing;data mining;mobile computing;mobile handsets","behavior-oriented time segmentation;individualized rule mining;mobile phone users;cellular phones;user phone call activities;individualized behavior rule discovery;phone call records;temporal context;individual phone behavior;temporal patterns;user behavior modeling;individual user activity patterns;BOTS technique;user dominant call response behavior;incoming call automated handling;intelligent call interruption management system","","9","","22","","26 Dec 2016","","","IEEE","IEEE Conferences"
"Profiling file repository access patterns for identifying data exfiltration activities","Y. Hu; C. Frank; J. Walden; E. Crawford; D. Kasturiratna","Computer Science Department, Northern Kentucky University, Highland Heights, 41099 USA; Computer Science Department, Northern Kentucky University, Highland Heights, 41099 USA; Computer Science Department, Northern Kentucky University, Highland Heights, 41099 USA; Computer Science Department, Northern Kentucky University, Highland Heights, 41099 USA; Dept. of Math. & Stat., Northern Kentucky Univ., Highland Heights, KY, USA","2011 IEEE Symposium on Computational Intelligence in Cyber Security (CICS)","11 Jul 2011","2011","","","122","128","Studies show that a significant number of employees steal data when changing jobs. Insider attackers who have the authorization to access the best-kept secrets of organizations pose a great challenge for organizational security. Although increasing efforts have been spent on identifying insider attacks, little research concentrates on detecting data exfiltration activities. This paper proposes a model for identifying data exfiltration activities by insiders. It uses statistical methods to profile legitimate uses of file repositories by authorized users. By analyzing legitimate file repository access logs, user access profiles are created and can be employed to detect a large set of data exfiltration activities. The effectiveness of the proposed model was tested with file access histories from the subversion logs of the popular open source project KDE.","","978-1-4244-9906-9","10.1109/CICYBS.2011.5949404","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5949404","Incident Response;Insider Attack;Data Exfiltration","Software;History;Testing;Organizations;Training;Data models;Estimation","authorisation;information retrieval;organisational aspects","profiling file repository access;data exfiltration activity detection;authorization;organizational security;statistical methods;profile legitimate;user access profiles;KDE;open source project;inside attacker","","3","","16","","11 Jul 2011","","","IEEE","IEEE Conferences"
"Using Medical Device Logs for Improving Medical Device Design","A. Cauchi; H. Thimbleby; P. Oladimeji; M. Harrison","Future Interaction Technol. Lab., Swansea Univ., Swansea, UK; Future Interaction Technol. Lab., Swansea Univ., Swansea, UK; Future Interaction Technol. Lab., Swansea Univ., Swansea, UK; Sch. of Electron. Eng. & Comput. Sci., Queen Mary Univ. of London, London, UK","2013 IEEE International Conference on Healthcare Informatics","12 Dec 2013","2013","","","56","65","User interfaces that employ the same display and buttons may look the same but can work very differently depending on exactly how they are implemented. In healthcare, it is critical that interfaces that look the same are the same. Hospitals typically have many types of visually similar infusion pumps, but with different software versions and variation between pump behavior, and this may lead to unexpected adverse events. For example, when entering drug doses into infusion pumps different results may arise when pushing identical sequences of buttons. These differences arise as a result of subtle implementation differences and may lead to large errors users do not notice. Previous work has explored different implementations of a 5-key interface for entering numbers using a new analysis technique, Differential Formal Analysis, where predictions relating to the distribution of errors in terms of the size of the error (out by 10, out by 100 and so on) can be made. The analysis described in the paper extends this work with models of use based on many hours of actual clinical use data. Specifically, we draw on 1,362 days of use of number entry systems, from 19 infusion pumps over a 3 year period in a UK hospital. The paper also suggests some improvements to medical device logs, which will help further evidence-based improvement to medical device safety.","","978-0-7695-5089-3","10.1109/ICHI.2013.14","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6680461","Human error;Number entry;Medical device logs;Differential Formal Analysis","Hospitals;Error analysis;Presses;User interfaces;Safety;Pressing","drugs;health care;medical information systems;user interfaces","medical device logs;medical device design;user interface;healthcare;pump behavior;drug doses;infusion pump;differential formal analysis;evidence-based improvement;medical device safety","","1","","14","","12 Dec 2013","","","IEEE","IEEE Conferences"
"On the Detection of Persistent Attacks using Alert Graphs and Event Feature Embeddings","B. Burr; S. Wang; G. Salmon; H. Soliman","Carleton University,Dept. of Mathematics and Statistics,Ottawa,Canada; Carleton University,Dept. of Mathematics and Statistics,Ottawa,Canada; RANK Software,Toronto,Canada; RANK Software,Toronto,Canada","NOMS 2020 - 2020 IEEE/IFIP Network Operations and Management Symposium","8 Jun 2020","2020","","","1","4","Intrusion Detection Systems (IDS) generate a high volume of alerts that security analysts do not have the resources to explore fully. Modelling attacks, especially the coordinated campaigns of Advanced Persistent Threats (APTs), in a visually-interpretable way is a useful approach for network security. Graph models combine multiple alerts and are well suited for visualization and interpretation, increasing security effectiveness. In this paper, we use feature embeddings, learned from network event logs, and community detection to construct and segment alert graphs of related alerts and networks hosts. We posit that such graphs can aid security analysts in investigating alerts and may capture multiple aspects of an APT attack. The eventual goal of this approach is to construct interpretable attack graphs and extract causality information to identify coordinated attacks.","2374-9709","978-1-7281-4973-8","10.1109/NOMS47738.2020.9110439","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9110439","word embedding;IP address;IP2Vec;graph model;unsupervised clustering;community detection","","computer network security;data visualisation;security of data","feature embeddings;IDS;security analysts;advanced persistent threats;network security;graph models;coordinated attacks;event feature embeddings;persistent attacks detection;alert graphs;intrusion detection systems;APT","","2","","15","","8 Jun 2020","","","IEEE","IEEE Conferences"
"A whole layer performance analysis method for Android platforms","N. Lee; S. Lim","LG Electronics, Seoul, Korea; School of Computer Science, Kookmin University, Seoul, Korea","2011 9th IEEE Symposium on Embedded Systems for Real-Time Multimedia","1 Dec 2011","2011","","","1","1","As the products based on Android platform have been widely spread in consumer electronics market, the needs for systematic performance analysis have significantly increased. We implemented a whole layer performance analysis tool set combining and integrating available open source performance analysis tools. Android framework layer profiling is performed DDMS mechanism with a little modification and the kernel layer profiling is based on Ftrace. The reason why we selected Ftrace instead of Oprofile as kernel layer profiler is that the Oprofile often misses the important kernel events to analyze since Oprofile is using purely time-based sampling. While Ftrace provides accuracy in performance profiling, the additional costs to perform kernel events profiling are significant. Therefore, minimizing the profiling cost maintaining the accuracy of event logging has been a primary issue in performance profiling at kernel layer.","2325-1301","978-1-4577-2122-9","10.1109/ESTIMedia.2011.6088515","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6088515","","","consumer electronics;operating system kernels;program debugging;software performance evaluation","whole layer performance analysis method;Android platform;consumer electronics market;open source performance analysis tools;Android framework layer profiling;Dalvik Debug Monitor Server mechanism;kernel layer profiling;Ftrace;event logging","","","","","","1 Dec 2011","","","IEEE","IEEE Conferences"
"Process discovery: Automated approach for block discovery","S. Boushaba; M. I. Kabbaj; Z. Bakkoury","Department of Computer Science, AMIPS Research Group, Ecole Mohammadia (d'Ingenieurs, Mohammed V University - Agdal, Av Ibn Sina, Rabat, Morocco; Department of Computer Science, AMIPS Research Group, Ecole Mohammadia (d'Ingenieurs, Mohammed V University - Agdal, Av Ibn Sina, Rabat, Morocco; Department of Computer Science, AMIPS Research Group, Ecole Mohammadia (d'Ingenieurs, Mohammed V University - Agdal, Av Ibn Sina, Rabat, Morocco","2014 9th International Conference on Evaluation of Novel Approaches to Software Engineering (ENASE)","2 Apr 2015","2014","","","1","8","Process mining is a set of techniques helping enterprises to avoid process modeling which is a time-consuming and error prone task. Process mining includes three topics: process discovery, conformance checking, and enhancement (IEEE Task Force on Process Mining: Process Mining Manifesto, 2012). The principle of process discovery is to extract information from event logs to capture the business process as it is being executed. Several techniques in literature (a algorithm, a+ algorithm and others) can be applied to discover a process model from a workflow log. However, as the amount of information grows exponentially, the log files (input of a process discovery algorithm) get bigger. In fact, classical techniques, which inspect relation between each couple of tasks will have problem dealing with big data. To this end, we introduced in (Boushaba et al., 2013) a new approach aiming to extract a block of tasks from event logs. In this paper, we present a new algorithm, based on a matrix representation, to detect a block of tasks. In addition, we develop an application to automate our technique.","","978-989-758-065-9","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7077136","Process Mining;Business Process Management;Process Discovery;Block Discovery","Business;Data mining;Filtering algorithms;Force;Complexity theory;Mathematical model;Indexes","","","","","","16","","2 Apr 2015","","","IEEE","IEEE Conferences"
"ARCS: A unified environment for autonomous robot control and simulation","S. Seifert; P. Meyer; C. Ramee; E. Evans; W. Roberts; K. Griendling; D. Mavris","Georgia Institute of Technology Atlanta, Georgia 30332-0250; Georgia Institute of Technology Atlanta, Georgia 30332-0250; Georgia Institute of Technology Atlanta, Georgia 30332-0250; Georgia Institute of Technology Atlanta, Georgia 30332-0250; Georgia Institute of Technology Atlanta, Georgia 30332-0250; Georgia Institute of Technology Atlanta, Georgia 30332-0250; Georgia Institute of Technology Atlanta, Georgia 30332-0250","OCEANS 2017 - Anchorage","25 Dec 2017","2017","","","1","8","A common approach for developing robotic systems leverages separate simulation and control softwares. Although this approach requires minimal coordination and orchestration between softwares, the separation of simulation and control applications presents the designer with unnecessary challenges during development. This paper describes the Autonomous Robot Control and Simulation (ARCS) software, a unified control and simulation environment developed on a common software stack for real world control and simulated experimentation. This unification is attained through the use of a modular data pipeline that allows for data to be generated in simulation, taken from real hardware, or taken from log files. The unification of real world control and simulation software and the adaptability of this pipeline allows for some unique advantages. These include more meaningful virtual experimentation, a streamlined functionality to perform hardware in the loop tests, and the ability to replay collected log files from both simulation and real world testing. This allows more rapid iterative development cycles of autonomous behaviors. This software stack was used to develop three marine robotics platforms, which will serve as an example to discuss the advantages and disadvantages of this unified approach.","","978-0-6929-4690-9","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8232219","","Hardware;Robots;Testing;Visualization;Linux","control engineering computing;digital simulation;marine systems;mobile robots;pipeline processing","robotic systems;real world control;autonomous robot control and simulation software;minimal coordination;control softwares;unified environment;unified approach;marine robotics platforms;autonomous behaviors;rapid iterative development cycles;log files;modular data pipeline;simulated experimentation;common software stack;simulation environment;unified control;ARCS;control applications","","","","11","","25 Dec 2017","","","IEEE","IEEE Conferences"
"Breeze: User-Level Access to Non-Volatile Main Memories for Legacy Software","A. Memaripour; S. Swanson","Dept. of Comput. Sci. & Eng., Univ. of California, San Diego, San Diego, CA, USA; Dept. of Comput. Sci. & Eng., Univ. of California, San Diego, San Diego, CA, USA","2018 IEEE 36th International Conference on Computer Design (ICCD)","17 Jan 2019","2018","","","413","422","Non-volatile main memory (NVMM) technologies, such as phase change memory and 3D XPoint, offer DRAM-like performance and byte-addressable access to persistent data. A wide range of applications (e.g., key-value stores and database systems) stand to benefit from the performance potential of these technologies. These potential benefits are greatest when applications can access memory directly via load/store instructions rather than conventional file-based interfaces. This approach presents several challenges. In particular, applications need guaranteed consistency and safety semantics to protect their data structures in the face of system failures and programming errors. Implementing data structures that meet these requirements is challenging and error-prone. Researchers have proposed several libraries and programming language extensions that simplify this task, but, to date, all the proposed solutions either require pervasive changes to existing software or rely on special hardware support. As a result, porting legacy applications to leverage NVMM is likely to be prohibitively difficult and time consuming. We propose Breeze, a NVMM toolchain that minimizes the changes necessary to enable legacy code to reap the benefits of directly accessing NVMM. Breeze guarantees data consistency and validity of persistent pointers regardless of failures. The toolchain transparently detects and logs writes to NVMM and provides a simple mechanism for identifying atomic sections while avoiding complications common in previous systems such as special persistent pointer types. Porting Memcached and MongoDB to use Breeze only requires changes to 5% of the source code compared to 7-14% for NVML and NVM-Direct. Breeze also provides equal or superior performance compared to NVML and NVM-Direct, outperforming them by up to 10x.","2576-6996","978-1-5386-8477-1","10.1109/ICCD.2018.00069","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8615719","Non volatile Memory;Persistent Objects;ACID Transactions;Legacy Software;Pointer Safety;Ease of Use","Nonvolatile memory;Data structures;Programming;Libraries;Buildings;Program processors","cache storage;data protection;data structures;database management systems;programming languages;software maintenance;storage management","user-level access;nonvolatile main memories;legacy software;byte-addressable access;database systems;programming language extensions;Breeze;NVMM toolchain;legacy code;data consistency;data structures;DRAM-like performance;porting memcached;MongoDB","","","","48","","17 Jan 2019","","","IEEE","IEEE Conferences"
"Business Process Discovery by Using Process Skeletonization","M. Kudo; A. Ishida; N. Sato","IBM Res. - Tokyo, Tokyo, Japan; IBM Res. - Tokyo, Tokyo, Japan; IBM Res. - Tokyo, Tokyo, Japan","2013 International Conference on Signal-Image Technology & Internet-Based Systems","30 Jan 2014","2013","","","976","982","Process Management software and solutions mainly address structured processes in enterprises, but there are still many business applications that are less structured by their nature, such as human-centric and ad hoc workflows. Existing process mining algorithms often have difficulties in extracting a skeletal structure of a less-structured process model from real-life event logs. We propose a new process mining algorithm by using a skeleton-extraction procedure, which brings structure to less structured business processes. We present experimental mining results from real insurance claim examination event logs and verified the effectiveness of the proposed algorithm.","","978-1-4799-3211-5","10.1109/SITIS.2013.158","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6727308","process mining;weakly structured process;process skeleton","","business data processing;data mining;insurance data processing","business process discovery;process skeletonization;process management software;structured processes;enterprises;business applications;human-centric workflows;ad hoc workflows;process mining algorithms;skeletal structure;less-structured process model;real-life event logs;skeleton-extraction procedure;less structured business processes;real insurance claim examination event logs","","4","","16","","30 Jan 2014","","","IEEE","IEEE Conferences"
"A study into the capabilities of NoSQL databases in handling a highly heterogeneous tree","D. Jayathilake; C. Sooriaarachchi; T. Gunawardena; B. Kulasuriya; T. Dayaratne","99X Technology, Colombo, 65, Walukarama Road, Colombo 3, Sri Lanka; 99X Technology, Colombo, 65, Walukarama Road, Colombo 3, Sri Lanka; 99X Technology, Colombo, 65, Walukarama Road, Colombo 3, Sri Lanka; 99X Technology, Colombo, 65, Walukarama Road, Colombo 3, Sri Lanka; 99X Technology, Colombo, 65, Walukarama Road, Colombo 3, Sri Lanka","2012 IEEE 6th International Conference on Information and Automation for Sustainability","28 Jan 2013","2012","","","106","111","This paper comprehends our work on assessing the feasibility of utilizing different NoSQL databases in handling a huge tree data structure with heterogeneous nodes in which heterogeneity implies that each node can embody a unique attribute set. It is a prominent requirement arising in structured log analysis where constituents in a software log file are scrutinized hierarchically. Traditional pills from relational databases fail in handling this efficiently. We lean towards NoSQL paradigm, which has been emerging as a prominent solution for dealing with high volumes of data with localized characteristics. Our exploration probes five different NoSQL models: wide column store, document store, tuple store, graph databases and multi-model databases that collectively account for a large fraction of the entire NoSQL spectrum. An experiment is designed to measure database performance against a generic tree API focusing on node insertion, node query and attribute-value query. The API is then implemented in a database selected from each of the five NoSQL models in concern. Implementations are used for testing the database performance with respect to the three operations by measuring time taken for a batch of similar operations in a machine with average hardware and software configuration. A summary of experiment results is provided along with the details on tree implementation methodology in each database. A discussion that highlights the congruence between observed performance differences among databases and the theoretical NoSQL models they represent is also included.","2151-1810","978-1-4673-1975-1","10.1109/ICIAFS.2012.6419890","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6419890","","Servers;Arrays;Indexing;Time measurement;Distributed databases","application program interfaces;distributed databases;SQL;tree data structures;trees (mathematics)","NoSQL databases;highly heterogeneous tree handling;huge tree data structure handling;heterogeneous nodes;attribute set;structured log analysis;software log file;localized characteristics;wide column store;document store;tuple store;graph databases;multimodel databases;NoSQL spectrum;generic tree API;node insertion;node query;attribute-value query;database performance testing;hardware configuration;software configuration","","9","","23","","28 Jan 2013","","","IEEE","IEEE Conferences"
"Kill Two Birds with One Stone: Auto-tuning RocksDB for High Bandwidth and Low Latency","Y. Jia; F. Chen","Louisiana State University,Computer Science and Engineering; Louisiana State University,Computer Science and Engineering","2020 IEEE 40th International Conference on Distributed Computing Systems (ICDCS)","23 Feb 2021","2020","","","652","664","Log-Structured Merge (LSM) tree based key-value stores are widely deployed in data centers. Due to its complex internal structures, appropriately configuring a modern key-value data store system, which can have more than 50 parameters with various hardware and system settings, is a highly challenging task. Currently, the industry still heavily relies on a traditional, experience-based, hand-tuning approach for performance tuning. Many simply adopt the default setting out of the box with no changes. Auto-tuning, as a self-adaptive solution, is thus highly appealing for achieving optimal or near-optimal performance in real-world deployment.In this paper, we quantitatively study and compare five optimization methods for auto-tuning the performance of LSM-tree based key-value stores. In order to evaluate the auto-tuning processes, we have conducted an exhaustive set of experiments over RocksDB, a representative LSM-tree data store. We have collected over 12,000 experimental records in 6 months, with about 2,000 software configurations of 6 parameters on different hardware setups. We have compared five representative algorithms, in terms of throughput, the 99th percentile tail latency, convergence time, real-time system throughput, and the iteration process, etc. We find that multi-objective optimization (MOO) methods can achieve a good balance among multiple targets, which satisfies the unique needs of key-value services. The more specific Quality of Service (QoS) requirements users can provide, the better performance these algorithms can achieve. We also find that the number of concurrent threads and the write buffer size are the two most impactful parameters determining the throughput and the 99th percentile tail latency across different hardware and workloads. Finally, we provide system-level explanations for the auto-tuning results and also discuss the associated implications for system designers and practitioners. We hope this work will pave the way towards a practical, high-speed auto-tuning solution for key-value data store systems.","2575-8411","978-1-7281-7002-2","10.1109/ICDCS47774.2020.00113","National Science Foundation; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9355766","Auto tuning;RocksDB;Multi objective Optimization;Log Structured Merge Tree;Genetic Algorithms","Software algorithms;Quality of service;Throughput;Hardware;Software;Task analysis;Tuning","computer centres;concurrency control;data handling;merging;NoSQL databases;optimisation;quality of service;storage management;tree data structures","auto-tuning RocksDB;log-structured merge tree;data centers;complex internal structures;system settings;performance tuning;near-optimal performance;real-world deployment;auto-tuning processes;representative LSM-tree data store;software configurations;hardware setups;real-time system throughput;multiobjective optimization methods;key-value services;system-level explanations;system designers;high-speed auto-tuning solution;key-value data store systems;self-adaptive solution;MOO method;quality of service;QoS;concurrent threads;write buffer size","","","","58","","23 Feb 2021","","","IEEE","IEEE Conferences"
"An Ontology for Workflow Organizational Model Mining","R. Sellami; W. Gaaloul; S. Moalla","Inst. TELECOM, Evry, France; Inst. TELECOM, Evry, France; Fac. of Sci. of Tunis, Univ. Tunis EL Manar, Tunis, Tunisia","2012 IEEE 21st International Workshop on Enabling Technologies: Infrastructure for Collaborative Enterprises","16 Aug 2012","2012","","","199","204","Continuous and unforeseeable evolution of business rules, processing logics, and organizational structures within enterprises, requires from business process management systems to integrate continuous design. Supporting business process rediscovery based on workflow logs analysis, workflow mining gathers retroactive (re)design techniques necessary to understand business process execution reality. Most of the works in this area focus on the control flow perspective, while very few of them address the organizational aspect. In addition, the used workflow mining techniques suffer from the lack of automation due to the purely syntactic logs. In this paper, we propose an Organizational Ontology (OrO) specifying the organizational model. This ontology is used to semantically annotate log files and establish an organizational knowledge base that will be used to discover relationships between performers in a workflow. Our approach has been implemented within the ProM framework.","1524-4547","978-1-4673-1888-4","10.1109/WETICE.2012.29","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6269728","Workflow Mining;Organizational Model;Organizational Ontology;SBPM;ProM","Ontologies;Business;Semantics;Knowledge based systems;Analytical models;Receivers;Automation","business process re-engineering;data mining;knowledge based systems;ontologies (artificial intelligence);organisational aspects;workflow management software","workflow organizational model mining;business rules;processing logics;organizational structures;business process management systems;business process rediscovery;workflow logs analysis;retroactive design techniques;business process execution;syntactic logs;organizational ontology;OrO;log files annotation;organizational knowledge base;ProM framework","","5","","20","","16 Aug 2012","","","IEEE","IEEE Conferences"
"HadoopWatch: A first step towards comprehensive traffic forecasting in cloud computing","Y. Peng; K. Chen; G. Wang; W. Bai; Z. Ma; L. Gu",Hong Kong University of Science and Technology; Hong Kong University of Science and Technology; Facebook; Hong Kong University of Science and Technology; Hong Kong University of Science and Technology; Hong Kong University of Science and Technology,"IEEE INFOCOM 2014 - IEEE Conference on Computer Communications","8 Jul 2014","2014","","","19","27","This paper presents our effort towards comprehensive traffic forecasting for big data applications using external, light-weighted file system monitoring. Our idea is motivated by the key observations that rich traffic demand information already exists in the log and meta-data files of many big data applications, and that such information can be readily extracted through run-time file system monitoring. As the first step, we use Hadoop as a concrete example to explore our methodology and develop a system called HadoopWatch to predict traffic demand of Hadoop applications. We further implement HadoopWatch in our real small-scale testbed with 10 physical servers and 30 virtual machines. Our experiments over a series of MapReduce applications demonstrate that HadoopWatch can forecast the traffic demand with almost 100% accuracy and time advance. Furthermore, it makes no modification of the Hadoop framework, and introduces little overhead to the application performance.","0743-166X","978-1-4799-3360-0","10.1109/INFOCOM.2014.6847920","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6847920","","Monitoring;Forecasting;Big data;Writing;Conferences;Computers;Pipelines","Big Data;cloud computing;meta data;parallel programming;public domain software;telecommunication traffic;virtual machines","HadoopWatch;comprehensive traffic demand forecasting;cloud computing;Big Data applications;external-light-weighted file system monitoring;log files;meta-data files;information extraction;traffic demand prediction;real-small-scale testbed;physical servers;virtual machines;MapReduce applications","","26","1","35","","8 Jul 2014","","","IEEE","IEEE Conferences"
"Continuous Analysis of Collaborative Design","J. Y. Bang; Y. Brun; N. Medvidovic","Kakao Corp., Seongnam, South Korea; Univ. of Massachusetts, Amherst, MA, USA; Univ. of Southern California, Los Angeles, CA, USA","2017 IEEE International Conference on Software Architecture (ICSA)","18 May 2017","2017","","","97","106","In collaborative design, architects' individual design decisions may conflict and, when joined, may violate system consistency rules or non-functional requirements. These design conflicts can hinder collaboration and result in wasted effort. Proactive detection of code-level conflicts has been shown to improve collaborative productivity, however, the computational resource requirements for proactively computing design conflicts have hindered its applicability in practice. Our survey and interviews of 50 architects from six large software companies find that 60% of their projects involve collaborative design, that architects consider integration costly, and that design conflicts are frequent and lead to lost work. To aid collaborative design, we re-engineer FLAME, our prior design conflict detection technique, to use cloud resources and a novel prioritization algorithm that, together, achieve efficient and nonintrusive conflict detection, and guarantee a bound on the time before a conflict is discovered. Two controlled experiments with 90 students trained in software architecture in a professional graduate program, demonstrate that architects using FLAME design more efficiently, produce higher-quality designs, repair conflicts faster, and prefer using FLAME. An empirical performance evaluation demonstrates FLAME's scalability and verifies its time-bound guarantees.","","978-1-5090-5729-0","10.1109/ICSA.2017.45","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7930205","collaborative design;conflict detection;proactive conflict detection;speculative analysis","Fires;Synchronization;Analytical models;Adaptation models;Tools;Collaboration;Engines","cloud computing;groupware;programming environments;software architecture","collaborative design;FLAME;design conflict detection technique;cloud resources;prioritization algorithm;nonintrusive conflict detection;software architecture;Framework for Logging and Analyzing Modeling Events","","2","1","61","","18 May 2017","","","IEEE","IEEE Conferences"
"Visualizing Self-Adaptive Plan Simulations Given Embedded Verification Concerns","S. Jahan; A. Marshall; R. Gamble","Tandy Sch. of Comput. Sci., Univ. of Tulsa, Tulsa, OK, USA; Tandy Sch. of Comput. Sci., Univ. of Tulsa, Tulsa, OK, USA; Tandy Sch. of Comput. Sci., Univ. of Tulsa, Tulsa, OK, USA","2017 IEEE 2nd International Workshops on Foundations and Applications of Self* Systems (FAS*W)","12 Oct 2017","2017","","","389","390","A system that dynamically self-adapts at runtime, should comply with critical requirements. However, runtime verification is difficult even when the system was originally formulated to expect adaptation and allowable changes are preconfigured or prespecified. Our approach examines verification processes originally performed for compliance with system requirements to identify specific verification concerns, such as variables, safety and liveness property conditions, and architecture properties. The expectation is that if a verification concern is impacted by an adaptation then the reuse of the original verification process may be restricted. If verification process reuse is inhibited, then there is increased likelihood that the requirements relying on that verification concern may no longer be guaranteed. In this demonstration, we illustrate our approach to take identified verification concerns for each requirement and embed them as checkpoints within the code, given the flow of the verification process from which they were derived. Simulating an adaptation plan produces log files based on which checkpoints are reached. Failure to complete a path through the checkpoints without raising a flag indicates that the verification process may not be repeatable and the adaptation plan may be risky to perform. We visualize the paths using ProM which shows where and how an adaptation plan may be problematic.","","978-1-5090-6558-5","10.1109/FAS-W.2017.185","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8064161","verification;self-adaptation;Linear Temporal Logic;verification process reuse;adaptive plans;ProM","PROM;Visualization;Adaptation models;Runtime;Computer architecture;Fuels;Conferences","checkpointing;data visualisation;embedded systems;formal verification;safety-critical software","self-adaptive plan simulations;embedded verification concerns;runtime verification;system requirements;safety;liveness property conditions;log files;checkpoints","","","","5","","12 Oct 2017","","","IEEE","IEEE Conferences"
"An OS-level Framework for Anomaly Detection in Complex Software Systems","A. Bovenzi; F. Brancati; S. Russo; A. Bondavalli","Dipartimento di Ingegneria Elettrica e delle Tecnologie dell’Informazione, Università di Napoli Federico II, Italy; Resiltech S.R.L., Pontedera (PI), Italy; Dipartimento di Ingegneria Elettrica e delle Tecnologie dell’Informazione, Università di Napoli Federico II, Italy; Departimento di Sistemi e Informatica, Università di Firenze","IEEE Transactions on Dependable and Secure Computing","11 May 2015","2015","12","3","366","372","Revealing anomalies at the operating system (OS) level to support online diagnosis activities of complex software systems is a promising approach when traditional detection mechanisms (e.g., based on event logs, probes and heartbeats) are inadequate or cannot be applied. In this paper we propose a configurable detection framework to reveal anomalies in the OS behavior, related to system misbehaviors. The detector is based on online statistical analyses techniques, and it is designed for systems that operate under variable and non-stationary conditions. The framework is evaluated to detect the activation of software faults in a complex distributed system for Air Traffic Management (ATM). Results of experiments with two different OSs, namely Linux Red Hat EL5 and Windows Server 2008, show that the detector is effective for mission-critical systems. The framework can be configured to select the monitored indicators so as to tune the level of intrusivity. A sensitivity analysis of the detector parameters is carried out to show their impact on the performance and to give to practitioners guidelines for its field tuning.","1941-0018","","10.1109/TDSC.2014.2334305","MIUR; FP7; Toscana Regional; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6847216","Anomaly-detection;system monitoring;operating system;mission-critical systems","Monitoring;Detectors;Linux;Operating systems;Software systems;Probes","operating systems (computers);software fault tolerance;statistical analysis","OS-level framework;operating system level;anomaly detection;complex software systems;configurable detection framework;online statistical analysis techniques;complex distributed system;air traffic management;Linux Red Hat EL5;Windows Server 2008;mission-critical systems","","14","","28","","1 Jul 2014","","","IEEE","IEEE Journals"
"ELF Analyzer Demo: Online Identification for IoT Malwares with Multiple Hardware Architectures","S. -M. Cheng; T. Ban; J. -W. Huang; B. -K. Hong; D. Inoue","Research Center for Information Technology Innovation, Academia Sinica,Taipei,Taiwan; Cybersecurity Laboratory, National Institute of Information and Communications Technology,Tokyo,Japan; National Taiwan University of Science and Technology,Department of Computer Science and Information Engineering,Taipei,Taiwan; National Taiwan University of Science and Technology,Department of Computer Science and Information Engineering,Taipei,Taiwan; Cybersecurity Laboratory, National Institute of Information and Communications Technology,Tokyo,Japan","2020 IEEE Security and Privacy Workshops (SPW)","18 Dec 2020","2020","","","126","126","This demonstration presents an automatic IoT runtime platform with a web interface, ELF Analyzer, where suspicious ELF files uploaded by users could be executed and dynamically analyzed for malicious behavior identification. The key component of our platform is a crafted IoT sandbox, where multiple hardware architectures are emulated using QEMU. With the introduction of strace functionality, we demonstrate that system call and traffic logs of an uploaded ELF file with different hardware architectures can be generated successfully. After proper analysis, malicious ELF files can be identified.","","978-1-7281-9346-5","10.1109/SPW50608.2020.00036","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9283851","architecture emulation;dynamic analysis;IoT malware;QEMU;strace","Privacy;Ground penetrating radar;Runtime;Geophysical measurement techniques;Hardware;Malware;Security","computer network security;Internet of Things;invasive software;Linux","IoT malwares;hardware architectures;automatic IoT runtime platform;web interface;suspicious ELF files;malicious behavior identification;crafted IoT sandbox;malicious ELF files;ELF Analyzer;QEMU","","","","1","","18 Dec 2020","","","IEEE","IEEE Conferences"
"An Approach towards Automation Firmware Modeling for an Exploration and Evaluation of Efficient Parallelization Alternatives","J. Bregenzer; J. Hartmann","Bosch Rexroth AG, Lohr am Main, Germany; Univ. of Appl. Sci., Fulda, Germany","2011 Sixth International Symposium on Parallel Computing in Electrical Engineering","19 May 2011","2011","","","13","18","Due to stagnating CPU cycles, future performance gains in automation firmware are unlikely to be achieved without parallelization for multi-core architectures. However, for a sophisticated system comprising millions of lines of code, this process induces significant effort, especially when having to keep real time and safety conditions. As efficiency matters in corporate software development, obtaining maximum speedup by spending no more implementation effort than necessary is intended. Thus, the design of a parallel firmware is recommended to base on the results of a model-based exploration and evaluation of efficient parallelization alternatives. For this purpose, we developed the EEEPA tool chain, that starts with graph-based firmware modeling on basis of dynamic event logs.","","978-1-4577-0078-1","10.1109/PARELEC.2011.35","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5770394","automation firmware;software modeling;static parallelization;multi-core architectures","Runtime;Synchronization;Schedules;Multicore processing;Adaptation model;Writing;Instruments","firmware;graph theory;parallel processing;software engineering","automation firmware modeling;parallelization alternative;multicore architecture;corporate software development;parallel firmware design;model-based exploration;EEEPA tool chain;graph-based firmware modeling;dynamic event logs","","2","","7","","19 May 2011","","","IEEE","IEEE Conferences"
"DESSERT: Debugging RTL Effectively with State Snapshotting for Error Replays across Trillions of Cycles","D. Kim; C. Celio; S. Karandikar; D. Biancolin; J. Bachrach; K. Asanović","Dept. of Electr. Eng. & Comput. Sci., Univ. of California, Berkeley, Berkeley, CA, USA; NA; Dept. of Electr. Eng. & Comput. Sci., Univ. of California, Berkeley, Berkeley, CA, USA; Dept. of Electr. Eng. & Comput. Sci., Univ. of California, Berkeley, Berkeley, CA, USA; Dept. of Electr. Eng. & Comput. Sci., Univ. of California, Berkeley, Berkeley, CA, USA; Dept. of Electr. Eng. & Comput. Sci., Univ. of California, Berkeley, Berkeley, CA, USA","2018 28th International Conference on Field Programmable Logic and Applications (FPL)","6 Dec 2018","2018","","","76","764","We present DESSERT, an FPGA-accelerated methodology for simulation-based RTL verification. The RTL design is automatically transformed and instrumented to allow deterministic simulation on the FPGA with initialization and state snapshot capture. Assert statements, which are present in RTL for error checking in software simulation, are automatically synthesized for quick hardware-based error checking. Print statements in the RTL design are also automatically transformed to generate logs from the FPGA, which are compared on the fly against a functional golden-model software simulator for more exhaustive error checking. To rapidly provide waveforms for debugging, two parallel deterministic FPGA-accelerated RTL simulations are run spaced apart in simulation time to support capture and replay of state snapshots immediately before an error. We demonstrate DESSERT, running on public-cloud FPGAs at extremely low cost, by catching bugs in a complex out-of-order processor hundreds of billions of cycles into SPEC2006int benchmarks running under Linux.","1946-1488","978-1-5386-8517-4","10.1109/FPL.2018.00021","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8533471","RTL debugging;hardware verification;simulation-based verification;FPGA-accelerated simulation","Field programmable gate arrays;Software;Timing;Debugging;Hardware;Load modeling;Computer bugs","computer simulation;field programmable gate arrays;formal verification;hardware description languages;program debugging","error replays;DESSERT;FPGA-accelerated methodology;simulation-based RTL verification;RTL design;deterministic simulation;assert statements;software simulation;print statements;hardware-based error checking;state snapshotting;RTL debugging","","1","","14","","6 Dec 2018","","","IEEE","IEEE Conferences"
"A Semantic Rule-Based Approach Towards Process Mining for Personalised Adaptive Learning","K. Okoye; A. R. H. Tawil; U. Naeem; R. Bashroush; E. Lamine","Sch. of Archit. Comput. & Eng., Univ. of East London, London, UK; Sch. of Archit. Comput. & Eng., Univ. of East London, London, UK; Sch. of Archit. Comput. & Eng., Univ. of East London, London, UK; Sch. of Archit. Comput. & Eng., Univ. of East London, London, UK; Mines-Albi, Univ. de Toulouse, Albi, France","2014 IEEE Intl Conf on High Performance Computing and Communications, 2014 IEEE 6th Intl Symp on Cyberspace Safety and Security, 2014 IEEE 11th Intl Conf on Embedded Software and Syst (HPCC,CSS,ICESS)","12 Mar 2015","2014","","","929","936","In recent years, automated learning systems are widely used for educational and training purposes within various organisations including, schools, universities and further education centres. A common challenge for automated learning approaches is the demand for an effectively well-designed and fit for purpose system that meets the requirements and needs of intended learners to achieve their learning goals. This paper proposes a novel approach for automated learning that is capable of detecting changing trends in learning behaviours and abilities through the use of process mining techniques. The goal is to discover user interaction patterns, and respond by making decisions based on adaptive rules centred on captured user profiles. The approach applies semantic annotation of activity logs within the learning process in order to discover patterns automatically by means of semantic reasoning. Therefore, our proposed approach is grounded on Semantic modelling and process mining techniques. To this end, it is possible to apply effective reasoning methods to make inferences over a Learning Process Knowledge-Base that leads to automated discovery of learning patterns or behaviour.","","978-1-4799-6123-8","10.1109/HPCC.2014.143","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7056857","process model;semantic rules;user profile;learning behaviour;event logs","Semantics;Ontologies;OWL;Cognition;Data mining;XML;Adaptive systems","computer aided instruction;data mining;educational institutions;inference mechanisms;knowledge based systems","semantic rule-based approach;process mining;personalised adaptive learning;automated learning systems;training purposes;educational purposes;organisations;schools;universities;further education centres;learning goals;automated learning;user interaction patterns;adaptive rules;captured user profiles;semantic activity logs annotation;semantic reasoning;reasoning methods;learning process knowledge-base","","2","","48","","12 Mar 2015","","","IEEE","IEEE Conferences"
"Adaptable exploit detection through scalable NetFlow analysis","A. Herbert; B. Irwin","Rhodes University, Grahamstown, RSA, South Africa; Rhodes University, Grahamstown, RSA, South Africa","2016 Information Security for South Africa (ISSA)","2 Jan 2017","2016","","","121","128","Full packet analysis on firewalls and intrusion detection, although effective, has been found in recent times to be detrimental to the overall performance of networks that receive large volumes of throughput. For this reason partial packet analysis technologies such as the NetFlow protocol have emerged to better mitigate these bottlenecks through log generation. This paper researches the use of log files generated by NetFlow version 9 and IPFIX to identify successful and unsuccessful exploit attacks commonly used by automated systems. These malicious communications include but are not limited to exploits that attack Microsoft RPC, Samba, NTP (Network Time Protocol) and IRC (Internet Relay Chat). These attacks are recreated through existing exploit implementations on Metasploit and through hand-crafted reconstructions of exploits via known documentation of vulnerabilities. These attacks are then monitored through a preconfigured virtual testbed containing gateways and network connections commonly found on the Internet. This common attack identification system is intended for insertion as a parallel module for Bolvedere in order to further the increase the Bolvedere system's attack detection capability.","","978-1-5090-2473-5","10.1109/ISSA.2016.7802938","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7802938","Digital forensics;Network security;Intrusion detection","Internet;Protocols;Computer crime;Monitoring;Companies;Hardware;Software","file organisation;firewalls;Internet;security of data;virtualisation","adaptable exploit detection;scalable NetFlow analysis;full packet analysis;firewalls;intrusion detection;partial packet analysis technologies;log files;IPFIX;automated systems;Microsoft RPC;Samba;NTP;network time protocol;IRC;Internet relay chat;Metasploit;virtual testbed","","","","33","","2 Jan 2017","","","IEEE","IEEE Conferences"
"Network Attack Detection based on Domain Attack Behavior Analysis","W. Wang; X. Zhang; L. Dong; Y. Fan; X. Diao; T. Xu","University of Jinan,University of Jinan Library, School of Information Science and Engineering,Jinan,China,250022; University of Jinan,University of Jinan Library, School of Information Science and Engineering,Jinan,China,250022; University of Jinan,University of Jinan Library, School of Information Science and Engineering,Jinan,China,250022; University of Jinan,University of Jinan Library, School of Information Science and Engineering,Jinan,China,250022; University of Jinan,University of Jinan Library, School of Information Science and Engineering,Jinan,China,250022; University of Jinan,University of Jinan Library, School of Information Science and Engineering,Jinan,China,250022","2020 13th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)","25 Nov 2020","2020","","","962","965","Network security has become an important issue in our work and life. Hackers' attack mode has been upgraded from normal attack to APT( Advanced Persistent Threat, APT) attack. The key of APT attack chain is the penetration and intrusion of active directory, which can not be completely detected via the traditional IDS and antivirus software. Further more, lack of security protection of existing solutions for domain control aggravates this problem. Although researchers have proposed methods for domain attack detection, many of them have not yet been converted into effective market-oriented products. In this paper, we analyzes the common domain intrusion methods, various domain related attack behavior characteristics were extracted from ATT&CK matrix (Advanced tactics, techniques, and common knowledge) for analysis and simulation test. Based on analyzing the log file generated by the attack, the domain attack detection rules are established and input into the analysis engine. Finally, the available domain intrusion detection system is designed and implemented. Experimental results show that the network attack detection method based on the analysis of domain attack behavior can analyze the log file in real time and effectively detect the malicious intrusion behavior of hackers , which could facilitate managers find and eliminate network security threats immediately.","","978-0-7381-0545-1","10.1109/CISP-BMEI51763.2020.9263663","University of Jinan; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9263663","network security;attack detection;active directory","Computer hacking;Feature extraction;Real-time systems;Tools;Communication networks;Databases;Forgery","computer crime;computer network security","network security threats;normal attack;APT attack chain;antivirus software;security protection;domain control;effective market-oriented products;common domain intrusion methods;domain related attack behavior characteristics;log file;domain attack detection rules;network attack detection method;domain intrusion detection system","","","","7","","25 Nov 2020","","","IEEE","IEEE Conferences"
"Dealing with noise in defect prediction","S. Kim; H. Zhang; R. Wu; L. Gong","Hong Kong University of Science and Technology, Hong Kong, China; Tsinghua University, Beijing, China; Tsinghua University, Beijing, China; Tsinghua University, Beijing, China","2011 33rd International Conference on Software Engineering (ICSE)","10 Oct 2011","2011","","","481","490","Many software defect prediction models have been built using historical defect data obtained by mining software repositories (MSR). Recent studies have discovered that data so collected contain noises because current defect collection practices are based on optional bug fix keywords or bug report links in change logs. Automatically collected defect data based on the change logs could include noises. This paper proposes approaches to deal with the noise in defect data. First, we measure the impact of noise on defect prediction models and provide guidelines for acceptable noise level. We measure noise resistant ability of two well-known defect prediction algorithms and find that in general, for large defect datasets, adding FP (false positive) or FN (false negative) noises alone does not lead to substantial performance differences. However, the prediction performance decreases significantly when the dataset contains 20%-35% of both FP and FN noises. Second, we propose a noise detection and elimination algorithm to address this problem. Our empirical study shows that our algorithm can identify noisy instances with reasonable accuracy. In addition, after eliminating the noises using our algorithm, defect prediction accuracy is improved.","1558-1225","978-1-4503-0445-0","10.1145/1985793.1985859","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6032487","buggy changes;buggy files;data quality;defect prediction;noise resistance","Noise;Training;Predictive models;Noise measurement;Resistance;Prediction algorithms;Electrical resistance measurement","data mining;program debugging;program testing","software defect prediction models;historical defect data;mining software repositories;MSR;optional bug fix keywords;bug report links;false positive noise;false negative noise","","88","","35","","10 Oct 2011","","","IEEE","IEEE Conferences"
"Research and Implementation of Data Loading and Synchronization in Memory Database","M. Wu; X. Zhao; X. Hou; J. Zhang; R. Liu","Beijing Institute of Technology,School of Software,Beijing,China,100081; Beijing Institute of Technology,School of Software,Beijing,China,100081; Beijing Institute of Technology,School of Software,Beijing,China,100081; Beijing Institute of Technology,School of Software,Beijing,China,100081; Beijing Institute of Technology,School of Software,Beijing,China,100081","2019 International Conference on Sensing, Diagnostics, Prognostics, and Control (SDPC)","17 Aug 2020","2019","","","790","796","This paper focuses on the data loading and synchronization strategy of memory database, and proposes a data loading and synchronization strategy for GBase8m, which is a memory database prototype system. The ping-pong checkpoint strategy is used when memory data are synchronized to the disk; The disk data are loaded into memory which can be divided into boot data loading and recovery data loading, in which boot data loading uses the Memory-Mapped Files provided by operating system, and the method of combining log with checkpoint technology is used for recovery loading. The test proves that the data loading and synchronization strategy of GBase8m is feasible, with some advantages in performance.","","978-1-7281-0199-6","10.1109/SDPC.2019.00150","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9169036","memory database;data synchronization;data loading","Synchronization;Loading;Memory management;Indexes;Concurrency control","checkpointing;data integrity;database management systems;fault tolerant computing;operating system kernels;security of data;synchronisation","synchronization strategy;recovery loading;Memory-Mapped Files;recovery data loading;boot data loading;disk data;memory data;ping-pong checkpoint strategy;memory database prototype system","","","","15","","17 Aug 2020","","","IEEE","IEEE Conferences"
"Design and Implementation of Meal Information Collection System Using IoT Wireless Tags","K. Kaiya; A. Koyama","Dept. of Inf., Yamagata Univ., Yamagata, Japan; Dept. of Inf., Yamagata Univ., Yamagata, Japan","2016 10th International Conference on Complex, Intelligent, and Software Intensive Systems (CISIS)","22 Dec 2016","2016","","","503","508","In recent years, people with lifestyle-related diseases are increasing, because of lack of exercise, overeating, stress and smoking. With this situation, applications of Personal Computer (PC) or smart phone which support healthy life are widespread. Turning to the system dealing with meal information, there are two type systems. One is that meal records are inputted by hand and managed as meal logs. Another one is that a meal image is shot by a smart phone, and meal contents are recorded by image recognition. However, these systems need a lot of time for inputting of meal contents or shooting of meal images. Therefore, it is difficult to continue use. Moreover, the second system has a problem for accuracy of image recognition. In this paper, to solve these problems, we propose a meal information collection system which uses IoT wireless tags. An IoT wireless tag is corresponded to a meal and the system can automatically get meal information such as meal contents, meal sequence, meal time and so on. We implemented the meal information collection system which aims automatic collection and getting of various information which do not get at other systems. From the evaluation results of collection accuracy and comparison with conventional systems, we verified effectiveness of the proposed system.","","978-1-5090-0987-9","10.1109/CISIS.2016.66","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7791934","IoT;Meal information collection;Meal record;Health care;Sensor;Tag","","health care;image recognition;information systems;Internet of Things;mobile computing;smart phones","meal information collection system;Internet of Things;IoT wireless tag;smart phone;image recognition;health care","","6","","10","","22 Dec 2016","","","IEEE","IEEE Conferences"
"Real-time classification of malicious URLs on Twitter using machine activity data","P. Burnap; A. Javed; O. F. Rana; M. S. Awan","School of Computer Science and Informatics, Cardiff University, Cardiff, UK; School of Computer Science and Informatics, Cardiff University, Cardiff, UK; School of Computer Science and Informatics, Cardiff University, Cardiff, UK; School of Computer Science and Informatics, Cardiff University, Cardiff, UK","2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)","11 Feb 2016","2015","","","970","977","Massive online social networks with hundreds of millions of active users are increasingly being used by Cyber criminals to spread malicious software (malware) to exploit vulnerabilities on the machines of users for personal gain. Twitter is particularly susceptible to such activity as, with its 140 character limit, it is common for people to include URLs in their tweets to link to more detailed information, evidence, news reports and so on. URLs are often shortened so the endpoint is not obvious before a person clicks the link. Cyber criminals can exploit this to propagate malicious URLs on Twitter, for which the endpoint is a malicious server that performs unwanted actions on the person's machine. This is known as a drive-by-download. In this paper we develop a machine classification system to distinguish between malicious and benign URLs within seconds of the URL being clicked (i.e. `real-time'). We train the classifier using machine activity logs created while interacting with URLs extracted from Twitter data collected during a large global event - the Superbowl - and test it using data from another large sporting event - the Cricket World Cup. The results show that machine activity logs produce precision performances of up to 0.975 on training data from the first event and 0.747 on a test data from a second event. Furthermore, we examine the properties of the learned model to explain the relationship between machine activity and malicious software behaviour, and build a learning curve for the classifier to illustrate that very small samples of training data can be used with only a small detriment to performance.","","978-1-4503-3854-7","10.1145/2808797.2809281","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7403664","","Twitter;Malware;Web pages;Data models;Uniform resource locators;Real-time systems","computer crime;invasive software;pattern classification;social networking (online)","real-time malicious URL classification;machine activity data;online social networks;cyber criminals;malicious software;malware;user machines;malicious server;drive-by-download;machine classification system;machine activity logs;Twitter data;Superbowl;Cricket World Cup;malicious software behaviour","","4","2","30","","11 Feb 2016","","","IEEE","IEEE Conferences"
"Dynamic malware detection using registers values set analysis","M. Ghiasi; A. Sami; Z. Salehi","CSE & IT Department, Shiraz University, Shiraz, Iran; CSE & IT Department, Shiraz University, Shiraz, Iran; CSE & IT Department, Shiraz University, Shiraz, Iran","2012 9th International ISC Conference on Information Security and Cryptology","10 Jan 2013","2012","","","54","59","The number of Malicious files increase every day because of existing open source malware and obfuscation techniques. It means that traditional signature-based techniques are not adequate for detecting new variant of malware. Researchers and anti malware companies recently focus on more advanced protection which needs influential pattern extraction techniques. In this paper, a novel method is proposed based on similarities of binaries behaviors. At first, Run-time behavior of the binary files are found and logged in a controlled environment tool which is developed in-house. The approach assumes that behavior of each binary can be represented by the values of memory contents in its run-time. That is, values stored in different registers while the malware is running in the controlled environment can be a distinguishing factor to set it apart from those of benign programs. Then, the register values for each Application Programming Interface (API) call are extracted before and after API is invoked. After that, we traced the distribution and changes of registers values throughout the executable file and created a vector for each of the values of EAX, EBX, EDX, EDI, ESI and EBP registers. With comparing the similarity measures between old and unseen malware vectors, we detected 98% of unseen samples and with 2.9% false positive.","","978-1-4673-2386-4","10.1109/ISCISC.2012.6408191","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6408191","Malware Detection;API Call;Dynamic Analysis;Memory Content;Register Value","Malware;Registers;Feature extraction;Monitoring;Accuracy;Indexes;Data mining","application program interfaces;digital signatures;invasive software;public domain software","dynamic malware detection;registers values set analysis;malicious files;open source malware;obfuscation techniques;signature-based techniques;antimalware companies;pattern extraction techniques;controlled environment tool;binary files;benign programs;application programming interface;API registers;EAX registers;EBX registers;EDX registers;EDI registers;ESI registers;EBP registers","","15","","20","","10 Jan 2013","","","IEEE","IEEE Conferences"
"Adopting Text Clustering in web-based application to facilitate searching of education information","N. Nilsson; Y. Liu","School of Software Engineering, Tongji University, Shanghai, China; School of Software Engineering, Tongji University, Shanghai, China","2014 IEEE 5th International Conference on Software Engineering and Service Science","23 Oct 2014","2014","","","393","396","Clustering, as a part of the Data Mining field, has been in the center of the research attention for the last decade. It is the task of finding subsets of data that are sharing the same type of attributes. Text Clustering becomes one of the most critical and important solutions in data mining to discover knowledge from fast grow up web data and log files. There are many challenges, algorithms needs to be tailored specific for each domain and scale well with growing data sets. Another interesting aspect is the design of the system. A complex set components need to interact well together. This article proposes an elegant way of clustering university educations based on their text attributes. The solution is integrated directly into a Spring Web Application. A comprehensive architecture is proposed, providing the frameworks needed. Clustering techniques such as Canopy Generation [4] and k-Means are demonstrated.","2327-0594","978-1-4799-3279-5","10.1109/ICSESS.2014.6933590","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6933590","Spring;Hibernate;Apache Hadoop;Apache Ma-hout;Lucene Index;Canopy Generation;k-Means","Clustering algorithms;Indexes;Servers;Educational institutions;Preforms","data mining;educational administrative data processing;educational institutions;pattern clustering;text analysis;Web sites","text clustering;Web-based application;university education information searching;data mining;knowledge discovery;canopy generation;k-means clustering","","","","5","","23 Oct 2014","","","IEEE","IEEE Conferences"
"Foreniscs analysis of cloud computing services","A. Ghafarian","Department of CSIS, University of North Georgia, Dahlonega, GA, 30597, USA","2015 Science and Information Conference (SAI)","3 Sep 2015","2015","","","1335","1339","Cloud computing services have become popular among businesses and individuals. A common application of cloud computing service is cloud storage service. Cloud storage services provide storage to the individuals and businesses. Businesses and individuals generally use cloud storage services for backup purposes and for sharing documents, pictures, videos, and multimedia files with others. When using cloud storage services, there are two main concerns namely, cloud security and forensics investigation in the cloud. The security concern is, the data stored in the cloud be compromised. The forensics concern is, the recovery of computer evidence from the cloud is more complicated. This research contributes to the forensics analysis of cloud storage services. We report the result of our experiment in obtaining forensics artifacts from cloud storage services such as Dropbox. We demonstrate that the use of computer forensics tools and systematic forensics methodologies can produce valuable artifacts to the computer forensics investigators. This is because criminals may use cloud services for illegal purposes. Moreover, we use Wireshark to analyze network traffic log files after working with cloud software to determine the details of activities in the cloud.","","978-1-4799-8547-0","10.1109/SAI.2015.7237316","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7237316","Cloud computing;cloud storage services;forensics;Dropbox;Security","Cloud computing;Forensics;Computers;Security;Business;Google","back-up procedures;cloud computing;digital forensics;storage management","cloud software;network traffic log file;Wireshark;computer forensics investigator;systematic forensics methodology;computer forensics tool;Dropbox;forensics artifact;computer evidence;data storage;security concern;forensics investigation;cloud security;backup purpose;cloud storage service;cloud computing service;foreniscs analysis","","4","","16","","3 Sep 2015","","","IEEE","IEEE Conferences"
"ADAMANT — An Anomaly Detection Algorithm for MAintenance and Network Troubleshooting","E. Martinez; E. Fallon; S. Fallon; M. Wang","Network Management Laboratory, Ericsson, Athlone, Ireland; Network Management Laboratory, Ericsson, Athlone, Ireland; Network Management Laboratory, Ericsson, Athlone, Ireland; Software Research Institute, Athlone Institute of Technology, Athlone, Ireland","2015 IFIP/IEEE International Symposium on Integrated Network Management (IM)","2 Jul 2015","2015","","","1292","1297","Network operators are increasingly using analytic applications to improve the performance of their networks. Telecommunications analytical applications typically use SQL and Complex Event Processing (CEP) for data processing, network analysis and troubleshooting. Such approaches are hindered as they require an in-depth knowledge of both the telecommunications domain and telecommunications data structures in order to create the required queries. Valuable information contained in free form text data fields such as “additional_info”, “user_text” or “problem_text” can also be ignored. This work proposes An Anomaly Detection Algorithm for MAintenance and Network Troubleshooting (ADAMANT), a text analytic based network anomaly detection approach. Once telecommunications data records have been indexed, ADAMANT uses distance based outlier detection within sliding windows to detect abnormal terms at configurable time intervals. Traditional approaches focus on a specific type of record and create specific cause and effect rules. With the ADAMANT approach all free form text fields of alarms, logs, etc. are treated as text documents similar to Twitter feeds. All documents within a window represent a snapshot of the network state that is processed by ADAMANT. The ADAMANT approach focuses on text analytics to provide automated analysis without the requirement for SQL/CEP queries. Such an approach provides distinct network insights in comparison to traditional approaches.","1573-0077","978-1-4799-8241-7","10.1109/INM.2015.7140484","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7140484","outlier;text anomaly;distance based;sliding windows;search Engine","Telecommunications;Search engines;Conferences;Detection algorithms;Indexes;Big data;Algorithm design and analysis","performance evaluation;search engines;security of data;text analysis","ADAMANT;anomaly detection algorithm for maintenance and network troubleshooting;network operators;network performance;telecommunications analytical applications;SQL;complex event processing;CEP;data processing;network analysis;telecommunications domain;telecommunications data structures;additional_info;user_text;problem_text;text analytic based network anomaly detection approach;telecommunications data records;distance based outlier detection;sliding windows;abnormal terms detection;configurable time intervals;text documents;Twitter feeds;network state;search engine","","4","","21","","2 Jul 2015","","","IEEE","IEEE Conferences"
"Real time implementation of intrusion detection system with reconfigurable architecture","A. Moghaddam","Department of Electrical & Computer Engineering, Shahid Beheshti University (SBU), Tehran, Iran","2012 IEEE Conference on Open Systems","24 Jan 2013","2012","","","1","5","Intrusion detection is the process of monitoring the events occurring in a computer system or network and analyzing them for signs of possible incidents, which are violations of computer security policies, or standard security practices. Intrusion detection system identifies possible incidents, logs information and provides report about them. In this article a real time intrusion detection system using SNORT rules and KMP algorithm is implemented in reconfigurable hardware. The parallel structure of this architecture let us to achieve a high real time performance at rate 100Mbps as it is shown by simulation and synthesis results on VIRTEX4.","","978-1-4673-1046-8","10.1109/ICOS.2012.6417634","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6417634","Intrusion Detection;Pattern Matching;KMP;Network Security;FPGA","Intrusion detection;Real-time systems;Search engines;Read only memory;Pattern matching;Field programmable gate arrays;Hardware","computer network security;field programmable gate arrays;parallel processing;public domain software;reconfigurable architectures;security of data;system monitoring","reconfigurable architecture;computer system;computer network;events monitoring process;computer security policies;standard security practices;logs information;real time intrusion detection system;Snort rules;KMP algorithm;reconfigurable hardware;parallel architecture structure;VIRTEX4","","1","","12","","24 Jan 2013","","","IEEE","IEEE Conferences"
"Provenance Collection in Reservoir Management Workflow Environments","F. Sun; J. Zhao; K. Gomadam; V. K. Prasanna","Dept. of Comput. Sci., Univ. of Southern California, Los Angeles, CA, USA; Dept. of Comput. Sci., Univ. of Southern California, Los Angeles, CA, USA; Ming Hsieh Dept. of Electr. Eng., Univ. of Southern California, Los Angeles, CA, USA; Ming Hsieh Dept. of Electr. Eng., Univ. of Southern California, Los Angeles, CA, USA","2010 Seventh International Conference on Information Technology: New Generations","1 Jul 2010","2010","","","82","87","There has been a recent push towards applying information technology principles, such as workflows, to bring greater efficiency to reservoir management tasks. These workflows are data intensive in nature, and the data is derived from heterogenous data sources. This has placed an emphasis on the quality and reliability of data that is used in reservoir engineering applications. Data provenance is metadata that pertains to the history of the data and can be used to assess data quality. In this paper, we present an approach for collecting provenance information from application logs in the domain of reservoir engineering. In doing so, we address challenges due to: 1) the lack of a workflow orchestration framework in reservoir engineering and 2) the inability of many reservoir engineering applications to collect provenance information. We present an approach that uses the workflow instances detection algorithm and the Open Provenance Model (OPM) for capturing provenance information from the logs.","","978-1-4244-6271-1","10.1109/ITNG.2010.222","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5501450","provenance;workflow;reservoir engineering","Reservoirs;Environmental management;Engineering management;History;Information technology;Technology management;Quality management;Predictive models;Application software;Conference management","chemical engineering;data handling;hydrocarbon reservoirs;recording;workflow management software","reservoir management workflow environments;information technology principles;heterogenous data sources;data quality;data reliability;reservoir engineering;data provenance;application logs;workflow orchestration framework;open provenance model;workflow instances detection algorithm","","2","","16","","1 Jul 2010","","","IEEE","IEEE Conferences"
"A GENERALIZED AGENT BASED FRAMEWORK FOR MODELING A BLOCKCHAIN SYSTEM","C. Kaligotla; C. M. Macal","Decision and Infrastructure Sciences, Argonne National Laboratory, Argonne, IL 60439, USA; Decision and Infrastructure Sciences, Argonne National Laboratory, Argonne, IL 60439, USA","2018 Winter Simulation Conference (WSC)","3 Feb 2019","2018","","","1001","1012","We describe an agent-based conceptual model of a Blockchain system. Blockchain technology enables distributed, encrypted and secure logging of digital transactions in an add-only ledger record. We model and simulate an expanding Blockchain network, describing the participating agents, the transactions, and the verified add-only public ledger record achieving decentralized consensus. All of the essential details of the functioning of the Blockchain including the behaviors and decisions made by agents deciding to join as well as participate in the market are detailed in the prototype model. The aim of this paper is to illustrate the essential elements and functioning of a Blockchain system, implement a generalized simulation and a measure of Blockchain efficiency from an agent choice and energy cost perspective. Our preliminary results indicate that mining choice (transaction block to verify) coupled with proof of work incentives are critical for energy efficiency.","1558-4305","978-1-5386-6572-5","10.1109/WSC.2018.8632374","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8632374","","Blockchain;Fault tolerance;Fault tolerant systems;Computational modeling;Bitcoin","cryptocurrencies;data mining;distributed databases;multi-agent systems;software agents","digital transactions;add-only public ledger record;prototype model;blockchain system;energy cost perspective;agent-based conceptual model;generalized agent based framework;decentralized consensus;mining choice;secure logging;distributed logging;encrypted logging","","2","","24","","3 Feb 2019","","","IEEE","IEEE Conferences"
"Lightweight Order-Based Deterministic Replay of Java Multithread Program","Z. Wang; K. Zhou; G. Zhao","Software Sch., Dalian Univ. of Technol., Dalian, China; Software Sch., Dalian Univ. of Technol., Dalian, China; Software Test & Evaluation Center, China Aerosp., Beijing, China","2015 2nd International Symposium on Dependable Computing and Internet of Things (DCIT)","17 Mar 2016","2015","","","50","58","Deterministic replay can reenact an earlier program execution, which can facilitate debugging the parallel program. There is non-determinism to access to shared resources when a parallel program runs on multicore processor, from which the major challenge of deterministic replay comes. An execution sequence inferred deterministic replay technique is proposed to reduce the record cost, ESIDR. The key idea of ESIDR is tracking the read actions of shared memory with a virtual cache and logging write-read dependence when a write action of shared memory happens. The dependence among shared memory accesses is obtained by analyzing the log that is a subset of exact dependence. We make an effective and feasible trade-off between recording and replaying to reduce the runtime overhead and the log size at record. Our replay approach guarantees the real and complete dependence between writing and reading of shared memory. A contrast experiment with state-of-the-art deterministic replay technique LEAP is conducted. The evaluation results show that ESIDR makes 2.38 times and 3.91 times reduction on the runtime overhead and the log size, comparing with LEAP.","","978-1-5090-0290-0","10.1109/DCIT.2015.7","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7434469","multicore processor;parallel prgram;deterministic replay;execution sequence;multithread program;shared memory","Internet of things","Java;multiprocessing programs;multi-threading;program debugging;shared memory systems;system monitoring;virtual storage","lightweight order-based deterministic replay;Java multithread program;program execution;program debugging;parallel program;multicore processor;ESIDR;virtual cache;logging write-read dependence;shared memory;LEAP","","","","22","","17 Mar 2016","","","IEEE","IEEE Conferences"
"Spark Streaming for Predictive Business Intelligence","N. S. G. Ganesh; A. Mummoorthy; R. R. Chandrika; A. Raman G.R.","MallaReddy College of Engineering and Technology,Department of Information Technology,Hyderabad,India; MallaReddy College of Engineering and Technology,Department of Information Technology,Hyderabad,India; MallaReddy College of Engineering and Technology,Department of Information Technology,Hyderabad,India; Department of Computer Science & Engg, Malla Reddy Institute of Engineering & Technology, Hyderabad, India","2019 International Conference on Emerging Trends in Science and Engineering (ICESE)","11 Sep 2020","2019","1","","1","4","Apache spark can process the data in real time with the test mining and natural language processing. The business intelligence can be improved by collecting and processing the data from web in real time. Process mining collects the data from event logs in process discovery. Then diagnosis the difference between the observed and reality through an event logs. And extended the data of the event. Dealing with huge data process mining finds difficulty in processing. Spark handles the data processing speed and real time. It receives the input data and segregated into batches put up in processing. The incoming data append into the already existing data for processing. It identifies the problems and quick report generation of processing data.","","978-1-7281-1871-0","10.1109/ICESE46178.2019.9194677","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9194677","Spark;Process Mining;Natural Language Processing;Business Intelligence","Sparks;Real-time systems;Business intelligence;Data mining;Distributed databases;Natural language processing;Open source software","competitive intelligence;data mining;Internet;natural language processing","spark streaming;predictive business intelligence;test mining;Apache Spark;natural language processing;event logs;process discovery;Web data processing;data process mining;data segregation","","","","21","","11 Sep 2020","","","IEEE","IEEE Conferences"
"Cloud Workload Analysis with SWAT","M. Breternitz; K. Lowery; A. Charnoff; P. Kaminski; L. Piga",NA; NA; NA; NA; NA,"2012 IEEE 24th International Symposium on Computer Architecture and High Performance Computing","6 Dec 2012","2012","","","92","99","This note describes the Synthetic Workload Application Toolkit (SWAT) and presents the results from a set of experiments on some key cloud workloads. SWAT is a software platform that automates the creation, deployment, provisioning, execution, and (most importantly) data gathering of synthetic compute workloads on clusters of arbitrary size. SWAT collects and aggregates data from application execution logs, operating system call interfaces, and micro architecture-specific program counters. The data collected by SWAT are used to characterize the effects of network traffic, file I/O, and computation on program performance. The output is analyzed to provide insight into the design and deployment of cloud workloads and systems. Each workload is characterized according to its scalability with the number of server nodes and Hadoop server jobs, sensitivity to network characteristics (bandwidth, latency, statistics on packet size), and computation vs. I/O intensity as these values adjusted via workload-specific parameters. (In the future, we will use SWAT's benchmark synthesizer capability.) We also characterize micro-architectural characteristics that give insight on the micro architecture of processors better suited for this class of workloads. We contrast our results with prior work on Cloud Suite [5], validating some conclusions and providing further insight into others. This illustrates SWAT's data collection capabilities and usefulness to obtain insight on cloud applications and systems.","1550-6533","978-0-7695-4907-1","10.1109/SBAC-PAD.2012.13","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6374776","cloud;scale-out;workload;servers","Benchmark testing;Operating systems;Radiation detectors;Servers;Delay;Electric breakdown;Microarchitecture","cloud computing;file servers;operating systems (computers);statistics","cloud workload analysis;SWAT;synthetic workload application toolkit;software platform;synthetic compute workloads;application execution logs;operating system call interfaces;microarchitecture-specific program counters;network traffic effects;file I-O;server nodes;Hadoop server jobs;bandwidth;latency;packet size statistics;Cloud Suite","","3","2","12","","6 Dec 2012","","","IEEE","IEEE Conferences"
"Process mining in business process management: Concepts and challenges","R. Saylam; O. K. Sahingoz","Department of Computer Engineering Turkish Air Force Academy 34149 - Yesilyurt, Istanbul, Turkey; Department of Computer Engineering Turkish Air Force Academy 34149 - Yesilyurt, Istanbul, Turkey","2013 International Conference on Electronics, Computer and Computation (ICECCO)","23 Jan 2014","2013","","","131","134","Process mining is an emerging research area that aims to improve the analysis of Business Process Models (BPMs) by extracting knowledge from event logs. What actually happened in the organization is set forth for consideration, not what people think about the organization. Therefore, it can be used in various industrial and scientific applications. This paper aims to provide information about the process mining concept, by revealing the differences with data mining, which is more commonly known in the literature, to point out the challenges in the use of BPM, to introduce the studies on this subject, to serve as a guide and to provide meta-information for researchers, scientists, software developers, etc. who are interested in the process mining.","","978-1-4799-3343-3","10.1109/ICECCO.2013.6718246","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6718246","Process Mining;Data Mining","Data mining;Business;Process control;Data models;Unified modeling language;Analytical models;Software","business data processing;data mining;knowledge management","business process management;process mining;business process models;BPM;knowledge extraction;event logs;organization;industrial applications;scientific applications;data mining;meta-information","","5","","7","","23 Jan 2014","","","IEEE","IEEE Conferences"
"Development of Debugging Exercise Extraction System using Learning History","K. Umezawa; M. Nakazawa; M. Goto; S. Hirasawa",Shonan Institute of Technology; Junior College of Aizu; Waseda University; Waseda University,"2019 IEEE Tenth International Conference on Technology for Education (T4E)","6 Feb 2020","2019","","","244","245","We have proposed an editing history visualization system which can confirm where and how the learner modified program. We utilized this system for actual flipped classroom and stored a large amount of learning logs. This learning log contains all the source code in the process of being modified until the program is completed. We developed a debugging exercise extraction system to automatically generate problems for debugging practice from this learning log. The debugging exercise extraction tool we developed extracted 18,680 source codes (which became practice problems) that included syntactic errors that could be used as a debugging exercise from 16 weeks of program edit history data (total number is 31,562 files). The execution time was 488 seconds. Since it can be analyzed only once every six months, we believe it is a sufficiently practical execution time.","","978-1-7281-4227-2","10.1109/T4E.2019.00056","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8983736","e-Learning, Self Study, Artificial Teacher, Language Leaning, Learning History","History;Debugging;Tools;Data mining;Data visualization;Education;Conferences","computer aided instruction;computer science education;data visualisation;program debugging;software engineering;source code (software)","program edit history data;learning history;editing history visualization system;actual flipped classroom;learning logs;source code;debugging exercise extraction system development;syntactic errors;e-learning","","1","","3","","6 Feb 2020","","","IEEE","IEEE Conferences"
"Data fusion at the source: Standards and technologies for seamless sensor integration — Alan Weber","J. Huang; A. Tuan","Cimetrix Incorporated 6979 South High Tech Drive; Salt Lake City, Utah 84047-3757; USA; Cimetrix Incorporated 6979 South High Tech Drive; Salt Lake City, Utah 84047-3757; USA","2015 Joint e-Manufacturing and Design Collaboration Symposium (eMDC) & 2015 International Symposium on Semiconductor Manufacturing (ISSM)","16 Nov 2015","2015","","","1","2","Process engineers are continually looking for ways to understand equipment behavior and improve process performance as feature sizes shrink and the related process windows narrow. In additional to collecting more and more data from the equipment directly, leading-edge factories often use custom, external sensors to improve process visibility and control. Some even extract information from internal equipment log files to look deeply into the mechanisms that may affect process performance and/or equipment productivity. As these efforts yield results, the number and variety of data sources required to support new analysis algorithms and applications will continue to grow with each manufacturing process node (see Figure 1). The problem with this explosion of data sources is that most of them are connected to the factory systems with custom, in-house software solutions, and the collected data is usually stored in whatever form/location is most convenient for the initial consuming application. Only rarely will any of this data be stored in a comprehensive, engineering database suitable for general use across the factory. This results in a “trail mix” style of system architecture that becomes increasingly difficult to understand and support over time. Clearly, a better approach is required.","","978-9-8691-7151-9","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7328913","Data Collection/Unification;External Sensor Integration;Standards;Predictive Analytics","Standards;Data models;Production facilities;Data collection;Systems architecture;Context;Process control","manufacturing data processing;manufacturing processes;process control;production equipment;productivity;sensor fusion","equipment behavior;process engineers;process performance;process windows;leading-edge factories;external sensors;process visibility;process control;internal equipment log files;equipment productivity;manufacturing process;factory systems;in-house software solutions;data fusion;seamless sensor integration","","","","1","","16 Nov 2015","","","IEEE","IEEE Conferences"
"A Self-Adaptive Bell–LaPadula Model Based on Model Training With Historical Access Logs","Z. Tang; X. Ding; Y. Zhong; L. Yang; K. Li","College of Information Science and Engineering, Hunan University, Changsha, China; College of Information Science and Engineering, Hunan University, Changsha, China; College of Information Science and Engineering, Hunan University, Changsha, China; College of Computer and Communication Engineering, Changsha University of Science and Technology, Hunan, China; College of Information Science and Engineering, Hunan University, Changsha, China","IEEE Transactions on Information Forensics and Security","16 Apr 2018","2018","13","8","2047","2061","In currently popular access control models, the security policies and regulations never change in the running system process once they are identified, which makes it possible for attackers to find the vulnerabilities in a system, resulting in the lack of ability to perceive the system security status and risks in a dynamic manner and exposing the system to such risks. By introducing the maximum entropy (MaxENT) models into the rule optimization for the Bell-LaPadula (BLP) model, this paper proposes an improved BLP model with the self-learning function: MaxENT-BLP. This model first formalizes the security properties, system states, transformational rules, and a constraint model based on the states transition of the MaxENT. After handling the historical system access logs as the original data sets, this model extracts the user requests, current states, and decisions to act as the feature vectors. Second, we use k -fold cross validation to divide all vectors into a training set and a testing set. In this paper, the model training process is based on the Broyden-Fletcher-Goldfarb-Shanno algorithm. And this model contains a strategy update algorithm to adjust the access control rules dynamically according to the access and decision records in a system. Third, we prove that MaxENT-BLP is secure through theoretical analysis. By estimating the precision, recall, and F1-score, the experiments show the availability and accuracy of this model. Finally, this paper provides the process of model training based on deep learning and discussions regarding adversarial samples from the malware classifiers. We demonstrate that MaxENT-BLP is an appropriate choice and has the ability to help running information systems to avoid more risks and losses.","1556-6021","","10.1109/TIFS.2018.2807793","National Natural Science Foundation of China; National Key Research and Development Program of China; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8295130","Adversarial sample;BLP;machine learning;mandatory access control;maximum entropy model;rule optimization","Hidden Markov models;Training;Access control;Data models;Feature extraction;Machine learning","authorisation;entropy;invasive software;learning (artificial intelligence);optimisation","historical access;security policies;regulations;running system process;system security status;maximum entropy models;rule optimization;improved BLP model;MaxENT-BLP;security properties;system states;transformational rules;constraint model;states transition;historical system access;training set;Broyden-Fletcher-Goldfarb-Shanno algorithm;access control rules;decision records;running information systems;training process;self-adaptive Bell-LaPadula model;access control models;precision;recall;F1-score","","2","","37","","19 Feb 2018","","","IEEE","IEEE Journals"
"Managing Security Control Assumptions Using Causal Traceability","A. Nhlabatsi; Y. Yu; A. Zisman; T. Tun; N. Khan; A. Bandara; K. M. Khan; B. Nuseibeh","Dept. of Comput. Sci. & Eng., Qatar Univ., Doha, Qatar; Dept. of Comput. & Commun., Open Univ., Milton Keynes, UK; Dept. of Comput. & Commun., Open Univ., Milton Keynes, UK; Dept. of Comput. & Commun., Open Univ., Milton Keynes, UK; Dept. of Comput. Sci. & Eng., Qatar Univ., Doha, Qatar; Dept. of Comput. & Commun., Open Univ., Milton Keynes, UK; Dept. of Comput. Sci. & Eng., Qatar Univ., Doha, Qatar; Lero, Univ. of Limerick, Dublin, Ireland","2015 IEEE/ACM 8th International Symposium on Software and Systems Traceability","6 Aug 2015","2015","","","43","49","Security control specifications of software systems are designed to meet their security requirements. It is difficult to know both the value of assets and the malicious intention of attackers at design time, hence assumptions about the operational environment often reveal unexpected flaws. To diagnose the causes of violations in security requirements it is necessary to check these design-time assumptions. Otherwise, the system could be vulnerable to potential attacks. Addressing such vulnerabilities requires an explicit understanding of how the security control specifications were defined from the original security requirements. However, assumptions are rarely explicitly documented and monitored during system operation. This paper proposes a systematic approach to monitoring design-time assumptions explicitly as logs, by using trace ability links from requirements to specifications. The work also helps identify which alternative specifications of security control can be used to satisfy a security requirement that has been violated based on the logs. The work is illustrated by an example of an electronic patient record system.","2157-2194","978-0-7695-5593-5","10.1109/SST.2015.14","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7181527","Causal Traceability;Assumptions;Security","Security;Monitoring;Global Positioning System;Context;Hospitals;Runtime","program diagnostics;security of data;software engineering","security control assumption;causal traceability link;software system;malicious intention;design-time assumption;security control specification;software development","","","","28","","6 Aug 2015","","","IEEE","IEEE Conferences"
"A Tool for Tuning Binarization Techniques","V. Sokratis; E. Kavallieratou","Inf. & Commun. Syst. Eng., Univ. of the Aegean, Samos, Greece; Inf. & Commun. Syst. Eng., Univ. of the Aegean, Samos, Greece","2011 International Conference on Document Analysis and Recognition","3 Nov 2011","2011","","","1","5","In this paper a user friendly tool appropriate to get user feedback for the application of binarization algorithms is presented. The human feedback is very useful in order to apply next the algorithm to similar images. The tool supports Image Selection and Display, Selection of Binarization Algorithm and Parameter Configuration, Feedback gathering and Creation of log file for further processing.","2379-2140","978-0-7695-4520-2","10.1109/ICDAR.2011.10","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6065265","document image processing;binarization algorithms;user-feedback","Noise;Databases;Modeling;Heuristic algorithms;Humans;Pattern recognition;Optical character recognition software","document image processing;user interfaces","binarization technique;user feedback;image selection;image display;binarization algorithm selection;binarization parameter configuration;feedback gathering;log file creation;document image","","4","1","8","","3 Nov 2011","","","IEEE","IEEE Conferences"
"Simplified Business Process Model Mining Based on Structuredness Metric","W. Zhao; X. Liu; A. Wang","Sch. of Software, Fudan Univ., Shanghai, China; Sch. of Software, Fudan Univ., Shanghai, China; Sch. of Software, Fudan Univ., Shanghai, China","2011 Seventh International Conference on Computational Intelligence and Security","12 Jan 2012","2011","","","1362","1366","Process mining is the automated acquisition of process models from event logs. Although many process mining techniques have been developed, most of them focus on mining models from the prospective of control flow while ignoring the structure of mined models. This directly impacts the understandability and quality of mined models. To address the problem, we have proposed a genetic programming (GP) approach to mining simplified process models. Herein, genetic programming is applied to simplify the complex structure of process models using a tree-based individual representation. In addition, the fitness function derived from process complexity metric provides a guideline for discovering low complexity process models. Finally, initial experiments are performed to evaluate the effectiveness of the method.","","978-1-4577-2008-6","10.1109/CIS.2011.303","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6128344","process mining;genetic programming;process complexity metric;Structuredness Metric","Complexity theory;Electronic countermeasures;Measurement;Genetic algorithms;Genetic programming;Business;Process control","business data processing;data mining;genetic algorithms;trees (mathematics)","simplified business process model mining;structuredness metric;process model acquisition;event logs;control flow;genetic programming;tree-based individual representation;fitness function;process complexity metric","","","","16","","12 Jan 2012","","","IEEE","IEEE Conferences"
"Accommodating Stealth Assessment in Serious Games: Towards Developing a Generic Tool","K. Georgiadis; G. van Lankveld; K. Bahreini; W. Westera","Res. Centre for Learning, Open Univ. of the Netherlands, Heerlen, Netherlands; Res. Centre for Learning, Open Univ. of the Netherlands, Heerlen, Netherlands; Res. Centre for Learning, Open Univ. of the Netherlands, Heerlen, Netherlands; Res. Centre for Learning, Open Univ. of the Netherlands, Heerlen, Netherlands","2018 10th International Conference on Virtual Worlds and Games for Serious Applications (VS-Games)","18 Oct 2018","2018","","","1","4","Stealth assessment derives the progression of learning in an unobtrusive way from observed gameplay captured in log files. To this end, it uses machine learning technologies to provide probabilistic reasoning over established latent competency variable models. Now that video games are increasingly being used for training and learning purposes, stealth assessment could provide an excellent means of monitoring learning progress without the need for explicit testing. However, applying stealth assessment is a complex and laborious process. This paper analyses the limitations of stealth assessment and conceptualizes the requirements for developing a generic tool that could overcome its barriers and accommodate its practical application. Hence, a framework is presented describing its user and functional requirements. The proposed generic solution could open up the wider uptake of stealth assessment in serious games.","2474-0489","978-1-5386-7123-8","10.1109/VS-Games.2018.8493409","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8493409","","Games;Tools;Task analysis;Education;Machine learning;Software;Psychology","computer aided instruction;computer games;inference mechanisms;learning (artificial intelligence);testing","stealth assessment;generic tool;machine learning technologies;monitoring learning progress;training purposes;log files;latent competency variable models;probabilistic reasoning;video games;explicit testing;functional requirements","","","","23","","18 Oct 2018","","","IEEE","IEEE Conferences"
"Challenges of digital forensic investigation in cloud computing","S. Hraiz","King Hussien School of information Technology, Princess Sumaya University for Technology, Amman, Jordan","2017 8th International Conference on Information Technology (ICIT)","23 Oct 2017","2017","","","568","571","Cloud computing is a widely used technology these days. Most of the companies use this technique, because it reduces the cost and efforts which will be used to purchasing and managing the equipment for the business. Cloud computing has many features including scalability, availability, flexibility and many others. Even though cloud computing offers very important benefits to both public and commercial entities, it arises a new challenges for many fields in computer science, one of these fields is digital forensic. The investigators face many challenges when they investigate crime occurred using cloud. Many researchers try to identify and solve these challenges by proposing new methods and techniques. This paper highlights some of these challenges and discusses some potential solutions can be applied to solve them. The challenges were discussed in this paper are log files, volatile data, making forensic images, and data integrity.","","978-1-5090-6332-1","10.1109/ICITECH.2017.8080060","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8080060","Cloud Computing;Digital Forensic;Volatile Data;Data Integrity;Forensic Images","Cloud computing;Computational modeling;Software as a service;Digital forensics","cloud computing;data integrity;digital forensics;image forensics","cloud computing;digital forensic;log files;volatile data;forensic images;data integrity","","1","","13","","23 Oct 2017","","","IEEE","IEEE Conferences"
"EKETour - Software Platform for Organizing Tours","B. Bakó; V. E. Bartha; K. Simon; C. Sulyok; L. Kintzel","Babes Bolyai University, Cluj-Napoca, Romania; Codespring LLC, Babes Bolyai University, Cluj-Napoca, Romania; Babes Bolyai University, Cluj-Napoca, Romania; Codespring LLC, Babes Bolyai University, Cluj-Napoca, Romania; Codespring LLC, Cluj-Napoca, Romania","2018 IEEE 16th International Symposium on Intelligent Systems and Informatics (SISY)","8 Nov 2018","2018","","","000045","000050","The target audience of the EKETour software system is any organization that manages hikes, tours and/or other outdoor events. The purpose is to create a unified registration interface for tours. This makes signing up easier for returning hikers, because the general information only requires completion upon first registration. The interface provides the possibility for a single user to register the information of multiple people into several profiles. Hikers receive NFC tags to validate at checkpoints. Organizers can use a mobile application to check passers, validate and log their data. The software is designed and implemented at the request of and based on requirements from the Transylvanian Carpathia Society (EKE). The system aims to replace/amend the current personnel on-paper hiker tracking mechanism with a digital solution.","1949-0488","978-1-5386-6841-2","10.1109/SISY.2018.8524797","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8524797","","Servers;Organizations;Smart phones;Software;Databases;Data models;Password","near-field communication","organizing tours;software platform;mobile application;NFC tags;multiple people;single user;general information;unified registration interface;EKETour software system;target audience","","","","11","","8 Nov 2018","","","IEEE","IEEE Conferences"
"Detecting the presence of pigeons on PV modules in a pico-solar system","P. E. Hertzog; A. J. Swart","Department of Electrical, Electronics and Computer Engineering, Central University of Technology, Private Bag X20539, Bloemfontein, 9300; Department of Electrical, Electronics and Computer Engineering, Central University of Technology, Private Bag X20539, Bloemfontein, 9300","2017 IEEE AFRICON","7 Nov 2017","2017","","","1528","1533","Shading caused by the presence of pigeons can negatively affect the output power of a PV module in a pico-solar system. This is due to the fact that the body of the pigeon may interrupt the direct beam radiation received by a cell, or number of cells, resulting in output power loss and internal power dissipation within the PV module, as the shaded cells become reverse biased. Detecting the presence of pigeons and then deploying some type of intervention to scare the pigeons away may assist in reducing the shading time, thereby enabling the availability of maximum output power and reduced hot spots. The purpose of this paper is to present the design and results of a LabVIEW software system that was used to detect the presence of pigeons on PV modules in a pico-solar system. The developed system monitors the output voltage and current of multiple identical PV systems, and if a significant drop in current is detected in one system, while the others remain constant, then the system registers an event and logs the amount of time in 10-second intervals. For each sample of this event, the system records an image of the PV modules using a high definition webcam. Correlating the images to the actual events reveals an 86.4% accuracy, thereby indicating that the developed system is indeed detecting the presence of pigeons on the PV modules. It is further recommended to now integrate into this system some type of intervention which may be used to scare the pigeons away.","2153-0033","978-1-5386-2775-4","10.1109/AFRCON.2017.8095709","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8095709","Arduino;LabVIEW;Webcam;shading","Birds;Webcams;Power generation;User interfaces;Voltage measurement;Software systems;Monitoring","cameras;object detection;photovoltaic power systems;solar power stations;virtual instrumentation","PV modules;pigeon presence detection;picosolar system;direct beam radiation;shading time reduction;output voltage monitoring;output current monitoring;high definition webcam;multiple identical PV systems;LabVIEW software system;maximum output power;shaded cells;internal power dissipation;output power loss","","","","23","","7 Nov 2017","","","IEEE","IEEE Conferences"
"Confidentiality of event data in policy-based monitoring","M. Montanari; R. H. Campbell","University of Illinois at Urbana-Champaign, USA; University of Illinois at Urbana-Champaign, USA","IEEE/IFIP International Conference on Dependable Systems and Networks (DSN 2012)","9 Aug 2012","2012","","","1","12","Monitoring systems observe important information that could be a valuable resource to malicious users: attackers can use the knowledge of topology information, application logs, or configuration data to target attacks and make them hard to detect. The increasing need for correlating information across distributed systems to better detect potential attacks and to meet regulatory requirements can potentially exacerbate the problem if the monitoring is centralized. A single zero-day vulnerability would permit an attacker to access all information. This paper introduces a novel algorithm for performing policy-based security monitoring. We use policies to distribute information across several hosts, so that any host compromise has limited impact on the confidentiality of the data about the overall system. Experiments show that our solution spreads information uniformly across distributed monitoring hosts and forces attackers to perform multiple actions to acquire important data.","2158-3927","978-1-4673-1625-5","10.1109/DSN.2012.6263954","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6263954","security;monitoring;policy compliance;confidentiality;distributed systems","Monitoring;Servers;Correlation;Security;Computers;Software;Organizations","distributed processing;security of data","event data confidentiality;policy-based security monitoring;topology information;application logs;configuration data;distributed systems;potential attack detection;single zero-day vulnerability","","1","","29","","9 Aug 2012","","","IEEE","IEEE Conferences"
"Coded Load Balancing in Cache Networks","M. J. Siavoshani; F. Parvaresh; A. Pourmiri; S. P. Shariatpanahi","Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Electrical Engineering, University of Isfahan, Isfahan, Iran; Department of Software Engineering, University of Isfahan, Isfahan, Iran; School of Computer Science, Institute for Research in Fundamental Sciences (IPM), Tehran, Iran","IEEE Transactions on Parallel and Distributed Systems","10 Jan 2020","2020","31","2","347","358","We consider load balancing problem in a cache network consisting of storage-enabled servers forming a distributed content delivery scenario. Previously proposed load balancing solutions cannot perfectly balance out requests among servers, which is a critical issue in practical networks. Therefore, in this paper, we investigate a coded cache content placement where coded chunks of original files are stored in servers based on the files popularity distribution. In our scheme, upon each request arrival at the delivery phase, by dispatching enough coded chunks to the request origin from the nearest servers, the requested file can be decoded. Here, we show that if n requests arrive randomly at n servers, the proposed scheme results in the maximum load of O(1) in the network. This result is shown to be valid under various assumptions for the underlying network topology. Our results should be compared to the maximum load of two baseline schemes, namely, nearest replica and power of two choices strategies, which are O(log n) and O(log log n), respectively. This finding shows that using coding, results in a considerable load balancing performance improvement, without compromising communications cost performance. This is confirmed by performing extensive simulation results, in non-asymptotic regimes as well.","1558-2183","","10.1109/TPDS.2019.2933839","IPM; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8792130","Distributed caching servers;content delivery networks;coded caching;request routing;load balancing;communication cost","Servers;Load management;Measurement;Encoding;Network topology;Wireless communication;Content distribution networks","cache storage;computational complexity;computer networks;resource allocation;telecommunication network topology","underlying network topology;baseline schemes;nearest replica;considerable load balancing performance improvement;coded load;cache network;load balancing problem;storage-enabled servers;distributed content delivery scenario;load balancing solutions;coded cache content placement;coded chunks;original files;files popularity distribution;request arrival;delivery phase;request origin;nearest servers;requested file","","","","35","IEEE","8 Aug 2019","","","IEEE","IEEE Journals"
"Interaction, Mediation, and Ties: An Analytic Hierarchy for Socio-Technical Systems","D. D. Suthers","Dept. of Inf. & Comput. Sci., Univ. of Hawaii, Honolulu, HI, USA","2011 44th Hawaii International Conference on System Sciences","22 Feb 2011","2011","","","1","10","To understand how technological designs encourage synergistic encounters between people and ideas within socio-technical systems, techniques are needed to bridge between levels of description from process traces such as log data, through individual trajectories of activity that interact with each other, to dynamic networks of associations that are both created by and further shape these interactions. Towards this end, we have developed an analytic hierarchy and associated representations. Process traces are abstracted to contingency and uptake graphs: directed graphs that record observed relationships (contingencies) between events that offer evidence for interaction and other influences between actors (uptake). Contingency graphs are further abstracted to associograms: two-mode directed graphs that record how associations between actors are mediated by digital artifacts. Patterns in associograms summarize sequential patterns of interaction. Transitive closure of associograms yields sociograms, to which existing network analytic techniques may be applied. We discuss how the hierarchy bridges between theoretical levels of analysis.","1530-1605","978-1-4244-9618-1","10.1109/HICSS.2011.248","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5718896","","Software;Bridges;Analytical models;Media;Mediation;Context modeling;Sociotechnical systems","decision making;directed graphs;graphs;social networking (online);social sciences computing","socio-technical system;analytic hierarchy system;technological design;log data;association network;directed graph;contingency graph;digital artifact;associogram pattern;transitive closure","","6","","25","","22 Feb 2011","","","IEEE","IEEE Conferences"
"Integrating a Security Operations Centre with an Organization’s Existing Procedures, Policies and Information Technology Systems","M. Mutemwa; J. Mtsweni; L. Zimba","Department of Peace, Safety and Security, The Council of Scientic and Industrial Research, Pretoria, South Africa; Department of Peace, Safety and Security, The Council of Scientic and Industrial Research, Pretoria, South Africa; Department of Peace, Safety and Security, The Council of Scientic and Industrial Research, Pretoria, South Africa","2018 International Conference on Intelligent and Innovative Computing Applications (ICONIC)","6 Jan 2019","2018","","","1","6","A Cybersecurity Operation Centre (SOC) is a centralized hub for network event monitoring and incident response. SOCs are critical when determining an organization's cybersecurity posture because they can be used to detect, analyze and report on various malicious activities. For most organizations, a SOC is not part of the initial design and implementation of the Information Technology (IT) environment but rather an afterthought. As a result, it is not natively a plug and play component therefore there are integration challenges when a SOC is introduced into an organization. A SOC is an independent hub that needs to be integrated with existing procedures, policies and IT systems of an organization such as the service desk, ticket logging system, reporting, etc. This paper discussed the challenges of integrating a newly developed SOC to an organization's existing IT environment. Firstly, the paper begins by looking at what data sources should be incorporated into the Security Information and Event Management (SIEM) such as which host machines, servers, network end points, software, applications, webservers, etc. for security posture monitoring. That is, which systems need to be monitored first and the order by which the rest of the systems follow. Secondly the paper also describes how to integrate the organization's ticket logging system with the SOC SIEM. That is how the cybersecurity related incidents should be logged by both analysts and nontechnical employees of an organization. Also, the priority matrix for incident types and notifications of incidents. Thirdly the paper looks at how to communicate awareness campaigns from the SOC and also how to report on incidents that are found inside the SOC. Lastly the paper looks at how to show value for the large investments that are poured into designing, building and running an SOC.","","978-1-5386-6477-3","10.1109/ICONIC.2018.8601251","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8601251","Cybersecurity Operation Centre;incident response;priority matrix;procedures and policies","Tools;Servers;Monitoring;Software;Computer security;Hardware","computer network security;information technology;matrix algebra","network event monitoring;information technology systems;cybersecurity operation centre;centralized hub;malicious activities;IT environment;plug and play component;data sources;security information and event management;priority matrix;ticket logging system organizations;cybersecurity posture organizations;SOC SIEM;security posture monitoring","","","","16","","6 Jan 2019","","","IEEE","IEEE Conferences"
"RCV: Network Monitoring and Diagnostic System with Interactive User Interface","S. Park; K. Talaulikar; C. Metz","KAIST, Daejeon, South Korea; Cisco Syst., Bangalore, India; Cisco Syst., San Jose, CA, USA","2016 International Conference on Collaboration Technologies and Systems (CTS)","6 Mar 2017","2016","","","578","583","Minimizing the impact of network convergence events has been an active area of research and innovation since it usually occurs unexpectedly and triggers (often jolting) alarms in network operations centers. Examination of router logs, network management system information, and router configuration reviews are the standard tools for assessment and often do not lead to satisfactory conclusions. We introduce Route Convergence Visualizer providing the network operator with a fast and simple tool to understand what exactly happened and who was impacted during a network convergence event. The main novel ingredients include (a) effectiveness of information delivery regarding convergence events, (b) programmability of the tool, and (c) application of the tool. At last, we will enlighten an indispensable collaboration between network operators and software engineers for SDN solutions which require contributions from two disparate engineering disciplines, networking and application development.","","978-1-5090-2300-4","10.1109/CTS.2016.0107","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7871045","computer network;network monitoring;routing;visualization;collaboration","Convergence;Routing protocols;Routing;Software;Databases;Floods;Collaboration","interactive systems;software defined networking;telecommunication network routing;user interfaces","RCV;interactive user interface;network convergence events;network operation centers;network management system information;router configuration reviews;route convergence visualizer;information delivery;tool programmability;tool application;SDN;router logs;network monitoring-and-diagnostic system","","","","16","","6 Mar 2017","","","IEEE","IEEE Conferences"
"High Temporal Accuracy for Extensive Sensor Networks","A. R. Wilson; T. E. Wilson","Maritime Division, Defence Science and Technology Organisation, Fishermens Bend, Australia; Maritime Division, Defence Science and Technology Organisation, Fishermens Bend, Australia","IEEE Sensors Journal","14 Feb 2014","2014","14","4","1061","1068","The Defence Science and Technology Organisation (DSTO) has developed a versatile, low power, and networked sensor interface design suitable for use with a wide range of sensors. The sensor interfaces consist of: 1) core hardware and software that is the same for any sensor and 2) sensor specific hardware and software. The core software provides basic communications routines and manages a simple file operating system. These files constitute the software for interfacing and control of the hardware for specific sensors. The whole system is implemented on the low power Texas Instruments MSP430 series of microcontrollers and results in a consistent interface to a data logging or network control system, no matter what sensor is in use. A network approach has been adopted to reduce the wiring requirements for large numbers of sensors. However, networking introduces complexity, especially in regards to time synchronization of medium-to-high speed sensors, considered here as sensors requiring measurements rates in the range 100-5000-Hz range, such as strain gauges and accelerometers. In these applications, it can also be useful to only acquire data when there is interesting activity to avoid collecting large amounts of essentially zero valued data points and thus reduce the data storage requirements. This paper covers the software and hardware design considerations to achieve temporally correlated measurements from a widely distributed sensor network based on the DSTO sensor interface. The ultimate performance of the DSTO implementation is investigated and experimental results are compared with the predicted response.","1558-1748","","10.1109/JSEN.2013.2292554","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6675022","Sensors;interfaces;timing;time correlation;networks;monitoring","Sensors;Clocks;Microcontrollers;Delays;Synchronization","computerised monitoring;data loggers;distributed sensors;microcontrollers","high temporal accuracy;extensive sensor networks;networked sensor interface;core hardware;core software;sensor specific hardware;sensor specific software;file operating system;Texas Instruments MSP430;microcontrollers;data logging;network control system;time synchronization;medium-to-high speed sensors;distributed sensor network;DSTO sensor interface;frequency 100 Hz to 5000 Hz","","1","","17","","25 Nov 2013","","","IEEE","IEEE Journals"
"The Promising Future of Process Mining with the Internet of Events in Context-Aware Environments","E. Batista; F. Falcone; A. Martínez-Ballesté; A. Solanas","Universitat Rovira i Virgili,Department of Computer Engineering and Mathematics,Av. Països Catalans 26, Tarragona, Catalonia,Spain,43007; Universidad Pública de Navarra,Electric, Electronic and Communication Engineering Department,Pamplona,Spain; Universitat Rovira i Virgili,Department of Computer Engineering and Mathematics,Av. Països Catalans 26, Tarragona, Catalonia,Spain,43007; Universitat Rovira i Virgili,Department of Computer Engineering and Mathematics,Av. Països Catalans 26, Tarragona, Catalonia,Spain,43007","2019 International Conference on Sensing and Instrumentation in IoT Era (ISSI)","23 Mar 2020","2019","","","1","6","Process Mining comprises a consolidated set of techniques that aim at discovering, checking and improving processes. Originally designed to cope with business processes, process mining techniques fed on log files automatically generated by enterprise resource planning (ERP) systems and software alike. However, nowadays, with the unstoppable deployment of the Internet of Things, multiple events are continuously generated and stored by distributed, highly uncoupled and heterogeneous devices able to sense their surroundings and report precise and timely updates on the environment. By embracing the Internet of Events, recently available contextual information opens the door to the application of process mining techniques in new domains, hence, generating opportunities for the analysis of contextual data with multiple implications in hot research areas such as, smart environments (e.g., smart cities, smart buildings, smart homes), cognitive systems, and industry 4.0. In this article, we discuss the opportunities and challenges of process mining in these new, events-rich, context-aware environments.","","978-1-7281-1022-6","10.1109/ISSI47111.2019.9043725","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9043725","Process mining;Context awareness;Internet of Things;Smart environments;Processes","Data mining;Internet of Things;Sensors;Biological system modeling;Smart homes","data mining;Internet of Things","distributed devices;highly uncoupled devices;heterogeneous devices;context-aware environments;business processes;process mining;Internet of Things;enterprise resource planning systems;ERP systems;Internet of Events","","","","31","","23 Mar 2020","","","IEEE","IEEE Conferences"
"Comfort route navigation providing high communication Quality and Energy Saving for mobile devices","K. Kanai; H. Konishi; J. Katto; T. Murase","Graduate School of Fundamental Science and Engineering, Waseda University, Tokyo, Japan; Graduate School of Fundamental Science and Engineering, Waseda University, Tokyo, Japan; Graduate School of Fundamental Science and Engineering, Waseda University, Tokyo, Japan; Cloud System Research Laboratories, NEC Corporation, Kanagawa, Japan","2014 IEEE 3rd Global Conference on Consumer Electronics (GCCE)","5 Feb 2015","2014","","","547","548","Extending battery life for smartphones while using wireless networks is important. In this paper, we propose Comfort Route for Energy Saving (CRFES), which helps users extend the battery life of their smartphones by navigating to their destinations via Quality of Service (QoS)-compliant energy-efficient spots such as Wi-Fi spots. To create CRFES, we construct maps of Wi-Fi spots using logging software we developed for Android phones. In addition to locations, we record actual throughput and energy consumption observed in cellular/Wi-Fi networks. So comfort route navigation is done by choosing a route which minimizes energy consumption instead of choosing the shortest path. Evaluations are carried out by computer simulation and one real city map. The results conclude that CRFES can save the battery life by approximately 50% by spending 1.6 times time cost to reach their destinations on average.","2378-8143","978-1-4799-5145-1","10.1109/GCCE.2014.7031167","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7031167","comfort route;energy efficient networks;wireless LANs;mobile user","IEEE 802.11 Standards;Energy efficiency;Batteries;Mobile communication;Quality of service;Throughput;Smart phones","cellular radio;energy conservation;energy consumption;quality of service;radionavigation;smart phones;telecommunication network routing;wireless LAN","high communication quality;mobile devices;smartphones;wireless networks;comfort route navigation for energy saving;CRFES;battery life extension;quality of service-compliant energy-efficient spots;QoS-compliant energy-efficient spots;logging software;Android phones;actual throughput recording;energy consumption minimization;cellular-Wi-Fi spot networks;computer simulation;real city map construction","","2","","4","","5 Feb 2015","","","IEEE","IEEE Conferences"
"Implementing Hadoop Platform on Eucalyptus Cloud Infrastructure","V. Amiry; S. Z. Rad; M. K. Akbari; M. S. Javan","Comput. Eng. & IT Dept., Qom Univ., Qom, Iran; Comput. Eng. & IT Dept., Mazandaran Univ. of Sci. & Technol., Babol, Iran; Comput. Eng. & IT Dept., Amirkabir Univ. of Technol., Tehran, Iran; Comput. Eng. & IT Dept., Amirkabir Univ. of Technol., Tehran, Iran","2012 Seventh International Conference on P2P, Parallel, Grid, Cloud and Internet Computing","29 Nov 2012","2012","","","74","78","Today, cloud computing is used as a model in most scientific, commercial, military, etc. In this model, the main body of the system is virtual servers, which currently provide services to customers around the world. Eucalyptus is an open source implementation of Cloud computing infrastructure that has a particular architecture. By using Eucalyptus, we can make public and private Clouds. Hadoop also is an open platform that is used for distributing data and jobs in large clusters. This platform can be used for processing and storing huge volumes of data and executing parallel jobs (e.g. analyzing system logs, weather data, and customer relationship behavior and so on.). In this article, we using integrating Hadoop platform on Eucalyptus infrastructure, facilitate and accelerate the deployment of a Hadoop cluster and moreover represent one of the uses of Eucalyptus system in serving services to the Cloud customers. It is worth mentioning that we have successfully implemented our method on the supercomputer of Amirkabir University and according to executed program on our cluster, efficiency has increased to about 80 percent in 32 cores compared to 4 cores clusters.","","978-1-4673-2991-0","10.1109/3PGCIC.2012.37","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6362952","Cloud Computing;Eucalyptus;Hadoop;Virtual Machine","Cloud computing;Computer architecture;Virtual machining;Computational modeling;Educational institutions;File systems;Computers","cloud computing;customer services;public domain software;software architecture;virtual machines;virtual storage","Eucalyptus cloud infrastructure;cloud computing;virtual server;open source architecture;distributing data;data processing;data storing;parallel job execution;Hadoop cluster;cloud customer service","","4","","12","","29 Nov 2012","","","IEEE","IEEE Conferences"
"Scientific Workflow Mining in Clouds","W. Song; F. Chen; H. Jacobsen; X. Xia; C. Ye; X. Ma","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Middleware Systems Research Group, University of Toronto, Toronto, ON, Canada; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; College of Information Science and Technology, Hainan University, Haikou, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China","IEEE Transactions on Parallel and Distributed Systems","8 Sep 2017","2017","28","10","2979","2992","Computing clouds have become the platform of choice for the deployment and execution of scientific workflows. Due to the uncertainty and unpredictability of scientific exploration, the execution plan for a scientific workflow may vary from the definition. It is therefore of great significance to be able to discover actual workflows from execution histories (event logs) to reproduce experimental results and to establish provenance. However, most existing process mining techniques focus on discovering control flow-oriented business processes in a centralized environment, and thus, they are mostly inapplicable to the discovery of data flow-oriented, unstructured scientific workflows in distributed cloud environments. In this paper, we present Scientific Workflow Mining as a Service (SWMaaS) to support both intra-cloud and inter-cloud scientific workflow mining. The approach is implemented as a ProM plug-in and is evaluated on event logs derived from real-world scientific workflows. Through experimental results, we demonstrate the effectiveness and efficiency of our approach.","1558-2183","","10.1109/TPDS.2017.2696942","National Basic Research Program of China; National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; China Scholarship Council; NSERC; Alexander von Humboldt Foundation; University of Toronto; Technische Universität München; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7907335","Scientific workflow;inter-cloud;workflow mining;event log;direct precedence","Business;Process control;Data mining;Petri nets;Electronic mail;Optimization;PROM","cloud computing;data mining;distributed processing","cloud computing;process mining;distributed cloud environments;scientific workflow mining as a service;SWMaaS;intra-cloud scientific workflow mining;inter-cloud scientific workflow mining;ProM plug-in","","11","","51","Traditional","24 Apr 2017","","","IEEE","IEEE Journals"
"Enabling reproducible cyber research - four labeled datasets","T. Bowen; A. Poylisher; C. Serban; R. Chadha; C. Jason Chiang; L. M. Marvel","Vencore Labs d/b/a/Applied Communication Sciences, Basking Ridge, NJ 07920, United States; Vencore Labs d/b/a/Applied Communication Sciences, Basking Ridge, NJ 07920, United States; Vencore Labs d/b/a/Applied Communication Sciences, Basking Ridge, NJ 07920, United States; Vencore Labs d/b/a/Applied Communication Sciences, Basking Ridge, NJ 07920, United States; Vencore Labs d/b/a/Applied Communication Sciences, Basking Ridge, NJ 07920, United States; U.S. Army Research Laboratory, APG, MD 21005, United States","MILCOM 2016 - 2016 IEEE Military Communications Conference","26 Dec 2016","2016","","","539","544","In this paper, we describe the design and creation of four publicly available datasets generated using a testbed with simulated benign users and a manual attacker. The datasets were created to provide examples of cyber exploitations and aid in the production of reproducible research that address cyber security challenges. The CyberVAN testbed provides sophisticated capabilities for high-fidelity cyber experimentation in strategic and tactical network environments. The representative network is sufficiently complex with synthetic users performing normal duties that generate traffic (webpage browsing, e-mail, etc.). Both network and host based facts/logs are included in the dataset along with a diagram of the network and a timeline of events. The four datasets encompass progressively complex scenarios: 1) malware infection injection via a phishing email attachment; 2) propagating botnet injection via phishing email attachment with a Single Fast Flux algorithm for bot master identification/communication; 3) propagating botnet injection via email link using a Domain Generation Algorithm for bot master identification/communication; 4) propagating botnet injection via corruption of a legitimate internal web server with Double Fast Flux for bot master identification/communication. The full datasets along with relevant documentation is available for public download. Additional datasets containing tactical network scenarios and environments will be added to the repository in the future with the goal of enabling reproducible cyber security research that will advance the science of cyber security.","2155-7586","978-1-5090-3781-0","10.1109/MILCOM.2016.7795383","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7795383","Cyber Security;Computer Network Defense;Computer Network Operations","Electronic mail;Servers;Internet;Malware;Firewalls (computing);Government","computer crime;computer network security;file servers;invasive software;unsolicited e-mail","reproducible cyber research;four labeled datasets;four publicly available datasets;simulated benign users;manual attacker;cyber exploitations;reproducible research production;cyber security challenges;CyberVAN;high-fidelity cyber experimentation;strategic network environments;tactical network environments;malware infection injection;phishing email attachment;cyber virtual assured networks;public download;double fast flux;legitimate internal Web server;domain generation algorithm;email link;propagating botnet injection;bot master identification-communication;single fast flux algorithm","","2","","15","","26 Dec 2016","","","IEEE","IEEE Conferences"
"Adaptively Learning and Assessing SPSS Operating Skills Using Online SPSS Simulator","Y. Chu; S. Tseng; J. Weng; H. Lin; N. Wang; A. Y. H. Liao; J. Su","Dept. of Comput. Sci., Nat. Chiao Tung Univ., Hsinchu, Taiwan; Dept. of Comput. Sci., Nat. Chiao Tung Univ., Hsinchu, Taiwan; Dept. of Comput. Sci., Nat. Chiao Tung Univ., Hsinchu, Taiwan; Dept. of Comput. Sci., Nat. Chiao Tung Univ., Hsinchu, Taiwan; Dept. of Comput. Sci., Nat. Chiao Tung Univ., Hsinchu, Taiwan; Dept. of Inf. Sci. & Applic., Asia Univ., Taichung, Taiwan; Dept. of Comput. Sci., Nat. Chiao Tung Univ., Hsinchu, Taiwan","2010 International Conference on Technologies and Applications of Artificial Intelligence","20 Jan 2011","2010","","","398","404","Online questionnaire is an emerging method for social science researchers to perform their survey. Although there are many data analysis software tools nowadays, such as SPSS, SAS, and Excel etc., to assist the students in analyzing the data of their surveys. However, the intentions of users for using the software tools cannot be probed and utilized to help the users operating them, so it will cause many errors on the students' analysis results due to the misusage of software and result in that learning how to use the data analysis software tools based upon the students' intentions becomes very time-consuming. Moreover, because the software operating procedures of the students are not recorded, the teachers cannot find out the students' misconceptions. In this paper, we acquire the knowledge of using SPSS software and then apply the Simulator Generator concept proposed in previous research to build an online web-based SPSS software simulator for the students to emulate the operating process of SPSS software. Based upon the acquired frequent patterns of misuse, the teacher can design action routines embedded in the simulator to instantly detect the frequent error action patterns and help the students to rectify their misconceptions. Moreover, the teacher can trace and analyze the students' learning statuses according to the logs generated from online SPSS Simulator. Using Simulator Generator, writing grammar rules is essential, but it is not easy for teachers. The knowledge of using SPSS software are largely identical but with minor differences. Thus, an SPSS Functional Ontology is proposed according to the features of SPSS software and the corresponding grammar rules are constructed in this research. Teachers just need to do some minor adjustments on the differences and the action routines, and then the new simulator will be generated automatically.","2376-6824","978-1-4244-8668-7","10.1109/TAAI.2010.70","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5695483","e-learning;Adaptive Learning;Assessment;Ontology","Data analysis;Generators;Ontologies;Grammar;Certification;Software tools","computer aided instruction;context-free grammars;data analysis;ontologies (artificial intelligence);software tools","adaptively learning;SPSS operating skill assessment;online questionnaire;data analysis software tools;simulator generator concept;online Web-based SPSS software simulator;writing grammar rules;SPSS functional ontology","","","","10","","20 Jan 2011","","","IEEE","IEEE Conferences"
"An Intelligent Framework for Malware Detection in Internet of Things (IoT) Ecosystem","O. Eboya; J. B. Juremi; M. Shahpasand","Asia Pacific University (APU),Faculty of Computing, Engineering, and Technology (FCET),Kuala Lumpur,Malaysia; Asia Pacific University (APU),Faculty of Computing, Engineering, and Technology (FCET),Kuala Lumpur,Malaysia; Staffordshire University Stoke-on-Trent,Department of Computing,Staffordshire,United Kingdom","2020 IEEE 8th R10 Humanitarian Technology Conference (R10-HTC)","23 Feb 2021","2020","","","1","6","The Heuristic-Based Analysis method, which is otherwise known as Behavioural-Based Analysis technique applied by several Anti-Virus and Anti-Malware vendors of computer network systems has not been sufficient. There are gaps in the field of research to develop a novel approach to solving the inherent and existential threats and attacks in the interconnected computer network systems, especially in the fast growing IoT Ecosystems that has now become ubiquitous to everyday life. We proposed and presented the experimental implementation of an intelligent framework, including the realization of the Multilayer Perceptron (MLP) with multiple hidden layers by exploiting the capabilities of Hierarchical Extreme Learning Machine (H-ELM). This has the preeminent underexplored potentials for accelerated speed, rapid feature learning, training, improved classification performance, and accuracy of Detecting, Recognizing, and Predicting (DRP) anomalies in an Internet of Things (IoT) ecosystem. The experimental implementation demonstrated the effectiveness of the proposed Intelligent DRP-Framework also known as the iDRP-Framework using the generalized MLP technique for demonstrating how a non-traditional malware detection mechanism can be applied to dynamically and intelligently Detect, Recognize, and Predict malwares in the generated big dataset of an IoT ecosystem through converted image file, such as the Log files, and Binary dataset to create machine learning model. The result indicates that the proposed iDRP-Framework delivered significant performance and accuracy in the detection of anomalies in an interconnected computer network system.","2572-7621","978-1-7281-1110-0","10.1109/R10-HTC49770.2020.9356961","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9356961","Intelligent IoT Framework;Malware Detection Framework;iDRP Framework;IoT Security Ecosystem","Training;Ecosystems;Multilayer perceptrons;Feature extraction;Malware;Computer networks;Internet of Things","Big Data;computer network security;Internet of Things;invasive software;learning (artificial intelligence);multilayer perceptrons;pattern classification","intelligent framework;heuristic-based analysis;anti-virus vendors;anti-malware vendors;existential threats;interconnected computer network system;multilayer perceptron;hierarchical extreme learning machine;feature learning;Internet of Things;MLP;IoT ecosystem;machine learning;nontraditional malware detection;iDRP;intelligent DRP;classification performance;behavioural-based analysis;H-ELM;detecting recognizing and predicting anomalies;big dataset;image file;log files;binary dataset","","","","29","","23 Feb 2021","","","IEEE","IEEE Conferences"
"Real-time regex matching with apache spark","S. Deaton; D. Brownfield; L. Kosta; Z. Zhu; S. J. Matthews","Department of Electrical Engineering & Computer Science, United States Military Academy, West Point, NY 10996; Department of Electrical Engineering & Computer Science, United States Military Academy, West Point, NY 10996; Department of Electrical Engineering & Computer Science, United States Military Academy, West Point, NY 10996; Department of Electrical Engineering & Computer Science, United States Military Academy, West Point, NY 10996; Department of Electrical Engineering & Computer Science, United States Military Academy, West Point, NY 10996","2017 IEEE High Performance Extreme Computing Conference (HPEC)","2 Nov 2017","2017","","","1","6","Network Monitoring Systems (NMS) are an important part of protecting Army and enterprise networks. As governments and corporations grow, the amount of traffic data collected by NMS grows proportionally. To protect users against emerging threats, it is common practice for organizations to maintain a series of custom regular expression (regex) patterns to run on NMS data. However, the growth of network traffic makes it increasingly difficult for network administrators to perform this process quickly. In this paper, we describe a novel algorithm that leverages Apache Spark to perform regex matching in parallel. We test our approach on a dataset of 31 million Bro HTTP log events and 569 regular expressions provided by the Army Engineer Research & Development Center (ERDC). Our results indicate that we are able to process 1, 250 events in 1.047 seconds, meeting the desired definition of real-time.","","978-1-5386-3472-1","10.1109/HPEC.2017.8091063","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8091063","Regular Expressions;Parallel Computing;Apache Spark;Bro","Sparks;Real-time systems;Organizations;Partitioning algorithms;Anomaly detection;Monitoring;Big Data","data protection;invasive software;parallel algorithms;pattern matching;public domain software","Network Monitoring Systems;enterprise networks;custom regular expression patterns;NMS data;network traffic;network administrators;regular expressions;traffic data collection;Apache Spark;Bro HTTP log events;real-time regex matching;Army Engineer Research & Development Center;time 1.047 s","","","","19","","2 Nov 2017","","","IEEE","IEEE Conferences"
"Replay debugging: Leveraging record and replay for program debugging","N. Honarmand; J. Torrellas","University of Illinois at Urbana-Champaign, USA; University of Illinois at Urbana-Champaign, USA","2014 ACM/IEEE 41st International Symposium on Computer Architecture (ISCA)","14 Jul 2014","2014","","","455","456","Hardware-assisted Record and Deterministic Replay (RnR) of programs has been proposed as a primitive for debugging hard-to-repeat software bugs. However, simply providing support for repeatedly stumbling on the same bug does not help diagnose it. For bug diagnosis, developers typically want to modify the code, e.g., by creating and operating on new variables, or printing state. Unfortunately, this renders the RnR log inconsistent and makes Replay Debugging (i.e., debugging while using an RnR log for replay) dicey at best. This paper presents rdb, the first scheme for replay debugging that guarantees exact replay. rdb relies on two mechanisms. The first one is compiler support to split the instrumented application into two executables: one that is identical to the original program binary, and another that encapsulates all the added debug code. The second mechanism is a runtime infrastructure that replays the application and, without affecting it in any way, invokes the appropriate debug code at the appropriate locations. We describe an implementation of rdb based on LLVM and Pin, and show an example of how rdb's replay debugging helps diagnose a real bug.","1063-6897","978-1-4799-4394-4","10.1109/ISCA.2014.6853229","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6853229","","Debugging;Program processors;Data structures;Runtime library;Computer bugs;Hardware;Algorithms","data flow analysis;program compilers;program debugging","replay debugging;program debugging;hardware-assisted record replay;hardware-assisted deterministic replay;hard-to-repeat software bugs;bug diagnosis;compiler support;runtime infrastructure","","5","1","47","","14 Jul 2014","","","IEEE","IEEE Conferences"
"An Integrated Data-Driven Framework for Computing System Management","T. Li; W. Peng; C. Perng; S. Ma; H. Wang","Sch. of Comput. Sci., Florida Int. Univ., Miami, FL, USA; Xerox Innovation Group, Xerox Corp., Rochester, NY, USA; IBM T J. Watson Res. Center, Yorktown Heights, NY, USA; IBM T J. Watson Res. Center, Yorktown Heights, NY, USA; IBM T J. Watson Res. Center, Yorktown Heights, NY, USA","IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans","15 Dec 2009","2010","40","1","90","99","With advancement in science and technology, computing systems are becoming increasingly more complex with a growing number of heterogeneous software and hardware components. They are thus becoming more difficult to monitor, manage, and maintain. Traditional approaches to system management have been largely based on domain experts through a knowledge acquisition solution that translates domain knowledge into operating rules and policies. This process has been well known as cumbersome, labor intensive, and error prone. In addition, traditional approaches for system management are difficult to keep up with the rapidly changing environments. There is a pressing need for automatic and efficient approaches to monitor and manage complex computing systems. In this paper, we propose an integrated data-driven framework for computing system management by acquiring the needed knowledge automatically from a large amount of historical log data. Specifically, we apply text mining techniques to automatically categorize the log messages into a set of canonical categories, incorporate temporal information to improve categorization performance, develop temporal mining techniques to discover the relationships between different events, and take a novel approach called event summarization to provide a concise interpretation of the temporal patterns.","1558-2426","","10.1109/TSMCA.2009.2030161","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5313888","Event mining;mining log data;summarization;system management","Knowledge management;Data mining;Hardware;Knowledge acquisition;Pressing;Computerized monitoring;Technology management;Environmental management;Text mining;Condition monitoring","data mining;text analysis","integrated data-driven framework;computing system management;cumbersome process;labor intensive process;error prone process;text mining techniques;data categorization;event summarization","","11","1","41","","6 Nov 2009","","","IEEE","IEEE Journals"
"Monitoring Informed Testing for IoT","A. Abdullah; H. Schmidt; M. Spichkova; H. Liu","Sch. of Sci., RMIT Univ., Melbourne, VIC, Australia; Sch. of Sci., RMIT Univ., Melbourne, VIC, Australia; Sch. of Sci., RMIT Univ., Melbourne, VIC, Australia; Eng. & Sci., Victoria Univ., Melbourne, VIC, Australia","2018 25th Australasian Software Engineering Conference (ASWEC)","27 Dec 2018","2018","","","91","95","Internet of Things (IoT) systems continuously collect a large amount of data from heterogeneous ""smart objects"" through standardised service interfaces. A key challenge is how to use these data and relevant event logs to construct continuously adapted usage profiles and apply them to enhance testing methods, i.e., prioritization of tests for the testing of continuous integration of an IoT system. In addition, these usage profiles provide relevance weightings to analyse architecture and behaviour of the system. Based on the analysis, testing methods can predict specific system locations that are susceptible to error, and therefore suggest where expanded runtime monitoring is necessary. Furthermore, IoT aims to connect billions of ""smart devices"" over the network. Testing even a small IoT system connecting a few dozens of smart devices would require a network of test Virtual Machines (VMs) possibly spreading across the Fog and the Cloud. In this paper we propose a framework for testing of each IoT layer in a separate VM environment, and discuss potential difficulties with optimal VM allocation.","2377-5408","978-1-7281-1241-1","10.1109/ASWEC.2018.00020","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8587291","Software Engineering;Testing","Testing;Monitoring;Middleware;Cloud computing;Runtime;Statistical analysis","cloud computing;Internet of Things;program testing;system monitoring;virtual machines","Internet of Things systems;smart objects;system locations;runtime monitoring;virtual machines test;service interfaces;VM allocation;event logs;cloud;fog;monitoring informed testing;IoT layer;smart devices;IoT system;continuous integration;testing methods","","","","24","","27 Dec 2018","","","IEEE","IEEE Conferences"
"Fighting Botnets with Cyber-Security Analytics: Dealing with Heterogeneous Cyber-Security Information in New Generation SIEMs","B. G. Crespo; A. Garwood","Atos Res. & Innvoation, Madrid, Spain; LSEC, Leuven, Belgium","2014 Ninth International Conference on Availability, Reliability and Security","11 Dec 2014","2014","","","192","198","One of the cyber-threats with the highest impact nowadays, in terms of number of compromised systems and the impact they can have on the Internet at large, is commonly known as the botnet. In the ACDC (Advanced Cyber Defence Centre) project, partners from 14 European countries, including public administrations, private sector organizations and academia, are trying to achieve a sustainable victory over botnets. This paper presents how a new generation SIEM is being used in the ACDC project to leverage its scalability and enhanced analytic capabilities and produce advance cyber-intelligence from the heterogeneous and massive streams of data continuously produced in the cyber-security context, in combination with traditional security events and system logs. The paper describes a case study where this approach is being tested. In the case study, the SIEM has been adapted to cope, not only with traditional security events and system logs, but also with pre-analyzed information about cyber-threats and incidents reported by the tools of some of the ACDC partner organizations. The case study also tests the adoption of the standard XML-based format called STIX, developed by the Mitre Corporation in the USA, and its suitability as a common specification for exchanging cybersecurity information between a subset of ACDC tools, the Atos SL SIEM and the ACDC's centralized data clearing house (CCH).","","978-1-4799-4223-7","10.1109/ARES.2014.33","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6980282","cyber-security;STIX;correlation;cyberdefense;cyber-threats;cyber-analytics","Computer security;Correlation;Organizations;Monitoring;Standards organizations","DP management;invasive software","botnets;cyber-security analytics;heterogeneous cyber-security information;SIEMs;cyber-threats;Internet;ACDC project;Advanced Cyber Defence Centre project;public administrations;private sector organizations;cyber-intelligence;security events;system logs;XML-based format;STIX;ACDC tools;Atos SL SIEM;centralized data clearing house;CCH;security information and event management","","","","15","","11 Dec 2014","","","IEEE","IEEE Conferences"
"Resource allocation optimization in LTE-A/5G networks using big data analytics","P. Kiran; M. G. Jibukumar; C. V. Premkumar","School of Engineering, Cochin University India; School of Engineering, Cochin University India; School of Engineering, Cochin University India","2016 International Conference on Information Networking (ICOIN)","10 Mar 2016","2016","","","254","259","The next generation wireless cellular communication networks are going to be completely internet protocol (IP) based. This leads to evolution of cellular networks into software controlled hierarchical abstract networks. Software controlling enables extraction of large volumes of networkdata usage and transactions related information in the form of log files, configuration files, database entries etc. This paper proposes a mechanism for efficient exploitation of this big volume of data for addressing the ever existing problem of optimum radio resource allocation to users. This work is based on the map-reduce data processing technique, by implementing non-orthogonal binary singular value decomposition (SVD) of binary data matrices for fuzzy pattern recognition and grouping. Patterns identified by proposed method can be interpreted efficiently for resource allocation in wireless networks.","","978-1-5090-1724-9","10.1109/ICOIN.2016.7427072","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7427072","Big Data;LTE-A;5G;Resource Optimization;binary non-orthogonal SVD","Resource management;Dictionaries;Big data;Pattern matching;Bandwidth;Matrix decomposition;Data mining","5G mobile communication;Big Data;fuzzy set theory;IP networks;Long Term Evolution;next generation networks;parallel programming;resource allocation;singular value decomposition","fuzzy pattern grouping;fuzzy pattern recognition;binary data matrices;SVD;nonorthogonal binary singular value decomposition;MapReduce data processing technique;radio resource allocation;database entries;configuration files;log files;network usage;information transactions;software controlled hierarchical abstract networks;cellular networks;IP;Internet protocol;next-generation wireless cellular communication networks;Big Data analytics;LTE-A/5G networks;resource allocation optimization","","7","","3","","10 Mar 2016","","","IEEE","IEEE Conferences"
"Interactive interface on a sports simulator for improved interaction and personalized services","S. Kang; K. Kim; S. Chi","Intelligent Cognitive Technology Research Department, ETRI, Daejeon, 305-700, Korea; Intelligent Cognitive Technology Research Department, ETRI, Daejeon, 305-700, Korea; Intelligent Cognitive Technology Research Department, ETRI, Daejeon, 305-700, Korea","2013 10th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)","2 Dec 2013","2013","","","236","237","In this paper, we present an interactive interface for a sports simulator to provide natural interoperation and customized functions suited to the user based on the identified user's logging and coaching data. The proposed system performs the control function between the user and sports simulator system. The system recommends a personalized exercise mode based on the user's historical information such as their identification data, exercise records, and coaching data. It can also analyze and evaluate for movements of the user during the riding time. The application of an interaction method to sport simulator systems for a customized coaching function can provide a depth of expression and effective educational training for the rider.","","978-1-4799-1197-4","10.1109/URAI.2013.6677353","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6677353","Sports simulator;interaction;coaching","Hardware;Training;Visualization;Robots;Control systems;Industries;Software","digital simulation;interactive systems;sport;user interfaces","interactive interface;interaction service;personalized service;natural interoperation;customized functions;user logging;coaching data;control function;sports simulator system;personalized exercise mode;user historical information;identification data;exercise records;sport simulator systems;customized coaching function;educational training","","1","","8","","2 Dec 2013","","","IEEE","IEEE Conferences"
"Predictive Process Monitoring using a Markov Model Technique","N. Khan; S. McClean; Z. Ali; A. Ali; D. Charles; P. Taylor; D. Nauck","School of Computing, Ulster University,Newtownabbey,UK; School of Computing, Ulster University,Newtownabbey,UK; School of Computing, Ulster University,Newtownabbey,UK; School of Computing, Ulster University,Newtownabbey,UK; School of Computing, Ulster University,Newtownabbey,UK; Applied Research, BT,Ipswich,UK; Applied Research, BT,Ipswich,UK","2019 International Conference on Computing, Electronics & Communications Engineering (iCCECE)","27 Dec 2019","2019","","","193","196","Information systems play a vital role in business process understanding through recording data where each process execution is represented in event logs. These event logs consist of a collection of traces which characterise the execution of a process. The techniques that analyse such types of data are broadly known as process mining. Hence, Process mining is a family of techniques to evaluate and analyse business process based on their pragmatic behaviour as recorded in event logs. Predictive process monitoring aims to predict how the completion of running process events can be anticipated. In this paper, Markov chain models have been investigated for prediction of future process events by considering a sequence of events. The Markov model is a special type of statistical (process) model that are used to evaluate systems where it is considered that future states depend only on the current state and not on previous states. Results have been evaluated using an accuracy metric for a dataset contains tasks from a ticketing management process of a software company.","","978-1-7281-2138-3","10.1109/iCCECE46942.2019.8941917","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8941917","Process analytic;monitoring;Markov chain Model","Markov processes;Data mining;Task analysis;Predictive models;Analytical models;Mathematical model","business data processing;data mining;information systems;Markov processes;process monitoring;statistical analysis","process mining;business process;predictive process monitoring;Markov chain models;statistical model;ticketing management process;Markov model technique;process execution;software company;information systems","","","","23","","27 Dec 2019","","","IEEE","IEEE Conferences"
"Visual Analytics for MOOC Data","H. Qu; Q. Chen",Hong Kong University of Science and Technology; Hong Kong University of Science and Technology,"IEEE Computer Graphics and Applications","18 Nov 2015","2015","35","6","69","75","With the rise of massive open online courses (MOOCs), tens of millions of learners can now enroll in more than 1,000 courses via MOOC platforms such as Coursera and edX. As a result, a huge amount of data has been collected. Compared with traditional education records, the data from MOOCs has much finer granularity and also contains new pieces of information. It is the first time in history that such comprehensive data related to learning behavior has become available for analysis. What roles can visual analytics play in this MOOC movement? The authors survey the current practice and argue that MOOCs provide an opportunity for visualization researchers and that visual analytics systems for MOOCs can benefit a range of end users such as course instructors, education researchers, students, university administrators, and MOOC providers.","1558-1756","","10.1109/MCG.2015.137","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7331178","computer graphics;visualization;MOOCs;web log data analysis;clickstreams;visual analytics","Data visualization;Distance education;Visual analytics;Data mining;Online services;Cryptography;Education courses","","","Computer Graphics;Computer-Assisted Instruction;Data Mining;Database Management Systems;Databases, Factual;Social Media;Software;User-Computer Interface","25","","6","","18 Nov 2015","","","IEEE","IEEE Magazines"
"Abnormal Payment Transaction Detection Scheme Based on Scalable Architecture and Redis Cluster","T. Lee; Y. Kim; E. Hwang","Korea University, School of Electrical and Computer Engineering, Seoul, Korea; Korea University, School of Electrical and Computer Engineering, Seoul, Korea; Korea University, School of Electrical and Computer Engineering, Seoul, Korea","2018 International Conference on Platform Technology and Service (PlatCon)","27 Sep 2018","2018","","","1","6","Log file based data analysis methods in the closed fault tolerant OS have shown several problems. First, it is not easy to add or change the data analysis direction while the service is running after the analysis process has been set and compiled. Second, in an independent closed system, due to the limited resource policy, it is difficult to perform real-time data analysis. Finally, it is not easy to utilize new technologies and open sources such as in-memory database and python. Due to these problems, existing methods have difficulty in detecting abnormal payment transactions in real time. In this paper, we propose an abnormal payment transaction detection scheme based on scalable network architecture and Redis cluster, which can collect transaction data quickly and perform their analysis in real-time. We show its performance by implementing a prototype system and performing several experiments on it. Furthermore, we show that our proposed scheme can be used for data analysis through the reproduction of data using in-memory storage, which can solve the aforementioned problem of unidirectional analysis by doing parallel processing on the distributed Redis repository.","","978-1-5386-4710-3","10.1109/PlatCon.2018.8472732","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8472732","abnormality detection;in-memory computing;realtime analysis;scalable network architecture","Servers;Real-time systems;Data analysis;Distributed databases;Network architecture;Data mining;Mission critical systems","data analysis;fault tolerant computing;financial data processing;operating systems (computers);parallel processing;pattern clustering;public domain software;real-time systems;software architecture","abnormal payment transaction detection scheme;scalable architecture;Redis cluster;closed fault tolerant OS;independent closed system;real-time data analysis;scalable network architecture;transaction data;log file based data analysis;open source system;distributed Redis repository","","","","17","","27 Sep 2018","","","IEEE","IEEE Conferences"
"Visual monitoring of numeric variables embedded in source code","F. Beck; F. Hollerich; S. Diehl; D. Weiskopf","VISUS, University of Stuttgart, Germany; University of Trier, Germany; University of Trier, Germany; VISUS, University of Stuttgart, Germany","2013 First IEEE Working Conference on Software Visualization (VISSOFT)","31 Oct 2013","2013","","","1","4","Numeric variables are one of the most frequently used data types. During the execution of a program, their values might change often. Tracing these changes can be necessary for understanding specific behavior of the program or for locating bugs. However, using a breakpoint debugger requires tedious stepping, and logging changes implies analyzing large text files. To make the monitoring of numeric variables easier, this work introduces a visualization approach that augments the source code view of an IDE by small, word-sized graphics: the visualizations accompanying the declarations of monitored variables plot read and write accesses on a timeline; detail views can be retrieved on demand. As suggested by a case study, this approach might support program comprehension and debugging.","","978-1-4799-1457-9","10.1109/VISSOFT.2013.6650545","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6650545","","Data visualization;Visualization;Time series analysis;Monitoring;Debugging;Software","checkpointing;data visualisation;program debugging;program visualisation;reverse engineering;system monitoring","visual monitoring;embedded numeric variables;data types;program execution;change tracing;program behavior understanding;bug location;breakpoint debugger;logging change;large text file analysis;numeric variable monitoring;visualization approach;source code view;IDE;small word-sized graphics;monitored variable declaration;read-and-write access plotting;program comprehension;program debugging","","12","","20","","31 Oct 2013","","","IEEE","IEEE Conferences"
"Mining Association Rules Consisting of Download Servers from Distributed Honeypot Observation","M. Ohrui; H. Kikuchi; M. Terada","Dept. of Inf. Sci. & Technol., Tokai Univ., Hiratsuka, Japan; Dept. of Inf. Sci. & Technol., Tokai Univ., Hiratsuka, Japan; Hitachi Incident Response Team (HIRT), Hitachi, Ltd., Kawasaki, Japan","2010 13th International Conference on Network-Based Information Systems","15 Nov 2010","2010","","","541","545","This paper aims to find interested association rules, known as data mining technique, out of the dataset of downloading logs by focusing on the coordinated activity among downloading servers. The result of the analysis shows the association rules of the downloading servers and that of the malwares.","2157-0426","978-1-4244-8053-1","10.1109/NBiS.2010.73","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5636261","Association Rules;Malware;Botnets;Coordinated Attack;Sequential Pattern","Malware;Association rules;Servers;IP networks;Grippers;Correlation","data mining;file servers;invasive software","association rule mining;download servers;distributed honeypot observation;data mining technique;malwares","","2","","6","","15 Nov 2010","","","IEEE","IEEE Conferences"
"Storage-Aware Network Stack for NVM-Assisted Key-Value Store","S. Chen; D. Li; X. Chen; W. Han; D. Zeng","Sch. of Electron. & Comput. Eng., Peking Univ., Shenzhen, China; Inst. of Big Data Technol., Peking Univ., Shenzhen, China; State Key Lab. of Functional Mater. for Inf., Shanghai Inst. of Microsyst. & Inf. Technol., Shanghai, China; State Key Lab. of Functional Mater. for Inf., Shanghai Inst. of Microsyst. & Inf. Technol., Shanghai, China; China Univ. of Geosci., Wuhan, China","2018 27th International Conference on Computer Communication and Networks (ICCCN)","11 Oct 2018","2018","","","1","9","This paper describes the design of a new software zero-copy network framework for NVM-assisted key-value stores, which directly stores and persists transactions from network into raw non-volatile memory used as write-ahead cache for data consistency. NVM is fast and bit-addressable which makes it the perfect choice for transient transaction log persistency than hard disks or even Flash drives, but its limited write cycle requires wear-leveling during direct access. However, popular RDMA-based zero-copy transmission normally needs to have the remote memory address beforehand and cannot cope with the address changing caused by wear-leveling easily. The software zero-copy solution proposed in this paper is designed with the awareness of NVM wear-leveling and log metadata management. Simulation results show that the new network framework improves performance by over 200× in throughput and decreases latency by more than 20× comparing to the traditional socket and hard disk based solution. When both equipped with NVM, the zero-copy network stack improves performance by 18 to 62% in throughput and 40 to 81% in latency comparing to the standard socket and with the lowest CPU consumption.","1095-2055","978-1-5386-5156-8","10.1109/ICCCN.2018.8487330","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8487330","","Nonvolatile memory;Servers;Databases;File systems;Computer architecture;Performance evaluation","cache storage;memory architecture;meta data","storage-aware network stack;NVM-assisted key-value store;software zero-copy network framework;nonvolatile memory;write-ahead cache;software zero-copy solution;NVM wear-leveling;zero-copy network stack;log metadata management","","","","36","","11 Oct 2018","","","IEEE","IEEE Conferences"
"SoftLearn: A Process Mining Platform for the Discovery of Learning Paths","B. V. Barreiros; M. Lama; M. Mucientes; J. C. Vidal","Center for Res. in Inf. Technol. (CiTIUS), Univ. of Santiago de Compostela, Santiago de Compostela, Spain; Center for Res. in Inf. Technol. (CiTIUS), Univ. of Santiago de Compostela, Santiago de Compostela, Spain; Center for Res. in Inf. Technol. (CiTIUS), Univ. of Santiago de Compostela, Santiago de Compostela, Spain; Center for Res. in Inf. Technol. (CiTIUS), Univ. of Santiago de Compostela, Santiago de Compostela, Spain","2014 IEEE 14th International Conference on Advanced Learning Technologies","22 Sep 2014","2014","","","373","375","One of the most challenging issues in learning analytics is the development of techniques and tools that facilitate the evaluation of the learning activities carried out by learners. In this paper, we faced this issue through a process mining-based platform, called Soft Learn, that is able to discover complete, precise and simple learning paths from event logs. This platform has a graphical interface that allows teachers to better understand the real learning paths undertaken by learners.","2161-377X","978-1-4799-4038-7","10.1109/ICALT.2014.111","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6901485","Learning Analytics;Process Mining;Learning Path Discovery","Data mining;Data visualization;Genetic algorithms;Visualization;Software architecture;Registers;Context","computer aided instruction;data mining;graphical user interfaces","SoftLearn;process mining platform;learning path discovery;learning analytics;event logs;graphical interface","","7","","9","","22 Sep 2014","","","IEEE","IEEE Conferences"
"Non-intrusive Performance Analysis of Parallel Hardware Accelerated Applications on Hybrid Architectures","R. Dietrich; T. Ilsche; G. Juckeland","Center for Inf. Services & High Performance Comput. (ZIH), Tech. Univ. Dresden, Dresden, Germany; Center for Inf. Services & High Performance Comput. (ZIH), Tech. Univ. Dresden, Dresden, Germany; Center for Inf. Services & High Performance Comput. (ZIH), Tech. Univ. Dresden, Dresden, Germany","2010 39th International Conference on Parallel Processing Workshops","11 Oct 2010","2010","","","135","143","New high performance computing (HPC) applications recently have to face scalability over an increasing number of nodes and the programming of special accelerator hardware. Hybrid composition of large computing systems leads to a new dimension in complexity of software development. This paper presents a novel approach to gain insight into accelerator interaction and utilization without any changes to the application. It leverages well established methods for performance analysis to accelerator hardware, allowing a holistic view on performance bottlenecks of hybrid applications. A general strategy is presented to get dynamic runtime information about hybrid program execution with minimal impact on the program ???ow. The achievable level of detail is exemplarily studied for the CUDA environment and the OpenCL framework. Combined with existing performance analysis techniques this facilitates obtaining the full potential of hybrid computing power.","2332-5690","978-1-4244-7918-4","10.1109/ICPPW.2010.30","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5599208","performance analysis;accelerators;GPGPU;tracing;event logging;monitoring libraries","Libraries;Kernel;Monitoring;Runtime;Instruments;Hardware;Synchronization","hybrid simulation;parallel processing;software engineering","nonintrusive performance analysis;parallel hardware accelerated applications;hybrid architectures;high performance computing;HPC applications;large computing systems;software development;CUDA environment;OpenCL framework","","16","","14","","11 Oct 2010","","","IEEE","IEEE Conferences"
"Integrating LSM Trees With Multichip Flash Translation Layer for Write-Efficient KVSSDs","S. -M. Wu; K. -H. Lin; L. -P. Chang","Department of Computer Science, College of Computer Science, National Chiao Tung University, Hsinchu, Taiwan; Department of Computer Science, College of Computer Science, National Chiao Tung University, Hsinchu, Taiwan; Department of Computer Science, College of Computer Science, National Chiao Tung University, Hsinchu, Taiwan","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","1 Jan 2021","2021","40","1","87","100","Log-structured-merge (LSM) trees are a highly write-optimized data structure for lightweight, high-performance key-value (KV) stores. Furthermore, solid-state drives (SSDs) are a crucial component for I/O acceleration. Conventional LSM-over-SSD designs involve multiple software layers, including the LSM tree, host file system, and flash translation layer (FTL), which introduce cascading write amplifications. To manage the write amplifications from different layers, we propose KVSSDs, which are a close integration of LSM trees and the FTL. KVSSDs exploit the FTL mapping mechanism to implement copy-free compaction of LSM trees, and they enables direct data allocation in flash memory for efficient garbage collection. Our design also uses a fine-grained, dynamic striping policy to fully exploit the rich internal parallelism of multichip SSDs. The experimental results indicated that our LSM-SSD integrated design reduced the write amplification by 86% and improved the throughput by 383% compared with a conventional LSM-over-SSD design.","1937-4151","","10.1109/TCAD.2020.2987781","Ministry of Science and Technology, Taiwan; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9066943","Flash memory;key-value (KV) store;log-structured-merge (LSM) tree;solid-state drive (SSD)","Compaction;Software;Parallel processing;Throughput;Writing;Resource management;Integrated design","flash memories;multichip modules;solid state drives;storage management;tree data structures","LSM-over-SSD designs;multiple software layers;LSM tree;write amplification;LSM-SSD integrated design;multichip flash translation layer;log-structured-merge trees;write-optimized data structure;high-performance key-value stores;write-efficient KVSSD;FTL mapping mechanism;direct data allocation;flash memory;garbage collection;fine-grained policy;dynamic striping policy;internal parallelism;multichip SSD;solid-state drives;I/O acceleration;copy-free compaction","","","","30","IEEE","14 Apr 2020","","","IEEE","IEEE Journals"
"Optimization of Storage Quota Based on User's Usage Distribution","Y. Kasahara; T. Kawatani; E. Ito; K. Simozono; N. Fujimura","Res. Inst. for IT, Kyushu Univ., Fukuoka, Japan; Dept. of ISEE, Kyushu Univ., Fukuoka, Japan; Res. Inst. for IT, Kyushu Univ., Fukuoka, Japan; Comput. & Commun. Center, Kagoshima Univ., Kagoshima, Japan; Dept. of Design, Kyushu Univ., Fukuoka, Japan","2015 IEEE 39th Annual Computer Software and Applications Conference","24 Sep 2015","2015","3","","149","154","To prevent shortage of storage space in a service system, an administrator usually set per-user quota as an upper limit of usable space for each user. To avoid service failure caused by resource exhaustion, the administrator tends to set a conservative quota value such as the storage capacity divided by the expected maximum number of users. In this research, we analyzed long-term storage usage history of our email system and file sharing system in Kyushu University. Mostly through the analyzed period, the usage pattern showed a long-tailed distribution similar to log-normal distribution. Also the overall storage consumption slowly increased during the analyzed period. Based on these analysis, we defined ""storage utilization ratio"" to evaluate how the storage was effectively used. By approximating a storage utilization pattern as a power-law distribution, we proposed a method to calculate the optimal quota value to maximize the utilization ratio.","0730-3157","978-1-4673-6564-2","10.1109/COMPSAC.2015.221","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7273344","","Postal services;Log-normal distribution;Electronic mail;Peer-to-peer computing;Approximation methods;Estimation;Gaussian distribution","statistical distributions;storage management","storage quota;user usage distribution;resource exhaustion;email system;file sharing system;Kyushu University;long-tailed distribution;storage consumption;storage utilization ratio;power-law distribution","","","","8","","24 Sep 2015","","","IEEE","IEEE Conferences"
"Android keylogging threat","F. Mohsen; M. Shehab","Department of Software and Information Systems, University of North Carolina at Charlotte, Charlotte, NC, USA; Department of Software and Information Systems, University of North Carolina at Charlotte, Charlotte, NC, USA","9th IEEE International Conference on Collaborative Computing: Networking, Applications and Worksharing","12 Dec 2013","2013","","","545","552","The openness of Android platform has attracted users, developers and attackers. Android offers bunch of capabilities and flexibilities, for instance, developers can write their own keyboard service-similar to Android soft keyboards-using the KeyboardView class. This class is available since api level 3.0 and can be part of the layout of an activity. Users prefer to download and install third-party keyboards that offer better experience and capabilities. However, there are security risks related to users installing and using these custom keyboards. Attackers can build or take advantage of existing third-party keyboards to create keyloggers to spy on smartphones users. Third-party keyboard once activated would substitute the Android standard keyboard, so all keys events pass this app. As results, many attacks can be launched identified by the permissions granted to these apps. The objective of this paper is to present these attacks, analyze their causes, and provide possible solutions.","","978-1-936968-92-3","10.4108/icst.collaboratecom.2013.254209","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6680023","mobile security;mobile apps;keyboard logging","Keyboards;Smart phones;Androids;Humanoid robots;Software;Mobile communication;Malware","Android (operating system);mobile computing;security of data","Android keylogging threat;Android platform;Android soft keyboards;KeyboardView class;third-party keyboards;security risks;Android standard keyboard","","8","","29","","12 Dec 2013","","","IEEE","IEEE Conferences"
"Scalable Crash Consistency for Staging-based In-situ Scientific Workflows","S. Duan; M. Parashar","Rutgers Discovery Informatics Institute Rutgers University,Piscataway,NJ,USA,08854; Rutgers Discovery Informatics Institute Rutgers University,Piscataway,NJ,USA,08854","2020 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)","28 Jul 2020","2020","","","340","348","As applications move towards extreme scales, data-related challenges are becoming significant concerns for scientific workflows, and in-situ/in-transit data processing have been proposed to address these challenges. However, increasing scales are expected to result in an increase in the rate of failures and the cost of resilience. Even worse, since coupled applications in workflows frequently interact and exchange a large amount of data, simply applying state of the art fault tolerance techniques to individual application components can not guarantee data consistency in workflows after failure recovery. Furthermore, naive use of fault tolerance techniques, such as checkpoint/restart, to the entire workflows prohibits the diversity of resilience of application components in workflows, and finally incurs a significant latency, storage overheads, and performance degradation. This paper addressed fault tolerance challenge for extreme scale in-situ scientific workflows. We present a loose coupled checkpoint/restart framework for in-situ workflows. This proposed approach provides a scalable and flexible fault tolerance scheme for in-situ workflows while still maintaining the data consistency and low resiliency cost. Specifically, we introduce a data logging mechanism in data staging which is composed by the queue based algorithm and user interface to keep data/events consistent during failure recovery. We have implemented our approach within the DataSpaces, an open-source data staging middleware, and evaluated it using synthetic workflows on a Cray XC40 system (Cori) at different scales. We demonstrated that, in the presence of failures, uncoordinated checkpoint and hybrid checkpoint with data logging scheme improved the workflow execution time by up to 13.48% in comparison with global coordinated checkpoint/restart approach.","","978-1-7281-7445-7","10.1109/IPDPSW50202.2020.00068","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9150356","crash consistency;checkpointing;in-situ workflows;data staging","Fault tolerance;Fault tolerant systems;Data models;Analytical models;Resilience;Couplings;Data visualization","checkpointing;data handling;middleware;public domain software;software fault tolerance;user interfaces;workflow management software","checkpoint;DataSpaces;crash consistency;open-source data staging middleware;user interface;queue based algorithm;data logging mechanism;fault tolerance scheme;failure recovery;data consistency;in-situ scientific workflows","","","","26","","28 Jul 2020","","","IEEE","IEEE Conferences"
"A New Framework for Detecting Insider Attacks in Cloud-Based E-Health Care System","F. M. Okikiola; A. M. Mustapha; A. F. Akinsola; M. A. Sokunbi","Yaba College of Technology,Department of Computer Technology,Lagos State,Nigeria; Federal University of Agriculture,Department of Computer Science,Abeokuta,Ogun State,Nigeria; Yaba College of Technology,Department of Computer Technology,Lagos State,Nigeria; Yaba College of Technology,Department of Computer Technology,Lagos State,Nigeria","2020 International Conference in Mathematics, Computer Engineering and Computer Science (ICMCECS)","27 Apr 2020","2020","","","1","6","The effect of insider attack on the e-Healthcare system can lead to false examination of patient's health records which have led to unaccountability of data usage and high financial cost as a result of data breaches in the e-healthcare without a highly efficient detection approach. A number of health centers have been faced with legal and reputational consequences as a result. This therefore requires the proposition of an efficient technique that can make this problem addressed most especially eHealth systems on the cloud environment as operations are currently operating with cloud services. Until such approaches are proposed, health records could be attacked and peradventure lead to poor treatment of patients due to misinformation and hence causing the death of individuals. This need serves as a key motivation for this research. In this, we proposed a new framework for detecting insider attacks in Cloud-based Healthcare system using watermarking extraction and logging detection technique. The framework was implemented using Microsoft Azure, Opennebula cloud management software emulation, PhP web scripting language, MySQL database server. The approach gave an output of the number of activities performed by users with the permission update of legal and illegal intrusion into the system using an audit trail. The approach showed high level of precision, recall and accuracy which makes it performance excellent to implement from the evaluation conducted at the end of the research.","","978-1-7281-3126-9","10.1109/ICMCECS47690.2020.240889","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9077627","e-Health;Cloud Computing;Insider attack;Health care system;Detection","","authoring languages;cloud computing;health care;medical information systems;public domain software;security of data;SQL;watermarking","cloud environment;cloud services;patient health records;Cloud-based Healthcare system;watermarking extraction;Opennebula cloud management software emulation;Cloud-based e-health care system;e-Healthcare system;data breaches;health centers;logging detection;insider attack detection;PhP web scripting language;Microsoft Azure;MySQL database server","","1","","18","","27 Apr 2020","","","IEEE","IEEE Conferences"
"Usability challenges in exception handling","R. Krischer; P. A. Buhr","David R. Cheriton School of Computer Science, University of Waterloo, Ontario, Canada, N2L 3G1; David R. Cheriton School of Computer Science, University of Waterloo, Ontario, Canada, N2L 3G1","2012 5th International Workshop on Exception Handling (WEH)","28 Jun 2012","2012","","","7","13","Two language mechanisms are presented and assessed for assisting understanding of exception complexity. First, passive and active exception assertions dynamically check if the reason for an exception raise matches with the purpose of a matching handler. A passive assertion is a check when an exception propagates out of a block it guards. The active form adds probabilistically injection of exceptions to provide more comprehensive testing. Second, runtime exception information is logged for dynamic query to check current handling status and provide a history of exceptional events.","","978-1-4673-1766-5","10.1109/WEH.2012.6226604","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6226604","exception;usability;asynchronous;assertion;injection;logging","Testing;Complexity theory;Usability;Probabilistic logic;Runtime;Semantics;Debugging","exception handling;probability;program diagnostics;program testing;program verification;query languages;software reusability","exception handling;language mechanism;exception complexity;active exception assertion;passive exception assertion;matching handling;probabilistic injection;comprehensive testing;runtime exception information;dynamic query;event handling status checking;software usability","","2","1","21","","28 Jun 2012","","","IEEE","IEEE Conferences"
"Distributed XPath query processing over large XML data based on MapReduce framework","H. Fan; D. Wang; J. Liu","School of Electronics Engineering and Computer Science, Peking University, Beijing, China; School of Software and Microelectronics, Peking University, Beijing, China; National Engineering Research Center for Software Engineering, Peking University, Beijing, China","2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)","24 Oct 2016","2016","","","1447","1453","The volume of XML data is tremendous in many areas, especially in data logging and scientific areas. XPath query is the core operation of XML process. It is a challenge to query massive XML data stored in a distributed manner. In this paper, we present an efficient distributed XPath query processing using MapReduce, which simultaneously processes queries for a massive volume of XML data. We first use virtual nodes to split the large scale XML data file into filesplits to the distributed storage system. Then we present the distributed XPath query algorithm to compute different fragments of the document tree in parallel using the MapReduce framework. Furthermore, in order to handle the large XML data efficiently, we build the partitional index and use random access mechanism to perform the query. The experimentation shows that our approach is efficient and scalable on this issue.","","978-1-5090-4093-3","10.1109/FSKD.2016.7603390","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7603390","XML;XPath Query;Hadoop;MapReduce","XML;Partitioning algorithms;Indexes;Distributed databases;Algorithm design and analysis;Query processing;Data models","data handling;parallel processing;query processing;trees (mathematics);XML","distributed XPath query processing;large XML data files;MapReduce;data logging;massive XML data querying;distributed storage system;document tree;partitional index;random access mechanism","","1","","14","","24 Oct 2016","","","IEEE","IEEE Conferences"
"SIEM Open Source Solutions: A Comparative Study","A. Vazão; L. Santos; M. B. Piedade; C. Rabadão","Centro de Investigação em Informática e Comunicações, Instituto Politécnico de Leiria, Escola Superior de Tecnologia e Gestão de Leiria, Instituto Politécnico de Leiria, Leiria, Portugal; Centro de Investigação em Informática e Comunicações, Instituto Politécnico de Leiria, Escola Superior de Tecnologia e Gestão de Leiria, Instituto Politécnico de Leiria, Leiria, Portugal; Centro de Investigação em Informática e Comunicações, Instituto Politécnico de Leiria, Escola Superior de Tecnologia e Gestão de Leiria, Instituto Politécnico de Leiria, Leiria, Portugal; Centro de Investigação em Informática e Comunicações, Instituto Politécnico de Leiria, Escola Superior de Tecnologia e Gestão de Leiria, Instituto Politécnico de Leiria, Leiria, Portugal","2019 14th Iberian Conference on Information Systems and Technologies (CISTI)","15 Jul 2019","2019","","","1","5","Computer attacks are increasing in complexity and number of occurrences making it imperative to implement tools such as Security Information and Event Management (SIEM) to mitigate risks, as Organizations increasingly rely each time more on computer systems for the development of their activities. The presented work compares several SIEM “open source” solutions, resorting to bibliographic research and the implementation of several tests' scenarios, with the aim of creating a prototype and evaluating it in a production context. The chosen solution will have to take in account the legal requirements of the EU-GDPR, General Data Protection Regulation, such as the anonymization and pseudo anonymization of sensitive data, retention time of “logs”, and its encryption and protection.","2166-0727","978-9-8998-4349-3","10.23919/CISTI.2019.8760980","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8760980","Security Information and Event Management;OSSIM;ELK Stack;Splunk Free;Graylog","Monitoring;Software;Linux;IP networks;Firewalls (computing)","cryptography;data protection;legislation;public domain software;risk management","SIEM open source solutions;computer attacks;computer systems;security information and event management;risk mitigation;legal requirements;EU-GDPR;general data protection regulation;pseudoanonymization;encryption","","","","","","15 Jul 2019","","","IEEE","IEEE Conferences"
"Data Infrastructure at LinkedIn","A. Auradkar; C. Botev; S. Das; D. De Maagd; A. Feinberg; P. Ganti; L. Gao; B. Ghosh; K. Gopalakrishna; B. Harris; J. Koshy; K. Krawez; J. Kreps; S. Lu; S. Nagaraj; N. Narkhede; S. Pachev; I. Perisic; L. Qiao; T. Quiggle; J. Rao; B. Schulman; A. Sebastian; O. Seeliger; A. Silberstein; B. Shkolnik; C. Soman; R. Sumbaly; K. Surlaker; S. Topiwala; C. Tran; B. Varadarajan; J. Westerman; Z. White; D. Zhang; J. Zhang",NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA,"2012 IEEE 28th International Conference on Data Engineering","2 Jul 2012","2012","","","1370","1381","Linked In is among the largest social networking sites in the world. As the company has grown, our core data sets and request processing requirements have grown as well. In this paper, we describe a few selected data infrastructure projects at Linked In that have helped us accommodate this increasing scale. Most of those projects build on existing open source projects and are themselves available as open source. The projects covered in this paper include: (1) Voldemort: a scalable and fault tolerant key-value store, (2) Data bus: a framework for delivering database changes to downstream applications, (3) Espresso: a distributed data store that supports flexible schemas and secondary indexing, (4) Kafka: a scalable and efficient messaging system for collecting various user activity events and log data.","2375-026X","978-0-7695-4747-3","10.1109/ICDE.2012.147","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6228206","","LinkedIn;Indexes;Companies;Servers;Routing;Pipelines","database indexing;distributed databases;public domain software;social networking (online);software fault tolerance;storage management","data infrastructure projects;LinkedIn;social networking sites;core data sets;request processing requirements;open source projects;Voldemort;fault tolerant key-value store;Data bus;database change delivery;Espresso;distributed data store;flexible schemas;secondary indexing;Kafka;messaging system;activity events;log data","","25","7","14","","2 Jul 2012","","","IEEE","IEEE Conferences"
"Development of Multichannel Data Acquisition System for Metal Oxide Sensor Array as an Electronic Nose","A. K. Keshari; R. Ramakrishnan; J. P. Rao; V. Jayaraman; A. S. Rama Murthy","HBNI, MC&MFCG Indira Gandhi Centre for Atomic Research, Kalpakkam, India; HBNI, MC&MFCG Indira Gandhi Centre for Atomic Research, Kalpakkam, India; HBNI, MC&MFCG Indira Gandhi Centre for Atomic Research, Kalpakkam, India; HBNI, MC&MFCG Indira Gandhi Centre for Atomic Research, Kalpakkam, India; HBNI, MC&MFCG Indira Gandhi Centre for Atomic Research, Kalpakkam, India","2019 6th International Conference on Signal Processing and Integrated Networks (SPIN)","13 May 2019","2019","","","574","578","The multichannel data acquisition system is developed for the measurement of the resistance of metal oxide sensor array response for electronic nose application. The resistance of the sensor varies with the concentration of analytes. The instrumentation is developed in-house which measures the output of sensor array and sends the data to the PC through LAN. The data acquisition software was developed for four channels to collect the data in PC in the form of counts and converts it into the resistance of sensors by using a suitable algebraic equation. Principal Component Analysis was implemented in the software to discriminate the analytes. The software is equipped with user-friendly features like online monitoring, on-demand pop-up window to check the overall trend of sensor behavior for a larger data etc. The data are stored with the time stamp in a file for post analysis. The data acquisition software was tested with various standard resistors and found the results were good. Further, the data acquisition software was also tested with the sensor and logged the sensor data for the study of sensors. The features of this development and the results obtained from the experiments are described in this paper.","","978-1-7281-1380-7","10.1109/SPIN.2019.8711631","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8711631","Data acquisition software;sensor array;analytes;tin dioxide sensor;resistance","Software;Resistance;Data acquisition;Standards;Sensor arrays;Electrical resistance measurement","computerised instrumentation;data acquisition;electronic noses;principal component analysis;resistors;sensor arrays","multichannel data acquisition system;electronic nose application;metal oxide sensor array;resistance measurement;analyte concentration;LAN;PC;algebraic equation;principal component analysis;on-demand pop-up window;standard resistors","","1","","18","","13 May 2019","","","IEEE","IEEE Conferences"
"Web-Based Content Management System for Equipment Information","R. Taylor","Coll. of Eng., Univ. of Utah, Salt Lake City, UT, USA","2012 19th Biennial University/Government/Industry, Micro/Nano Symposium (UGIM)","23 Jul 2012","2012","","","1","1","Summary form only given. Our lab has developed a database driven content management system for lab members to obtain up to date information on the status and availability of tools from any web browser. The system displays whether a tool is up, down or in a caution state. It provides a calendar of upcoming reservations-either in-browser or as an ical file. Tools can be pinpointed on a map. Tool specific downloads can be stored in a versionable file repository with tiered access to control visibility to each of 3 groups: lab staff, lab members and the general public. Administration views and controls are available to lab staff when logged in. The system can be configured to use OpenCoral as the data backend. By installing the JasperServer reporting engine, a number of reports covering lab usage, equipment metrics and repair history can also be hosted and linked to the same set of information. This presentation will demonstrate how our web system is accessed by first going through the typical experience of a lab member, from authentication to looking up tool information, including the availability of reservation slots and any problems that have been reported on the tool. Then the user interface that is used by administrators for updating information and uploading files will be shown. The second portion of the demonstration will show the software components that make up the system and give an overview of how to install the application onto an existing web server. The software is based on a typical LAMP (Linux, Apache, MySQL and PHP) stack with LDAP authentication on the backend. Database storage can also use PostgreSQL. The JasperReports reporting engine is used for displaying reports. I will discuss the various caveats that need to be considered when setting up and connecting these systems, as well as plans for future improvements.","2375-5350","978-1-4577-1752-9","10.1109/UGIM.2012.6247085","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6247085","","Content management;Educational institutions;Databases;Availability;Engines;Authentication;Software","content management;Internet;laboratory techniques;virtual instrumentation","Web-based content management system;equipment information;database driven content management system;tool status;tool availability;Web browser;file repository;OpenCoral;JasperServer reporting engine;user interface;Web server;Linux;Apache;MySQL;PHP;PostgreSQL","","2","1","","","23 Jul 2012","","","IEEE","IEEE Conferences"
"An Automated Testing Platform for Mobile Applications","X. Ma; N. Wang; P. Xie; J. Zhou; X. Zhang; C. Fang","Nanjing Inst. of Product Quality Inspection, Nanjing, China; Nanjing Inst. of Product Quality Inspection, Nanjing, China; Nanjing Inst. of Product Quality Inspection, Nanjing, China; Nanjing Inst. of Product Quality Inspection, Nanjing, China; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; Nanjing Inst. of Product Quality Inspection, Nanjing, China","2016 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)","22 Sep 2016","2016","","","159","162","With the growing popularity and complexity of mobile apps, quality assurance becomes more and more important in mobile app development. Unfortunately, Android is also suffering from the notorious fragmentation problem. To tackle these problems, we introduce an automated testing platform, BugRocket. BugRocket combines a distributed testing system with automated testing techniques equipped on mobile devices. In this paper, we developed a toolset for automated testing, and set up such a testing platform with 40 of the most popular Android devices. An case study is conducted and the results show that BugRocket can work for functional testing and compatibility testing. Besides, BugRocket can record a failed run as long as annotated GUI model and system logs to alleviate locating bugs and bug fixing.","","978-1-5090-3713-1","10.1109/QRS-C.2016.25","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7573738","Testing platform;Automated testing;Mobile Testing","Testing;Servers;Androids;Humanoid robots;Computer bugs;Graphical user interfaces;Mobile communication","Android (operating system);distributed programming;mobile computing;program debugging;program testing;smart phones","automated testing platform;mobile applications;quality assurance;mobile app development;Bugrocket;distributed testing system;mobile devices;automated testing toolset;Android devices;functional testing;compatibility testing;annotated GUI model;bugs location;bug fixing","","6","","5","","22 Sep 2016","","","IEEE","IEEE Conferences"
"A novel design of bow-tie antennas array for uplink C-band applications based on fast and efficient computational equivalent model","S. Didouh; M. Abri; H. Abri Badaoui","Telecommunications Laboratory, Faculty of Technology, university of Tlemcen, Algeria; Telecommunications Laboratory, Faculty of Technology, university of Tlemcen, Algeria; STIC Laboratory, Faculty of Technology, university of Tlemcen, Algeria","2017 Seminar on Detection Systems Architectures and Technologies (DAT)","30 Mar 2017","2017","","","1","6","This paper proposes an original application and for the first time a novel manufactured design of series fed Logperiodic microstrip antennas array LPA composed of seven bow-tie radiating elements functioning in the uplink C-band frequency from 6.12 to 6.58 GHz based on the bow-tie antenna equivalent model (BAEM). The model is used to optimize the antenna array in a record time and calculate the return loss with a sufficient precision. To prove the effectiveness of the suggested BAEM model, a comparison has been achieved between the CST Microwave studio software, Momentum of Agilent, BAEM model and measurements. The comparison of measured and simulated results shows good agreement. The measured results show that a bandwidth from 6.12 to 6.58 GHz is obtained. The obtained results show the effectiveness of this method to analyze such types of antennas.","","978-1-5090-4508-2","10.1109/DAT.2017.7889176","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7889176","LPA antennas array;bow-tie antenna;BAEM model;uplink C-band fabrication;measurements","Covariance matrices;Mathematical model;Program processors;Information filters;Estimation;Seminars;Uncertainty","antenna feeds;antenna radiation patterns;bow-tie antennas;log periodic antennas;microstrip antenna arrays","bow-tie antennas array;uplink C-band application;computational equivalent model;series fed logperiodic microstrip antennas array LPA;bow-tie radiating elements;bow-tie antenna equivalent model;BAEM model;CST Microwave studio software;Momentum of Agilent;frequency 6.12 GHz to 6.58 GHz;bandwidth 6.12 GHz to 6.58 GHz","","","","13","","30 Mar 2017","","","IEEE","IEEE Conferences"
"Experience Report: How to Design Web-Based Competitions for Legal Proceedings: Lessons from a Court Case","A. Van Moorsel; M. Forshaw; F. Rocha","Sch. of Comput., Newcastle Univ., Newcastle upon Tyne, UK; Sch. of Comput., Newcastle Univ., Newcastle upon Tyne, UK; Sch. of Comput., Newcastle Univ., Newcastle upon Tyne, UK","2017 IEEE 28th International Symposium on Software Reliability Engineering (ISSRE)","16 Nov 2017","2017","","","240","249","In this practical experience report we discuss a court case in which one of the authors was expert witness. This UK civil case considered possible fraud in an online product promotion competition, with participants being denied prizes because they were considered to have cheated. The discussion in this paper aims to provide a practice-led perspective on the link between technology and legal issues in the design of online games and web applications. The paper presents the court's questions and the witness responses, and also provides a synopsis of analysis of data in the web server log file presented to court. Based on the insights gained, we present guidelines for the design of online competitions and for client-server web applications implementing it. As we will see, the case turned out to be about design of socio-technical systems, not about advanced technologies. It illustrates the need to identify practically relevant threat models and pragmatic security solutions that balance business, legal and usability concerns.","2332-6549","978-1-5386-0941-5","10.1109/ISSRE.2017.41","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8109090","web site design;online games;software engineering;web logs;practical experience report;legal;fraud","Games;IP networks;Servers;Browsers;Companies;Law","computer games;data analysis;Internet;law administration;product design;security of data;Web design","experience report;legal proceedings;court case;UK civil case;online product promotion competition;legal issues;online games;witness responses;web server;online competitions;client-server web applications;legal usability concerns;Web-Based Competitions design;fraud;socio-technical systems","","","","15","","16 Nov 2017","","","IEEE","IEEE Conferences"
"Aarohi: Making Real-Time Node Failure Prediction Feasible","A. Das; F. Mueller; B. Rountree",North Carolina State University; North Carolina State University; Lawrence Livermore National Laboratory,"2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020","2020","","","1092","1101","Large-scale production systems are well known to encounter node failures, which affect compute capacity and energy. Both in HPC systems and enterprise data centers, combating failures is becoming challenging with increasing hardware and software complexity. Several data mining solutions of logs have been investigated in the context of anomaly detection in such systems. However, with subsequent proactive failure mitigation, the existing log mining solutions are not sufficiently fast for real-time anomaly detection. Machine learning (ML)-based training can produce high accuracy but the inference scheme needs to be enhanced with rapid parsers to assess anomalies in real-time. This work tackles online anomaly prediction in computing systems by exploiting context free grammar-based rapid event analysis. We present our framework Aarohi1, which describes an effective way to predict failures online. Aarohi is designed to be generic and scalable making it suitable as a real-time predictor. Aarohi obtains more than 3 minutes lead times to node failures with an average of 0.31 msecs prediction time for a chain length of 18. The overall improvement obtained w.r.t. the existing state-of-the-art is over a factor of 27.4×. Our compiler-based approach provides new research directions for lead time optimization with a significant prediction speedup required for the deployment of proactive fault tolerant solutions in practice.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00115","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9139847","Online Prediction;HPC;Node Failures;Parsing","Training;Real-time systems;Production;Hardware;Software;Xenon;Anomaly detection","context-free grammars;data mining;learning (artificial intelligence);parallel processing;program compilers;software fault tolerance;system recovery","HPC systems;enterprise data centers;software complexity;subsequent proactive failure mitigation;real-time anomaly detection;machine learning-based training;rapid parsers;online anomaly prediction;context free grammar-based rapid event analysis;real-time predictor;node failures;compiler-based approach;lead time optimization;proactive fault tolerant solutions;large-scale production systems;prediction speedup;real-time node failure prediction;Aarohi;time 0.31 ms","","","","43","","14 Jul 2020","","","IEEE","IEEE Conferences"
"Consideration of the Essential Topics for Role Estimation for AIWolf","S. Kato; T. Okumura; I. Toda; T. Fukui; K. Iwata; N. Ito",Aichi Institute of Technology; Aichi Institute of Technology; Aichi Institute of Technology; Aichi Institute of Technology; Aichi University; Aichi Institute of Technology,"2019 6th International Conference on Computational Science/Intelligence and Applied Informatics (CSII)","28 Nov 2019","2019","","","72","77","In recent years, research on using artificial intelligence (AI) to play the Werewolf game has been actively conducted. The Werewolf game is a multiplayer communication game, where the players are divided into two teams who each aim to eliminate the opposing team through discussion. In the Werewolf game, a different role (possibly repeated) is assigned to each player. The role is referred to as the ""Role."" The Role of a player is known only to himself/herself. Good strategies for the Werewolf game are predicting and inferring other players' Roles. The AI player that plays the Werewolf game is called ""AIWolf."" In studies on AIWolf, the research on estimating the Role using machine learning has attracted a great deal of attention. However, most of the research on estimating the Role has not sufficiently discussed the required information for Role estimation, although it has described the efficiency of Role estimation. In this paper, we focus on this problem and aim to clarify the type of utterance, which is a feature that is sufficient for estimating the Role, by analyzing log files. The type of utterance is referred to as the ""Topic."" First, we select the required features from the Topic based on the results of the analysis of log files and propose the features required for the estimation of the Role. Second, we design and implement a prediction model with proposed features to evaluate their effectiveness. Additionally, we also implement prediction models from related research and compare them with our model. Finally, we evaluate the effectiveness of the proposed features for estimating the Role by comparing the Role estimation accuracy for the prediction models. From the experimental results, we confirmed that the accuracy of the prediction model in this paper is higher than that of one of the prediction models in the related research, and is statistically sufficient for estimating the Role. Therefore, we demonstrated that the features selected by analyzing the log files are effective in estimating the Role of AIWolf.","","978-1-7281-2553-4","10.1109/CSII.2019.00020","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8916754","AIWolf;Coefficient of variation;Artificial neural network","Games;Estimation;Predictive models;Dispersion;Urban areas;Business","computer games;feature extraction;learning (artificial intelligence);software performance evaluation","machine learning;role estimation accuracy;prediction model;log files;AI player;multiplayer communication game;Werewolf game;AIWolf","","","","10","","28 Nov 2019","","","IEEE","IEEE Conferences"
"On Evolving Software Defined Storage Architecture","A. Raghunath; Y. Zou; A. Chagam","Intel labs Intel Corp.,Hillsboro,OR,USA; Intel labs Intel Corp.,Hillsboro,OR,USA; Data Platforms Group Intel Corp.,Chandler,AZ,USA","2020 IEEE International Conference on Cloud Computing Technology and Science (CloudCom)","26 Apr 2021","2020","","","57","64","Software Defined Storage (SDS) architectures offer storage services on standard high-volume servers by providing durability, availability, reliability and on-demand scale-out elasticity. To meet different application needs, various storage interfaces like block, file, object are provided, as well as higher level interfaces like streaming write-ahead-logs, and NoSQL databases. Typically, each SDS service is implemented as an independent distributed framework, often with redundant capabilities intertwined with service-specific functions within a monolithic stack. Such monolithic SDS architectures prevent the benefits of new storage media and protocol innovations from reaching the applications using SDS services. In this paper, we advocate for a decoupled architecture by redefining the boundaries of separation within SDS. We describe an instantiation of this architecture built using a popular open-source SDS, where the SDS back-end is decoupled. Evaluation results demonstrate a 99% and 35% bandwidth consumption reduction in cluster network and total network respectively, as well as up to a 35% latency reduction, in a disaggregated storage environment just via careful decoupling. We discuss the benefits of this approach in the context of containerization for agility, scaling NVMe-oF and leveraging emerging storage accelerators.","2330-2186","978-1-6654-0388-7","10.1109/CloudCom49646.2020.00008","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9407322","storage architecture;software defined storage;cloud storage;disaggregation;NVMe over Fabrics","Industries;Cloud computing;Technological innovation;Protocols;Computer architecture;Media;Software reliability","","","","","","69","","26 Apr 2021","","","IEEE","IEEE Conferences"
"Precision Evaluation of a class of Timed Workflow Nets","D. Lefebvre; E. López-Mellado","GREAH UNILEHAVRE Normandie University,Le Havre,France,76600; CINVESTAV Unidad Guadalajara,Zapopan,Mexico,45019","2020 7th International Conference on Control, Decision and Information Technologies (CoDIT)","27 Nov 2020","2020","1","","843","848","In this paper, the precision evaluation of a class of timed workflow nets (WN) is addressed. A novel method for assessing the precision of a labelled stochastic timed WN (LSWN) with respect to a timed log λt composed by traces of dated events is proposed. The evaluation involves the combined use of two separated metrics of precision regarding a) the exceeding language of the untimed WN with respect to the untimed log, and b) the surplus covering of the timed sequences by the stochastic model. The assessment of the temporal precision (b) considers the mean durations extracted from the timed log and frequency of occurrence of the traces in λt.","2576-3555","978-1-7281-5953-9","10.1109/CoDIT49905.2020.9263811","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9263811","Precision evaluation;timed log;labelled stochastic timed workflow nets","Task analysis;Stochastic processes;Hidden Markov models;Firing;Petri nets;Labeling;Servers","Petri nets;stochastic processes;workflow management software","precision evaluation;timed workflow nets;untimed WN;untimed log;timed sequences;temporal precision","","","","21","","27 Nov 2020","","","IEEE","IEEE Conferences"
"Behavior based network traffic analysis tool","S. Kakuru","Electrical Engineering Department, San Jose State University, San Jose CA, 95112","2011 IEEE 3rd International Conference on Communication Software and Networks","8 Sep 2011","2011","","","649","652","Pattern matching systems are mainly based on network models, which are formed from detailed analysis of user statistics and network traffic. These models are used in developing traffic analysis tools. This paper focuses on development of a behavior analysis tool on any operating system and its use on detecting internal active/passive attacks. Many kinds of tools and firewalls are in market to help network administrator to prevent intrusion from outside network, but very few tools to stop attacks from internal part of the network. This tool provides a way to detect any unusual behavior by a legitimate user in a network. It uses packet sniffer like Wireshark to record log traffic over a network. Furthermore, behavioral analysis is carried in two phases. In the first phase, Wireshark records the user's interaction with the network for a period of time and is stored in database. In second phase, current activity is compared to the past activity and notifies any new behavior to network administrator. This tool adds an additional layer of security along with the intrusion detection systems available from any network attacks. Many additional features can be incorporated in this tool for future enhancement.","","978-1-61284-486-2","10.1109/ICCSN.2011.6014810","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6014810","Packet capture;network administrator (NA);Network behavior analysis (NBA)","Internet;Information services;Electronic publishing;Filtering","pattern matching;security of data","behavior based network traffic analysis tool;pattern matching system;network model;user statistics;behavior analysis tool;operating system;firewalls;network administrator;packet sniffer;Wireshark;log traffic;behavioral analysis;intrusion detection system","","5","","8","","8 Sep 2011","","","IEEE","IEEE Conferences"
"Resource Classification from Version Control System Logs","K. Agrawal; M. Aschauer; T. Thonhofer; S. Bala; A. Rogge-Solti; N. Tomsich","Inst. for Inf. Bus., Wirtschaftsuniv. Wien, Vienna, Austria; Inst. for Inf. Bus., Wirtschaftsuniv. Wien, Vienna, Austria; Inst. for Inf. Bus., Wirtschaftsuniv. Wien, Vienna, Austria; Inst. for Inf. Bus., Wirtschaftsuniv. Wien, Vienna, Austria; Inst. for Inf. Bus., Wirtschaftsuniv. Wien, Vienna, Austria; Inst. for Inf. Bus., Wirtschaftsuniv. Wien, Vienna, Austria","2016 IEEE 20th International Enterprise Distributed Object Computing Workshop (EDOCW)","10 Oct 2016","2016","","","1","10","Collaboration in business processes and projects requires a division of responsibilities among the participants. Version control systems allow us to collect profiles of the participants that hint at participants' roles in the collaborative work. The goal of this paper is to automatically classify participants into the roles they fulfill in the collaboration. Two approaches are proposed and compared in this paper. The first approach finds classes of users by applying k-means clustering to users based on attributes calculated for them. The classes identified by the clustering are then used to build a decision tree classification model. The second approach classifies individual commits based on commit messages and file types. The distribution of commit types is used for creating a decision tree classification model. The two approaches are implemented and tested against three real datasets, one from academia and two from industry. Our classification covers 86% percent of the total commits. The results are evaluated with actual role information that was manually collected from the teams responsible for the analyzed repositories.","2325-6605","978-1-4673-9933-3","10.1109/EDOCW.2016.7584383","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7584383","","Data mining;Software;Control systems;Algorithm design and analysis;Computer bugs;Social network services;Business","decision trees;pattern classification;pattern clustering","resource classification;version control system logs;participants automatic classification;k-means clustering;decision tree classification model","","4","","20","","10 Oct 2016","","","IEEE","IEEE Conferences"
"Implementation of Real-Time Monitoring System for Hazardous Chemical Gas Distribution","J. Lee; D. Welsan; E. Hidayat; B. R. Trilaksono","School of Electrical Engineering, Institut Teknologi Bandung, West Java, Indonesia; School of Electrical Engineering, Institut Teknologi Bandung, West Java, Indonesia; School of Electrical Engineering, Institut Teknologi Bandung, West Java, Indonesia; School of Electrical Engineering, Institut Teknologi Bandung, West Java, Indonesia","2018 IEEE 8th International Conference on System Engineering and Technology (ICSET)","10 Jan 2019","2018","","","102","107","This paper covers design and implementation of monitoring system using unmanned vehicle to monitor the spread of hazardous chemical gas. This system is applicable to wide variety of unmanned vehicles ranging from ground vehicles to aerial vehicles. It can also be implemented with multi-vehicle concept. In a simple scenario, the system uses chemical sensor to measure and monitor level of hazardous chemical gas in a specified region explored by the unmanned vehicles. Then the measurement is sent to a computer in Ground Control Station (GCS) to be visualized on 2D virtual map in real-time. In addition to data visualization, the system also provides data logging saved in a CSV file for further data processing. Users can control the vehicles remotely with a user interface in the GCS as well. This monitoring system is developed and implemented in Robot Operating System (ROS) and open source software QgroundControl (QGC). Data acquisition and communication between the vehicle and the GCS are implemented in ROS, whereas the visualization system is developed from well-known flight control user interface named QGC. Implementation of this system shows that it is successfully tested to visualize hazardous chemical gas on a 2D virtual map in real-time.","2470-640X","978-1-5386-9180-9","10.1109/ICSEngT.2018.8606368","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8606368","Monitoring System;Hazardous Chemical Gas;Unmanned Vehicle;Robot Operating System;QGroundControl","Pollution measurement;Chemicals;Unmanned vehicles;Monitoring;Data visualization;Global Positioning System;Position measurement","autonomous aerial vehicles;chemical sensors;computerised monitoring;control engineering computing;data acquisition;data visualisation;graphical user interfaces;hazards;mobile robots;multi-robot systems;public domain software","real-time monitoring System;hazardous chemical gas distribution;unmanned vehicle;ground vehicles;aerial vehicles;chemical sensor;GCS;2D virtual map;data visualization;Robot Operating System;ground control station;data logging;CSV file;QGC open source software;QgroundControl;flight control user interface","","","","16","","10 Jan 2019","","","IEEE","IEEE Conferences"
"A Temporal Logic-Based Measurement Framework for Process Mining","A. Cecconi; G. D. Giacomo; C. D. Ciccio; F. M. Maggi; J. Mendling","WU Vienna,Vienna,Austria; Sapienza University of Rome,Rome,Italy; Sapienza University of Rome,Rome,Italy; Free University of Bolzano,Bolzano,Italy; WU Vienna,Vienna,Austria","2020 2nd International Conference on Process Mining (ICPM)","22 Oct 2020","2020","","","113","120","The assessment of behavioral rules with respect to a given dataset is key in several research areas, including declarative process mining, association rule mining, and specification mining. The assessment is required to check how well a set of discovered rules describes the input data, as well as to determine to what extent data complies with predefined rules. In declarative process mining, in particular, some measures have been taken from association rule mining and adapted to support the assessment of temporal rules on event logs. Among them, support and confidence are used most often, yet they are reportedly unable to provide a sufficiently rich feedback to users and often cause spurious rules to be discovered from logs. In addition, these measures are designed to work on a predefined set of rules, thus lacking generality and extensibility. In this paper, we address this research gap by developing a general measurement framework for temporal rules based on Linear-time Temporal Logic with Past on Finite Traces (LTLpf). The framework is independent from the rule-specification language of choice and allows users to define new measures. We show that our framework can seamlessly adapt well-known measures of the association rule mining field to declarative process mining. Also, we test our software prototype implementing the framework on synthetic and real-world data, and investigate the properties characterizing those measures in the context of process analysis.","","978-1-7281-9832-3","10.1109/ICPM49681.2020.00026","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9229935","Declarative Process Mining;Specification Mining;Association Rule Mining;Quality Measures;Temporal Rules","Data mining;Software measurement;Probabilistic logic;Particle measurements;Atmospheric measurements;Time measurement;Software","data mining;formal specification;specification languages;temporal logic","behavioral rules;declarative process mining;specification mining;temporal rules;linear-time temporal logic;association rule mining;temporal logic-based measurement;event logs;past on finite traces;rule-specification language","","","","25","","22 Oct 2020","","","IEEE","IEEE Conferences"
"The analysis of youths' searching behavior","C. Li; B. Wu; Y. Li","Beijing Key Laboratory of Intelligent Telecommunications, Software and Multimedia, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Laboratory of Intelligent Telecommunications, Software and Multimedia, Beijing University of Posts and Telecommunications, Beijing, China; Internet Management and Law Research Center, Beijing University of Posts and Telecommunications, Beijing, China","2011 Second Worldwide Cybersecurity Summit (WCS)","8 Aug 2011","2011","","","1","4","In order to create a safe and harmonious cyberspace environment for youth and protect them from the impact of bad information such as pornography, violence and gambling, most of the current solutions are to shield the sites containing this information. However some youth will take the initiative to seek out this bad information. In this paper we propose a method to analyze the searching behavior of youth and then to shield those keywords that may impact their health over time. We suggest that search engine operators provide keywords filtering technology to limit the searching behavior of youth. Operators should return to warning users who search keywords that contain bad information and record the searching log as feedback to parents. Of course parents should provide their IP addresses to the search engine operators first, to allow operators to set their IP addresses within the scope of supervising. Relying on search logs we count and classify keywords to understand the trend of youths' searching behavior and the areas that they often are interested in. Then we cluster the keywords in each area to get some main clusters and frequent keywords. Using this method we can grasp youths' searching behavior and protect them from the impact of bad information effectively.","","978-0-615-51608-0","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5978793","classify;cluster;searching behavior;bad informations;shield","Classification algorithms;Clustering algorithms;Internet;Search engines;Training;Text categorization;Machine learning algorithms","behavioural sciences computing;information retrieval;Internet;pattern classification;pattern clustering;search engines;security of data","youth searching behavior analysis;cyberspace environment;pornography;gambling;search engine operators;keywords filtering technology;searching log;IP addresses;Internet era;cyber security","","","","","","8 Aug 2011","","","IEEE","IEEE Conferences"
"An Implementation of Scalable High Throughput Data Platform for Logging Semiconductor Testing Results","C. Tsung; H. Hsieh; C. Yang","Department of Computer Science and Information Engineering, National Chin-Yi University of Technology, Taichung, Taiwan; Department of Computer Science, Tunghai University, Taichung, Taiwan; Department of Computer Science, Tunghai University, Taichung, Taiwan","IEEE Access","12 Mar 2019","2019","7","","26497","26506","Unlike graded data of common semiconductor test results storing in relational databases, log data in the standard test data format (STDF) contain millions of test data entries. In a semiconductor packaging and testing factory, a semiconductor wafer or integrated circuit tests generate thousands of STDF files each day; therefore, how to store these massive databases is a crucial topic. Different products correspond to different test items and STDF content; if a relational database is used to store all forms of data, the practical operation becomes challenging. This paper used a NoSQL document-oriented database collocated with a Docker container to build a system, named the scalable STDF data (SSD) framework, for storing semiconductor test data. According to semiconductor test operations, the SSD framework first converts STDF files into an open standard format for data transmission and subsequently transfers them to the database. The use of NoSQL databases allows for flexibility of specifications of STDF content, and a Docker container exhibits features such as rapid deployment and high scalability. The SSD framework meets the requirements of semiconductor testing for throughput, latency, and parallel experimental projects; possesses excellent execution efficiency; and provides flexible data storage services in a semiconductor testing environment where processing a large quantity of data is required. From our simulation results, the major performance of the proposed system depends on the hardware properties. The higher hardware distribution degree provides better performance. Docker container provides more connections and the scalability of storage, but higher software distribution contributes limited performance enhancement.","2169-3536","","10.1109/ACCESS.2019.2901115","Ministry of Science and Technology, Taiwan; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8649578","Flexible data storage;scalable STDF data;semiconductor testing;standard test data format","Containers;NoSQL databases;Testing;Security;Hardware;Throughput","data analysis;database management systems;electronic engineering computing;integrated circuit testing;relational databases;semiconductor device testing;SQL;storage management","simulation results;semiconductor testing environment;flexible data storage services;high scalability;NoSQL databases;data transmission;SSD framework;semiconductor test operations;semiconductor test data;scalable STDF data framework;NoSQL document-oriented database;STDF content;massive databases;STDF files;integrated circuit tests;semiconductor wafer;testing factory;semiconductor packaging;test data entries;standard test data;relational database;common semiconductor test results;graded data;semiconductor testing results;scalable high throughput data platform;test items;Docker container","","2","","33","","22 Feb 2019","","","IEEE","IEEE Journals"
"Approximation and Online Algorithms for NFV-Enabled Multicasting in SDNs","Z. Xu; W. Liang; M. Huang; M. Jia; S. Guo; A. Galis","Univ. Coll. London, London, UK; Australian Nat. Univ., Canberra, ACT, Australia; Australian Nat. Univ., Canberra, ACT, Australia; Australian Nat. Univ., Canberra, ACT, Australia; Hong Kong Polytech. Univ., Hong Kong, China; Univ. Coll. London, London, UK","2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS)","17 Jul 2017","2017","","","625","634","Multicasting is a fundamental functionality of networks for many applications including online conferencing, event monitoring, video streaming, and system monitoring in data centers. To ensure multicasting reliable, secure and scalable, a service chain consisting of network functions (e.g., firewalls, Intrusion Detection Systems (IDSs), and transcoders) usually is associated with each multicast request. Such a multicast request is referred to as an NFV-enabled multicast request. In this paper we study NFV-enabled multicasting in a Software-Defined Network (SDN) with the aims to minimize the implementation cost of each NFV-enabled multicast request or maximize the network throughput for a sequence of NFV-enabled requests, subject to network resource capacity constraints. We first formulate novel NFV-enabled multicasting and online NFV-enabled multicasting problems. We then devise the very first approximation algorithm with an approximation ratio of 2K for the NFV-enabled multicasting problem if the number of servers for implementing the network functions of each request is no more than a constant K (1). We also study dynamic admissions of NFV-enabled multicast requests without the knowledge of future request arrivals with the objective to maximize the network throughput, for which we propose an online algorithm with a competitive ratio of O(log n) when K = 1, where n is the number of nodes in the network. We finally evaluate the performance of the proposed algorithms through experimental simulations. Experimental results demonstrate that the proposed algorithms outperform other existing heuristics.","1063-6927","978-1-5386-1792-2","10.1109/ICDCS.2017.43","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7980006","Network function virtualization;software-defined networks;multicasting;NFV-enabled multicasting;service chains;virtualized network functions;approximation and online algorithms","Servers;Multicast communication;Approximation algorithms;Heuristic algorithms;Bandwidth;Optical switches;Middleboxes","approximation theory;computer network reliability;firewalls;multicast communication;software defined networking;virtualisation","SDN;online algorithm;network fundamental functionality;online conferencing;event monitoring;video streaming;system monitoring;data centers;service chain;network functions;firewalls;intrusion detection systems;IDS;transcoders;NFV-enabled multicast request;software-defined network;network throughput maximization;network resource capacity constraints;online NFV-enabled multicasting problem;first approximation algorithm;approximation ratio;NFV-enabled multicasting problem;dynamic admissions;request arrivals;competitive ratio;multicasting reliability;multicasting security;multicasting scalability","","32","","28","","17 Jul 2017","","","IEEE","IEEE Conferences"
"A Distributed Multi-Target Software Vulnerability Discovery and Analysis Infrastructure for Smart Phones","S. P. T. Krishnan; L. W. Hao; S. A. Sathya; L. Devi","Inst. for Infocomm Res., Singapore, Singapore; Inst. for Infocomm Res., Singapore, Singapore; Inst. for Infocomm Res., Singapore, Singapore; Inst. for Infocomm Res., Singapore, Singapore","2010 IEEE Global Telecommunications Conference GLOBECOM 2010","10 Jan 2011","2010","","","1","5","Smart phones of today have increasingly sophisticated software. As the feature set grows further, the probability of system security related defects is likely to increase as well. Today, the security of mobile platforms and applications comes under great scrutiny as they are getting widely adopted. It is therefore crucial that code for mobile devices gets well tested and security bugs eliminated where possible. A popular and effective testing technique to identify severe security bugs in source code is fuzz testing. However, it is extremely time consuming to generate randomized input and test them on each version of the mobile phone and its software. This paper presents, MAFIA - Multi-target Automated Fuzzing Infrastructure and Arsenal, a composite, distributed client-server fuzz testing infrastructure for software applications and libraries in virtually any smartphone platform. The set of tools in MAFIA is file-format agnostic and can be used across various applications & libraries. With MAFIA, we conducted a large number of tests against image-handling libraries and logged more than 13,000 mutated inputs that successfully crash several Symbian OS retail phones models. The system is scalable and can be easily extended to be used on new devices and operating systems.","1930-529X","978-1-4244-5638-3","10.1109/GLOCOM.2010.5684011","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5684011","","Computer crashes;Mobile communication;Servers;Mobile handsets;Libraries;Security;Software","mobile computing;mobile handsets;operating systems (computers);security of data;source coding;telecommunication computing","distributed multitarget software vulnerability discovery;analysis infrastructure;smart phones;system security;mobile platforms;mobile devices;security bugs;source code;fuzz testing;MAFIA;multitarget automated fuzzing infrastructure and arsenal;image-handling libraries;OS retail phones models;operating systems","","1","","15","","10 Jan 2011","","","IEEE","IEEE Conferences"
"Software framework for development and running multiplayer games","P. Goloskokovic; I. Anctelkovic; B. Nikolić","Elektrotehnički fakultet u Beogradu, Bulevar kralja Aleksandra 73, 11120 Beograd, Srbija; Elektrotehnički fakultet u Beogradu, Bulevar kralja Aleksandra 73, 11120 Beograd, Srbija; Elektrotehnički fakultet u Beogradu, Bulevar kralja Aleksandra 73, 11120 Beograd, Srbija","2012 20th Telecommunications Forum (TELFOR)","24 Jan 2013","2012","","","1421","1424","Game theory as a field of study for the Expert Systems course at the Faculty of Electrical Engineering at the University of Belgrade is fertile ground for the practical work of the students. For this purpose, a software system was designed in the Java programming language that enables implementation of these requirements. The system consists of a server that runs the games and programs that students write in order to remotely access the server and participate in the games. Software-based players extend the interfaces in the Java programming language, which allows students to focus on writing artificial intelligence algorithms, while leaving our interfaces to handle the flow of the game and communicate with the server in the background. Server is able to run arbitrary number of games. After starting the server, logged players play matches with each other, while server monitors statistics of each game. Server has a graphical user interface witch displays all server events in text format, the flow of each party and statistics of all matches played in every game.","","978-1-4673-2984-2","10.1109/TELFOR.2012.6419485","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6419485","Ekspertski sistemi;igre za više igrača;Java softverski sistem;minimax algoritam;teorija igara;veštačka inteligencija","Servers;Java;Games;Educational institutions;Electronic mail;Robots;Artificial intelligence","computer aided instruction;computer games;computer science education;educational courses;expert systems;game theory;graphical user interfaces;Java;statistics","software framework;multiplayer game development;game theory;expert systems course;Faculty of Electrical Engineering;University of Belgrade;software system design;Java programming language;remotely server access;software-based players;artificial intelligence algorithms;game flow handling;game statistics;graphical user interface","","","","7","","24 Jan 2013","","","IEEE","IEEE Conferences"
"Probabilistic model checking for AMI intrusion detection","M. Q. Ali; E. Al-Shaer","Department of Software and Information Systems, University of North Carolina Charlotte, USA; Department of Software and Information Systems, University of North Carolina Charlotte, USA","2013 IEEE International Conference on Smart Grid Communications (SmartGridComm)","19 Dec 2013","2013","","","468","473","Smart grids provide bi-directional communication between smart meters at user premises and utility provider for the purpose of efficient energy management through Advanced Metering Infrastructure (AMI). Recent studies have shown that the potential threats targeting AMI are significant. Despite the need of developing intrusion detection systems (IDS) tailored for the smart grid [4], very limited progress has been made in this area so far. Unlike traditional networks, smart grid has its unique challenges, such as limited computational power devices and potentially high deployment cost, which restrict the deployment options of intrusion detectors. However, smart grid exhibits behavior that can be accurately modeled based on its configuration, which can be exploited to design efficient intrusion detectors. In this paper, we show that AMI behavior can be modeled using event logs collected at smart collectors, which in turn can be verified using the specifications invariant generated from the configurations of the AMI devices. We model the AMI behavior using the fourth order Markov chain and the stochastic model is then probabilistically verified using specifications written in Linear Temporal Logic. Our model is capable of detecting malicious behavior in the AMI network due to intrusions or device malfunctioning. We validate our approach on a real-world dataset of thousands of meters collected at the AMI of a leading utility provider.","","978-1-4799-1526-2","10.1109/SmartGridComm.2013.6688002","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6688002","","Markov processes;Smart grids;Correlation;Entropy;Monitoring;Accuracy","energy management systems;Markov processes;power engineering computing;safety systems;smart meters;smart power grids;temporal logic","advanced metering infrastructure;AMI;intrusion detection systems;IDS;probabilistic model checking;smart grids;bidirectional communication;smart meters;user premises;utility provider;energy management;event log collection;smart collectors;fourth order Markov chain;stochastic model;linear temporal logic;malicious behavior detection;device malfunctioning;real-world dataset","","3","","14","","19 Dec 2013","","","IEEE","IEEE Conferences"
"Visiting Styles in an Art Exhibition Supported by a Digital Fruition System","S. Cuomo; P. De Michele; A. Galletti; G. Ponti","Dept. of Math. & Applic., Univ. of Naples “Federico II”, Naples, Italy; Dept. of Math. & Applic., Univ. of Naples “Federico II”, Naples, Italy; Univ. of Naples “Parthenope”, Naples, Italy; DTE, ENEA Portici Res. Center, Naples, Italy","2015 11th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)","8 Feb 2016","2015","","","775","781","We investigate the user dynamics related to the interaction with artworks in an exhibition. In a first step, we characterize visitors in a cultural heritage scenario and after, we study how these interact with available technologies. Accordingly with the fact that the technology plays a crucial role in supporting spectators and enhancing their experiences, the starting point of this research is the analysis of real data coming from visitors of the art exhibition named The Beauty or the Truth that was located in Naples, Italy. The event was equipped with several technological tools arranged within the halls of the exhibition, with the aim to create a novel metaphor that stimulates the user enjoyment and the knowledge diffusion. The collected log files from a suitable expert software system are used in a flexible framework in order to analyse how the supporting pervasive technology influence and modify behaviours and visiting styles. Finally, we carried out some experiments to exploit the clustering facilities for finding groups that reflect visiting styles. The obtained results have revealed interesting issues also to understand hidden aspects in the data and unattended in the analysis.","","978-1-4673-9721-6","10.1109/SITIS.2015.87","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7400651","clustering;data mining;user profiling;cultural heritage","Cultural differences;Art;Global communication;Electronic mail;Information systems;Data models;Media","art;data mining;history;pattern clustering;ubiquitous computing","visiting style;art exhibition;digital fruition system;user dynamics;cultural heritage;log files;pervasive technology;clustering facility","","12","","15","","8 Feb 2016","","","IEEE","IEEE Conferences"
"Improvements to a graphical technique for seafloor image mosaicking","M. Woolsey; A. Woolsey","National Institute for Undersea Science and Technology, University of Southern Mississippi, Abbeville, MS; Mississippi Mineral Resources Institute, University of Mississippi, University, Mississippi","2014 Oceans - St. John's","8 Jan 2015","2014","","","1","8","A technique was developed by which seafloor images gathered by a hovering AUV can be mosaicked based on the vehicle position at each image. In the first phase of development, software was produced to interpret the AUV logs and output parametric files for each image. The images and their parameters are then assembled into a mosaic using GIS software. The improvements to the overall chain of mosaic production span from operational changes in the way the vehicle performs to revisions in the mosaicking operations themselves. Postprocessing of AUV navigation using tie points within an intermediate mosaic is a significant improvement, although similar results can be obtained in future developments by utilizing the multibeam sonar bathymetry that is acquired along with the imagery.","0197-7385","978-1-4799-4918-2","10.1109/OCEANS.2014.7003101","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7003101","mosaicking;seafloor survey","Vehicles;Cameras;Software;Educational institutions;Sonar navigation;Accuracy","bathymetry;geophysical image processing;image segmentation;oceanographic techniques;seafloor phenomena","graphical technique;seafloor image mosaicking;seafloor images;vehicle position;AUV logs;output parametric files;GIS software;mosaic production chain;AUV navigation processing;intermediate mosaic;multibeam sonar bathymetry","","1","","6","","8 Jan 2015","","","IEEE","IEEE Conferences"
"An Empirical Exploratory Analysis of Failure Sequences in a Commodity Operating System","C. A. R. Dos Santos; R. Matias; K. S. Trivedi","Federal University of Uberlandia,School of Computer Science,Uberlandia,MG,Brazil; Federal University of Uberlandia,School of Computer Science,Uberlandia,MG,Brazil; Duke University,Electrical and Computer Engineering Department,Durham,NC,USA","2019 IX Brazilian Symposium on Computing Systems Engineering (SBESC)","26 Mar 2020","2019","","","1","8","A fundamental need for software reliability engineering is to comprehend how software systems fail, which means understanding the dynamics that govern different types of failure manifestation. In this paper, we present an exploratory study on multiple-event failures, looking for systematic patterns of sequences of failures in logs of a commodity operating system. This study is based on real failure data collected from hundreds of computers. The major contribution of this paper is the method proposed to discover patterns of failure sequences and their attributes. The method is generic enough to be applied to any other software system, with minor changes. The empirical findings of this study include 153 different patterns of OS failure sequences discovered, along with statistical analyses of their properties.","2324-7894","978-1-7281-6318-5","10.1109/SBESC49506.2019.9046072","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9046072","operating systems;failures;patterns","","failure analysis;operating systems (computers);software fault tolerance;statistical analysis","OS failure sequences;empirical exploratory analysis;commodity operating system;software reliability engineering;software system;failure manifestation;multiple-event failures;systematic patterns;failure data;statistical analyses","","1","","27","","26 Mar 2020","","","IEEE","IEEE Conferences"
"RTA: Real Time Actionable Events Detection as a Service","Y. Qiu; Y. Chen; L. Jiao; S. Huang","IBM Res. - China, Shanghai, China; Fudan Univ. - China, Shanghai, China; IBM Res. - China, Shanghai, China; IBM Res. - China, Shanghai, China","2016 IEEE International Conference on Web Services (ICWS)","1 Sep 2016","2016","","","514","521","Nowadays vast amounts of data are being produced in continuous ways. They may come from sensors, smart meters, application logs, monitoring software etc. The data need to be processed in realtime to gain actionable insights. Services like smart grid load balancing, cloud platform maintenance, can be carried out in an efficient way. Stream processing is the programming paradigm that answers such demand. When talking about stream processing, we can easily recall several famous open-source software frameworks such as Spark Streaming, Samza, Flink and Storm. Although they provide distributed, robust, low-latency stream processing engines, it's still difficult for an end user to set up a usable stream processing application from scratch. Firstly, users are required to write code to define their business related stream processing logic. Secondly, the submission and update of the stream processing logic require service restart, therefore it may lead to service unavailability for minutes. Thirdly, extra operation effort are required for handling scaling and failover issues. In this paper, we present RTA, a released research service on realtime data processing. The RTA service fills the gap between the stream processing requester and the existing software stacks. It offers a SQL-like stream query language for defining stream processing logic definition over streaming data. It allows users easily define their stream processing logic without programming. In RTA service, stream processing logic is also treated as a type of input, which enables online logic update without service downtime. The RTA service also provides scalability, high availability and resource isolation for serving multiple tenants. In this paper, we also provide a comprehensive evaluation of our service through a case study.","","978-1-5090-2675-3","10.1109/ICWS.2016.110","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7558042","","Engines;Sparks;Program processors;Monitoring;Programming;Data processing;Real-time systems","data handling;real-time systems;Web services","real time actionable events detection as a service;real time data processing;smart grid load balancing;cloud platform maintenance;programming paradigm;Spark streaming;Samza;Flink;Storm;RTA service;stream processing requester;software stacks;SQL-like stream query language;streaming data;stream processing logic;online logic update;resource isolation;Web service","","3","","24","","1 Sep 2016","","","IEEE","IEEE Conferences"
"Detecting Fraud on Websites","R. Fly",salesforce.com,"IEEE Security & Privacy","8 Dec 2011","2011","9","6","80","85","Beyond some specific vertical markets that have been dealing with fraud for ages such as financial institutions and retailers most software and services have zero ability to detect someone committing fraud against them or their users. Most sites happily take whatever requests and input users give them, thinking it's legitimate.Fortunately, there are fairly straightforward things every website can do to detect online fraud.","1558-4046","","10.1109/MSP.2011.161","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6096620","logging events;fraud detection;computer security;website security;http requests","Computer crime;Online services;Internet;Behavioral science","security of data;Web sites","online fraud detection;Web sites","","1","","4","","8 Dec 2011","","","IEEE","IEEE Magazines"
"Graphically Display Database Transactions to Enhance Database Forensics","C. Orosco; C. Varol; N. Shashidhar","Sam Houston State University,Department of Computer Science,Huntsville,TX,USA; Sam Houston State University,Department of Computer Science,Huntsville,TX,USA; Sam Houston State University,Department of Computer Science,Huntsville,TX,USA","2020 8th International Symposium on Digital Forensics and Security (ISDFS)","15 Jun 2020","2020","","","1","6","Database forensics presents a set of unique challenges and these may contribute to the reason for the lack of available tools and methods for database forensics. Yet considering the pervasiveness of databases in society, the amount of sensitive data stored within these repositories, and the number of data breaches, it is perplexing why there are not more database forensic software applications. This paper contains a discussion of a design for the graphical display of database transactions from a MySQL database. Using tools such as Elasticsearch and Kibana to aggregate the log files and pre-process the data into an intelligent format facilitates the expedient analysis of volumes of database transactions. The capability to search through terabytes of data within mere seconds and visually display this data is invaluable and considered as the vital contribution of this work.","","978-1-7281-6939-2","10.1109/ISDFS49300.2020.9116412","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9116412","database forensics;data visualization;Elasticsearch;Kibana;MySQL Binary Log;MySQL General Log","","data visualisation;digital forensics;SQL","MySQL database;database forensics;sensitive data;data breaches;database forensic software applications;database transaction graphical display","","","","19","","15 Jun 2020","","","IEEE","IEEE Conferences"
"Social Data Driven SDN Network Operation using Northbound Interface","T. Tairaku; A. Nakao; S. Yamamoto; S. Yamaguchi; M. Oguchi","Ochanomizu University, 2-1-1 Otsuka, Bunkyo-ku, Tokyo, JAPAN; The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, JAPAN; The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, JAPAN; Kogakuin University, 1-24-2 Nishi-Shinjuku, Shinjuku-ku, Tokyo, JAPAN; Ochanomizu University, 2-1-1 Otsuka, Bunkyo-ku, Tokyo, JAPAN","2018 International Conference on Computing, Networking and Communications (ICNC)","21 Jun 2018","2018","","","702","706","Software Defined Networking (SDN) enables highly flexible routing and traffic engineering. The network using the centralized SDN control can be operated by globally viewing the entire network. On the other hand, the network operation of the routing and traffic engineering based on internal network traffic monitoring does not immediately perform the network recovery specifically in case of the multiple network failures induced by the large scale disaster such as Great East Japan Earthquake. Hence, we will investigate the new network operation utilizing the external information abstracted from the social data. The social data contains the real-time user experience of the network quality degradation due to the emergency events, natural disaster etc. and useful for the immediate network recovery. The integration of the social data with SDN will enable the new network operation. In order to obtain the real-time information from the social data, Twitter is most relevant for our purpose. First, we constructed the SDN enabled network testbed in the wide area network controlled by the centralized system using th northbound interface (NBI). Next using the testbed, the network restoration experiment was successfully performed by the network path computation algorithm using actual Tweet logs of the Great East Japan Earthquake.","","978-1-5386-3652-7","10.1109/ICCNC.2018.8390390","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8390390","","Twitter;Monitoring;Containers;Earthquakes;Computer architecture;Performance evaluation;Switches","disasters;social networking (online);software defined networking;telecommunication control;telecommunication network reliability;telecommunication network routing;telecommunication traffic;traffic engineering computing;wide area networks","network path computation;Great East Japan Earthquake;social data driven SDN network operation;northbound interface;traffic engineering;centralized SDN control;internal network traffic monitoring;multiple network failures;network quality degradation;immediate network recovery;wide area network;network restoration experiment;software defined networking;flexible routing;emergency events;natural disaster;real-time information;Twitter;actual Tweet logs","","1","","11","","21 Jun 2018","","","IEEE","IEEE Conferences"
"Extracting brand names and product names from product review webpages: A case of mixed languages","K. Atchariyachanvanich; S. Jantararoungtong; V. Jirabovolvanit","Faculty of Information Technology, King Mongkut's Institute of Technology Ladkrabang 1 Soi Chalongkrung, Ladkrabang, Bangkok, Thailand; Faculty of Information Technology, King Mongkut's Institute of Technology Ladkrabang 1 Soi Chalongkrung, Ladkrabang, Bangkok, Thailand; Faculty of Information Technology, King Mongkut's Institute of Technology Ladkrabang 1 Soi Chalongkrung, Ladkrabang, Bangkok, Thailand","2016 13th International Joint Conference on Computer Science and Software Engineering (JCSSE)","21 Nov 2016","2016","","","1","5","Most of Thai product review websites display mixed contents by showing the product name and brand name in English language and reviewed contents in Thai language. However, the current product extraction application programming interfaces (API) could not extract product name written in English language from Thai product review websites. Therefore, this paper proposed the algorithm to extract brand names and product names from mixed Thai-English language product review websites. The experimental result revealed the proposed algorithm had a 97.48% of precision. The contribution of the proposed algorithm can be a part of location-based m-commerce. The algorithm extracts product name from the logs file of wireless access point in a shopping mall in order to recognize customers' interest. The sale promotion or advertising will be offered to the customers based on their interest via their mobile phone.","","978-1-5090-2033-1","10.1109/JCSSE.2016.7748923","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7748923","product name extraction;pattern matching","Feature extraction;Pattern matching;Computer science;Software engineering;Business;Web pages;Databases","application program interfaces;natural language processing;Web sites","extracting brand names;product names;product review Web pages;mixed languages;Web sites display;English language;Thai language;product extraction application programming interfaces;API;mixed Thai-English language product review Websites;m-commerce;wireless access point;customers interest;sale promotion;sale advertising;mobile phone","","","","6","","21 Nov 2016","","","IEEE","IEEE Conferences"
"Early-Stage Engagement: Applying Big Data Analytics on Collaborative Learning Environment for Measuring Learners' Engagement Rate","O. H. T. Lu; A. Y. Q. Huang; J. C. H. Huang; C. S. J. Huang; S. J. H. Yang","Dept. of Comput. Sci. & Inf. Eng., Nat. Central Univ., Taoyuan, Taiwan; Dept. of Comput. Sci. & Inf. Eng., Nat. Central Univ., Taoyuan, Taiwan; Dept. of Comput. Sci. & Inf. Eng., Hwa Hsia Univ. of Technol., Taipei, Taiwan; Dept. of Comput. Sci. & Inf. Eng., Nat. Central Univ., Taoyuan, Taiwan; Dept. of Comput. Sci. & Inf. Eng., Nat. Central Univ., Taoyuan, Taiwan","2016 International Conference on Educational Innovation through Technology (EITT)","2 Feb 2017","2016","","","106","110","Computer-supported Collaborative Learning (CSCL) is a pedagogical strategy associated with how learners construct knowledge with a group by computer-based learning system. In recent years, most of the computer-based learning systems record the interaction log of each learner when developing course assignments. However, the recorded data is facing a challenge to expose the learners behaviors during the course and to design a computer-supported collaborative learning activity. To address those challenges in this paper, a novel collaborative programming tool called Software Project Development and Insight Learning Environment (SPDI Learning Environment) is described. The SPDI Learning environment allows learners of computer science to develop course assignments collaboratively. Besides, it allows instructors to investigate the learners behaviors by associating a web-based integrated development environment (IDE) with Big Data analysis pipeline and Visualization Dashboard. In addition to collect real data from courses, we designed learning activities to help teachers to engage the field of CSCL and Learning Analytics.","","978-1-5090-6138-9","10.1109/EITT.2016.28","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7839503","Computer-supported Collaborative Learning;Learning Analytics;Collaborative Programming;Clickstream Data;Big-Data Analysis","Programming;Collaboration;Collaborative work;Big data;Software;Algorithm design and analysis;Education","Big Data;computer science education;data analysis;data visualisation;educational courses;Internet;learning management systems;programming environments","learning analytics;visualization dashboard;IDE;Web-based integrated development environment;computer science learning;SPDI learning environment;software project development and insight learning environment;learner behaviors;course assignment development;interaction log recording;computer-based learning system;learner knowledge construction;CSCL;computer-supported collaborative learning;learner engagement rate measurement;collaborative learning environment;Big Data analytics;early-stage engagement","","2","","20","","2 Feb 2017","","","IEEE","IEEE Conferences"
"Proposal of a Method for Identifying the Infection Route for Targeted Attacks Based on Malware Behavior in a Network","M. Sato; A. Sugimoto; N. Hayashi; Y. Isobe; R. Sasaki","Grad. Sch. of Adv. Sci. & Technol., Tokyo Denki Univ., Tokyo, Japan; Yokohama Res. Lab., Hitachi, Ltd., Yokohama, Japan; Yokohama Res. Lab., Hitachi, Ltd., Yokohama, Japan; Yokohama Res. Lab., Hitachi, Ltd., Yokohama, Japan; Grad. Sch. of Adv. Sci. & Technol., Tokyo Denki Univ., Tokyo, Japan","2015 Fourth International Conference on Cyber Security, Cyber Warfare, and Digital Forensic (CyberSec)","16 Jun 2016","2015","","","40","45","A targeted attack affects all terminals in a network. Therefore, in order to properly deal with such an attack, it is necessary to analyze the event information for each terminal in the network as well as all event information within the terminal. We have been studying a dynamic diagnostic method based on malware behavior in a network. We herein propose a malware detection method that works by dynamically converting collected process logs into CybOX and analyzing the converted data. In the present paper, we focus on the observables of the penetration/exploration phase of targeted attacks. We propose a method for identifying the route of infection by analyzing the process and a communication attempt associated with the process of the detected malware. We confirmed the ability to find the source of the infection process in the initially infected terminal by analyzing the behavior of the malware in a secondarily infected terminal.","","978-1-4673-8499-5","10.1109/CyberSec.2015.17","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7491559","Security;Malware;Targeted attack;CybOX;Log analysis","Malware;Ports (Computers);IP networks;Organizations;Trademarks;Monitoring;Electronic mail","invasive software","infection route identification;malware behavior;malware detection method;process logs;CybOX;targeted attacks exploration phase;infection process source;targeted attacks penetration phase","","","","15","","16 Jun 2016","","","IEEE","IEEE Conferences"
"Detection of Recovery Patterns in Cluster Systems Using Resource Usage Data","N. Gurumdimma; A. Jhumka","Dept. of Comput. Sci., Univ. of Warwick, Coventry, UK; Dept. of Comput. Sci., Univ. of Warwick, Coventry, UK","2017 IEEE 22nd Pacific Rim International Symposium on Dependable Computing (PRDC)","8 May 2017","2017","","","58","67","The failure of large-scale distributed systems such as cluster systems has adverse effects on the performance of high-performance computing applications such as scientific applications. Techniques to handle these failures, such as checkpointing, typically incur a prohibitively high computational cost. To reduce or prevent the occurrences of such failures, system administrators have employed a divide and conquer approach to diagnosing the root-cause of such failures, in order to take corrective or preventive measures. Most times, event logs are the main sources of information about the failures. However, it is also important to be able to predict when the system is recovering to avoid such costly error handling. To this end, we present a novel technique, based on system resource usage information, to detect recovery runs. Our approach uses an unsupervised learning technique, namely change point detection, to predict recovery. We run our approach on data from Ranger Supercomputer System and the results are positive: our approach have an F-measure of 64%.","2473-3105","978-1-5090-5652-1","10.1109/PRDC.2017.17","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7920597","Change point detection;resource usage data;recovery sequence;detection;large-scale HPC systems","Supercomputers;Radiation detectors;Error correction codes;Monitoring;Unsupervised learning;Signal processing;Software","divide and conquer methods;fault diagnosis;parallel processing;software fault tolerance;system recovery;unsupervised learning","F-measure;Ranger supercomputer system;change point detection;unsupervised learning;recovery runs detection;system resource usage information;error handling;system recovery;event logs;preventive measures;corrective measures;failure diagnosis;divide and conquer approach;system administrators;failure occurrences;checkpointing;scientific applications;high-performance computing;cluster systems;large-scale distributed systems;resource usage data;recovery patterns detection","","2","","41","","8 May 2017","","","IEEE","IEEE Conferences"
"Cross Platform Bug Correlation Using Stack Traces","M. A. Ghafoor; J. H. Siddiqui","Sch. of Sci. & Eng., Dept. of Comput. Sci., LUMS, Lahore, Pakistan; Sch. of Sci. & Eng., Dept. of Comput. Sci., LUMS, Lahore, Pakistan","2016 International Conference on Frontiers of Information Technology (FIT)","2 Mar 2017","2016","","","199","204","Crashing of program is an annoying experience for users. Whenever a program crashes, an event log is generated. Sometimes built in crash reporting programs send crash reports automatically to developing site whereas sometimes, user is presented with an option to report the crash himself. This reporting is often useful for the development team to diagnose and fix the problem. It happens quite often that code that crashes a program is not the reason for the crash itself but crash is due to the use of some faulty function or library of some other program. We propose a method to identify cross platform bug correlation to detect faulty functions, using function call stack information given in bug reports. We collect and process bug reports from multiple platforms to compute similarity based on our similarity metric between occurrence sequences of the function calls within different bug reports. For the solved bug we extract information about faulty function by analyzing its bug report to propose fix of the similar bug with same function within correlated bug report. If there exists fix of one bug in one application, it can be used to resolve similar bug in some other application. Using our technique we found similar bug reports. We were able to find cases for similar bug reports that have same reason for the cause of bug and were using same fix.","","978-1-5090-5300-1","10.1109/FIT.2016.044","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7866753","Bug reports;Stack traces","Computer bugs;Measurement;Correlation;Data mining;Software;Libraries","program debugging;software fault tolerance","cross platform bug correlation;stack traces;program crashing;event log;crash reporting programs;faulty function detection;function call stack;bug reports;similarity metric","","2","2","13","","2 Mar 2017","","","IEEE","IEEE Conferences"
"DHTM: Durable Hardware Transactional Memory","A. Joshi; V. Nagarajan; M. Cintra; S. Viglas","Univ. of Edinburgh, Edinburgh, UK; Univ. of Edinburgh, Edinburgh, UK; Intel, Munich, Germany; Google, USA","2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)","23 Jul 2018","2018","","","452","465","The emergence of byte-addressable persistent (non-volatile) memory provides a low latency and high bandwidth path to durability. However, programmers need guarantees on what will remain in persistent memory in the event of a system crash. A widely accepted model for crash consistent programming is ACID transactions, in which updates within a transaction are made visible as well as durable in an atomic manner. However, existing software based proposals suffer from significant performance overheads. In this paper, we support both atomic visibility and durability in hardware. We propose DHTM (durable hardware transactional memory) that leverages a commercial HTM to provide atomic visibility and extends it with hardware support for redo logging to provide atomic durability. Furthermore, we leverage the same logging infrastructure to extend the supported transaction size (from being L1-limited to LLC-limited) with only minor changes to the coherence protocol. Our evaluation shows that DHTM outperforms the state-of-the-art by an average of 21% to 25% on TATP, TPC-C and a set of microbenchmarks. We believe DHTM is the first complete and practical hardware based solution for ACID transactions that has the potential to significantly ease the burden of crash consistent programming.","2575-713X","978-1-5386-5984-7","10.1109/ISCA.2018.00045","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8416847","persistent memory;non volatile memory;ACID transactions;logging;hardware transactional memory;crash consistency","Hardware;Software;Coherence;Computer crashes;Protocols;Nonvolatile memory;Complexity theory","random-access storage;system recovery;transaction processing","DHTM;ACID transactions;crash consistent programming;durable hardware transactional memory;high bandwidth path;atomic visibility;atomic durability;byte-addressable persistent memory;non-volatile memory;system crash;logging infrastructure","","8","","45","","23 Jul 2018","","","IEEE","IEEE Conferences"
"Classifying social groups and change point detection for prioritizing social data","K. Dayanandan; J. Vepa","GAPG, Samsung India Software Operations, Bangalore, India; ASCG, Samsung India Software Operations, Bangalore, India","2011 IEEE 12th International Symposium on Computational Intelligence and Informatics (CINTI)","22 Dec 2011","2011","","","303","306","Internet technologies and web based social networking have seen wide adopted in the recent years and the user is exposed to lots of data from his friends and acquaintances. The data presented to the user needs to be filtered based on his current state and prioritized. In this paper we propose a method for identifying change points related to busy and free periods based on mobile phone call detail records. This could be used to measure the level of willingness level of users for social interaction. This work is useful for preparing elastic list of updates from close friends and also in optimizing the caching of recent updates at mobile handsets. For validation of our results, we used actual call logs of 100 users collected at MIT. The experimental results show that our method is able to detect the bands achieves results with high accuracy.","","978-1-4577-0045-3","10.1109/CINTI.2011.6108519","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6108519","","Social network services;Humans;Mobile handsets;Data mining;Noise;Information filters","Internet;mobile computing;mobile handsets;social networking (online)","social group classification;change point detection;social data prioritization;Internet technologies;Web based social networking;mobile phone call detail records;social interaction;mobile handsets","","","1","10","","22 Dec 2011","","","IEEE","IEEE Conferences"
"Predictive Analysis of Business Processes Using Neural Networks with Attention Mechanism","P. Philipp; R. Jacob; S. Robert; J. Beyerer","Vision and Fusion Laboratory IES, Karlsruhe Institute of Technology KIT,Karlsruhe,Germany; Vision and Fusion Laboratory IES, Karlsruhe Institute of Technology KIT,Karlsruhe,Germany; Fraunhofer Institute of Optronics, System Technologies and Image Exploitation IOSB, Fraunhofer Center for Machine Learning,Karlsruhe,Germany; Fraunhofer Institute of Optronics, System Technologies and Image Exploitation IOSB, Fraunhofer Center for Machine Learning,Karlsruhe,Germany","2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)","16 Apr 2020","2020","","","225","230","The analysis of ongoing processes is an important task in business process management. This is not surprising, since (e.g.) being able to predict future events in processes enables companies to intervene at an early stage if deviations from a desired workflow are likely to occur. Subsequently, errors and associated financial losses can be prevented or avoided. A common basis for being able to predict future events is a sequence of previous events that are typically stored in a socalled event log. In this work we present a neural network with attention mechanism, which is trained using publicly available event logs (e.g. BPI Challenge 2013). Furthermore, we elaborate the proposed model with an extensive dataset of events of a worldwide known German software company. In addition to promising results (e.g.) with regard to n-gram models, the training time of the proposed model is shorter than that of typical reference models such as neural networks with a long short-term memory architecture (LSTM).","","978-1-7281-4985-1","10.1109/ICAIIC48513.2020.9065057","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9065057","Process Prediction;Deep Learning;Neural Networks;Attention Mechanism;Transformer","Training;Neural networks;Companies;Task analysis;Predictive models;Unified modeling language","business data processing;data mining","neural network;predictive analysis;attention mechanism;business process management;financial losses;publicly available event logs;BPI Challenge 2013;German software company;n-gram model;long short-term memory architecture;LSTM","","","","23","","16 Apr 2020","","","IEEE","IEEE Conferences"
"A framework for detection and prevention of novel keylogger spyware attacks","M. Wazid; A. Katal; R. H. Goudar; D. P. Singh; A. Tyagi; R. Sharma; P. Bhakuni","Department of CSE, Graphic Era University, Dehradun, India; Department of CSE, Graphic Era University, Dehradun, India; Department of CSE, Graphic Era University, Dehradun, India; Department of CSE, Graphic Era University, Dehradun, India; Department of CSE, Graphic Era University, Dehradun, India; Department of IT, Graphic Era University, Dehradun, India; Department of IT, Graphic Era University, Dehradun, India","2013 7th International Conference on Intelligent Systems and Control (ISCO)","21 Mar 2013","2013","","","433","438","Cyber world is susceptible to various attacks, out of which malware attack is the malignant one. It is very difficult to detect and defend. A keylogger spyware contains both scripts keylogger and spyware in a single program. The functionality of this program is that it can capture all key strokes which are pressed by a system user and stores them in a log file, the spyware email this log file to the designer's specified address. It is very harmful for those systems which are used in daily transaction process i.e. online banking system. The prevention of these attacks is necessary. In this paper we have proposed a framework for detection and prevention of novel keylogger spyware attack. It is capable to defend against such kind of attacks.","","978-1-4673-4603-0","10.1109/ISCO.2013.6481194","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6481194","Keylogger Spyware Attack;Honeypot Base Monitoring;Keylogger Spyware Detection and Prevention;Keylogger Spyware Algorithm;Keylogger Spyware Monitoring and Inspection Algorithm;Keylogger Spyware Removal Algorithm","Helium;Spyware","electronic mail;invasive software;transaction processing","keylogger spyware attack detection;keylogger spyware attack prevention;cyber world;malware attack;spyware email;log file;transaction process;online banking system;keylogger spyware monitoring algorithm;keylogger spyware inspection algorithm","","9","1","15","","21 Mar 2013","","","IEEE","IEEE Conferences"
"Performance evaluation of distributed maximum weighted matching algorithms","C. U. Ileri; O. Dagdeviren","International Computer Institute, Ege University, Izmir, Turkey; International Computer Institute, Ege University, Izmir, Turkey","2016 Sixth International Conference on Digital Information and Communication Technology and its Applications (DICTAP)","18 Aug 2016","2016","","","103","108","Graph matching is a fundamental graph theory problem which has a broad application range including information retrieval, pattern recognition, graph partitioning, chemical structure analysis, protein function prediction, backup placement and cellular coverage. This problem has gained attention in distributed computing as there are distributed matching algorithms with asymptotically guaranteed time bounds and approximation ratios. On the other side, we do not know the practical performance of these algorithms. In this paper, we provide a detailed performance evaluation of asynchronous distributed maximum weighted matching (MWM) algorithms. We assume a message-passing system in CONGEST model in which the message size is limited to O(log n) where n is the number of nodes. This model is popular for energy-efficient networks such as wireless sensor networks. We used a discrete event simulator, SimPy, to model the assumed network structures. We provide the implementations of Watthenhofer and Wattenhofer's algorithm, Hoepman's algorithm, Lotker et al.'s algorithm and Lotker et al.'s improvement algorithm. The results show that the greedy algorithm of Hoepman performed best in approximating the optimum result in all types of networks, even achieving an approximation ratio of 0.99 in some instances. To the best of our knowledge this is the first study which provides an extensive performance evaluation of distributed MWM algorithms.","","978-1-4673-9609-7","10.1109/DICTAP.2016.7544009","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7544009","Graph Matching;Distributed Computing;Performance Evaluation","Approximation algorithms;Algorithm design and analysis","discrete event simulation;energy conservation;graph theory;information retrieval;message passing;pattern recognition;software performance evaluation","performance evaluation;graph matching;graph theory;information retrieval;pattern recognition;graph partitioning;chemical structure analysis;protein function prediction;backup placement;cellular coverage;distributed computing;asynchronous distributed maximum weighted matching algorithms;MWM algorithms;message passing system;CONGEST model;energy-efficient networks;discrete event simulator;SimPy","","4","","24","","18 Aug 2016","","","IEEE","IEEE Conferences"
"2B-SSD: The Case for Dual, Byte- and Block-Addressable Solid-State Drives","D. Bae; I. Jo; Y. A. Choi; J. Hwang; S. Cho; D. Lee; J. Jeong","Memory Bus., Samsung Electron. Co., Ltd., Yongin, South Korea; Memory Bus., Samsung Electron. Co., Ltd., Yongin, South Korea; Memory Bus., Samsung Electron. Co., Ltd., Yongin, South Korea; Memory Bus., Samsung Electron. Co., Ltd., Yongin, South Korea; Memory Bus., Samsung Electron. Co., Ltd., Yongin, South Korea; Memory Bus., Samsung Electron. Co., Ltd., Yongin, South Korea; Memory Bus., Samsung Electron. Co., Ltd., Yongin, South Korea","2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)","23 Jul 2018","2018","","","425","438","Performance critical transaction and storage systems require fast persistence of write data. Typically, a non-volatile RAM (NVRAM) is employed on the datapath to the permanent storage, to temporarily and quickly store write data before the system acknowledges the write request. NVRAM is commonly implemented with battery-backed DRAM. Unfortunately, battery-backed DRAM is small and costly, and occupies a precious DIMM slot. In this paper, we make a case for dual, byte- and block-addressable solid-state drive (2B-SSD), a novel NAND flash SSD architecture designed to offer a dual view of byte addressability and traditional block addressability at the same time. Unlike a conventional storage device, 2B-SSD allows accessing the same file with two independent byte- and block-I/O paths. It controls the data transfer between its internal DRAM and NAND flash memory through an intuitive software interface, and manages the mapping of the two address spaces. 2B-SSD realizes a wholly different way and speed of accessing files on a storage device; applications can access them directly using memory-mapped I/O, and moreover write with a DRAM-like latency. To quantify the benefits of 2B-SSD, we modified logging subsystems of major database engines to store log records directly on it without buffering them in the host memory. When running popular workloads, we measured throughput gains in the range of 1.2x and 2.8x with no risk of data loss.","2575-713X","978-1-5386-5984-7","10.1109/ISCA.2018.00043","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8416845","2B-SSD;non volatile memory;WAL","Random access memory;Nonvolatile memory;Software;Memory management;Hardware;Integrated circuit interconnections","flash memories;NAND circuits;random-access storage","NAND flash SSD architecture;NAND flash memory;internal DRAM;byte- block-addressable solid-state drive;dual block-addressable solid-state drive;battery-backed DRAM;NVRAM;nonvolatile RAM;block-addressable solid-state drives;2B-SSD","","1","","67","","23 Jul 2018","","","IEEE","IEEE Conferences"
"Shipwright: A Human-in-the-Loop System for Dockerfile Repair","J. Henkel; D. Silva; L. Teixeira; M. d’Amorim; T. Reps","University of Wisconsin–Madison,Madison,WI,USA; Federal University of Pernambuco,Recife,PE,Brazil; Federal University of Pernambuco,Recife,PE,Brazil; Federal University of Pernambuco,Recife,PE,Brazil; University of Wisconsin–Madison,Madison,WI,USA","2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)","7 May 2021","2021","","","1148","1160","Docker is a tool for lightweight OS-level virtualization. Docker images are created by performing a build, controlled by a source-level artifact called a Dockerfile. We studied Dockerfiles on GitHub, and—to our great surprise— found that over a quarter of the examined Dockerfiles failed to build (and thus to produce images). To address this problem, we propose SHIPWRIGHT, a human-in-the-loop system for finding repairs to broken Dockerfiles. SHIPWRIGHT uses a modified version of the BERT language model to embed build logs and to cluster broken Dockerfiles. Using these clusters and a search-based procedure, we were able to design 13 rules for making automated repairs to Dockerfiles. With the aid of SHIPWRIGHT, we submitted 45 pull requests (with a 42.2% acceptance rate) to GitHub projects with broken Dockerfiles. Furthermore, in a “time-travel” analysis of broken Dockerfiles that were later fixed, we found that SHIPWRIGHT proposed repairs that were equivalent to human-authored patches in 22.77% of the cases we studied. Finally, we compared our work with recent, state-of-the-art, static Dockerfile analyses, and found that, while static tools detected possible build-failure-inducing issues in 20.6–33.8% of the files we examined, SHIPWRIGHT was able to detect possible issues in 73.25% of the files and, additionally, provide automated repairs for 18.9% of the files.","1558-1225","978-1-6654-0296-5","10.1109/ICSE43902.2021.00106","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9402069","Docker;DevOps;Repair","Bit error rate;Maintenance engineering;Tools;Virtualization;Software development management;Software engineering","","","","","","45","","7 May 2021","","","IEEE","IEEE Conferences"
"HerpRap: A Hybrid Array Architecture Providing Any Point-in-Time Data Tracking for Datacenter","L. Zeng; D. Feng; B. Mao; J. Chen; Q. Wei; W. Liu","Wuhan Nat. Lab. for Optoelectron, Huazhong Univ. of Sci. & Technol., Wuhan, China; Wuhan Nat. Lab. for Optoelectron, Huazhong Univ. of Sci. & Technol., Wuhan, China; Univ. of Nebraska-Lincoln, Lincoln, NE, USA; Wuhan Nat. Lab. for Optoelectron, Huazhong Univ. of Sci. & Technol., Wuhan, China; Data Storage Inst., A*STAR, Singapore, Singapore; Wuhan Nat. Lab. for Optoelectron, Huazhong Univ. of Sci. & Technol., Wuhan, China","2012 IEEE International Conference on Cluster Computing","25 Oct 2012","2012","","","311","319","Both physical disk failure and logical errors such as software error, user abuse and virus attacks may cause data lose. The risk of logical errors is far greater than physical disk failure. Moreover, existing RAID solution cannot satisfy the reliability requirement in face of the logical errors in data centers. It is therefore becoming increasingly important for RAID-based storage systems to be able to recover data to any point-in-time when logical errors occur. We proposed a novel storage array architecture, Herp Rap, which is able to recover data from both physical disk failure and logical errors. We have implemented a prototype of Herp Rap and carried out extensive performance measurements using DBT-2 and file system benchmarks. Our experiments demonstrated that the proposed Herp Rap is able to track or recover data to any point-in-time quickly by tracing back the history of block logs. Moreover, Herp Rap outperforms existing HDD-based or SSD-based RAID5 with copy-on-write (COW) snapshot in terms of performance, energy efficiency, failure recovery ability and reliability.","2168-9253","978-0-7695-4807-4","10.1109/CLUSTER.2012.19","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6337793","RAID;solid state drive;continuous data protection;reliability;performance","Arrays;Reliability;Monitoring;Software;History;Linux","computer centres;data integrity;disc drives;hard discs;memory architecture;RAID;risk analysis;storage management;system recovery","HerpRap;hybrid array architecture;any point-in-time data tracking;physical disk failure;software error;user abuse;virus attack;data loss;logical error risk;reliability requirement;data center;RAID-based storage system;data recovery;storage array architecture;DBT-2;file system;block log history;HDD-based RAID5;SSD-based RAID5;copy-on-write snapshot;COW snapshot;energy efficiency;failure recovery ability","","3","1","42","","25 Oct 2012","","","IEEE","IEEE Conferences"
"From Sensing to Action: Quick and Reliable Access to Information in Cities Vulnerable to Heavy Rain","S. Gaitan; L. Calderoni; P. Palmieri; M. t. Veldhuis; D. Maio; M. B. van Riemsdijk","Department of Water Management, Delft University of Technology, Delft, The Netherlands; Department of Computer Science and Engineering, Università di Bologna, Bologna, Italy; Department of Software and Computer Technology, Delft University of Technology, Delft, The Netherlands; Department of Water Management, Delft University of Technology, Delft, The Netherlands; Department of Computer Science and Engineering, Università di Bologna, Bologna, Italy; Department of Intelligent Systems, Delft University of Technology, Delft, The Netherlands","IEEE Sensors Journal","23 Oct 2014","2014","14","12","4175","4184","Cities need to constantly monitor weather to anticipate heavy storm events and reduce the impact of floods. Information describing precipitation and ground conditions at high spatio-temporal resolution is essential for taking timely action and preventing damages. Traditionally, rain gauges and weather radars are used to monitor rain events, but these sources provide low spatial resolutions and are subject to inaccuracy. Therefore, information needs to be complemented with data from other sources: from citizens' phone calls to the authorities, to relevant online media posts, which have the potential of providing timely and valuable information on weather conditions in the city. This information is often scattered through different, static, and not-publicly available databases. This makes it impossible to use it in an aggregate, standard way, and therefore hampers efficiency of emergency response. In this paper, we describe information sources relating to a heavy rain event in Rotterdam on October 12-14, 2013. Rotterdam weather monitoring infrastructure is composed of a number of rain gauges installed at different locations in the city, as well as a weather radar network. This sensing network is currently scarcely integrated and logged data are not easily accessible during an emergency. Therefore, we propose a reliable, efficient, and low-cost ICT infrastructure that takes information from all relevant sources, including sensors as well as social and user contributed information and integrates them into a unique, cloud-based interface. The proposed infrastructure will improve efficiency in emergency responses to extreme weather events and, ultimately, guarantee more safety to the urban population.","1558-1748","","10.1109/JSEN.2014.2354980","SHINE project of the Delft University of Technology; Delft Institute for Research on ICT; Climate-KIC; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6892929","Smart city;urban ICT infrastructures;weather sensing;heavy rain","Meterology;Urban areas;Sensor systems;Meteorological radar","atmospheric techniques;meteorological radar;rain;remote sensing by radar;storms","heavy storm events;flood impact;rain gauges;weather radars;weather conditions;heavy rain event;AD 2013 10 12 to 14;Rotterdam weather monitoring infrastructure;ICT infrastructure;cloud-based interface;extreme weather events;urban population","","12","","38","","5 Sep 2014","","","IEEE","IEEE Journals"
"Trusted Transport Telemetry by Using Distributed Databases","I. S. Shipunov; K. S. Voevodskiy; A. P. Nyrkov; Y. F. Katorin; Y. A. Gatchin","Chair of Complex Provision of Information Security Admiral, Makarov State University of Maritime and Inland Shipping, St. Petersburg, Russia; Chair of Complex Provision of Information Security Admiral, Makarov State University of Maritime and Inland Shipping, St. Petersburg, Russia; Chair of Complex Provision of Information Security Admiral, Makarov State University of Maritime and Inland Shipping, St. Petersburg, Russia; Chair of Complex Provision of Information Security Admiral, Makarov State University of Maritime and Inland Shipping, St. Petersburg, Russia; Chair of Complex Provision of Information Security Admiral, Makarov State University of Maritime and Inland Shipping, St. Petersburg, Russia","2019 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (EIConRus)","3 Mar 2019","2019","","","344","347","There are several companies that provide their own hardware and software solutions for the collection and storage of telemetry data from transport. However, how to be sure that the system stores the correct data? There is always the possibility of bribery of employees of the Operator of the telemetry data. In this matter, the use of distributed databases is a reliable solution. Telemetry data from devices immediately gets into the database and can no longer be changed. When using a private distributed databases within the data Operator company, the possibility of various manipulations with the data is still preserved. To solve this problem, it is proposed to use the public distributed database Emercoin, one of the functional features of which is the NVS (Name Value Storage), - the ability to write to the database NVS-records of arbitrary content up to 20KB.","2376-6565","978-1-7281-0339-6","10.1109/EIConRus.2019.8657215","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8657215","keland logistic;telemetry logging;hardware;distributed databases;Emercoin;public system;insurance","Distributed databases;Telemetry;Insurance;Companies;Servers;Vehicles","distributed databases;telecommunication computing;telecommunication security;telemetry","telemetry data;Emercoin public distributed database;name value storage;NVS-records database","","2","","14","","3 Mar 2019","","","IEEE","IEEE Conferences"
"Regularized Deconvolution-Based Approaches for Estimating Room Occupancies","A. Ebadat; G. Bottegal; D. Varagnolo; B. Wahlberg; K. H. Johansson","Department of Automatic Control, School of Electrical engineering, KTH Royal Institute of Technology, Sweden; Department of Automatic Control, School of Electrical engineering, KTH Royal Institute of Technology, Sweden; Division of Signals and Systems, Department of Computer Science, Electrical and Space Engineering, Luleå University of Innovation and Technology, Luleå, Sweden; Department of Automatic Control, School of Electrical engineering, KTH Royal Institute of Technology, Sweden; Department of Automatic Control, School of Electrical Engineering, KTH Royal Institute of Technology, Sweden","IEEE Transactions on Automation Science and Engineering","2 Oct 2015","2015","12","4","1157","1168","We address the problem of estimating the number of people in a room using information available in standard HVAC systems. We propose an estimation scheme based on two phases. In the first phase, we assume the availability of pilot data and identify a model for the dynamic relations occurring between occupancy levels, CO2 concentration and room temperature. In the second phase, we make use of the identified model to formulate the occupancy estimation task as a deconvolution problem. In particular, we aim at obtaining an estimated occupancy pattern by trading off between adherence to the current measurements and regularity of the pattern. To achieve this goal, we employ a special instance of the so-called fused lasso estimator, which promotes piecewise constant estimates by including an ℓ1 norm-dependent term in the associated cost function. We extend the proposed estimator to include different sources of information, such as actuation of the ventilation system and door opening/closing events. We also provide conditions under which the occupancy estimator provides correct estimates within a guaranteed probability. We test the estimator running experiments on a real testbed, in order to compare it with other occupancy estimation techniques and assess the value of having additional information sources. Note to Practitioners - Home automation systems benefit from automatic recognition of human presence in the built environment. Since dedicated hardware is costly, it may be preferable to detect occupancy with software-based systems which do not require the installation of additional devices. The object of this study is the reconstruction of occupancy patterns in a room using measurements of concentration, temperature, fresh air inflow, and door opening/closing events. All these signals are information sources often available in HVAC systems of modern buildings and homes. We assess the value of such information sources in terms of their relevance in detecting occupancy in small and medium-sized rooms. The proposed estimation scheme is composed of two distinct phases. The first is a training phase where the goal is to derive a mathematical model relating the number of occupants with the concentration. It is required to record the actual occupants in the room for a time period spanning few days, a task that can be performed either with manual logging or with temporary dedicated hardware counting systems. In a second phase, we use the derived model to design an online software which collects measurements of the environmental signals and provides the number of people currently in the room. The estimated occupancy levels can then be employed to enhance the efficiency of the HVAC system of the building. We notice that, in modern residential buildings composed by structurally equal flats, the training phase can be run in one flat only, since the obtained model will be reasonably valid for the other flats.","1558-3783","","10.1109/TASE.2015.2471305","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7254252","Deconvolution;occupancy estimation;regularization;system identification","Estimation;Temperature measurement;System identification;Air conditioning;Deconvolution","home automation;HVAC;probability","regularized deconvolution-based approach;HVAC system;occupancy level;CO2 concentration;room temperature;occupancy estimation task;fused lasso estimator;cost function;ventilation system;probability;occupancy estimation techniques;home automation system;information sources;mathematical model;time period spanning;manual logging","","21","","48","","10 Sep 2015","","","IEEE","IEEE Journals"
"Botnets Detection in DNS logs using machine learning","F. Fernández-Peña; A. Zurita-Amores","FISEI, Universidad Técnica de Ambato, ESAI, Universidad Espíritu Santo, Ambato, Guayaquil, Ecuador; Black Label Tech cia. Ltda, Ambato, Ecuador","2019 14th Iberian Conference on Information Systems and Technologies (CISTI)","15 Jul 2019","2019","","","1","5","Botnets detection is a computationally expensive problem for which there is no deterministic solution yet. The scientific problem that raises is how to define a procedure for botnet detection with limited resources. In this paper, a botnets' detection method, based on machine learning, is formalized and evaluated. This proposal makes use of Splunk, a tool that allowed us to use the Random Forest algorithm to analyze DNS logs in order to detect connections to C&C servers. The resulting procedure complements the use of machine learning with the verification against other data sources for improving the results. The achieved results showed an error margin of +/- 5.44 for 18,748,713 events which were analyzed. This way, the validity of this proposal was proved.","2166-0727","978-9-8998-4349-3","10.23919/CISTI.2019.8760760","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8760760","security;botnet attack;machine learning;randomforest;dns server","Botnet;Machine learning;Malware;Hardware;IP networks","invasive software;learning (artificial intelligence)","botnets detection;machine learning;computationally expensive problem;scientific problem;botnet detection;procedure complements","","","","","","15 Jul 2019","","","IEEE","IEEE Conferences"
"Adapting level of detail in user interfaces for Cybersecurity operations","C. Inibhunu; S. Langevin; S. Ralph; N. Kronefeld; H. Soh; G. A. Jamieson; S. Sanner; S. W. Kortschot; C. Carrasco; M. White","Uncharted Software Inc., Toronto, Ontario, Canada; Uncharted Software Inc., Toronto, Ontario, Canada; Uncharted Software Inc., Toronto, Ontario, Canada; Uncharted Software Inc., Toronto, Ontario, Canada; University of Toronto, Ontario, Canada; University of Toronto, Ontario, Canada; University of Toronto, Ontario, Canada; University of Toronto, Ontario, Canada; University of Toronto, Ontario, Canada; University of Toronto, Ontario, Canada","2016 Resilience Week (RWS)","22 Sep 2016","2016","","","13","16","As cybersecurity threats increasingly appear in news headlines, the security industry continues to build state of the art firewall and intrusion detection systems for monitoring activities in complex cyber networks. These systems generate millions of log files and continuous alerts. In order to make sense of cyber data, cyber security and system administrators review and analyze millions of logs using highly summarized views and manual cycles of click-intensive details-on-demand. This is laborious, induces cognitive overload, and is prone to errors resulting in important information and impacts not being seen when most needed. Our research focus is on developing “FocalPoint” a system that provides Adaptive Level of Detail (LOD) in user interfaces for cybersecurity operations. FocalPoint is a recommender system tailored for complex network information structures that reasons about contextual information associated with the network, user tasks, and cognitive load. This facilitates tuning cyber visualization displays thereby improving user performance in perception, comprehension and projection of current Cybersecurity Situational Awareness (Cyber SA). For cyber analysts, having the right information, in context, when most needed without cognitive overload could lead to effective decision making in cyber operations. We provide a use case scenario for FocalPoint with an in-progress prototype and highlight various challenges and potential considerations for building an effective adaptive system.","","978-1-5090-2002-7","10.1109/RWEEK.2016.7573300","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7573300","Adaptive User Interfaces;Cybersecurity;Network Situation Awareness;Human Computer Interaction;Context-aware Reasoning;Adaptive Visualization","Context;Adaptation models;Cognition;Adaptive systems;Computer security;Monitoring;Data visualization","data visualisation;firewalls;human computer interaction;security of data;user interfaces","cybersecurity operation;firewall;intrusion detection;complex cyber network;click-intensive details-on-demand;FocalPoint;adaptive level-of-detail;recommender system;user task;cognitive load;cyber visualization;cybersecurity situational awareness","","1","","38","","22 Sep 2016","","","IEEE","IEEE Conferences"
"GuidedTracker: Track the victims with access logs to finding malicious web pages","H. Sha; Q. Liu; Z. Zhou; C. Zheng","School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China 100876; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China 100093; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China 100093; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China 100093","2014 IEEE Global Communications Conference","12 Feb 2015","2014","","","564","569","Malicious web pages have become a malignant tumour for the Internet, which spread malicious code, steal people's private information, and deliver spamming advertisements. And how to distinguish them from the huge number of normal web pages effectively remains a huge challenge in the era of big data. To detect malicious pages, one needs to first collect candidate web pages that are live on the web; then filter massive legitimate pages using fast filters and finally examine the remaining pages using precisely but slow analyzer. However, there are new challenges recently for these conventional techniques, including large scale, imbalance data and the usage of cloaking techniques. To cope with these challenges, the malicious URL detection system should perform more efficiently. In this paper, we propose a system, named GuidedTracker, to search for suspicious malicious pages. GuidedTracker starts from the seed set which includes known malicious pages. Then, it automatically figures out those victims based on the seed set and the visit relation database. Finally, the access records of these victims are used to identify other malicious pages. In this way, GuidedTracker increase the percentages of malicious URLs in the input URL stream submitted to the precisely analyzer. To our best knowledge, GuidedTracker is the first to introduce visit relations to tackle the malicious URL detection problem. The introduction of visit relations limits the scope of URL inspection and enables this approach to have the ability of self-learning. Experimental results show that the overall ""toxicity"" can be improved by 6.97%-50.38% compared with full inspection of access logs.","1930-529X","978-1-4799-3512-3","10.1109/GLOCOM.2014.7036867","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7036867","","Web pages;Uniform resource locators;Inspection;Security;Detectors;Information systems","computer crime;invasive software;relational databases;unsupervised learning;Web sites","self-learning;input URL stream;malicious URL;visit relation database;suspicious malicious pages;malicious web pages;GuidedTracker system","","1","1","28","","12 Feb 2015","","","IEEE","IEEE Conferences"
"Big Data Analytics: Performance Evaluation for High Availability and Fault Tolerance using MapReduce Framework with HDFS","J. P. Verma; S. H. Mankad; S. Garg","Ccomputer Science and Engineering Department, Institute of Technology, Nirma University, Ahmedabad, India; Ccomputer Science and Engineering Department, Institute of Technology, Nirma University, Ahmedabad, India; Ccomputer Science and Engineering Department, Institute of Technology, Nirma University, Ahmedabad, India","2018 Fifth International Conference on Parallel, Distributed and Grid Computing (PDGC)","27 Jun 2019","2018","","","770","775","Big data analytics helps in analyzing structured data transaction and analytics programs that contain semi-structured and unstructured data. Internet clickstream data, mobile-phone call details, server logs are examples of big data. Relational database-oriented dataset doesn't fit in traditional data warehouse since big data set is updated frequently and large amount of data are generated in real time. Many open source solutions are available for handling this large scale data. The Hadoop Distributed File System (HDFS) is one of the solutions which helps in storing, managing, and analyzing big data. Hadoop has become a standard for distributed storage and computing in Big Data Analytic applications. It has the capability to manage distributed nodes for data storage and processing in distributed manner. Hadoop architecture is also known as Store everything now and decide how to process later. Challenges and issues of multi-node Hadoop cluster setup and configuration are discussed in this paper. The troubleshooting for high availability of nodes in different scenarios for Hadoop cluster failure are experimented with different sizes of datasets. Experimental analysis carried out in this paper helps to improve uses of Hadoop cluster effectively for research and analysis. It also provides suggestions for selecting size of Hadoop cluster as per data size and generation speed.","2573-3079","978-1-7281-0646-5","10.1109/PDGC.2018.8745770","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8745770","Big Data Analytics;Big Data;MapReduce;Hadoop Cluster;High Availability","Big Data;Task analysis;Distributed databases;Tools;File systems;Computer architecture;Metadata","Big Data;data analysis;data handling;data mining;data warehouses;distributed databases;Internet;parallel processing;pattern clustering;public domain software;relational databases","Hadoop cluster;data size;big data analytics;Hadoop distributed file system;big data analytic applications;relational database-oriented dataset;data storage;big data set;traditional data warehouse;internet clickstream data;unstructured data;analytics programs;structured data transaction;generation speed","","3","","15","","27 Jun 2019","","","IEEE","IEEE Conferences"
"SPBC: Leveraging the characteristics of MPI HPC applications for scalable checkpointing","T. Ropars; T. V. Martsinkevich; A. Guermouche; A. Schiper; F. Cappello","École Polytechnique Fédérale de Lausanne (EPFL), Switzerland; INRIA, University of Paris Sud, France; Université de Versailles, Saint-Quentin en Yveline, France; École Polytechnique Fédérale de Lausanne (EPFL), Switzerland; Argonne National Laboratory, USA","SC '13: Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis","14 Aug 2014","2013","","","1","12","The high failure rate expected for future supercomputers requires the design of new fault tolerant solutions. Most checkpointing protocols are designed to work with any message-passing application but sudder from scalability issues at extreme scale. We take a different approach: We identify a property common to many HPC applications, namely channel-determinism, and introduce a new partial order relation, called always-happens-before relation, between events of such applications. Leveraging these two concepts, we design a protocol that combines an unprecedented set of features. Our protocol called SPBC combines in a hierarchical way coordinated checkpointing and message logging. It is the first protocol that provides failure containment without logging any information reliably apart from process checkpoints, and this, without penalizing recovery performance. Experiments run with a representative set of HPC workloads demonstrate a good performance of our protocol during both, failure-free execution and recovery.","2167-4337","978-1-4503-2378-9","10.1145/2503210.2503271","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6877441","Algorithms;Reliability","Protocols;Checkpointing;Fault tolerance;Fault tolerant systems;Libraries;Payloads","application program interfaces;checkpointing;failure analysis;message passing;parallel processing;protocols;software fault tolerance;software reliability","SPBC protocol;MPI HPC;scalable checkpointing;high failure rate;supercomputers;fault tolerant solutions;checkpointing protocols;message-passing;channel-determinism;partial order relation;always-happens-before relation;hierarchical way coordinated checkpointing;message logging;failure containment;HPC workloads","","9","","31","","14 Aug 2014","","","IEEE","IEEE Conferences"
"Process Discovery Algorithms Using Numerical Abstract Domains","J. Carmona; J. Cortadella","Software Department, Universitat Politècnica de Catalunya, Barcelona, Spain; Software Department, Universitat Politècnica de Catalunya, Barcelona, Spain","IEEE Transactions on Knowledge and Data Engineering","4 Nov 2014","2014","26","12","3064","3076","The discovery of process models from event logs has emerged as one of the crucial problems for enabling the continuous support in the life-cycle of an information system. However, in a decade of process discovery research, the algorithms and tools that have appeared are known to have strong limitations in several dimensions. The size of the logs and the formal properties of the model discovered are the two main challenges nowadays. In this paper we propose the use of numerical abstract domains for tackling these two problems, for the particular case of the discovery of Petri nets. First, numerical abstract domains enable the discovery of general process models, requiring no knowledge (e.g., the bound of the Petri net to derive) for the discovery algorithm. Second, by using divide and conquer techniques we are able to control the size of the process discovery problems. The methods proposed in this paper have been implemented in a prototype tool and experiments are reported illustrating the significance of this fresh view of the process discovery problem.","1558-2191","","10.1109/TKDE.2013.156","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6606789","Software Engineering Process;Formal methods;Workflow management;Mining methods and algorithms;Business;Process discovery;numerical abstract domains;petri nets;formal methods;concurrency","Abstracts;Petri nets;Lattices;Artificial neural networks;Data models","information systems;Petri nets","process discovery algorithms;numerical abstract domains;process model discovery;information system;process discovery research;formal properties;Petri nets discovery;general process models","","13","","36","","23 Sep 2013","","","IEEE","IEEE Journals"
"An I/O Request Packet (IRP) Driven Effective Ransomware Detection Scheme using Artificial Neural Network","M. A. Ayub; A. Continella; A. Siraj","Tennessee Tech University,Department of Computer Science,Cookeville,USA; University of Twente,Faculty of Electrical Engineering, Mathematics and Computer Science,Enschede,NL; Tennessee Tech University,Department of Computer Science,Cookeville,USA","2020 IEEE 21st International Conference on Information Reuse and Integration for Data Science (IRI)","10 Sep 2020","2020","","","319","324","In recent times, there has been a global surge of ransomware attacks targeted at industries of various types and sizes from retail to critical infrastructure. Ransomware researchers are constantly coming across new kinds of ransomware samples every day and discovering novel ransomware families out in the wild. To mitigate this ever-growing menace, academia and industry-based security researchers have been utilizing unique ways to defend against this type of cyber-attacks. I/O Request Packet (IRP), a low-level file system I/O log, is a newly found research paradigm for defense against ransomware that is being explored frequently. As such in this study, to learn granular level, actionable insights of ransomware behavior, we analyze the IRP logs of 272 ransomware samples belonging to 18 different ransomware families captured during individual execution. We further our analysis by building an effective Artificial Neural Network (ANN) structure for successful ransomware detection by learning the underlying patterns of the IRP logs. We evaluate the ANN model with three different experimental settings to prove the effectiveness of our approach. The model demonstrates outstanding performance in terms of accuracy, precision score, recall score, and F1 score, i.e., in the range of 99.7%±0.2%.","","978-1-7281-1054-7","10.1109/IRI49571.2020.00053","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9191509","Artificial Neural Network;I/O Monitoring;Malware;Ransomware","Ransomware;Cryptography;Neural networks;Microsoft Windows;Kernel;Data collection","invasive software;learning (artificial intelligence);neural nets","driven effective ransomware detection scheme;global surge;ransomware attacks;retail to critical infrastructure;ransomware researchers;ransomware samples every day;discovering novel ransomware families;industry-based security researchers;cyber-attacks;low-level file system;newly found research paradigm;ransomware behavior;IRP logs;ransomware samples;ransomware families;ransomware detection;effective artificial neural network structure","","","","15","","10 Sep 2020","","","IEEE","IEEE Conferences"
"Shared dataset on natural human-computer interaction to support continuous authentication research","C. Murphy; J. Huang; D. Hou; S. Schuckers","Electrical and Computer Engineering Department, Clarkson University, Potsdam NY USA 13699; Electrical and Computer Engineering Department, Clarkson University, Potsdam NY USA 13699; Electrical and Computer Engineering Department, Clarkson University, Potsdam NY USA 13699; Electrical and Computer Engineering Department, Clarkson University, Potsdam NY USA 13699","2017 IEEE International Joint Conference on Biometrics (IJCB)","1 Feb 2018","2017","","","525","530","Conventional one-stop authentication of a computer terminal takes place at a user's initial sign-on. In contrast, continuous authentication protects against the case where an intruder takes over an authenticated terminal or simply has access to sign-on credentials. Behavioral biometrics has had some success in providing continuous authentication without requiring additional hardware. However, further advancement requires benchmarking existing algorithms against large, shared datasets. To this end, we provide a novel large dataset that captures not only keystrokes, but also mouse events and active programs. Our dataset is collected using passive logging software to monitor user interactions with the mouse, keyboard, and software programs. Data was collected from 103 users in a completely uncontrolled, natural setting, over a time span of 2.5 years. We apply Gunetti & Picardi's algorithm, a state-of-the-art algorithm in free text keystroke dynamics, as an initial benchmarkfor the new dataset.","2474-9699","978-1-5386-1124-1","10.1109/BTAS.2017.8272738","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8272738","","Mice;Microsoft Windows;Task analysis;Authentication;Data collection;Presses;Keyboards","authorisation;biometrics (access control);keyboards;message authentication;natural language processing;text analysis","completely uncontrolled setting;natural setting;shared dataset;continuous authentication research;computer terminal;authenticated terminal;behavioral biometrics;user interactions;natural human-computer interaction;conventional one-stop authentication;user initial sign-on;sign-on credentials;mouse events;active programs;active programs;passive logging software;user interaction monitoring;keyboard;software programs;time 2.5 year","","7","","16","","1 Feb 2018","","","IEEE","IEEE Conferences"
"NetCapVis: Web-based Progressive Visual Analytics for Network Packet Captures","A. Ulmer; D. Sessler; J. Kohlhammer","Technische Universität,Fraunhofer IGD,Darmstadt,Germany; Technische Universität,Fraunhofer IGD,Darmstadt,Germany; Technische Universität,Fraunhofer IGD,Darmstadt,Germany","2019 IEEE Symposium on Visualization for Cyber Security (VizSec)","7 Aug 2020","2019","","","1","10","Network traffic log data is a key data source for forensic analysis of cybersecurity incidents. Packet Captures (PCAPs) are the raw information directly gathered from the network device. As the bandwidth and connections to other hosts rise, this data becomes very large quickly. Malware analysts and administrators are using this data frequently for their analysis. However, the currently most used tool Wireshark is displaying the data as a table, making it difficult to get an overview and focus on the significant parts. Also, the process of loading large files into Wireshark takes time and has to be repeated each time the file is closed. We believe that this problem poses an optimal setting for a client-server infrastructure with a progressive visual analytics approach. The processing can be outsourced to the server while the client is progressively updated. In this paper we present NetCapVis, an web-based progressive visual analytics system where the user can upload PCAP files, set initial filters to reduce the data before uploading and then instantly interact with the data while the rest is progressively loaded into the visualizations.","2639-4332","978-1-7281-3876-3","10.1109/VizSec48167.2019.9161633","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9161633","Network Traffic;PCAP;Progressive Visual Analytics;Packet Capture;Web-Application;Human-centered computing—Visual analytics;Human-centered computing—Interaction design—User interface design","Visual analytics;Data visualization;Malware;Tools;Protocols;Usability","client-server systems;data analysis;data visualisation;digital forensics;Internet;invasive software;telecommunication traffic","NetCapVis;network packet captures;network traffic log data;forensic analysis;cybersecurity;network device;client-server infrastructure;PCAP files;Web-based progressive visual analytics system;visualizations;malware","","1","","34","","7 Aug 2020","","","IEEE","IEEE Conferences"
"Real-time wireless vibration monitoring system using LabVIEW","M. Y. Upadhye; P. B. Borole; A. K. Sharma","Electrical Engineering Department, Veermata Jijabai Technological Institute, Mumbai 400019, India; Electrical Engineering Department, Veermata Jijabai Technological Institute, Mumbai 400019, India; Control Instrumentation Division, Bhabha Atomic Research Centre, Mumbai 400094, India","2015 International Conference on Industrial Instrumentation and Control (ICIC)","9 Jul 2015","2015","","","925","928","Vibration analysis provides relevant information about abnormal working condition of machine parts. Vibration measurement is prerequisite for vibration analysis which is used for condition monitoring of machinery. Also, wireless vibration monitoring has many advantages over wired monitoring. This Paper presents, implementation of a reliable and low cost wireless vibration monitoring system. Vibration measurement has been done using 3-Axis digital output MEMS Accelerometer sensor. This sensor can sense vibrations in the range 0.0156g to 8g where, 1g is 9.81m/s2. Accelerometer Sensor is interfaced with Arduino-derived microcontroller board having Atmel's AT-mega328p microcontroller. The implemented system uses ZigBee communication protocol i.e. standard IEEE 802.15.4, for wireless communication between Sensor Unit and Vibration Monitoring Unit. The wireless communication has been done using XBee RF modules. National Instruments's LabVIEW software has been used for development of graphical user interface, data-logging and alarm indication on the PC. Experimental results show continuous real-time monitoring of machine's vibrations on charts. These results, along with data-log file have been used for vibration analysis. This analysis is used to ensure safe working condition of machinery and used in predictive maintenance.","","978-1-4799-7165-7","10.1109/IIC.2015.7150876","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7150876","Vibration Measurement;Predictive Maintenance;Condition Monitoring;MEMS Accelerometer;ZigBee;Arduino-derived Microcontroller Board;LabVIEW GUI","Vibrations;Monitoring;Wireless communication;Software;Accelerometers;Wireless sensor networks;Radio frequency","accelerometers;graphical user interfaces;maintenance engineering;microcontrollers;micromechanical devices;vibration measurement;virtual instrumentation;Zigbee","real-time wireless vibration monitoring system;LabVIEW;machine parts;vibration measurement;vibration analysis;3-axis digital output MEMS accelerometer sensor;Arduino-derived microcontroller board;Atmel AT-mega328p microcontroller;ZigBee communication protocol;IEEE 802.15.4;vibration monitoring unit;wireless communication;XBee RF modules;National Instruments;graphical user interface;data-logging;alarm indication;PC;predictive maintenance;safe working condition","","8","","14","","9 Jul 2015","","","IEEE","IEEE Conferences"
"MementoMap Framework for Flexible and Adaptive Web Archive Profiling","S. Alam; M. Weigle; M. Nelson; F. Melo; D. Bicho; D. Gomes",Old Dominion University; Old Dominion University; Old Dominion University; FCT: Arquivo.pt; FCT: Arquivo.pt; FCT: Arquivo.pt,"2019 ACM/IEEE Joint Conference on Digital Libraries (JCDL)","8 Aug 2019","2019","","","172","181","In this work we proposed MementoMap, a flexible and adaptive framework to summarize holdings of a web archive efficiently. We described a simple, yet extensible, file format suitable for MementoMap. We used the complete index of the Arquivo.pt comprising 5B mementos (archived web pages/files) to understand the nature and shape of its holdings. We generated MementoMaps with varying amount of detail from its HTML pages that have an HTTP status code of 200 OK. Additionally, we designed a single-pass, memory-efficient, and parallelization-friendly algorithm to compact a large MementoMap into a small one and an in-file binary search method for efficient lookup. We analyzed more than three years of MemGator (a Memento aggregator) logs to understand the response behavior of 14 public web archives. We evaluated MementoMaps by measuring their Accuracy using 3.3M unique URIs from MemGator logs. We found that a MementoMap of less than 1.5% Relative Cost (as compared to the comprehensive listing of all the unique original URIs) can correctly identify the presence or absence of 60% of the lookup URIs in the corresponding archive while maintaining 100% Recall (i.e., zero false negatives).","","978-1-7281-1547-4","10.1109/JCDL.2019.00033","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8791219","Memento;Web Archiving;Archive Profiling;MementoMap","Tools;Indexes;Computer science;Routing;Open source software;Time factors;Internet","hypermedia markup languages;information retrieval systems;Internet;parallel processing;search engines;storage management","in-file binary search method;mementomap framework;adaptive Web archive profiling;single-pass algorithm;flexible Web archive profiling;memory-efficiency algorithm;parallelization-friendly algorithm;MemGator logs;URI;HTML pages","","1","","40","","8 Aug 2019","","","IEEE","IEEE Conferences"
"An effective long string searching algorithm towards component security testing","J. Chen; Lili Zhu; Zhibin Xie; M. Omari; H. Ackah-Arthur; Saihua Cai; R. Huang","School of Computer Science and Telecommunication Engineering, Jiangsu University, Zhenjiang, China; School of Computer Science and Telecommunication Engineering, Jiangsu University, Zhenjiang, China; School of Electronic and Information, Jiangsu University of Science and Technology, Zhenjiang, China; School of Computer Science and Telecommunication Engineering, Jiangsu University, Zhenjiang, China; School of Computer Science and Telecommunication Engineering, Jiangsu University, Zhenjiang, China; School of Computer Science and Telecommunication Engineering, Jiangsu University, Zhenjiang, China; School of Computer Science and Telecommunication Engineering, Jiangsu University, Zhenjiang, China","China Communications","12 Dec 2016","2016","13","11","153","169","In the execution of method invocation sequences to test component security, abnormal or normal information is generated and recorded in a monitor log. By searching abnormal information from monitor log, the exceptions that the component has can be determined. To facilitate the searching process, string searching methods could be employed. However, current approaches are not effective enough to search long pattern string. In order to mine the specific information with less number of matches, we proposed an improved Sunday string searching algorithm in this paper. Unlike Sunday algorithm which does not make use of the already matched characters, the proposed approach presents two ideas - utilizing and recycling these characters. We take advantage of all matched characters in main string, if they are still in the matchable interval compared with pattern string, to increase the distance that pattern string moves backwards. Experimental analysis shows that, compared to Sunday algorithm, our method could greatly reduce the matching times, if the scale of character set constituting both main string and pattern string is small, or if the length of pattern string is long. Also, the proposed approach can improve the search effectiveness for abnormal information in component security testing.","1673-5447","","10.1109/CC.2016.7781726","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7781726","component testing;security detection;monitor log;abnormal information;string-searching","Pattern matching;Security;Monitoring;Algorithm design and analysis;Testing;Manganese;Software engineering","object-oriented programming;program testing;security of data;string matching","long string searching algorithm;component security testing;monitor log;abnormal information searching;normal information searching","","2","","","","12 Dec 2016","","","IEEE","IEEE Magazines"
"Osmotic Computing: Software Defined Membranes meet Private/Federated Blockchains","M. Villari; A. Galletta; A. Celesti; L. Carnevale; M. Fazio","MIFT Department, University of Messina, Messina, Italy; MIFT Department, University of Messina, Messina, Italy; MIFT Department, University of Messina, Messina, Italy; MIFT Department, University of Messina, Messina, Italy; MIFT Department, University of Messina, Messina, Italy","2018 IEEE Symposium on Computers and Communications (ISCC)","18 Nov 2018","2018","","","01292","01297","This paper presents an innovative solution to manage security and trustiness in Osmotic Computing. Osmotic Computing dynamically manages Cloud, Edge and IoT resources across federated environments set up by different and cooperating stakeholders. In this context, the Software Defined Membrane (SDMem) is the main component responsible to orchestrate the osmotic transfer of microelements (MELs) across different environments straightening the security needs of such a complex ecosystem. The basic idea presented in this paper is to leverage Private Blockchain technologies in SDMem implementation over federated systems. Data access activities will be logged in a private distributed Blockchain-based ledger. This will allow to have a certified, non-repudiable record of all the data accessed performed by distributed computing, thus assuring the overall ownership and integrity of data and processes running in MELs. The resulting SDMem solution allow us to isolate data and workflows in distributed environments where heterogeneous resources and devices are exploited.","1530-1346","978-1-5386-6950-1","10.1109/ISCC.2018.8538546","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8538546","Osmotic Computing;Blockchain;federation;microservices","Bitcoin;Peer-to-peer computing;Software;Distributed databases;Fabrics","authorisation;cloud computing;Internet of Things","federated systems;data access activities;distributed computing;distributed environments;Osmotic Computing;federated environments;private-federated blockchains;software defined membranes;SDMem solution;private distributed blockchain-based ledger;private blockchain technologies","","1","","33","","18 Nov 2018","","","IEEE","IEEE Conferences"
"Preserving Network Privacy on Fine-grain Path-tracking Using P4-based SDN","A. Indra Basuki; D. Rosiyadi; I. Setiawan","Indonesian Institute of Sciences,Research Center for Informatics,Bandung,Indonesia; Indonesian Institute of Sciences,Research Center for Informatics,Bandung,Indonesia; Indonesian Institute of Sciences,Research Center for Informatics,Bandung,Indonesia","2020 International Conference on Radar, Antenna, Microwave, Electronics, and Telecommunications (ICRAMET)","25 Dec 2020","2020","","","129","134","Path-tracking is essential to provide complete information regarding network breach incidents. It records the direction of the attack and its source of origin thus giving the network manager proper information for the next responses. Nevertheless, the existing path-tracking implementations expose the network topology and routing configurations. In this paper, we propose a privacy-aware path-tracking which mystifies network configurations using in-packet bloom filter. We apply our method by using P4 switch to supports a fine-grain (per-packet) path-tracking with dynamic adaptability via in-switch bloom filter computation. We use a hybrid scheme which consists of a destination-based logging and a path finger print-based marking to minimize the redundant path inferring caused by the bloom filter's false positive. For evaluation, we emulate the network using Mininet and BMv2 software switch. We deploy a source routing mechanism to run the evaluations using a limited testbed machine implementing Rocketfuel topology. By using the hybrid marking and logging technique, we can reduce the redundant path to zero percent, ensuring no-collision in the path-inferring. Based on the experiments, it has a lower space efficiency (56 bit) compared with the bloom filter-only solution (128 bit). Our proposed method guarantees that the recorded path remains secret unless the secret keys of every switch are known.","","978-1-7281-8922-2","10.1109/ICRAMET51080.2020.9298588","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9298588","Privacy-aware;Path-tracking;fine-grain;Bloom filter;P4","Switches;Routing;IP networks;Security;Protocols;Privacy;Network topology","data privacy;data structures;software defined networking;telecommunication network management;telecommunication network routing;telecommunication network topology;telecommunication security","secret keys;Mininet;path finger print-based marking;network breach incidents;P4-based SDN;network privacy preservation;path-inferring;source routing mechanism;BMv2 software switch;destination-based logging;in-switch bloom filter computation;in-packet bloom filter;network configurations;privacy-aware path-tracking;routing configurations;network topology;network manager;fine-grain path-tracking;word length 56.0 bit;word length 128.0 bit","","","","16","","25 Dec 2020","","","IEEE","IEEE Conferences"
"Hadoop multi node cluster resource analysis","K. Pandey; A. Gadwal; P. Lakkadwala","Acropolis Technical Campus, Indore (MP) Truba College of Engineering and Technology, Indore (MP); Acropolis Technical Campus, Indore (MP) Truba College of Engineering and Technology, Indore (MP); Acropolis Technical Campus, Indore (MP) Truba College of Engineering and Technology, Indore (MP)","2016 Symposium on Colossal Data Analysis and Networking (CDAN)","19 Sep 2016","2016","","","1","5","In the Computer System we have basically three types of Resources; they are Software, Hardware and Data. Data is the most important resource of computer system, because whatever computing we are doing is just because of data. Data Science deals with large amount of data to infer knowledge from the data sets to rationalize the information to achieve business values. Traditionally information was in structured form, means that was generally generated by business transactions. This information was easily process with our traditional data management system. In past few years the amount of digital data generated was grown exponentially. This large amount of data was unstructured, which cannot be processed and extracted efficiently from our traditional system. It includes Text files, sensor data, log data, web data, social networking data etc. The major reason behind the generation of this unstructured data is various applications used via internet e.g. Smart devices, web, mobile, social media and sensor Devices. For achieving the business goals this data is essential to mine. This large amount of unstructured data is called BIG DATA [1]. It's in large volume, its varying. There are various tools are available for processing of this large amount of data. Hadoop is one of the popular and efficient tool for the processing of Big Data. Hadoop provide a framework that allows us distributed computing and run tasks in parallel such that such type of complex data can be processed efficiently with respect to time, performance and resources. This paper covers the major resources used by Hadoop cluster.","","978-1-5090-0669-4","10.1109/CDAN.2016.7570925","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7570925","Hadoop;Big Data;Map Reduce;HDFS;Hadoop Multi Node Cluster","Distributed databases;Big data;Business;Data mining;Social network services;Servers;Google","Big Data;business data processing;data analysis;Internet;parallel processing","Hadoop multinode cluster resource analysis;computer system;data science;business values;business transactions;data management system;digital data;text files;sensor data;log data;Web data;social networking data;unstructured data;Internet;data mining;Big Data processing;distributed computing;parallel computing","","2","","21","","19 Sep 2016","","","IEEE","IEEE Conferences"
"A reliable framework for adaptive scientific workflow management systems based on SOA","A. Azimi; S. Parsa","Department of Computer Engineering, Islamic Azad University, Sari Branch, Sari, Iran; Department of Computer Engineering, Iran University of Science and Technology, Narmak, Iran","13th International Conference on Advanced Communication Technology (ICACT2011)","7 Apr 2011","2011","","","1358","1363","In this paper a reliable framework for scientific workflow management system is presented. The reliability of workflow management systems could be guaranteed by supplying workflow engines with fault control components. A fault control component may receive fault notifications through exceptions raised by web service handlers. Receiving a fault exception, the program execution state has to be rolled back to the state before the fault. To achieve this, the execution state of each component could be kept in a file, separately. Using the log kept in this file, the program status could be returned to the states before invoking the fault web service. The program execution is continued after the fault web service is substituted with a correct one. The applicability of the proposed framework is practically evaluated by applying it to design three different scientific workflow engines.","1738-9445","978-89-5519-155-4","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5746056","grid Service;QOS;reliable scientific workflow engine;SOA;web service","Web services;Workflow management software;Engines;Reliability;Quality of service;Conferences;Runtime","service-oriented architecture;software fault tolerance;Web services;workflow management software","scientific workflow management systems;service-oriented architecture;SOA;fault control component;Web service;fault exception;program execution","","","","41","","7 Apr 2011","","","IEEE","IEEE Conferences"
"Innovation pattern analysis","C. Diamantini; L. Genga; D. Potena; E. Storti","Dipartimento di Ingegneria dell'Informazione, Università Politecnica delle Marche, via Brecce Bianche, 60131 Ancona, Italy; Dipartimento di Ingegneria dell'Informazione, Università Politecnica delle Marche, via Brecce Bianche, 60131 Ancona, Italy; Dipartimento di Ingegneria dell'Informazione, Università Politecnica delle Marche, via Brecce Bianche, 60131 Ancona, Italy; Dipartimento di Ingegneria dell'Informazione, Università Politecnica delle Marche, via Brecce Bianche, 60131 Ancona, Italy","2013 International Conference on Collaboration Technologies and Systems (CTS)","25 Jul 2013","2013","","","628","629","The evolution of innovation management in last decades was strongly influenced and led by the theory of the “Open Innovation” introduced by Chesbrough [1], and has become one of the hottest topic in business Literature. In the current economical scenario an increasingly number of organizations decide to adopt a more open approach in their innovation policy, trying to establish more or less strong relations with external partners, directly involving them in innovative projects. Consequently the collaborative work is gaining a growing importance in innovation practices of organizations, since the success or failure of innovative projects is often strictly related to results of collaborative tasks. Therefore, to support innovation processes of an organization one can investigate and improve its collaboration practices, with the aim to discover the best ones, i.e. those that maximize the success probability of organizations innovative projects. However, this kind of analysis is often prevented by the lack of real world data, mainly due to the limited diffusion of innovation management systems capable to collect innovation activities traces. Nevertheless, the daily activities of an enterprise, both internal and external, are almost completely performed by software systems. Both explicitly and implicitly, these systems keep track of users activities, e.g. ERP logs, versioning systems, list of emails, file timestamps, and so forth. In the present work we propose a methodology aimed to discover relevant collaboration patterns based on real data daily collected by enterprises, with the aim of providing business users with a better understanding of the dynamics of the interactions among members of collaborating groups. Our idea is firstly to collect any kind of data produced during the collaborative development of an innovation project, then to integrate them into a unique knowledge base storing traces of enterprise activities. Through preprocessing analysis, such traces are translated into process schemas, that can be considered as a representation of collaborative innovation processes in the organization, on which we can perform pattern discovery. To this aim we consider hierarchical clustering, which is capable to extracts frequent subprocesses representing common collaboration patterns and to arrange them in a hierarchy with different level of abstractions. The rest of this work is organized in two sections, the former aimed to describe the main ideas of the methodology, the latter to sketch out future extensions we plan to conduct.","","978-1-4673-6404-1","10.1109/CTS.2013.6567301","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6567301","open innovation;innovation process;collaboration pattern discovery;hierarchical clustering","Collaboration;Technological innovation;Organizations;Electronic mail;Data mining;Innovation management","innovation management;organisational aspects;pattern clustering","innovation pattern analysis;open innovation;business literature;collaborative work;organizations innovative projects;innovation management;innovation activities traces;ERP logs;versioning systems;emails;file timestamps;enterprise activities;hierarchical clustering","","3","","3","","25 Jul 2013","","","IEEE","IEEE Conferences"
"Detection of Internet robots using a Bayesian approach","G. Suchacka; M. Sobków","Faculty of Mathematics, Physics and Computer Science, Opole University, Poland; Faculty of Mathematics, Physics and Computer Science, Opole University, Poland","2015 IEEE 2nd International Conference on Cybernetics (CYBCONF)","6 Aug 2015","2015","","","365","370","A large part of Web traffic on e-commerce sites is generated not by human users but by Internet robots: search engine crawlers, shopping bots, hacking bots, etc. In practice, not all robots, especially the malicious ones, disclose their identities to a Web server and thus there is a need to develop methods for their detection and identification. This paper proposes the application of a Bayesian approach to robot detection based on characteristics of user sessions. The method is applied to the Web traffic from a real e-commerce site. Results show that the classification model based on the cluster analysis with the Ward's method and the weighted Euclidean metric is very effective in robot detection, even obtaining accuracy of above 90%.","","978-1-4799-8322-3","10.1109/CYBConf.2015.7175961","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7175961","Bayesian approach;Bayesian statistics;cluster analysis;correlation analysis;data mining;Web mining;Web server;Web traffic;log file analysis;e-commerce;Internet robot;Web bot;Web robot detection;Matlab","Robots;Bayes methods;Internet;Testing;Euclidean distance;Correlation","Bayes methods;electronic commerce;Internet;invasive software;pattern classification;pattern clustering;telecommunication traffic;Web sites","Internet robots detection;Bayesian approach;Web traffic;e-commerce sites;search engine crawlers;shopping bots;hacking bots;malicious robots;Web server;Internet robots identification;user sessions characteristics;classification model;cluster analysis;Ward method;weighted Euclidean metric","","17","","28","","6 Aug 2015","","","IEEE","IEEE Conferences"
"Business Process Configuration in the Cloud: How to Support and Analyze Multi-tenant Processes?","W. M. P. van der Aalst","Dept. of Math. & Comput. Sci., Eindhoven Univ. of Technol., Eindhoven, Netherlands","2011 IEEE Ninth European Conference on Web Services","27 Oct 2011","2011","","","3","10","Lion's share of cloud research has been focusing on performance related problems. However, cloud computing will also change the way in which business processes are managed and supported, e.g., more and more organizations will be sharing common processes. In the classical setting, where product software is used, different organizations can make ad-hoc customizations to let the system fit their needs. This is undesirable, especially when multiple organizations share a cloud infrastructure. Configurable process models enable the sharing of common processes among different organizations in a controlled manner. This paper discusses challenges and opportunities related to business process configuration. Causal nets (C-nets) are proposed as a new formalism to deal with these challenges, e.g., merging variants into a configurable model is supported by a simple union operator. C-nets also provide a good representational bias for process mining, i.e., process discovery and conformance checking based on event logs. In the context of cloud computing, we focus on the application of C-nets to cross-organizational process mining.","","978-1-4577-1532-7","10.1109/ECOWS.2011.8","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6061094","configurable process models;cross-organizational process mining;cloud computing;causal nets","Unified modeling language;Organizations;Books;Semantics;Biological system modeling;Software","business data processing;cloud computing","business process configuration;multi-tenant process;cloud research;cloud computing;product software;ad hoc customization;cloud infrastructure;configurable process model;causal nets;C-nets;conformance checking;event logs;cross-organizational process mining","","25","1","17","","27 Oct 2011","","","IEEE","IEEE Conferences"
"TAAU: A tool to study the amino acid usage in different organisms","Z. Mungloo-Dilmohamud; S. Nundoo","Department of Computer Science and Engineering, Faculty of Engineering, University of Mauritius, Mauritius; Accenture, Mauritius","2016 8th Cairo International Biomedical Engineering Conference (CIBEC)","30 Jan 2017","2016","","","15","18","Worldwide there are scientists and researchers who deal with proteins in an attempt to find valuable information about the various species of organisms present on earth. With the expansion of available biological data, analysis is a major challenge facing researchers wishing to explore the massive amount of information. This paper presents the features of TAAU, a tool to study the amino acid usage in protein sequences. The analysis tools can be applied to either the protein sequences of different species or different proteins families. The tool allows the user to either download protein sequence information and files from online databases or to browse locally for a file or even to copy and paste a sequence for performing calculations like Grand Average of Hydropathy (GRAVY), the Codon Adaptation index (CAI) and frequency of amino acids amongst others. The results of the various analyses will be presented in the form of graphs and phylogenetic trees. TAAU has been implemented using Java, BioJava, PostgreSQL, JMoI, ClustalW, JFreeChart, JeUtils, and TreeView. The data have been organized in six (6) PostgreSQL tables to represent information pertaining to protein files, protein details, alignment of protein sequences and log files.","2156-6100","978-1-5090-2987-7","10.1109/CIBEC.2016.7836089","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7836089","amino-acid usage;codon usage;tool;GRAVY;CAI;Amino acid composition","Amino acids;Databases;Protein sequence;Organisms;Software;Java","bioinformatics;biological techniques;genetics;macromolecules;molecular biophysics;proteins","TAAU tool;amino acid usage;organisms;biological data;protein families;protein sequence information;online databases;grand-average-of-hydropathy;codon adaptation index;graphs;phylogenetic trees;BioJava;PostgreSQL;JMoI;ClustalW;JFreeChart;JeUtils;TreeView;protein files;protein details","","","","16","","30 Jan 2017","","","IEEE","IEEE Conferences"
"Exploration and Analysis of Undocumented Processes Using Heterogeneous and Unstructured Business Data","S. Pospiech; S. Mielke; R. Mertens; M. Pelke; K. Jagannath; M. Stadler","Cologne Intell. GmbH, Cologne, Germany; Hochschule Weserbergland, Hameln, Germany; Hochschule Weserbergland, Hameln, Germany; Hochschule Weserbergland, Hameln, Germany; Hochschule Weserbergland, Hameln, Germany; Hochschule Weserbergland, Hameln, Germany","2014 IEEE International Conference on Semantic Computing","25 Aug 2014","2014","","","191","198","The business world has become more dynamic than ever before. Global competition and today's rapid pace of development in many fields has led to shorter time-to-market intervals, as well as more complex products and services. These developments do often imply impromptu changes to existing business processes. These dynamics are aggravated when unforeseen paths have to be taken like it is often the case when problems are solved in customer support situations. This leads to undocumented business processes which pose a serious problem for management. In order to cope with this problem the discipline of Process Mining has emerged. In classical Process Mining, event logs generated for example by workflow management systems are used to create a process model. In order for classical Process Mining to work, the process therefore has to be implemented in such a system, it just lacks documentation. The above mentioned impromptu changes and impromptu processes do, however, lack any such documentation. In many cases event logs do not exist, at least not in the strict sense of the definition. Instead, traces left by a process might include unstructured data, such as emails or notes in a human readable format. In this paper we will demonstrate how it is possible to search and locate processes that exist in a company, but that are neither documented, nor implemented in any business process management system. The idea is to use all data stores in a company to find a trace of a process instance and to reconstruct and visualize it. The trace of this single instance is then generalized to a process template that covers all instances of that process. This generalization step generates a description that can manually be adapted in order to fit all process instances. While retrieving instances from structured data can be described by simple queries, retrieving process steps from unstructured data often requires more elaborate approaches. Hence, we have modified a search-engine to combine a simple word-search with ad-hoc ontologies that allow for defining synonym relations on a query-by-query basis.","","978-1-4799-4003-5","10.1109/ICSC.2014.24","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6882022","Process Mining;Process Discovery;heterogeneous business data;ontology;semantic layer;synonym search","Companies;Connectors;Customer relationship management;Electronic mail;Documentation;Turing machines","business data processing;data mining;query processing;storage management;workflow management software","ad-hoc ontology;simple word-search;search-engine;retrieving process steps;query;structured data;template processing;business process management system;documentation;impromptu changes;impromptu processes;classical process mining;workflow management systems;event logs;undocumented business processes;time-to-market intervals;heterogeneous business data;unstructured business data","","3","","18","","25 Aug 2014","","","IEEE","IEEE Conferences"
"A Case for Secure Virtual Append-Only Storage for Virtual Machines","Z. Lin; K. Gopalan; P. Yang","Dept. of Comput. Sci., State Univ. of New York at Binghamton, Binghamton, NY, USA; Dept. of Comput. Sci., State Univ. of New York at Binghamton, Binghamton, NY, USA; Dept. of Comput. Sci., State Univ. of New York at Binghamton, Binghamton, NY, USA","2010 39th International Conference on Parallel Processing Workshops","11 Oct 2010","2010","","","245","250","Traditional operating systems and applications use logs extensively to monitor system activity and perform intrusion detection. Consequently, logs have also become prime targets for intruders. When a malware or intruder obtains root privileges in a system, one of its first actions is to hide its footprint by deleting or modifying system logs, especially the log entry recording the intrusion activity (such as unauthorized root login). A key weakness of most current logging mechanisms is that logs are stored on a storage device over which the system being logged has complete control, including the ability to delete/modify the logs arbitrarily. Once the root privileges of such a system are compromised, so are the logs. Virtualization offers a unique opportunity to eliminate this point of weakness. In this paper, we propose a new virtual storage abstraction for virtual machines (VMs) called Virtual Append-only Storage (VAS) that secures and preserves all system and/or application logs in a VM and can prevent an intruder from deleting/modifying past logs even after the root privileges of a VM are compromised. Our VAS-based logging complements existing intrusion detection techniques which mainly monitor the in-memory execution state and data, but do not protect the storage device on which logs are stored. Since logs can become voluminous over time, VAS also provides administrators the ability to secure either system-wide or application-specific logs, rather than blindly logging all system activity.","2332-5690","978-1-4244-7918-4","10.1109/ICPPW.2010.15","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5599196","","Driver circuits;Kernel;Monitoring;Virtual machine monitors;Linux;Cloud computing","Internet;invasive software;storage management;virtual machines","secure virtual append-only storage;virtual machines;operating systems;system activity monitoring;malware;virtual storage abstraction;VAS-based logging complements;intrusion detection techniques;in-memory execution state","","","2","14","","11 Oct 2010","","","IEEE","IEEE Conferences"
"Design and implementation of an embedded audio-on-demand system","S. Huan","Department of Automatic, College of Electronics, Information Engineering, Inner Mongolia University, Hohhot, China","2012 International Conference on Systems and Informatics (ICSAI2012)","25 Jun 2012","2012","","","1172","1175","This paper introduces the design and implementation of C/S structure audio-on-demand system based on S3C2410 processor and embedded Linux operation system. It focuses on the design and implementation process of software. In this system, the communication between the client and server is achieved. Users log into the server through the client, read the song files on the list, select and download their favorite tracks to play on the client. The process management is used in the server, multiple users can select the same or different songs simultaneously. This system can be widely used in many locations such as KTV and public broadcasting.","","978-1-4673-0199-2","10.1109/ICSAI.2012.6223242","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6223242","Embedded system;process;communication;C/S;network;sever;audio-on-demand","Servers;Embedded systems;Linux;Educational institutions;Programming;Kernel;Sockets","audio systems;client-server systems;cloud computing;embedded systems;information retrieval;Linux;microprocessor chips;music","embedded audio-on-demand system implementation;embedded audio-on-demand system design;C/S structure audio-on-demand system;S3C2410 processor;embedded Linux operation system;software design;software implementation process;client-server system communication;song file reading;track downloading;track selection;process management;KTV;public broadcasting","","","","6","","25 Jun 2012","","","IEEE","IEEE Conferences"
"Discovering and analyzing patterns of usage to detect usability problems in web applications","A. Vargas; H. Weffers; H. V. da Rocha","Institute of Computing (IC), University of Campinas (UNICAMP), Campinas, Brazil; Laboratory for Quality Software (LaQuSo), Eindhoven University of Technology (TU/e), Eindhoven, The Netherlands; Institute of Computing (IC), University of Campinas (UNICAMP), Campinas, Brazil","2011 11th International Conference on Intelligent Systems Design and Applications","2 Jan 2012","2011","","","575","580","In this paper we describe two usability studies in which the interaction of users with an online application was remotely and automatically captured and analyzed. The usability studies were performed using the WebHint method for usability analysis of web applications. We evaluate two different versions of the application in order to observe the applicability of the method for successive usability analysis. The results show how the WebHint method can be used as an alternative approach to carry out successive evaluations of the usability of an application in order to analyze the evolution of different versions of its interface.","2164-7151","978-1-4577-1676-8","10.1109/ISDA.2011.6121717","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6121717","usability evaluation;remote usability evaluation;log file analysis;usage mining;pattern mining;sequence mining;user behavior analysis","Usability;Portfolios;Postal services;Data mining;Context;Electronic mail;Data analysis","Internet","Web applications;online application;WebHint method;successive usability analysis;usage patterns","","1","","13","","2 Jan 2012","","","IEEE","IEEE Conferences"
"Exploring students' experimentation strategies in engineering design using an educational CAD tool","Ying Ying Seah; C. Vieira; C. Dasgupta; A. J. Magana","Purdue Polytechnic Institute, Purdue University, West Lafayette, IN, United States; Purdue Polytechnic Institute, Purdue University, West Lafayette, IN, United States; Purdue Polytechnic Institute, Purdue University, West Lafayette, IN, United States; Purdue Polytechnic Institute, Purdue University, West Lafayette, IN, United States","2016 IEEE Frontiers in Education Conference (FIE)","1 Dec 2016","2016","","","1","5","Engineering design is an iterative process that supports the solution of problems by applying scientific knowledge to make informed decisions. Assessing different levels of expertise in experimentation is a difficult task since these are not usually visible as part of a student's final design solution. The purpose of this research is to investigate and characterize students' experimentation strategies while working on a design challenge. We conducted a concurrent think-aloud to capture students' thinking while they were working on a design challenge using an educational computer-aided design (CAD) software. We showed how the design replays generated from the log files collected from the CAD software can be used to represent students' experimentation strategies and how these representations can be validated by the data collected from the think-aloud. Our preliminary results show that technology-based assessment by the educational CAD tool allows us to identify the differences between different experimentation strategies and that the result of this assessment is supported by the result obtained from the concurrent think-aloud. Implications of this work would be relevant to engineering educators and researchers who are interested in understanding and assessing students' experimentation strategies in engineering design.","","978-1-5090-1790-4","10.1109/FIE.2016.7757470","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7757470","engineering design;experimentation;think-aloud;assessment","Buildings;Design automation;Software;Energy efficiency;Systematics;Prototypes;Solar panels","CAD;educational administrative data processing;educational courses;engineering education;student experiments","concurrent think-aloud;technology-based assessment;data collection;CAD software;educational computer-aided design software;informed decision making;scientific knowledge application;iterative process;educational CAD tool;engineering design;student experimentation strategies","","1","","16","","1 Dec 2016","","","IEEE","IEEE Conferences"
"Big data analysis architecture for multi IDS sensors using memory based processor","F. A. Saputra; M. Salman; K. Ramli; A. Abdillah; I. Syarif","Department of Electrical Engineering, Universitas Indonesia, Jakarta, Indonesia; Department of Electrical Engineering, Universitas Indonesia, Jakarta, Indonesia; Department of Electrical Engineering, Universitas Indonesia, Jakarta, Indonesia; Department of Informatics and Computer Engineering, Politeknik Elektronika Negeri Surabaya, Surabaya, Indonesia; Department of Informatics and Computer Engineering, Politeknik Elektronika Negeri Surabaya, Surabaya, Indonesia","2017 International Electronics Symposium on Knowledge Creation and Intelligent Computing (IES-KCIC)","21 Dec 2017","2017","","","40","45","The massive internet usage is followed by the rise of cyber-related crime such as information stealing, denial-of-service (DoS) attack, trojan and malware. To cope with the threats, one of most popular choice is using Intrusion Detection System (IDS). The logs produced by IDS in a day is huge and the limitation of computing power is the main problem to process that logs files. In this paper, we propose a big data analysis architecture of multi IDS sensors using in-memory data processing. Deployed IDS sensors are taking an extra role as computation slave to build scalable data analysis platform for network security analysis. So, adding more sensors means expanding computational resources. Adding to three sensors are helping data computation of clustering algorithm faster up to 27% comparing to the computation by using only one sensor. This research also introduces the use of memory-based processor, this system provides 7,9 times faster data processing than conservative MapReduce operation. And moreover, we also have performed botnets classification over Spark RDD that give high accuracy result to 99%.","","978-1-5386-0716-9","10.1109/KCIC.2017.8228456","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8228456","intrusion detection system;big data architecture;network data analysis;Memory-based Processor","","Big Data;computer crime;computer network security;data analysis;data mining;Internet;invasive software;parallel processing;pattern clustering","big data analysis architecture;multiIDS sensors;memory based processor;cyber-related crime;trojan;malware;Intrusion Detection System;logs files;in-memory data processing;deployed IDS sensors;computation slave;scalable data analysis platform;network security analysis;computational resources;data computation;massive Internet usage;clustering algorithm;conservative MapReduce operation;botnets classification;Spark RDD","","1","","16","","21 Dec 2017","","","IEEE","IEEE Conferences"
"Design and Implementation of Windows Service Client Based on HTTP","S. Qi; J. Hongbo; W. Li","NA; Coll. of Software, Beijing Univ. of Technol., Beijing, China; NA","2010 International Conference on E-Business and E-Government","30 Sep 2010","2010","","","3868","3871","In some cases, we need to perform certain functions after computer is started, but not logged. This is the role of the service program. This article discusses the establishment of an installation, operation, start and stop function of the windows service program process and associated communication protocol implementations. Before logged in the case of the operating system, the function of the service program is to achieve communications and data exchange with the remote server. Communication protocol consists request mainly based on http protocol and text file to receive and resolve in two parts based on XML. In this paper, it is beginning with architecture design presentation, introduced the concept of service program and process design, based on http protocol session design and XML parsing, and made some analysis of the whole structure.","","978-1-4244-6647-4","10.1109/ICEE.2010.970","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5591822","Windows service;HTTP protocol;XML protocol","XML;Protocols;HTML;Presses;Software;Educational institutions;Computers","client-server systems;electronic data interchange;transport protocols;user interfaces;utility programs;XML","Windows service client;Windows service program;communication protocol;remote server;data exchange;XML;HTTP protocol session design","","","","","","30 Sep 2010","","","IEEE","IEEE Conferences"
"Cloud Adapted Workflow e-Assessment System: Cloud-AWAS","F. Hajjej; Y. B. Hlaoui; L. Jemni Ben Ayed","Lab. LaTICE, Univ. of Tunis, Tunis, Tunisia; Lab. LaTICE, Univ. of Tunis, Tunis, Tunisia; Lab. LaTICE, Univ. of Tunis, Tunis, Tunisia","2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC)","11 Sep 2017","2017","1","","438","447","In this paper, we present an approach to adapt the e-assessment workflow by considering learner's profiles. We have started by creating a learner profile ontology based on extraction data from e-assessment activities, file log and personal information. Then, we have defined three adaptation actions: Add Activity, Edit Activity and Delete Activity, applied on the workflow assessment and using information extracted from learner profile ontology instances. Each action is applied according to conditions. After that, we present some results of the empirical evaluation of our system.","0730-3157","978-1-5386-0367-3","10.1109/COMPSAC.2017.86","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8029640","e-assessment;wokflow;adaptation;cloud computing","Ontologies;Data mining;Cloud computing;Adaptive systems;Tools;Electronic learning;Unified modeling language","cloud computing;ontologies (artificial intelligence)","workflow e-assessment system;cloud-AWAS;extraction data;e-assessment activities;file log;personal information;adaptation actions;add activity;edit activity;delete activity;learner profile ontology instances","","","","32","","11 Sep 2017","","","IEEE","IEEE Conferences"
"Achieving cloud security using third party auditor, MD5 and identity-based encryption","B. P. Gajendra; V. K. Singh; M. Sujeet","Department of Computer Science and Engineering, National Institute of Technology Jalandhar, Jalandhar, Punjab 144011, India; Indian Institute of Technology, Delhi, India; Jawaharlal Nehru Engineering College, Aurangabad, Maharashtra, India","2016 International Conference on Computing, Communication and Automation (ICCCA)","16 Jan 2017","2016","","","1304","1309","In recent years, Cloud computing has enjoyed a tremendous rise in popularity. It provides facilities for users to use cloud applications or platform without the need of any software installation and access the data or application from anywhere, with the help of internet. The cloud computing is divided into three fragments: Application, Service and Platform. Each of fragment provides different cloud services for business and individuals. Many organizations use this cloud service without any software, they run all their applications on the cloud and use on demand services. In Cloud, consumers, store their personal files or data on cloud server and consumers use that data or files whenever needed. Many consumers store or place their personal data on the cloud, so security and privacy are very important issue in cloud. These two issues can lead to a number of security concerns related to data transmission, integrity control, access control, identity management, logging and auditing, etc. Yet, research in the area of cloud computing receiving great attention from industry, academia and government. Since these domains have numerous complex issues, there are multiple open problems for research and opportunities for making noteworthy contributions, one of these issues is the data transmission in cloud computing. This proposal is concerned to overcome the security trade-off and improve the performance of data transmission and increase the security through Third Party Auditor and Identity Based Encryption.","","978-1-5090-1666-2","10.1109/CCAA.2016.7813920","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7813920","Cloud computing;Third Party Auditor;Identity Based Encryption;RSA;MD5","Cloud computing;Servers;Encryption;Algorithm design and analysis","auditing;authorisation;cloud computing;cryptography;data integrity;data privacy","cloud security;third party auditor;MD5;identity-based encryption;cloud computing;cloud applications;cloud platform;software installation;data access;Internet;cloud service;on-demand service;personal file storage;personal data storage;cloud server;privacy;data transmission;integrity control;access control;identity management;logging;auditing;identity based encryption","","2","","11","","16 Jan 2017","","","IEEE","IEEE Conferences"
"AutoSOC: A low budget flexible security operations platform for enterprises and organizations","G. W. P. Chamiekara; M. I. M. Cooray; L. S. A. M. Wickramasinghe; Y. M. S. Koshila; K. Y. Abeywardhana; A. N. Senarathna","Department of Information Systems Engineering, Faculty of Computing, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Department of Information Systems Engineering, Faculty of Computing, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Department of Information Systems Engineering, Faculty of Computing, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Department of Information Systems Engineering, Faculty of Computing, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Department of Information Systems Engineering, Faculty of Computing, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Department of Information Systems Engineering, Faculty of Computing, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka","2017 National Information Technology Conference (NITC)","8 Feb 2018","2017","","","100","105","Most of today's existing Security Operations Center (SOC) platforms follow a hybrid methodology in Security operations execution. However, these systems consist of a number of drawbacks. As there is a human component, there is a possibility of identification of false positives as true threat alerts. This will impact inversely towards the overall system. Currently there exists some automated SOCs as well, however their cost is considerably high for most small and medium scale companies. That is why we propose AutoSOC, a fully automated security operations center platform except for the Forensic investigation system, which requires a ticket to be generated with the approval of the user. This low budget enterprise solution comprises of an Intelligent Intrusion Detection and Prevention System (IIDPS), a Security Incident and Event Management System (SIEM), a Malware Analysis System and a Simple Forensic Investigation System. The Intelligent IIDPS contains an Intelligent Intrusion Detection System (IIDS) and an Intelligent Intrusion Prevention System (IIPS). IIDS is an alert system, which comprises components that notify and communicate in between integrated components when an attack or a breach occurs. The IIPS will understand the behavior of applications, and protocols are supposed to be according to their published standards. The SIEM is responsible for analyzing security event data, and it collects logs, stores, analyzes and reports on log data for incident response, forensics and regulatory compliance. The malware analysis process runs parallel to a forensic toolkit in order to accurately predict possible root causes for a certain incident. The forensic toolkit targets on the key components of analysis including processes running, packets captured etc. Therefore, the suggested solution will be able to reduce the cost of security implementations, increase the efficiency and accuracy of analysis results by eliminating false positives or the reporting of incorrect vulnerabilities by learning about the SOC network and environment.","","978-1-5386-2425-8","10.1109/NITC.2017.8285644","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8285644","SOC;IDS;IPS;SIEM;Forensics","Forensics;Malware;IP networks;Monitoring;Intrusion detection;Organizations","invasive software;organisational aspects;small-to-medium enterprises","AutoSOC;low budget flexible security operations platform;enterprises;organizations;hybrid methodology;Security operations execution;false positives;threat alerts;automated SOCs;small scale companies;medium scale companies;fully automated security operations center platform;low budget enterprise solution;Security Incident;SIEM;Malware Analysis System;Simple Forensic Investigation System;Intelligent IIDPS;Intelligent Intrusion Detection System;IIDS;Intelligent Intrusion Prevention System;IIPS;alert system;integrated components;security event data;incident response;forensics;malware analysis process;forensic toolkit targets;security implementations;Security Operations Center;Security Incident and Event Management System","","4","","23","","8 Feb 2018","","","IEEE","IEEE Conferences"
"Uscope: A scalable unified tracer from kernel to user space","J. Rhee; H. Zhang; N. Arora; G. Jiang; K. Yoshihira","NEC Laboratories America, USA; NEC Laboratories America, USA; NEC Laboratories America, USA; NEC Laboratories America, USA; NEC Laboratories America, USA","2014 IEEE Network Operations and Management Symposium (NOMS)","19 Jun 2014","2014","","","1","8","Unified tracing is the process of collecting trace logs across the boundary of kernel and user spaces, and has been used to understand the in-depth correspondence between low level events and application program context for diagnosing system failures and performance problems. Crossing the boundary from the kernel space to a user space to collect trace events from dual spaces imposes challenges compared to crossing the boundary in the other way from a user space to the kernel space due to multiple scheduled programs and diverse code layouts in the user space regarding the tracing target. In this paper, we propose a novel unified tracing system called Uscope to systematically trace kernel and unprecedented user code with low overhead. The key idea is to use an efficient variant of stack walking. Uscope lowers stack walking overhead by adjusting the scope of walking in two ways: (1) a highly configurable focus within the call stack, and (2) a per-application tracing that systematically tracks a dynamic set of new, exiting, or transforming processes and threads of an application software. This system is realized by using a flexible stack walking algorithm and a runtime kernel structure, Trace Map. These key features lead to low run-time overhead under 6% relative to native execution on a set of widely used benchmarks.","2374-9709","978-1-4799-0913-1","10.1109/NOMS.2014.6838328","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6838328","black-box unified event tracing;performance diagnostics;system troubleshooting;data centers","Kernel;Legged locomotion;Libraries;Monitoring;Runtime;Probes","data structures;program diagnostics","Uscope;scalable unified tracer;kernel space;user space;trace logs collection;application program context;call stack;per-application tracing;application software;flexible stack walking algorithm;runtime kernel structure;Trace Map","","1","","23","","19 Jun 2014","","","IEEE","IEEE Conferences"
"Store-and-Feedforward Adaptive Gaming System for Hand-Finger Motion Tracking in Telerehabilitation","D. Lockery; J. F. Peters; S. Ramanna; B. L. Shay; T. Szturm","Computational Intelligence Laboratory, Department of ECE, University of Manitoba, Winnipeg, Canada; Computational Intelligence Laboratory, Department of ECE, University of Manitoba, Winnipeg, Canada; Department of Applied Computer Science, University of Winnipeg, Winnipeg, Canada; School of Medical Rehabilitation , University of Manitoba, Winnipeg, Canada; School of Medical Rehabilitation , University of Manitoba, Winnipeg, Canada","IEEE Transactions on Information Technology in Biomedicine","2 May 2011","2011","15","3","467","473","This paper presents a telerehabilitation system that encompasses a webcam and store-and-feedforward adaptive gaming system for tracking finger-hand movement of patients during local and remote therapy sessions. Gaming-event signals and webcam images are recorded as part of a gaming session and then forwarded to an online healthcare content management system (CMS) that separates incoming information into individual patient records. The CMS makes it possible for clinicians to log in remotely and review gathered data using online reports that are provided to help with signal and image analysis using various numerical measures and plotting functions. Signals from a 6 degree-of-freedom magnetic motion tracking system provide a basis for video-game sprite control. The MMT provides a path for motion signals between common objects manipulated by a patient and a computer game. During a therapy session, a webcam that captures images of the hand together with a number of performance metrics provides insight into the quality, efficiency, and skill of a patient.","1558-0032","","10.1109/TITB.2011.2125976","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5741727","Computational intelligence;content management system;motion tracking;rheumatoid arthritis;telerehabilitation;therapeutic gaming","Games;Tracking;Hardware;Software;Content management;Databases;Instruments","computer games;content management;image motion analysis;medical image processing;motion measurement;patient rehabilitation;patient treatment;telemedicine","store-and-feedforward adaptive gaming system;hand-finger motion tracking;telerehabilitation;remote therapy session;gaming-event signals;webcam images;online healthcare content management system;6 degree-of-freedom magnetic motion tracking;video-game sprite control","Adult;Arthritis, Rheumatoid;Fingers;Hand;Humans;Internet;Middle Aged;Monitoring, Ambulatory;Range of Motion, Articular;Rehabilitation;Signal Processing, Computer-Assisted;Telemedicine;Video Games","17","","18","","5 Apr 2011","","","IEEE","IEEE Journals"
"The introduction of elog technology to the fishing industry, a case study: Ground fish sector project","A. Barkai; H. Henninger","OLRAC, Silvermine House, Steenberg Office Park, Tokai 7945, Cape Town, South Africa, OLFISH-AOLA, 54 Chatham Drive, Bedford, New Hampshire 03110, U.S.A.; OLRAC, Silvermine House, Steenberg Office Park, Tokai 7945, Cape Town, South Africa, OLFISH-AOLA, 54 Chatham Drive, Bedford, New Hampshire 03110, U.S.A.","OCEANS 2010 MTS/IEEE SEATTLE","10 Dec 2010","2010","","","1","11","Fisheries management is continually frustrated by the lack, or poor quality, of critical data on fishing operations. While quantitative methods for managing fisheries have developed considerably, the quality of available data prevents advances in fisheries management. With the advent of satellite communication systems and broadband on larger vessels, the transmission of reports from fishing vessels has proved manageable and it is clear that more modern means should be utilized for the collection and reporting of data from active fishing operations. In response, OLRAC, a South African company, developed a fisheries data-logging software, Olfish, capable of collecting, analyzing, plotting, mapping, reporting, tracing and transmitting all data related to fishing operations. Data can be compressed, encrypted and digitally authenticated prior to transmission to the company or management authorities. Olfish can be used by skippers, fleet/company managers, scientists, observers, and compliance inspectors and fisheries management authorities. It includes a dynamic report generator, eliminating the need for paper-logbooks. Olfish includes an onboard version (Olfish Dynamic Data Logger: Olfish-DDL), a shore component, and a web-based data management hub (Olfish Reports Management System: Olfish-RMS). Olfish-DDL captures data in real-time and/or in post-event mode. It can read GPS input and incorporates GIS capabilities for viewing of vessel movements and other operational data. Users can collect any type of data in any form, (images, video-clips, numerical and alphanumeric fields, free-text, date, time, location, etc). Olfish is currently installed on hundreds of vessels around the world, including Europe, Australia, Africa and the USA. This paper will focus on one such US case-study, in New England. Olrac and its North-American partner, Olfish-AOLA, were contracted by the Sustainable Harvest Sector (SHS) to customize the Olfish-DDL to meet the requirements of New England Groundfish sector vessels. The Groundfish fleet in New England is managed under a days-at-sea management regime. Previously, they were required to report to the NMFS (National Marine Fisheries Service) Northeast Regional Office using the paper log sheet (VTR form). As of May 1, 2010 the groundfish fleet has been managed under a system whereby groundfish landings are managed by group allocations (sectors). Vessels are designated to sectors and their landings are then attributed to that sector. The sector will hold a quota for each species of groundfish which it can divide up amongst the sector vessels as it sees fit. Sector managers will need to see effort reports on a weekly basis in order to attribute landings to stock areas and fulfil their sector weekly reporting requirements. The Olfish-DDL SHS project has been initiated to provide the required reports at required intervals. The project is currently operating effectively with two sectors (Sustainable Harvest Sector and Tri State Sector), with plans to expand to other sectors and to Olfish-RMS. The Olfish software is flexible enough to adjust to expected rule-changes each year under sector management.","0197-7385","978-1-4244-4332-1","10.1109/OCEANS.2010.5663993","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5663993","","Aquaculture;Databases;Companies;Real time systems;Global Positioning System;Computers","aquaculture;authorisation;cryptography;data communication;data compression;fishing industry;geographic information systems;Global Positioning System;production engineering computing;satellite communication","elog technology;fishing industry;ground fish sector project;fisheries management;fishing operations;satellite communication systems;broadband;fishing vessels;OLRAC;South African company;fisheries data-logging software;data compression;data encryption;authentication;data transmission;web-based data management hub;Olfish-DDL;GPS;GIS;sustainable harvest sector;New England;days-at-sea management regime;NMFS;national marine fisheries service;Northeast regional office;paper log sheet","","1","","7","","10 Dec 2010","","","IEEE","IEEE Conferences"
"Experiments with personal ownership of quality at the University of Texas at El Paso","O. Mondragon; J. T. Mallikarjan; E. Smith","Industrial Manufacturing and Systems Engineering, University of Texas at El Paso, USA; Industrial Manufacturing and Systems Engineering, University of Texas at El Paso, USA; Industrial Manufacturing and Systems Engineering, University of Texas at El Paso, USA","2015 IEEE Frontiers in Education Conference (FIE)","7 Dec 2015","2015","","","1","5","The lack of commitment to create quality work is a long-standing problem in education, and it is a direct negative driver of student performance, disturbing students' ability to: apply imparted concepts, build team quality work, and foster industry's economy. The quality of delivered work is poor mainly because students do not spend needed time and effort to review their own work; e.g., research papers, group projects, and assignments. At the University of Texas at El Paso (UTEP), experimental developments targeted students' commitment to create quality work, creating infrastructure to conduct personal reviews for different types of work products, and teaching students to conduct effective personal reviews of their own work. A partial implementation of check lists, review processes, and teaching material has been used in courses at UTEP. The work to be developed includes measuring time distributions of effort (time to build the product vs. time to correct the product), recording the defects injected in products, creating checklists based on the recorded defects, creating a process to review products, and defining a defect log. The goal is to create a habit within students to create quality work by using personal reviews to improve the quality of submitted work.","","978-1-4799-8454-1","10.1109/FIE.2015.7344079","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7344079","Quality;Personal Review;CMMI;Personal Software Process (PSP)","Software;Testing;Industries;Data analysis;Complexity theory;Education;Standards","computer aided instruction;economics;educational courses;educational institutions;educational technology;quality management;student experiments","personal ownership;University of Texas at El Paso;quality work;education;student performance;economy;UTEP;teaching","","2","","8","","7 Dec 2015","","","IEEE","IEEE Conferences"
"Measurement, Modeling, and Analysis of a Large-scale Blog Sever Workload","M. Jeon; J. Hwang; Y. Kim; J. Jang; J. Lee; E. Seo","NA; NA; Oak Ridge Nat. Lab., Oak Ridge, TN, USA; NA; NA; NA","2010 IEEE Second International Conference on Social Computing","30 Sep 2010","2010","","","558","563","Despite the growing popularity of Online Social Networks (OSNs), the workload characteristics of OSN servers, such as those hosting blog services, are not well understood. Understanding workload characteristics is important for optimizing and improving the performance of current systems and software based on observed trends. Thus, in this paper, we characterize the system workload of the largest blog hosting servers in South Korea, Tistory. In addition to understanding the system workload of the blog hosting server, we have developed synthesized workloads and obtained the following major findings: (i) the transfer size of non-multimedia files and blog articles can be modeled by a truncated Pareto distribution and a log-normal distribution respectively, and (ii) users' accesses to blog articles do not show temporal locality, but they are strongly biased toward those posted along with images or audio.","","978-1-4244-8439-3","10.1109/SocialCom.2010.88","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5591340","","Information services;Web sites;Bandwidth;Histograms;Web server","normal distribution;social networking (online);software metrics","large-scale blog sever workload;online social networks;blog services;system workload;Pareto distribution;nonmultimedia files;blog articles;log-normal distribution;blog hosting servers;South Korea;Tistory","","","","18","","30 Sep 2010","","","IEEE","IEEE Conferences"
"Bootstrapping a Blockchain Based Ecosystem for Big Data Exchange","J. Chen; Y. Xue","Sch. of Inf., Renmin Univ. of China, Beijing, China; Inst. of Software, Beijing, China","2017 IEEE International Congress on Big Data (BigData Congress)","11 Sep 2017","2017","","","460","463","In recent years, data is becoming the most valuable asset. There are more and more data exchange markets on Internet. These markets help data owners publish their datasets and data consumers find appropriate services. However, different from traditional goods like clothes and food, data is a special commodity. For current data exchange markets, it is very hard to protect copyright and privacy. Moreover, maintaining data services requires special IT techniques, which is a difficult job for many organizations who own big datasets, such as hospitals, government departments, planetariums and banks. In this paper, we propose a decentralized solution for big data exchange. This solution aims at cultivating an ecosystem, inside which all participators can cooperate to exchange data in a peer-to-peer way. The core part of this solution is to utilize blockchain technology to record transaction logs and other important documents. Unlike existing data exchange markets, our solution does not need any third-parties. It also provides an convenient way for data owners to audit the use of data, in order to protect data copyright and privacy. We will explain the ecosystem, and discuss the technical challenges and corresponding solutions.","","978-1-5386-1996-4","10.1109/BigDataCongress.2017.67","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8029360","data exchange;blockchain;ecosystem","Ecosystems;Peer-to-peer computing;Contracts;Protocols;Data privacy;Companies;Bitcoin","Big Data;data protection;electronic data interchange","blockchain based ecosystem bootstrapping;Big Data exchange;peer-to-peer system;transaction logs;data auditing;data copyright protection;data privacy protection","","9","","8","","11 Sep 2017","","","IEEE","IEEE Conferences"
"Wind resource assessment and numerical simulation for wind turbine airfoils","D. W. Wekesa; C. Wang; Y. Wei; J. N. Kamau","Institute of Dynamics and Control of Spacecrafts, School of Astronautics, Harbin Institute of Technology, Heilongjiang, 150001, China; Institute of Dynamics and Control of Spacecrafts, School of Astronautics, Harbin Institute of Technology, Heilongjiang, 150001, China; Institute of Dynamics and Control of Spacecrafts, School of Astronautics, Harbin Institute of Technology, Heilongjiang, 150001, China; Department of Physics, Jomo Kenyatta University of Agriculture & Technology, Nairobi City 00200, Kenya","15th International Workshop on Research and Education in Mechatronics (REM)","13 Oct 2014","2014","","","1","9","This paper investigates a feasibility measurement study for wind resource and a numerical simulation for wind regimes of Marsabit and Garissa stations, both rural-urban towns in Eastern region of Kenya. The experimental wind speed measurement was done using optical anemometer sensors while the wind direction was detected by wind vane sensors. The data logging instrumentation with calibrated wind sensors recorded higher resolution wind data. Based on the wind regimes at the sites, an analysis of aerodynamic performance and flow physics of two-dimensional numerical simulations on a NACA0015 airfoil was done using CFD FLUENT software. The Meteorological wind speed measurement correlated with the calibrated cup anemometer sensors measurement with a correlation coefficient of 0.98. The strong correlation confirms that the experimental anemometer gave the correct readings and practical readings can be approximated to the data from the meteorological station. The wind regimes showed attached flow acceleration over the airfoil surface for various angles of attack. The results provide a reference to the research and development of vertical axis wind turbine for target markets.","","978-1-4799-3029-6","10.1109/REM.2014.6920224","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6920224","Wind sensors;Data logging;correlation;CFD;Airfoil","Wind speed;Automotive components;Atmospheric modeling;Wind turbines;Mathematical model;Fluid flow measurement","aerodynamics;anemometers;optical sensors;velocity measurement;wind turbines","wind resource assessment;wind turbine airfoils;wind regimes;Marsabit station;Garissa station;rural-urban towns;Eastern region;Kenya;optical anemometer sensors;wind direction;wind vane sensors;data logging instrumentation;wind sensors;higher resolution wind data;aerodynamic performance;flow physics;two-dimensional numerical simulations;NACA0015 airfoil;CFD FLUENT software;meteorological wind speed measurement;calibrated cup anemometer sensors measurement;airfoil surface;vertical axis wind turbine","","2","","20","","13 Oct 2014","","","IEEE","IEEE Conferences"
"Alchemy: Stochastic Feature Regeneration for Malicious Network Traffic Classification","B. Hu; A. Kumagai; K. Kamiya; K. Takahashi; D. Dalek; O. Soderstrom; K. Okada; Y. Sekiya; A. Nakao","NTT, Japan; The University of Tokyo; NTT; NTT; NTT Security, US; NTT Security, Sweden; NTT Security, Sweden; The University of Tokyo; The University of Tokyo; The University of Tokyo","2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC)","9 Jul 2019","2019","1","","346","351","As signature-based techniques have ever more difficulty detecting increasing and varying malicious activities through network traffic, machine learning has become a promising approach in network security. Many previous studies have aggregated traffic data into groups by hosts or flows for generating features and training detection models. However, two problems degrade detection performance. One is the scarcity of training sets due to the rarity of new types of malicious traffic, and the other is variations in feature values generated from incomplete data due to limited observed traffic. In this paper, we propose a stochastic method called Alchemy that regenerates a set of feature vectors by randomly resampling raw traffic data of each bag into several subsets. Alchemy can increase training sets and represent raw traffic robustly to correct the influence of variations in feature vectors, regardless of types of traffic data and classifiers. We evaluated Alchemy with real-world traffic data of network flows, passive DNS records, and HTTP logs, and demonstrated that it improves detection performance of various classifiers more effectively than the conventional methods in all three types of traffic data.","0730-3157","978-1-7281-2607-4","10.1109/COMPSAC.2019.00057","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8754288","malicious traffic;machine learning;feature generation","Feature extraction;Training;Machine learning;IP networks;Stochastic processes;Security;Data models","computer network security;Internet;learning (artificial intelligence);stochastic processes;telecommunication traffic","feature vectors;Alchemy;real-world traffic data;stochastic feature regeneration;malicious network traffic classification;signature-based techniques;network security;training detection models;HTTP logs;passive DNS records;machine learning","","","","17","","9 Jul 2019","","","IEEE","IEEE Conferences"
"Analysis of Android malware detection performance using machine learning classifiers","Hyo-Sik Ham; Mi-Jung Choi","Dept. of Computer Science, Kangwon National University, Chuncheon, Korea; Dept. of Computer Science, Kangwon National University, Chuncheon, Korea","2013 International Conference on ICT Convergence (ICTC)","2 Dec 2013","2013","","","490","495","As mobile devices have supported various services and contents, much personal information such as private SMS messages, bank account information, etc. is scattered in mobile devices. Thus, attackers extend the attack range not only to the existing environment of PC and Internet, but also to the mobile device. Previous studies evaluated the malware detection performance of machine learning classifiers through collecting and analyzing event, system call, and log information generated in Android mobile devices. However, monitoring of unnecessary features without understanding Android architecture and malware characteristics generates resource consumption overhead of Android devices and low ratio of malware detection. In this paper, we propose new feature sets which solve the problem of previous studies in mobile malware detection and analyze the malware detection performance of machine learning classifiers.","2162-1241","978-1-4799-0698-7","10.1109/ICTC.2013.6675404","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6675404","Malware Detection;Android;Machine Learning Classifiers;Detection Performance","Malware;Smart phones;Performance evaluation;Monitoring;Feature extraction;Androids;Humanoid robots","Internet;invasive software;learning (artificial intelligence);mobile computing;operating systems (computers);software architecture","Android malware detection performance;machine learning classifiers;private SMS messages;bank account information;PC;Internet;system call;log information;Android mobile devices;Android architecture;malware characteristics;resource consumption overhead;Android devices;mobile malware detection","","8","","14","","2 Dec 2013","","","IEEE","IEEE Conferences"
"A cost-effective interactive platform for the management of a small scale lap-based jogging competition using low-frequency RFID technology","P. Satitsuksanoh; R. Jiamthapthaksin; S. W. Kim; P. Setthawong","Department of Computer Science Assumption University Samut Prakan, Thailand; Department of Computer Science Assumption University Samut Prakan, Thailand; Department of Computer Science Assumption University Samut Prakan, Thailand; Department of Business Information Systems Assumption University Samut Prakan, Thailand","2017 3rd International Conference on Science in Information Technology (ICSITech)","15 Jan 2018","2017","","","360","365","There are many formats for arranging jogging and running events. One popular format allows participants to compete over a period of time to collect the most amount of laps. These types of competition are aimed towards making the participants more healthy by being active and engage in regular exercise. To motivate the participants, the runners with the most laps are usually rewarded for their effort. Traditionally most of these events are usually small local community events, in which there is little budget and manpower that could be allocated towards the management of the events. This makes investment on existing interactive ultra-high frequency (UHF) radio-frequency identification (RFID) race timing systems too expensive to be considered for these events. Due to the lack of cost-effective solution for the problem domain, this study proposes a cost-effective interactive platform that is based on low-frequency (LF) RFID devices. The proposed platforms helps manage the running events, log runner specifics, manages data through a centralized database, and shows participants' data such as lap times and total laps in real time. The proposed system was deployed at the Fit for Run event at Assumption University between January 23-February 28, 2017.","","978-1-5090-5866-2","10.1109/ICSITech.2017.8257139","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8257139","LF RFID;RFID;timing system;interactive systems","Global Positioning System;RFID tags;Systems architecture;Databases;Smart phones;Software","radiofrequency identification;sport","Run event;cost-effective interactive platform;jogging competition;low-frequency RFID technology;running events;regular exercise;interactive ultra-high frequency radio-frequency identification race;low-frequency RFID devices","","","","16","","15 Jan 2018","","","IEEE","IEEE Conferences"
"Chapter 2 Data Analysis Software Requirements","J. Sergers",JS Engineering BVBA,"Analysis Techniques for Racecar Data Acquisition","","2014","","","17","37","Racecar data acquisition used to be limited to well-funded teams in high-profile championships. Today, the cost of electronics has decreased dramatically, making them available to everyone. But the cost of any data acquisition system is a waste of money if the recorded data is not interpreted correctly. This book, updated from the best-selling 2008 edition, contains techniques for analyzing data recorded by any vehicle's data acquisition system. It details how to measure the performance of the vehicle and driver, what can be learned from it, and how this information can be used to advantage next time the vehicle hits the track. Such information is invaluable to racing engineers and managers, race teams, and racing data analysts in all motorsports. Whether measuring the performance of a Formula One racecar or that of a road-legal street car on the local drag strip, the dynamics of vehicles and their drivers remain the same. Identical analysis techniques apply. Some race series have restricted data logging to decrease the team’s running budgets. In these cases it is extremely important that a maximum of information is extracted and interpreted from the hardware at hand. A team that uses data more efficiently will have an edge over the competition. However, the ever-decreasing cost of electronics makes advanced sensors and logging capabilities more accessible for everybody. With this comes the risk of information overload. Techniques are needed to help draw the right conclusions quickly from very large data sets. In addition to updates throughout, this new edition contains three new chapters: one on techniques for analyzing tire performance, one that provides an introduction to metric-driven analysis, a technique that is used throughout the book, and another that explains what kind of information the data contains about the track.","","9780768080810","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/xpl/ebooks/bookPdfWithBanner.jsp?fileName=8505513.pdf&bkn=8503628&pdfType=chapter","","","","","","","","","","17 Dec 2018","","","SAE","SAE eBook Chapters"
"Peculiarities of the interaction of electromagnetic waves with bio tissue and tool for early diagnosis, prevention and treatment","A. Trunov","Petro Mohyla Black Sea State University, Mykolayiv, Ukraine","2016 IEEE 36th International Conference on Electronics and Nanotechnology (ELNANO)","16 Jun 2016","2016","","","169","174","The problem about influence of electromagnetic wave (EMW) on the electron of terminal enzyme of respiratory chain from cytochrome-c-oxidase and other photo acceptors is considered and solved. The solution of problems of encoding and decoding interferogram data and calculation of radiation dose for different spectral composition are proposed. Applied aspects of construction software (SW) of early diagnosis are considered. The results of implementation and testing of SW are discussed for modules such as: calibration of sensitive elements of camera matrix for image analysis; choice of file format for recording, reading and logging; identification of structural elements and determination of the distribution concentration, intensity and spectral composition of radiation dose. The results of testing SW for photodynamic therapy are demonstrated of its efficiency. Experimentally studied and proven that irradiation of muscle by device of resonant conformational photon therapy improves the mechanical properties of the muscles.","","978-1-5090-1431-6","10.1109/ELNANO.2016.7493041","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7493041","EMW interaction;module of software;identification of structural elements;determination of the distribution concentration;irradiation of muscles","Magnetic fields;Mathematical model;Biochemistry;Calibration;Cameras;Radiation effects;Photonics","","","","8","","26","","16 Jun 2016","","","IEEE","IEEE Conferences"
"PyTrigger: A System to Trigger & Extract User-Activated Malware Behavior","D. Fleck; A. Tokhtabayev; A. Alarif; A. Stavrou; T. Nykodym",NA; NA; NA; NA; NA,"2013 International Conference on Availability, Reliability and Security","7 Nov 2013","2013","","","92","101","We introduce PyTrigger, a dynamic malware analysis system that automatically exercises a malware binary extracting its behavioral profile even when specific user activity or input is required. To accomplish this, we developed a novel user activity record and playback framework and a new behavior extraction approach. Unlike existing research, the activity recording and playback includes the context of every object in addition to traditional keyboard and mouse actions. The addition of the context makes the playback more accurate and avoids dependencies and pitfalls that come with pure mouse and keyboard replay. Moreover, playback can become more efficient by condensing common activities into a single action. After playback, PyTrigger analyzes the system trace using a combination of multiple states and behavior differencing to accurately extract the malware behavior and user triggered behavior from the complete system trace log. We present the algorithms, architecture and evaluate the PyTrigger prototype using 3994 real malware samples. Results and analysis are presented showing PyTrigger extracts additional behavior in 21% of the samples.","","978-0-7695-5008-4","10.1109/ARES.2013.16","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6657230","user activated malware;malware;malware triggering","Malware;Noise;Context;Equations;Mice;Browsers;Keyboards","behavioural sciences;feature extraction;human factors;invasive software;keyboards;mouse controllers (computers)","user-activated malware behavior extraction;user-activated malware behavior trigger;dynamic malware analysis system;malware binary extraction;user activity record and playback framework;PyTrigger analyzes;behavior differencing;user triggered behavior;complete system trace log;PyTrigger prototype;keyboard actions;mouse actions;keyboard replay;mouse replay","","8","","28","","7 Nov 2013","","","IEEE","IEEE Conferences"
"A New Graphical Password Scheme Resistant to Shoulder-Surfing","H. Gao; Z. Ren; X. Chang; X. Liu; U. Aickelin","Software Eng. Inst., Xidian Univ., Xi'an, China; Software Eng. Inst., Xidian Univ., Xi'an, China; Software Eng. Inst., Xidian Univ., Xi'an, China; Software Eng. Inst., Xidian Univ., Xi'an, China; Sch. of Comput. Sci., Univ. of Nottingham, Nottingham, UK","2010 International Conference on Cyberworlds","3 Dec 2010","2010","","","194","199","Shoulder-surfing is a known risk where an attacker can capture a password by direct observation or by recording the authentication session. Due to the visual interface, this problem has become exacerbated in graphical passwords. There have been some graphical schemes resistant or immune to shoulder-surfing, but they have significant usability drawbacks, usually in the time and effort to log in. In this paper, we propose and evaluate a new shoulder-surfing resistant scheme which has a desirable usability for PDAs. Our inspiration comes from the drawing input method in DAS and the association mnemonics in Story for sequence retrieval. The new scheme requires users to draw a curve across their password images orderly rather than click directly on them. The drawing input trick along with the complementary measures, such as erasing the drawing trace, displaying degraded images, and starting and ending with randomly designated images provide a good resistance to shoulder-surfing. A preliminary user study showed that users were able to enter their passwords accurately and to remember them over time.","","978-1-4244-8301-3","10.1109/CW.2010.34","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5656379","graphical password;shoulder-surfing;PDA;authentication","Usability;Authentication;Resistance;Prototypes;Personal digital assistants;Head","computer graphics;security of data;user interfaces","graphical password scheme;shoulder surfing;password;authentication session;visual interface;PDA;DAS;association mnemonics;sequence retrieval;password images","","20","2","28","","3 Dec 2010","","","IEEE","IEEE Conferences"
"Precise calling context encoding","W. N. Sumner; Y. Zheng; D. Weeratunge; X. Zhang",Purdue University; Purdue University; Purdue University; Purdue University,"2010 ACM/IEEE 32nd International Conference on Software Engineering","27 Oct 2011","2010","1","","525","534","Calling contexts are very important for a wide range of applications such as profiling, debugging, and event logging. Most applications perform expensive stack walking to recover contexts. The resulting contexts are often explicitly represented as a sequence of call sites and hence bulky. We propose a technique to encode the current calling context of any point during an execution. In particular, an acyclic call path is encoded into one number through only integer additions. Recursive call paths are divided into acyclic subsequences and encoded independently. We leverage stack depth in a safe way to optimize encoding: if a calling context can be safely and uniquely identified by its stack depth, we do not perform encoding. We propose an algorithm to seamlessly fuse encoding and stack depth based identification. The algorithm is safe because different contexts are guaranteed to have different IDs. It also ensures contexts can be faithfully decoded. Our experiments show that our technique incurs negligible overhead (1.89% on average). For most medium-sized programs, it can encode all contexts with just one number. For large programs, we are able to encode most calling contexts to a few numbers.","1558-1225","978-1-60558-719-6","10.1145/1806799.1806875","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6062056","calling context;dynamic context sensitivity;profiling","Context;Encoding;Instruments;Decoding;Runtime;Testing;Legged locomotion","program debugging;program testing","precise calling context encoding;profiling;debugging;event logging;expensive stack walking;acyclic call path;integer addition;recursive call path;acyclic subsequence;stack depth based identification;medium-sized program","","3","3","23","","27 Oct 2011","","","IEEE","IEEE Conferences"
"Thin and Light Video Editing Extensions for Education with Opencast Matterhorn","G. Logan; J. Greer; G. McCalla","Univ. of Saskatchewan, Saskatoon, SK, Canada; Univ. of Saskatchewan, Saskatoon, SK, Canada; Univ. of Saskatchewan, Saskatoon, SK, Canada","2012 IEEE International Symposium on Multimedia","31 Jan 2013","2012","","","467","470","This paper presents the current state of our research project which aims to give users a simple, easy to use, and computationally light way of creating mashups of lecture content within the Opencast Matter horn lecture capture system. The system modifies the playback components of Matter horn to deliver thin and light video clipping functionality without requiring installation of any additional software. We plan to make use of the extensive logging framework built into Matter horn to examine the effects of this tool on learner engagement.","","978-1-4673-4370-1","10.1109/ISM.2012.95","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6424708","lecture recording;lecture capture;video editing;matterhorn;opencast;mashups","Streaming media;Media;Educational institutions;Software;Context","computer aided instruction;video signal processing","light video editing extensions;Opencast Matterhorn lecture capture system;light video clipping functionality;thin video clipping functionality;extensive logging framework;learner engagement","","","","9","","31 Jan 2013","","","IEEE","IEEE Conferences"
"Data Mining and Student e-Learning Profiles","M. Zhou","Nat. Inst. of Educ., Nanyang Technol. Univ., Singapore, Singapore","2010 International Conference on E-Business and E-Government","30 Sep 2010","2010","","","5405","5408","Data mining techniques have been applied to educational research in various ways. In this paper, I presented the application of sequential data mining algorithms to analyze computer logs to profile learners in terms of their learning tactic use and motivation in a web-based learning environment (gStudy). The data mining algorithms are employed to discover patterns which characterize learners either across session or groups based on their study tactic choice and goal orientation. The use of this method is illustrated through a sequential pattern analysis of gStudy log files generated by university students.","","978-1-4244-6647-4","10.1109/ICEE.2010.1352","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5592904","data mining;sequential patterns;e-learning;learning profiles","Data mining;Software;Pattern analysis;Presses;Electronic learning;Learning systems","computer aided instruction;data mining;educational institutions;Internet","university student e-learning;educational research;sequential data mining algorithms;Web-based learning environment;tactic choice;goal orientation;learning tactic;gStudy;sequential pattern analysis;computer logs analysis","","","","26","","30 Sep 2010","","","IEEE","IEEE Conferences"
"Sensing Teamwork During Multi-objective Optimization","I. Winder; D. Delaporte; S. Wanaka; K. Hiekata","Massachusetts Institute of Technology,Engineering Systems Laboratory,Cambridge,MA; University of Tokyo,Graduate School of Frontier Sciences,Tokyo,Japan; National Maritime Research Institute,Knowledge and Data Systems,Tokyo,Japan; University of Tokyo,Graduate School of Frontier Sciences,Tokyo,Japan","2020 IEEE 6th World Forum on Internet of Things (WF-IoT)","13 Oct 2020","2020","","","1","6","We describe, build, and test a collaborative design environment with IoT sensors to help researchers study engineering teamwork behavior as it relates to design performance during multi-objective optimization. As a case study, we use an instrumented environment to observe engineers designing a fleet of crude oil tankers with the assistance of an interactive computer simulation. Recorded observations include user-triggered software events, simulation inputs and results, conversation activity, and video data. We describe the results of a trial run that demonstrates successful logging and cross-referencing of data from multiple sensor feeds. This work lays the foundation for continued experimentation with larger sample sizes, so that we may answer research questions about the nature of teamwork behavior and design performance.","","978-1-7281-5503-6","10.1109/WF-IoT48130.2020.9221086","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9221086","teamwork;sensors;simulation;systems engineering;multi-objective optimization;Pareto frontier;computer supported cooperative work","","computer simulation;crude oil;design engineering;groupware;human factors;interactive systems;Internet of Things;optimisation;sensor fusion;tanks (containers);team working","interactive computer simulation;user-triggered software events;multiple sensor;design performance;multiobjective optimization;collaborative design environment;IoT sensors;engineering teamwork behavior;instrumented environment;crude oil tankers;conversation activity;video data;sensor feeds","","","","16","","13 Oct 2020","","","IEEE","IEEE Conferences"
"Evaluation of Student's 3D Modeling Capability Based on Model Completeness and Usage Pattern in K-12 Classrooms","Y. Wu; W. Liao; C. Liu; T. Li; M. Chi","Dept. of Comput. Sci., Nat. Chengchi Univ., Taipei, Taiwan; Dept. of Comput. Sci., Nat. Chengchi Univ., Taipei, Taiwan; Dept. of Comput. Sci., Nat. Chengchi Univ., Taipei, Taiwan; Dept. of Comput. Sci., Nat. Chengchi Univ., Taipei, Taiwan; Dept. of Comput. Sci., Nat. Chengchi Univ., Taipei, Taiwan","2018 IEEE 18th International Conference on Advanced Learning Technologies (ICALT)","13 Aug 2018","2018","","","244","248","As more schools incorporate 3D printing into their curriculum to stimulate the creativity of K-12 students with a learning-by-doing approach, it becomes crucial to understand how users work with 3D modeling tools. In this paper, we aim to develop model and usage-pattern-related features to quantize students' performance on 3D modeling operation. The dataset is gathered from the Affiliated Experimental Elementary School (AEES) of National Chengchi University. Participants' operation log and finished work for specific 3D modeling software are recorded and analyzed. In all our lesson plans, students are required to create structurally stable and printable 3D models. Three modeling software with different levels of difficulty have been introduced and tested. The collected data include screen recording, software operation log, experts evaluation, and interviews with students, which are employed for subsequent qualitative evaluation as well as quantitative analysis. With our proposed approach, we are able to identify the key factors affecting students' learning experience and performance in terms of model completeness and usage pattern. Through these indicators, instructors can understand student's learning status of 3D modeling software more comprehensively.","2161-377X","978-1-5386-6049-2","10.1109/ICALT.2018.00063","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8433506","3D modeling software;STEAM lesson plan;Qmodel Creator;Performance evaluation","Three-dimensional displays;Solid modeling;Complexity theory;Software;Entropy;Analytical models;Computational modeling","computer aided instruction;solid modelling","usage pattern;3D modeling tools;Affiliated Experimental Elementary School;students learning experience;National Chengchi University;K-12 classrooms;3D printing","","","","9","","13 Aug 2018","","","IEEE","IEEE Conferences"
"DeltaDB: A Scalable Database Design for Time-Varying Schema-Free Data","P. Ivie; D. Thain","Dept. of Comput. Sci. & Eng., Univ. of Notre Dame, Notre Dame, IN, USA; Dept. of Comput. Sci. & Eng., Univ. of Notre Dame, Notre Dame, IN, USA","2014 IEEE International Congress on Big Data","25 Sep 2014","2014","","","104","111","DeltaDB is a model for a database consisting of records with no fixed schema whose entire history is captured over time. It is designed to support efficient queries against the current state of the database, any point in the history of the database, and historical data aggregations over time. In this paper, we present the DeltaDB data model, the associated query algebra, and highlight the fundamental query optimization concerns. To gain experience with the DeltaDB model, we have created a single-node implementation of the database and used it to collect one year's worth of monitoring data from a distributed software system, reducing over 5TB of snapshots into 11GB of log data. We give examples of novel types of queries that exploit the time-varying nature of the data and evaluate their performance. We conclude with a discussion of how the single-node implementation will serve as the building block for a future distributed implementation.","2379-7703","978-1-4799-5057-7","10.1109/BigData.Congress.2014.24","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6906767","Database;Schema-Free;Time-Variant;Scalable;Data Model;Data;Temporal;Spatial;Algebra;Query;Distributed;Log-only;Multiversion;Multi-version;Time Series;Reduction","Databases;Algebra;Data models;History;Servers;Software;Big data","data models;database management systems;distributed processing;query processing","DeltaDB data model;database design;time-varying schema-free data;historical data aggregations;query algebra;fundamental query optimization concerns;distributed software system;single-node implementation","","","","24","","25 Sep 2014","","","IEEE","IEEE Conferences"
"A Traceable and Pseudonymous P2P Information Distribution System","N. Tsujio; Y. Okabe","Grad. Sch. of Infomatics, Kyoto Univ., Kyoto, Japan; Acad. Center for Comput. & Media Studies, Kyoto Univ., Kyoto, Japan","2015 IEEE 39th Annual Computer Software and Applications Conference","24 Sep 2015","2015","3","","67","72","Anyone can publish various kinds of information on the Internet almost freely, but in some cases such information distribution is inhibited by the authorities. In order to resist such censorship, there have been developed many anonymous information distribution systems such as Free net and Tor, but some people argue that such a system may also be a hotbed of crime since scrupulous anonymity disturbs investigations. In this paper, an article distribution system is proposed, which protects pseudonymity of users from surveillance by the authorities as with existing anticensorship systems. As a novel point, the proposed system allows a user to trace the publisher of an article by cooperation of the users who have relayed the article. This will suppress criminal acts abusing pseudonymity in the system. On the other hand, it is difficult to trace the publisher for a single government or an organization alone unless it obtains cooperation of multiple users. The proposed system will therefore be able to avoid authoritarian censorship or surveillance by the authorities. The system adopts P2P architecture, and a user can publish articles to other users like Netnews. A published article is relayed by node to node and spreads over the network of the system. In order to trace the publisher of an article, a user records a relaying log when he relays an article. A relaying log contains information about the predecessor from whom the user received the article. A user can trace the publisher by gathering relaying logs. Each user has responsibility to determine whether to disclose a relaying log or not, considering the content of the article. If all users along the path from the publisher agree to cooperate in gathering their logs, they will be able to trace to the publisher. The performance of the system is discussed in evaluation, how users' actions affect traceability, and what should users do if governments intervene in the system.","0730-3157","978-1-4673-6564-2","10.1109/COMPSAC.2015.215","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7273326","peer-to-peer;censorship;privacy;traceable;information distribution","Peer-to-peer computing;Government;Relays;Internet;Censorship;Surveillance;Privacy","authorisation;computer crime;Internet;law;peer-to-peer computing","traceable P2P information distribution system;pseudonymous P2P information distribution system;Internet;Freenet;Tor;crime hotbed;scrupulous anonymity;users pseudonymity;anticensorship systems;authoritarian censorship;P2P architecture;Netnews;relaying log;governments","","","","15","","24 Sep 2015","","","IEEE","IEEE Conferences"
"DLOOP: A Flash Translation Layer Exploiting Plane-Level Parallelism","A. R. Abdurrab; T. Xie; W. Wang","Microsoft Corp., Bellevue, WA, USA; San Diego State Univ., San Diego, CA, USA; San Diego State Univ., San Diego, CA, USA","2013 IEEE 27th International Symposium on Parallel and Distributed Processing","29 Jul 2013","2013","","","908","918","A flash translation layer (FTL) is a software layer running in the flash controller of a NAND flash memory solid-state disk (hereafter, flash SSD). It translates logical addresses received from a file system to physical addresses in flash SSD so that the linear flash memory appears to the system like a block storage device. Since the effectiveness of an FTL significantly impacts the performance and durability of a flash SSD, FTL design has attracted significant attention from both industry and academy in recent years. In this research, we propose a new FTL called DLOOP (Data Log On One Plane), which fully exploits plane-level parallelism supported by modern flash SSDs. The basic idea of DLOOP is to allocate logs (updates) onto the same plane where their associated original data resides so that valid page copying operations triggered by garbage collection can be carried out by intraplane copy-back operations without occupying the external I/O bus. Further, we largely extend a validated simulation environment DiskSim3.0/FlashSim to implement DLOOP. Finally, we conduct comprehensive experiments to evaluate DLOOP using realistic enterprise-scale workloads. Experimental results show that DLOOP consistently outperforms a classical hybrid FTL named FAST and a morden page-mapping FTL called DFTL.","1530-2075","978-1-4673-6066-1","10.1109/IPDPS.2013.58","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6569873","flash translation layer;copy-back;merge operations;solid state disk;garbage collection","Ash;Parallel processing;Registers;Random access memory;Equations;Drives;Electronic mail","flash memories;storage management","DLOOP;flash translation layer;plane-level parallelism;FTL;software layer;flash controller;NAND flash memory solid-state disk;flash SSD;logical address;file system;physical address;linear flash memory;block storage device;data log on one plane;page copying operation;garbage collection;intraplane copy-back operation;DiskSim3.0;FlashSim;enterprise-scale workload","","13","","19","","29 Jul 2013","","","IEEE","IEEE Conferences"
"Establishing Independent Audit Mechanisms for Database Management Systems","A. Rasin; J. Wagner; K. Heart; J. Grier","School of Computing DePaul University Chicago, IL, USA; School of Computing DePaul University Chicago, IL, USA; School of Computing DePaul University Chicago, IL, USA; Grier Forensics Pikesville, MD, USA","2018 IEEE International Symposium on Technologies for Homeland Security (HST)","13 Dec 2018","2018","","","1","7","The pervasive use of databases for the storage of critical and sensitive information in many organizations has led to an increase in the rate at which databases are exploited in computer crimes. While there are several techniques and tools available for database forensic analysis, such tools usually assume an apriori database preparation, such as relying on tamper-detection software to already be in place and the use of detailed logging. Further, such tools are built-in and thus can be compromised or corrupted along with the database itself. In practice, investigators need forensic and security audit tools that work on poorlyconfigured systems and make no assumptions about the extent of damage or malicious hacking in a database.In this paper, we present our database forensics methods, which are capable of examining database content from a storage (disk or RAM) image without using any log or file system metadata. We describe how these methods can be used to detect security breaches in an untrusted environment where the security threat arose from a privileged user (or someone who has obtained such privileges). Finally, we argue that a comprehensive and independent audit framework is necessary in order to detect and counteract threats in an environment where the security breach originates from an administrator (either at database or operating system level).","","978-1-5386-3443-1","10.1109/THS.2018.8574150","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8574150","database forensics;security audit;evidence gathering","Forensics;Databases;Tools;Security;Image reconstruction;Random access memory;Metadata","auditing;computer crime;digital forensics;meta data","independent audit mechanisms;independent audit framework;comprehensive audit framework;security threat;security breaches;database forensics methods;security audit tools;tamper-detection software;apriori database preparation;database forensic analysis;computer crimes;database management systems;file system metadata;storage image;malicious hacking;forensic audit tools;organizations;logging","","","","16","","13 Dec 2018","","","IEEE","IEEE Conferences"
"A Review of Unstructured Data Analysis and Parsing Methods","S. Jain; A. de Buitléir; E. Fallon","Software Research Institute, Athlone Institute of Technology,Athlone,Ireland; Network Management Lab, Ericsson,Athlone,Ireland; Software Research Institute, Athlone Institute of Technology,Athlone,Ireland","2020 International Conference on Emerging Smart Computing and Informatics (ESCI)","17 Aug 2020","2020","","","164","169","Computer applications generate an enormous amount of data every day through their logs, system-generated files or other reports. This generated data depicts the state of the running system and contains abundant information that can be used for system diagnostics and monitoring. Network monitoring systems produce a wide variety of unstructured information, so there is a need for an automated way to extract the relevant data, which currently requires multitude of custom parsers. Developing and testing custom parsers can be time-consuming. Instead, data can be automatically processed and parsed into a machine-readable format, building a generic model for standard or vendor-specific data, and generating insights for analytics, anomaly detection, intrusion detection, node failures and various other applications. This paper reviews some existing approaches for unstructured data mining and parsing and discusses the challenges in information extraction, creation of knowledge bases and presents a generic framework for automatic parsing.","","978-1-7281-5263-9","10.1109/ESCI48226.2020.9167588","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9167588","Data Mining;Information Extraction;Similarity;NLP;Knowledge base","Feature extraction;Knowledge based systems;Clustering algorithms;Information retrieval;Natural language processing;Text analysis","data analysis;data mining;grammars;security of data","anomaly detection;intrusion detection;unstructured data mining;information extraction;automatic parsing;unstructured data analysis;parsing methods;computer applications;system-generated files;running system;abundant information;system diagnostics;network monitoring systems;unstructured information;custom parsers;machine-readable format;vendor-specific data","","2","","33","","17 Aug 2020","","","IEEE","IEEE Conferences"
"Identifying Routine and Telltale Activity Patterns in Knowledge Work","O. Brdiczka; V. Bellotti","Palo Alto Res. Center (PARC), Palo Alto, CA, USA; Palo Alto Res. Center (PARC), Palo Alto, CA, USA","2011 IEEE Fifth International Conference on Semantic Computing","27 Oct 2011","2011","","","95","101","Our research addresses the question as to whether automatically collected quantitative data about people's behavior online can be analyzed to spot patterns that indicate behaviors of interest. Based on ethnographic studies, we find that people, going about their routine work, exhibit patterns in terms of their routine online activities and work rhythms. Such patterns can be comprised of many diverse types of events occurring over arbitrary durations. For example, they might include timing, duration and frequency of particular uses of hardware and software resources, manipulations of content, communication acts and so on. We use ethnography to identify and target significant patterns and computer logging to collect data on computer events that can be analyzed to find reliable correlates of those patterns. In this paper we discuss our methods and their potential for the development of novel types of applications that can identify normal activities and also spot telltale or deviant patterns. Such applications could be useful to users directly by providing helpful resources and content automatically or to the enterprise in general by automatically detecting performance problems, deleterious behaviors, or malicious activities.","","978-1-4577-1648-5","10.1109/ICSC.2011.12","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6061417","human behavior semantics;human activity detection;routine activity patterns;ethnography","Humans;Software;Semantics;Accuracy;Computers;Decision trees;Shadow mapping","behavioural sciences computing","telltale activity patterns;knowledge work;ethnography;computer logging;computer events;deviant patterns","","2","","36","","27 Oct 2011","","","IEEE","IEEE Conferences"
"Systematic configuration and automatic tuning of neuromorphic systems","S. Sheik; F. Stefanini; E. Neftci; E. Chicca; G. Indiveri","Institute of Neuroinformatics, University of Zurich and ETH, Switzerland; Institute of Neuroinformatics, University of Zurich and ETH, Switzerland; Institute of Neuroinformatics, University of Zurich and ETH, Switzerland; Institute of Neuroinformatics, University of Zurich and ETH, Switzerland; Institute of Neuroinformatics, University of Zurich and ETH, Switzerland","2011 IEEE International Symposium of Circuits and Systems (ISCAS)","4 Jul 2011","2011","","","873","876","In the past recent years several research groups have proposed neuromorphic Very Large Scale Integration (VLSI) devices that implement event-based sensors or biophysically realistic networks of spiking neurons. It has been argued that these devices can be used to build event-based systems, for solving real-world applications in real-time, with efficiencies and robustness that cannot be achieved with conventional computing technologies. In order to implement complex event-based neuromorphic systems it is necessary to interface the neuromorphic VLSI sensors and devices among each other, to robotic platforms, and to workstations (e.g. for data-logging and analysis). This apparently simple goal requires painstaking work that spans multiple levels of complexity and disciplines: from the custom layout of microelectronic circuits and asynchronous printed circuit boards, to the development of object oriented classes and methods in software; from electrical engineering and physics for analog/digital circuit design to neuroscience and computer science for neural computation and spike-based learning methods. Within this context, we present a framework we developed to simplify the configuration of multi-chip neuromorphic VLSI systems, and automate the mapping of neural network model parameters to neuromorphic circuit bias values.","2158-1525","978-1-4244-9474-3","10.1109/ISCAS.2011.5937705","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5937705","","Hardware;Neurons;Neuromorphics;Software;Object oriented modeling;Optimization","neural nets;VLSI","automatic tuning;neuromorphic very large scale integration devices;VLSI devices;event-based sensors;biophysically realistic networks;spiking neurons;complex event-based neuromorphic systems;neuromorphic VLSI sensors;microelectronic circuits;asynchronous printed circuit boards;object oriented classes;electrical engineering;physics;analog/digital circuit design;neuroscience;computer science;neural computation;spike-based learning;multichip neuromorphic VLSI systems;neural network model;neuromorphic circuit","","7","2","14","","4 Jul 2011","","","IEEE","IEEE Conferences"
"Sonification of Network Traffic for Detecting and Learning About Botnet Behavior","M. Debashi; P. Vickers","Department of Computer and Information Sciences, Northumbria University, Newcastle upon Tyne, U.K.; Department of Computer and Information Sciences, Northumbria University, Newcastle upon Tyne, U.K.","IEEE Access","4 Jul 2018","2018","6","","33826","33839","Today's computer networks are under increasing threat from malicious activity. Botnets (networks of remotely controlled computers, or “bots”) operate in such a way that their activity superficially resembles normal network traffic which makes their behavior hard to detect by current intrusion detection systems (IDS). Therefore, new monitoring techniques are needed to enable network operators to detect botnet activity quickly and in real time. Here, we show a sonification technique using the SoNSTAR system that maps characteristics of network traffic to a real-time soundscape enabling an operator to hear and detect botnet activity. A case study demonstrated how using traffic log files alongside the interactive SoNSTAR system enabled the identification of new traffic patterns characteristic of botnet behavior and subsequently the effective targeting and real-time detection of botnet activity by a human operator. An experiment using the 11.39 GiB ISOT botnet data set, containing labeled botnet traffic data, compared the SoNSTAR system with three leading machine learning-based traffic classifiers in a botnet activity detection test. SoNSTAR demonstrated greater accuracy (99.92%), precision (97.1%), and recall (99.5%) and much lower false positive rates (0.007%) than the other techniques. The knowledge generated about characteristic botnet behaviors could be used in the development of future IDSs.","2169-3536","","10.1109/ACCESS.2018.2847349","Ministry of Higher Education and Scientific Research; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8385098","Botnet detection;intrusion detection systems;network monitoring;situational awareness;sonification","Botnet;IP networks;Monitoring;Security;Servers;Tools;Real-time systems","computer network security;invasive software;learning (artificial intelligence);telecommunication traffic","effective targeting time detection;real-time detection;11.39 GiB ISOT botnet data;botnet traffic data;botnet activity detection test;characteristic botnet behaviors;computer networks;malicious activity;remotely controlled computers;normal network traffic;traffic log files;interactive SoNSTAR system;traffic patterns;machine learning;intrusion detection systems;botnet behavior detection;botnet behavior learning;network traffic sonification;real-time soundscape;traffic classifiers","","3","","38","CCBY","14 Jun 2018","","","IEEE","IEEE Journals"
"Hiding the long latency of persist barriers using speculative execution","S. Shin; J. Tuck; Y. Solihin","Dept. of Electrical and Computer Engineering North Carolina State University, NC, USA; Dept. of Electrical and Computer Engineering North Carolina State University, NC, USA; Dept. of Electrical and Computer Engineering North Carolina State University, NC, USA","2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)","14 Dec 2017","2017","","","175","186","Byte-addressable non-volatile memory technology is emerging as an alternative for DRAM for main memory. This new Non-Volatile Main Memory (NVMM) allows programmers to store important data in data structures in memory instead of serializing it to the file system, thereby providing a substantial performance boost. However, modern systems reorder memory operations and utilize volatile caches for better performance, making it difficult to ensure a consistent state in NVMM. Intel recently announced a new set of persistence instructions, clflushopt, clwb, and pcommit. These new instructions make it possible to implement fail-safe code on NVMM, but few workloads have been written or characterized using these new instructions. In this work, we describe how these instructions work and how they can be used to implement write-ahead logging based transactions. We implement several common data structures and kernels and evaluate the performance overhead incurred over traditional non-persistent implementations. In particular, we find that persistence instructions occur in clusters along with expensive fence operations, they have long latency, and they add a significant execution time overhead, on average by 20.3% over code with logging but without fence instructions to order persists. To deal with this overhead and alleviate the performance bottleneck, we propose to speculate past long latency persistency operations using checkpoint-based processing. Our speculative persistence architecture reduces the execution time overheads to only 3.6%.","","978-1-4503-4892-8","10.1145/3079856.3080240","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8192470","Non-Volatile Main Memory;Speculative Persistence;Failure Safety","Nonvolatile memory;Safety;Data structures;Force;Random access memory;Software","cache storage;checkpointing;data structures;DRAM chips;random-access storage;storage management","speculative persistence architecture;persist barriers;speculative execution;nonvolatile memory technology;NonVolatile Main Memory;NVMM;file system;substantial performance boost;modern systems reorder memory operations;volatile caches;consistent state;persistence instructions;fail-safe code;logging based transactions;common data structures;performance overhead;nonpersistent implementations;expensive fence operations;significant execution time overhead;performance bottleneck;long latency persistency operations;checkpoint-based processing;DRAM;clflushopt;clwb;pcommit","","16","","49","","14 Dec 2017","","","IEEE","IEEE Conferences"
"Android Malware Detection: Building Useful Representations","L. Sayfullina; E. Eirola; D. Komashinsky; P. Palumbo; J. Karhunen","Aalto Univ., Espoo, Finland; Arcada Univ. of Appl. Sci., Helsinki, Finland; F-Secure Corp., Helsinki, Finland; F-Secure Corp., Helsinki, Finland; Aalto Univ., Espoo, Finland","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","2 Feb 2017","2016","","","201","206","The problem of proactively detecting Android Malware has proven to be a challenging one. The challenges stem from a variety of issues, but recent literature has shown that this task is hard to solve with high accuracy when only a restricted set of features, like permissions or similar fixed sets of features, are used. The opposite approach of including all available features is also problematic, as it causes the features space to grow beyond reasonable size. In this paper we focus on finding an efficient way to select a representative feature space, preserving its discriminative power on unseen data. We go beyond traditional approaches like Principal Component Analysis, which is too heavy for large-scale problems with millions of features. In particular we show that many feature groups that can be extracted from Android application packages, like features extracted from the manifest file or strings extracted from the Dalvik Executable (DEX), should be filtered and used in classification separately. Our proposed dimensionality reduction scheme is applied to each group separately and consists of raw string preprocessing, feature selection via log-odds and finally applying random projections. With the size of the feature space growing exponentially as a function of the training set's size, our approach drastically decreases the size of the feature space of several orders of magnitude, this in turn allows accurate classification to become possible in a real world scenario. After reducing the dimensionality we use the feature groups in a light-weight ensemble of logistic classifiers. We evaluated the proposed classification scheme on real malware data provided by the antivirus vendor and achieved state-of-the-art 88.24% true positive and reasonably low 0.04% false positive rates with a significantly compressed feature space on a balanced test set of 10,000 samples.","","978-1-5090-6167-9","10.1109/ICMLA.2016.0041","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7838145","malware classification;logistic regression;random projection;Android;dimensionality reduction;feature selection","Feature extraction;Androids;Humanoid robots;Malware;Principal component analysis;Sparse matrices;Logistics","Android (operating system);feature extraction;feature selection;invasive software;pattern classification","Android malware detection;useful representations;representative feature space;feature extraction;Android application packages;manifest file;Dalvik executable;DEX;raw string preprocessing;feature selection;log-odds;random projections;logistic classifiers;classification scheme;mobile devices","","4","","17","","2 Feb 2017","","","IEEE","IEEE Conferences"
"DeepGuard: Deep Generative User-behavior Analytics for Ransomware Detection","G. O. Ganfure; C. -F. Wu; Y. -H. Chang; W. -K. Shih","Taiwan International Graduate Program,Social Networks and Human Centered Computing Program; Institute of Information Science,Nangang,Taipei,Taiwan,115; Institute of Information Science,Nangang,Taipei,Taiwan,115; National Tsing Hua University,Department of Computer Science,Hsinchu City,Taiwan","2020 IEEE International Conference on Intelligence and Security Informatics (ISI)","8 Dec 2020","2020","","","1","6","In the last couple of years, the move to cyberspace provides a fertile environment for ransomware criminals like ever before. Notably, since the introduction of WannaCry, numerous ransomware detection solution has been proposed. However, the ransomware incidence report shows that most organizations impacted by ransomware are running state of the art ransomware detection tools. Hence, an alternative solution is an urgent requirement as the existing detection models are not sufficient to spot emerging ransomware treat. With this motivation, our work proposes ""DeepGuard,"" a novel concept of modeling user behavior for ransomware detection. The main idea is to log the file-interaction pattern of typical user activity and pass it through deep generative autoencoder architecture to recreate the input. With sufficient training data, the model can learn how to reconstruct typical user activity (or input) with minimal reconstruction error. Hence, by applying the three-sigma limit rule on the model's output, DeepGuard can distinguish the ransomware activity from the user activity. The experiment result shows that DeepGuard effectively detects a variant class of ransomware with minimal false-positive rates. Overall, modeling the attack detection with user-behavior permits the proposed strategy to have deep visibility of various ransomware families.","","978-1-7281-8800-3","10.1109/ISI49825.2020.9280508","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9280508","Ransomware Detection;User behavior Analytics;Deep Autoencoders;Cybersecurity","Training;Training data;Organizations;Tools;Ransomware;Security;Informatics","data mining;invasive software;learning (artificial intelligence);neural nets;system monitoring","ransomware incidence report;ransomware detection;DeepGuard;user behavior modeling;user activity;deep generative autoencoder architecture;ransomware activity;attack detection;deep generative user-behavior analytics;ransomware criminals;WannaCry;file-interaction pattern logging;three-sigma limit rule","","","","18","","8 Dec 2020","","","IEEE","IEEE Conferences"
"Detecting Malicious Server Based on Server-to-Server Realation Graph","Z. Wang; F. Zou; B. Pei; W. He; L. Pan; Z. Mao; L. Li","Sch. of Inf. Security Eng., Shanghai Jiao Tong Univ. Shanghai, Shanghai, China; Sch. of Inf. Security Eng., Shanghai Jiao Tong Univ. Shanghai, Shanghai, China; Minist. of Public Security, Key Lab. of Inf. Network Security, Shanghai, China; Sch. of Inf. Security Eng., Shanghai Jiao Tong Univ. Shanghai, Shanghai, China; Sch. of Inf. Security Eng., Shanghai Jiao Tong Univ. Shanghai, Shanghai, China; Sch. of Inf. Security Eng., Shanghai Jiao Tong Univ. Shanghai, Shanghai, China; Sch. of Inf. Security Eng., Shanghai Jiao Tong Univ. Shanghai, Shanghai, China","2016 IEEE First International Conference on Data Science in Cyberspace (DSC)","2 Mar 2017","2016","","","698","702","The rapid development of Internet attack has posed severe threats to information security. Therefore, it's of great interest to both the Internet security companies and researchers to develop novel methods which are capable of protecting users against new threats. However, the sources of these network attack varies. Existing malware detectors and intrusion detectors mostly treat the web logs separately using supervised learning algorithms. Meanwhile, using features beyond network connection content are starting to be leveraged for Internet server classification. In this paper, based on the Server-to-Server Relation Graph, we present a network Server classification method by analyzing the client distribution of each server. When constructing Server-to-Server Relation graph, k-nearest neighbors are chosen as adjacent nodes for each server node, and being compared with radial basis function network. Files are connected with edges representing the similarity of their client set. In the machine learning part, we used Label propagation algorithm, a semi-supervised learning algorithm which propagates class labels on a graph. We evaluate the effectiveness of our proposed method on a real and large dataset. Experimental results demonstrate that the precision of our method is acceptable and worthwhile.","","978-1-5090-1192-6","10.1109/DSC.2016.79","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7866212","Malicious Server Detection;Server-to-Server Relation graph;kNN;Label propagation","Servers;Internet;Malware;Classification algorithms;Machine learning algorithms;Telecommunication traffic;IP networks","Internet;invasive software;learning (artificial intelligence);network servers;pattern classification;radial basis function networks","malicious server;server-to-server realation graph;Internet attack;information security;Internet security;network attack;malware detectors;intrusion detectors;Web logs;network connection content;Internet server classification;network server classification;k-nearest neighbors;server node;radial basis function network;files;client set;machine learning;Label propagation algorithm;semisupervised learning algorithms","","","","12","","2 Mar 2017","","","IEEE","IEEE Conferences"
"Monitoring Infrastructure: The Challenges of Moving Beyond Petascale","A. Bonnie; M. Mason; D. Illescas","Los Alamos Nat. Lab., Los Alamos, NM, USA; Los Alamos Nat. Lab., Los Alamos, NM, USA; Los Alamos Nat. Lab., Los Alamos, NM, USA","2017 IEEE International Conference on Cluster Computing (CLUSTER)","25 Sep 2017","2017","","","785","788","Scaling clusters is no longer the only struggle in moving towards exascale in HPC. While scaling components such as the network and file systems is a widely accepted need, monitoring, on the other hand, is often left behind in the procurement of these large systems. Monitoring is often quite an afterthought that is expected to be incorporated in existing infrastructure. While that often works for small systems, even petascale systems are starting to push the capabilities of monitoring infrastructure and their ability to collect and analyze complete system wide logs. The need and desire to do more cross-component relations will only get more complex with scale. Preparing for monitoring an exascale class machine is no small task.This paper presents the current redesign of our commodity monitoring infrastructure, the upgraded sub-system monitoring for Trinity, and ideas and concepts for moving towards exascale class monitoring.","2168-9253","978-1-5386-2326-8","10.1109/CLUSTER.2017.89","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8049017","monitoring;infrastructure;HPC;exascale;Splunk;ZFS","Monitoring;Measurement;Tools;Software;Licenses;Hardware;Production","parallel processing;system monitoring","petascale systems;complete system wide logs;cross-component relations;exascale class machine;commodity monitoring infrastructure;upgraded sub-system monitoring;exascale class monitoring;scaling clusters;HPC;Trinity;high performance computing","","1","","7","","25 Sep 2017","","","IEEE","IEEE Conferences"
"Rapid FPGA development framework using a custom Simulink library for MTCA.4 modules","P. Prędki; M. Heuer; Ł. Butkowski; A. Napieralski","Department of Microelectronics and Computer Science of the Lodz University of Technology, Poland; Deutsches Elektronen-Synchrotron, Germany; Deutsches Elektronen-Synchrotron, Germany; Department of Microelectronics and Computer Science of the Lodz University of Technology, Poland","2014 19th IEEE-NPSS Real Time Conference","30 Apr 2015","2014","","","1","2","The recent introduction of advanced hardware architectures such as the Micro Telecommunications Computing Architecture (MTCA) caused a change in the approach to implementation of control schemes in many fields. It required the development to move away from traditional programming languages (C/C++) to hardware description languages (VHDL, Verilog), which are used in FPGA development. With MATLAB/Simulink it is possible to describe complex systems with block diagrams and simulate their behavior. Those diagrams are then used by the HDL experts, to implement exactly the required functionality in hardware. Both the porting of existing applications and adaptation of new ones requires a lot of development time from them. To solve this, Xilinx System Generator, a toolbox for MATLAB/Simulink, allows rapid prototyping of those block diagrams using hardware modelling. It is still up to the firmware developer to merge this structure with the hardware-dependent HDL project. This prevents the application engineer from quickly verifying the proposed schemes in real hardware. The framework described in this article overcomes these challenges, offering a hardware-independent library of components that can be used in Simulink/System Generator models. The components are subsequently translated into VHDL entities and integrated with a pre-prepared VHDL project template. Furthermore, the entire implementation process is run in the background, giving the user an almost one-click path from control scheme modelling and simulation to bit-file generation. This approach allows the control theory engineers to quickly develop new schemes and test them in real hardware environment. The applications may range from simple data logging or signal generation ones to very advanced controllers. Taking advantage of the Simulink simulation capabilities and user-friendly hardware implementation routines, the framework significantly decreases the development time of FPGA-based applications.","","978-1-4799-3659-5","10.1109/RTC.2014.7097549","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7097549","DESY;FLASH;European XFEL;feedback control;control system;MTCA;Simulink;hardware description languages;System Generator;Xilinx","Hardware;Field programmable gate arrays;Software packages;Libraries;Hardware design languages;Synchronization;Generators","field programmable gate arrays;firmware;hardware description languages;mathematics computing;rapid prototyping (industrial)","rapid FPGA development framework;custom Simulink library;MTCA.4 modules;advanced hardware architectures;Micro Telecommunications Computing Architecture;control schemes;C programming languages;C++ programming languages;hardware description languages;Verilog;MATLAB;Xilinx System Generator;rapid prototyping;block diagrams;hardware modelling;firmware developer;hardware-dependent HDL project;hardware-independent library;System Generator models;VHDL project template;data logging;signal generation","","","","8","","30 Apr 2015","","","IEEE","IEEE Conferences"
"Analysis of Learning Patterns and Performance—A Case Study of 3-D Modeling Lessons in the K-12 Classrooms","Y. -C. Wu; W. -H. Liao","Department of Computer Science, National Chengchi University, Taipei, Taiwan; Department of Computer Science, National Chengchi University, Taipei, Taiwan","IEEE Access","22 Oct 2020","2020","8","","186976","186992","As more schools incorporate 3D printing into their curriculum to stimulate the creativity of K-12 students with a learning-by-doing approach, it becomes crucial to understand how users work with 3D modeling tools and to evaluate integrated lesson plans in the STEAM (Science, Technology, Engineering, Arts and Math) educational framework. Our work consists of two stages: an investigation of usage patterns of modeling, and an evaluation of the usability of Qmodel Creator, in collaboration with Lanyu Primary and Junior High School, Sanchong High School, and Affiliated Experimental Elementary School of National Chengchi University. Participants operation logs, screen recordings, and finished work for respective 3D modeling software were recorded and analyzed. Two types of indicators have been developed. One is concerned with the quantification of learning behavior, including Effective Operating Period (EOP), Trialand-Error Period (TEP), and Implementation Period (IP). The other has to do with the evaluation of learning outcome, i.e., the complexity of 3D models, including the Degree of Detail (DoD), shape (Cf ), partition (Cp), and block-ratio (Cr) complexity. Based on the proposed features, we are able to identify the key factors that affect students' learning experiences and performance in terms of learning patterns and model completeness. Through these indicators, instructors can gain better insights into student's learning status of 3D modeling software.","2169-3536","","10.1109/ACCESS.2020.3029947","Ministry of Science and Technology Taiwan; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9218951","Learning patterns;3D modeling software;STEAM lesson plan;Qmodel creator;performance evaluation;K-12 education","Three-dimensional displays;Solid modeling;Software;Education;Task analysis;Three-dimensional printing;Complexity theory","computational complexity;computer aided instruction;educational courses;educational institutions;solid modelling","3D modeling tools;integrated lesson plans;Qmodel Creator;Lanyu Primary;Junior High School;National Chengchi University;screen recordings;3D modeling software;implementation period;3-D modeling lessons;K-12 Classrooms;3D printing;learning-by-doing approach;learning patterns;effective operating period;trialand-error period;Sanchong high school;block-ratio complexity;degree of detail;DoD","","","","37","CCBY","9 Oct 2020","","","IEEE","IEEE Journals"
"Checks and balances: A tripartite public key infrastructure for secure web-based connections","J. Chen; S. Yao; Q. Yuan; R. Du; G. Xue","State Key Laboratory of Software Engineering, Computer School, Wuhan University, Wuhan, China 430072; State Key Laboratory of Software Engineering, Computer School, Wuhan University, Wuhan, China 430072; University of Texas-Permian Basin, TX, 79762; Collaborative Innovation Center of Geospatial Technology, Wuhan, China 430072; Arizona State University, Tempe, AZ 85287","IEEE INFOCOM 2017 - IEEE Conference on Computer Communications","5 Oct 2017","2017","","","1","9","Recent real-world attacks against Certification Authorities (CAs) and fraudulently issued certificates arouse the public to rethink the security of public key infrastructure for web-based connections. To distribute the trust of CAs, notaries, as an independent party, are introduced to record certificates, and a client can request an audit proof of certificates from notaries directly. However, there are two challenges. On one hand, existing works consider the security of notaries insufficiently. Due to lack of systematic mutual verification, notaries might bring safety bottlenecks. On the other hand, the service of these works is not sustainable, when any party leaks its private key or fails. In this paper, we propose a Tripartite Public Key Infrastructure (TriPKI), using Certificates Authorities, Integrity Log Servers, and Domain Name Servers, to provide a basis for establishing secure SSL/TLS connections. Specifically, we apply checks-and balances among those three parties in the structure to make them verify mutually, which avoids any single party compromise. Furthermore, we design a collaborative certificate management scheme to provide sustainable services. The security analysis and experiment results demonstrate that our scheme is suitable for practical usage with moderate overhead.","","978-1-5090-5336-0","10.1109/INFOCOM.2017.8057201","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8057201","Public Key Infrastructure;DNS-based;Mutual Verification","Public key;Servers;Collaboration;Electronic mail;Conferences;Authentication","authorisation;certification;Internet;public key cryptography","balances;tripartite public key infrastructure;Certification Authorities;CAs;fraudulently issued certificates;notaries;independent party;record certificates;systematic mutual verification;party leaks;private key;Certificates Authorities;secure SSL/TLS;checks;single party compromise;collaborative certificate management scheme;security analysis;secure Web-based connections","","3","","26","","5 Oct 2017","","","IEEE","IEEE Conferences"
"Studying Multi-threaded Behavior with TSViz","M. Nunes; H. Lalh; A. Sharma; A. Wong; S. Miucin; A. Fedorova; I. Beschastnikh","Comput. Sci., Univ. Fed. de Minas Gerais, Belo Horizonte, Brazil; Electr. & Comput. Eng., Univ. of British Columbia, Vancouver, BC, Canada; Electr. & Comput. Eng., Univ. of British Columbia, Vancouver, BC, Canada; Electr. & Comput. Eng., Univ. of British Columbia, Vancouver, BC, Canada; Electr. & Comput. Eng., Univ. of British Columbia, Vancouver, BC, Canada; Electr. & Comput. Eng., Univ. of British Columbia, Vancouver, BC, Canada; Univ. of British Columbia, Vancouver, BC, Canada","2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C)","3 Jul 2017","2017","","","35","38","Modern high-performing systems make extensive use of multiple CPU cores. These multi-threaded systems are complex to design, build, and understand. Debugging performance of these multi-threaded systems is especially challenging. This requires the developer to understand the relative execution of dozens of threads and their inter-dependencies, including data-sharing and synchronization behaviors. We describe TSViz, a visualization tool to help developers study and understand the activity of complex multi-threaded systems. TSviz depicts the partial order of concurrent events in a time-space diagram, and simultaneously scales this diagram according to the physical clock timestamps that tag each event. A developer can then interact with the visualization in several ways, for example by searching for events of interest, studying the distribution of critical sections across threads and zooming the diagram in and out. We overview TSviz design and describe our experience with using it to study a high-performance multi-threaded key-value store based on MongoDB. A video demo of TSViz is online: https://youtu.be/LpuiOZ3PJCk.","","978-1-5386-1589-8","10.1109/ICSE-C.2017.9","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7965251","performance debugging;concurrency;multi-threaded systems;tracing;log analysis;visualization","Tools;Instruction sets;Visualization;Debugging;Clocks;Synchronization","multi-threading;system monitoring","TSViz;visualization tool;multithreaded systems;time-space diagram;high-performance multithreaded key-value store;MongoDB","","1","","9","","3 Jul 2017","","","IEEE","IEEE Conferences"
"Analysis of Cloud Forensics Techniques for Emerging Technologies","S. A. Ali; S. Memon; F. Sahito","University of Sindh,Faculty of Engineering and Technology,Jamshoro,Pakistan; University of Sindh,Faculty of Engineering and Technology,Jamshoro,Pakistan; Graz University of Technology,Institute of Software Technology,Graz,Austria","2020 International Conference on Computing, Networking, Telecommunications & Engineering Sciences Applications (CoNTESA)","28 Dec 2020","2020","","","106","111","Cloud computing is the technology of the modern era having invaluable features that convince enterprises to adopt. It provides opportunities to the enterprises to compete with their opponents, to facilitate their customers, and grow up their day to day business by reducing cost and infrastructure management burden. Regardless of the enormous features offered by cloud computing, enterprises generally hesitate to adopt it due to the physical inaccessibility and deployment of their intellectual properties to a location that is unknown to them. On the other hand, the cyber-criminals sniffing around to find the loopholes to breach in, and compromise the confidentiality, integrity, and availability of the resources managed by Cloud Service Provider (CSP). The emergence of the Internet of Things (IoT) and Cyber-Physical Cloud Systems (CPCS) where the data is processed and stored in the cloud environment, demands the need to tighten up the security of the cloud itself. Identifying and tracing the source of a cyber-attack plays an integral role in the investigation process. Digital forensics in the cloud termed Cloud Forensics which boosts the confidence of the stack holders by reconstructing the past cloud events to give a hypothetical view of the crime scene and describe how the malicious activity had happened? The collection of evidence especially in the cloud environment is the most complex task due to its scattered architecture. Moreover, the collected evidence was used for the investigation to shield the cloud environment from a future security breach and bring the cybercriminal to the court of justice. In this paper, we have presented an analysis of cloud forensics techniques and their practical challenges/limitations to cope with cloud forensic investigators.","","978-1-7281-8488-3","10.1109/CoNTESA50436.2020.9302862","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9302862","Cloud computing;Cyber-attack;Security logs;Digital Forensics;Evidence collection;Investigation","Cloud computing;Digital forensics;Tools;Security;Object recognition;Distributed databases;Computer architecture","cloud computing;computer crime;digital forensics;security of data","cloud environment;digital forensics;cloud events;cloud forensics techniques;cloud forensic investigators;cloud computing;invaluable features;enterprises;day business;infrastructure management burden;enormous features;physical inaccessibility;cyber-criminals;Cloud Service Provider;Cyber-Physical Cloud Systems","","","","16","","28 Dec 2020","","","IEEE","IEEE Conferences"
"A norms mining approach to norms detection in multi-agent systems","M. A. Mahmoud; M. S. Ahmad; A. Ahmad; M. Z. Mohd Yusoff; A. Mustapha","College of Graduate Studies, Universiti Tenaga Nasional, Malaysia; College of Graduate Studies, Universiti Tenaga Nasional, Malaysia; College of Information Technology, Universiti Tenaga Nasional, Malaysia; College of Information Technology, Universiti Tenaga Nasional, Malaysia; Faculty of Computer Science & Information Technology, Universiti Putra Malaysia, Malaysia","2012 International Conference on Computer & Information Science (ICCIS)","10 Sep 2012","2012","1","","458","463","In this paper, we propose a norms mining technique for a visitor agent to detect the norms of a community of local agents to comply with the community's normative protocol. In this technique, the visitor agent is equipped with an algorithm, which detects the potential norms through the system's log file, interactions with the local agents, and observing the local agents in action. The visitor agent detects the norms from these sources depending on their availability. Due to security issues, access is prevented to one or more of these sources. The norms mining technique entails the process of data formatting and filtering and identifying the norms' components that contribute to the strength of the norms. The results of an example mining operation show that the technique is successful in discovering the potential norms.","","978-1-4673-1938-6","10.1109/ICCISci.2012.6297289","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6297289","Norms;Normative Systems;Intelligent Software Agents;Norms Detection;Norms Identification","Multiagent systems;Security;Area measurement;Data mining","data mining;information filtering;multi-agent systems;protocols","norms mining approach;norms detection;multiagent systems;norms mining technique;visitor agent;local agent community;community normative protocol;system log file;local agent interaction;data formatting;data filtering;norm component identification","","14","","27","","10 Sep 2012","","","IEEE","IEEE Conferences"
"Rapid-X - An FPGA Development Toolset Using a Custom Simulink Library for MTCA.4 Modules","P. Prędki; M. Heuer; Ł. Butkowski; K. Przygoda; H. Schlarb; A. Napieralski","Department of Microelectronics and Computer Science of the Lodz University of Technology, Poland; Deutsches Elektronen-Synchrotron, Germany; Deutsches Elektronen-Synchrotron, Germany; Department of Microelectronics and Computer Science of the Lodz University of Technology, Poland; Deutsches Elektronen-Synchrotron, Germany; Department of Microelectronics and Computer Science of the Lodz University of Technology, Poland","IEEE Transactions on Nuclear Science","12 Jun 2015","2015","62","3","940","946","The recent introduction of advanced hardware architectures such as the Micro Telecommunications Computing Architecture (MTCA) caused a change in the approach to implementation of control schemes in many fields. The development has been moving away from traditional programming languages ( C/C++), to hardware description languages (VHDL, Verilog), which are used in FPGA development. With MATLAB/Simulink it is possible to describe complex systems with block diagrams and simulate their behavior. Those diagrams are then used by the HDL experts to implement exactly the required functionality in hardware. Both the porting of existing applications and adaptation of new ones require a lot of development time from them. To solve this, Xilinx System Generator, a toolbox for MATLAB/Simulink, allows rapid prototyping of those block diagrams using hardware modelling. It is still up to the firmware developer to merge this structure with the hardware-dependent HDL project. This prevents the application engineer from quickly verifying the proposed schemes in real hardware. The framework described in this article overcomes these challenges, offering a hardware-independent library of components that can be used in Simulink/System Generator models. The components are subsequently translated into VHDL entities and integrated with a pre-prepared VHDL project template. Furthermore, the entire implementation process is run in the background, giving the user an almost one-click path from control scheme modelling and simulation to bit-file generation. This approach allows the application engineers to quickly develop new schemes and test them in real hardware environment. The applications may range from simple data logging or signal generation ones to very advanced controllers. Taking advantage of the Simulink simulation capabilities and user-friendly hardware implementation routines, the framework significantly decreases the development time of FPGA-based applications.","1558-1578","","10.1109/TNS.2015.2413673","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7110631","Control systems;DESY;European XFEL;feedback control;FLASH;hardware description languages;MTCA;Simulink;system generator;xilinx","Hardware;Field programmable gate arrays;Libraries;Software packages;Generators;Adaptive optics;Ultrafast optics","control systems;field programmable gate arrays;firmware;software prototyping","Rapid-X;FPGA development toolset;custom Simulink library;MTCA.4 modules;advanced hardware architecture;Micro Telecommunications Computing Architecture;hardware description languages;MATLAB;Xilinx System Generator;rapid prototyping;firmware;bit file generation","","2","","24","","20 May 2015","","","IEEE","IEEE Journals"
"Motion-Activated Facial Recognition-Based Electronic Residential Logbook using PCA Algorithm with Web Application","R. G. Maramba; G. V. Magwili; J. M. A. Labuac; J. R. S. Reyes; P. J. A. Tibayan","School of Electrical, Electronics and Computer Engineering, Mapúa University, Intramuros Manila, 1002, Philippines; School of Electrical, Electronics and Computer Engineering, Mapúa University, Intramuros Manila, 1002, Philippines; School of Electrical, Electronics and Computer Engineering, Mapúa University, Intramuros Manila, 1002, Philippines; School of Electrical, Electronics and Computer Engineering, Mapúa University, Intramuros Manila, 1002, Philippines; School of Electrical, Electronics and Computer Engineering, Mapúa University, Intramuros Manila, 1002, Philippines","2018 IEEE 10th International Conference on Humanoid, Nanotechnology, Information Technology,Communication and Control, Environment and Management (HNICEM)","14 Mar 2019","2018","","","1","6","This paper embarks on the development of an electronic residential logbook that is motion-activated and utilizes facial recognition as its biometrics authentication. This paper aims to access log information through a web application. The algorithm that was used for the facial recognition process is known as the Principal Component Analysis, a linear technique that is widely used in recognition and compression applications. Moreover, an application of PCA which is the Eigenface technique was used in capturing the most necessary factor in the facial recognition process. The hardware part of the electronic residential logbook is driven by AT89S52 microcontroller where a PIR sensor is connected. The PIR sensor is responsible for the motion activation of this design. The log record information can be viewed through a web application which reflects the time in/out of the residents. In conclusion, the researchers were able to meet all of their objectives and provided the expected output with a 13% error rate for the facial recognition system with only 5 training faces conducted for each resident.","","978-1-5386-7767-4","10.1109/HNICEM.2018.8666441","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8666441","Facial Recognition;Principal Component Analysis (PCA);AT89S52;PIR Sensor;Eigenface","Face recognition;Databases;Principal component analysis;Software;Light emitting diodes;Hardware;Cameras","biometrics (access control);face recognition;image motion analysis;principal component analysis","motion-activated facial recognition;electronic residential logbook;principal component analysis;PCA algorithm;facial recognition system;log record information;motion activation;PIR sensor;compression applications;facial recognition process;web application","","2","","17","","14 Mar 2019","","","IEEE","IEEE Conferences"
"Exploring data mining possibilities on computer based problem solving data","A. Pejić; P. S. Molcer","University of Szeged, Szeged, Hungary; SuboticaTech/Department of Informatics, Subotica, Serbia","2016 IEEE 14th International Symposium on Intelligent Systems and Informatics (SISY)","20 Oct 2016","2016","","","171","176","This article deals with the data mining process on a database containing the strategic steps made during complex problem solving in a computer-based assessment environment. The used database was constructed based on the log-files generated by the computer-based assessment software and published on-line by PISA. Preprocessing, attribute extraction and classification of the data by naïve Bayes classifier is presented. Evaluation parameter values such as the ROC, PR curves, the areas under these curves, as well as confusion matrices are given. Results indicated that the data contained in the database can be used for classification of the achievement in resolving the complex problems given in the PISA tasks.","1949-0488","978-1-5090-2866-5","10.1109/SISY.2016.7601491","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7601491","data mining;classification;n-grams;PISA","Data mining;Problem-solving;Databases;Computers;Software;Meteorology;Feature extraction","Bayes methods;data mining;pattern classification;problem solving","data mining;computer based problem solving data;complex problem solving;computer-based assessment software;PISA;attribute extraction;data classification;naïve Bayes classifier;ROC;PR curves;confusion matrices","","3","","13","","20 Oct 2016","","","IEEE","IEEE Conferences"
"On-Grid Photovoltaic System Power Monitoring with A Responsive Dashboard Design","H. Prasetyo; A. Nugroho","Politeknik ATMI Surakarta,Mechatronics Study Program,Surakarta,Indonesia; Politeknik ATMI Surakarta,Manufacture Design Study Program,Surakarta,Indonesia","2020 International Conference on ICT for Smart Society (ICISS)","4 Jan 2021","2020","CFP2013V-ART","","1","5","This research proposes the on-grid photovoltaic system power monitoring with a responsive dashboard design. The power monitoring utilizes open source and low-cost resources namely NodeMCU, MQTT, Node-RED, and the Raspberry Pi as a local server. A CT (Current Transformers) is used as an electric current sensor. The monitoring dashboard is created using the Node-RED UIbuilder node instead of using the default dashboard nodes module. There are eight displays of information presented on the dashboard, namely Real-Time Power, Runtime, Highest Power Record, Total Energy Savings, Total Cost Savings, Daily Savings Rate, Summary Last Update, and Power Monitoring Log. The power monitoring dashboard performs well on the widescreen and the mobile screen devices. The accuracy of power readings is tolerable.","2640-0545","978-1-6654-0422-8","10.1109/ICISS50791.2020.9307563","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9307563","Mobile Responsive Web;Internet of Things;Node-RED;PV Monitoring Application","Monitoring;Real-time systems;Internet of Things;Databases;Current transformers;Servers;Arrays","current transformers;electric sensing devices;energy conservation;photovoltaic power systems;power engineering computing;power grids;power system measurement;public domain software","responsive dashboard design;on-grid photovoltaic system power monitoring;open source resources;low-cost resources;Current Transformers;electric current sensor;Node-RED UIbuilder node;Real-Time Power;Highest Power Record;Total Energy Savings;Total Cost Savings;Power Monitoring Log;power monitoring dashboard;power readings;default dashboard node module;daily saving rate;summary last update;mobile screen devices;power reading accuracy;NodeMCU;MQTT;Node-RED;Raspberry Pi;local server","","","","14","","4 Jan 2021","","","IEEE","IEEE Conferences"
"A Visualization Technique for Access Patterns and Link Structures of Web Sites","M. Kawamoto; T. Itoh","Ochanomizu Univ., Tokyo, Japan; Ochanomizu Univ., Tokyo, Japan","2010 14th International Conference Information Visualisation","13 Sep 2010","2010","","","11","16","There have been two types of Web visualization techniques: visualization of Web sites themselves based on such as link structures or lexical contents, and visualization of browsers' behaviors. We think that integration of such two visualization techniques is very useful for Web site management, and therefore we are currently studying on visualization of access pattern and link structure on a single screen. This paper presents a Web visualization technique using our own multiple-category-embedded graph visualization technique. The presented technique constructs link structures using crawler software, and access patterns from access log files. It then integrates them and visualizes by our graph visualization technique. We expect that users can visually understand the relationship between access patterns and link structures, and utilize the knowledge for design and management of Web sites. This paper shows our case study and discusses typical access patterns we observed by the technique.","2375-0138","978-1-4244-7846-0","10.1109/IV.2010.11","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5571381","","Browsers;Web pages;Visualization;Layout;Software;Image color analysis","data visualisation;graph theory;Web sites","Web visualization technique;access patterns;link structures;web sites;graph visualization technique;crawler software","","4","","11","","13 Sep 2010","","","IEEE","IEEE Conferences"
"Monitoring design of dumper control system based on KingView","J. Yan","Anshan Normal University, Anshan 114016","2017 29th Chinese Control And Decision Conference (CCDC)","17 Jul 2017","2017","","","5879","5882","Taking an actual Car Dumper Control System as an example, the control system is composed of two leves, the first is to execute the PLC logic and the other is the CRT monitoring system which could record the process log, reports, process monitoring and so on. According to the function of the second level, this paper is concerned with the monitoring system design by the KingView so that the system man-machine interface is simple and friendly, easy to use.","1948-9447","978-1-5090-4657-7","10.1109/CCDC.2017.7978219","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7978219","Tipping machine;monitoring control systems;KingView","Automobiles;Monitoring;Control systems;Software;Real-time systems;Market research;Process control","automobiles;control system synthesis;man-machine systems;process monitoring;programmable controllers","KingView;CRT monitoring system;car dumper control system;process log;process monitoring;monitoring system design;PLC logic;man-machine interface","","","","12","","17 Jul 2017","","","IEEE","IEEE Conferences"
"Automated Bug Reporting System in Web Applications","Y. Sharma; Shatakshi; Palvika; A. Dagur; R. Chaturvedi","Krishna Engineering College, Department of Computer Science and Engineering, Ghaziabad, UP, India; Krishna Engineering College, Department of Computer Science and Engineering, Ghaziabad, UP, India; Krishna Engineering College, Department of Computer Science and Engineering, Ghaziabad, UP, India; Krishna Engineering College, Department of Computer Science and Engineering, Ghaziabad, UP, India; Krishna Engineering College, Department of Computer Science and Engineering, Ghaziabad, UP, India","2018 2nd International Conference on Trends in Electronics and Informatics (ICOEI)","2 Dec 2018","2018","","","1484","1488","Bug reporting system is the approach for tracking and reporting bugs of a web application. It is the system which filters the duplicate bugs and generates the bug report. Bug reporting system permits individual or group of developers to observe and scan the bugs in their product adequately. The bug reporting system can greatly raise the work rate and responsibilities of particular working stiff by accommodating a recorded progress and optimistic response for pleasing performance. Manually generated the bug report is a time-consuming process and it is very expensive to maintain. In this paper, we designed a framework that automatically generates the bug report. It provides the facility of report logging, reusable method and multiple browser support which helps to reduce the human effort and time requires to performing regression testing.","","978-1-5386-3570-4","10.1109/ICOEI.2018.8553850","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8553850","","Computer bugs;Testing;Software;Tools;Conferences;Selenium;Automation","Internet;online front-ends;program debugging;program testing;regression analysis","duplicate bugs;report logging;Web application;bug reporting system automation;tracking bugs;time-consuming process;regression testing;reusable method;multiple browser support","","","","12","","2 Dec 2018","","","IEEE","IEEE Conferences"
"On Defense and Detection of SQL SERVER Injection Attack","Q. Xue; P. He","Shannxi Coll. of Commun. Technol., Xi'an, China; Shannxi Coll. of Commun. Technol., Xi'an, China","2011 7th International Conference on Wireless Communications, Networking and Mobile Computing","10 Oct 2011","2011","","","1","4","The mechanism of SQL injection attack is introduced in this paper. Differing from the works of the predecessors, the authors categorize the injection attacks according to the characteristics of the injection codes. For the type of web databases with SQL Server as the backend, a DDL (Detection-Defense-Log) Model against SQL injection is created. Both the client computer and the server are included in the model. The model is intended to prevent as many attacks as possible and record the dangerous attack actions by deploying some smart program on the client computer and the server respectively, which can check the length and data type of the submitted variables, and detect the injection-sensitive characters and keywords.","2161-9654","978-1-4244-6252-0","10.1109/wicom.2011.6040534","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6040534","","Servers;Databases;Computers;Software;Internet;Security;Computational modeling","Internet;SQL","SQL server injection attack;predecessors;Web databases;DDL;detection-defense-log model;client computer;injection-sensitive characters;smart program","","4","","15","","10 Oct 2011","","","IEEE","IEEE Conferences"
"Service Mining: Using Process Mining to Discover, Check, and Improve Service Behavior","W. v. d. Aalst","Eindhoven University of Technology, Eindhoven and Queensland University of Technology, Brisbane","IEEE Transactions on Services Computing","9 Dec 2013","2013","6","4","525","535","Web services are an emerging technology to implement and integrate business processes within and across enterprises. Service orientation can be used to decompose complex systems into loosely coupled software components that may run remotely. However, the distributed nature of services complicates the design and analysis of service-oriented systems that support end-to-end business processes. Fortunately, services leave trails in so-called event logs and recent breakthroughs in process mining research make it possible to discover, analyze, and improve business processes based on such logs. Recently, the task force on process mining released the process mining manifesto. This manifesto is supported by 53 organizations and 77 process mining experts contributed to it. The active participation from end-users, tool vendors, consultants, analysts, and researchers illustrate the growing significance of process mining as a bridge between data mining and business process modeling. In this paper, we focus on the opportunities and challenges for service mining, i.e., applying process mining techniques to services. We discuss the guiding principles and challenges listed in the process mining manifesto and also highlight challenges specific for service-orientated systems.","1939-1374","","10.1109/TSC.2012.25","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6275430","Process mining;business process management;service discovery;conformance checking","Data mining;Web services;Computational modeling;Context;Analytical models;Customer services","business data processing;data mining;Web services","service mining;business process modeling;data mining;tool vendors;end-users;process mining experts;process mining manifesto;task force;event logs;end-to-end business processes;service-oriented systems;software components;complex systems;service orientation;enterprises;Web services;service behavior","","34","","39","","20 Aug 2012","","","IEEE","IEEE Journals"
"Visualizing Network Activity Using Parallel Coordinates","S. Tricaud; K. Nance; P. Saadé",NA; NA; NA,"2011 44th Hawaii International Conference on System Sciences","22 Feb 2011","2011","","","1","8","Detecting and analyzing the complex problems introduced by today's cybercriminal are challenging undertakings. System pirates are organized and exploit available machines worldwide to conduct their attacks. The attack patterns are complex, multi-variate, and, in the case of botnets, can generate a significant amount of traffic that is difficult to interpret. In order to understand these complex event structures and ascertain their possible correlations in multiple dimensions, a visualization method called parallel coordinates can be used. This paper introduces the basic theory behind parallel coordinates, and demonstrates the visualization of real-world examples of attacks observed through a month of Snort logs on a production server. The parallel coordinates-based visualization is accomplished using an open source visual intrusion detection system called Picviz, which can aid in the analysis of potentially malicious network traffic.","1530-1605","978-1-4244-9618-1","10.1109/HICSS.2011.488","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5718652","","Visualization;Data visualization;IP networks;Intrusion detection;Graphical user interfaces;Monitoring;Software","computational geometry;computer crime;data visualisation;public domain software;telecommunication traffic","network activity visualization;parallel coordinate;system pirate;attack pattern;cybercriminal botnet;snort log;production server;open source visual intrusion detection system;Picviz;potentially malicious network traffic","","8","","11","","22 Feb 2011","","","IEEE","IEEE Conferences"
"A tiny-capacitor-backed non-volatile buffer to reduce storage writes in smartphones","M. Son; J. Ahn; S. Yoo","Department of Electrical Engineering, Pohang University of Science and Technology (POSTECH) Pohang, Republic of Korea; Department of Electrical and Computer Engineering, Seoul National University (SNU) Seoul, Republic of Korea; Department of Computer Science and Engineering, Seoul National University (SNU) Seoul, Republic of Korea","2015 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)","19 Nov 2015","2015","","","21","29","Mobile storage writes are often dominated by writes to SQLite database files. Our characterization shows that they mostly consist of frequent overwrites with small new data (which we call small writes) and relatively infrequent writes with large data updates. In order to reduce writes to the Flash memory on smartphones, we propose exploiting these characteristics and present a low-cost nonvolatile write buffer for write coalescing. The key challenge in it is that the stringent resource constraints of mobile devices force the write buffer size to be minimized down to a single Flash page in order to reduce the overhead of SRAM buffer on the controller chip and a backing capacitor that maintains non-volatility of the buffer on power failure. As a solution to this problem, we propose three optimizations that make the best use of this small single-page nonvolatile write buffer. First, we propose managing only the difference between old and new data (i.e., differential logs) in the write buffer, based on the observation that small writes are frequent. Second, we develop a dynamic bypass scheme which judiciously bypasses overwrite-unfriendly pages from the write buffer. Third, we devise an incremental flush policy which controls the number of write buffer entries to be flushed according to the size of the newly written data. According to our experiments using four representative mobile applications on a real storage platform, OpenSSD, the proposed method gives average 69.5% and 64.5% reductions in Flash memory writes in single- and multi-application runs, respectively. In addition, our scheme introduces a very small cost into existing systems, including 8-18.5KB SRAM on the controller chip and a tiny capacitor occupying only 1.7% of eMMC package volume.","","978-1-4673-8321-9","10.1109/CODESISSS.2015.7331364","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7331364","Differential logging;single-page non-volatile write buffer;smartphone;SQLite;storage write reduction;write coalescing","Flash memories;Databases;Nonvolatile memory;Buffer storage;Smart phones;Mobile applications;Mobile communication","buffer circuits;flash memories;smart phones;SRAM chips","tiny-capacitor-backed non-volatile buffer;smartphones;mobile storage writes;mobile devices;SRAM buffer;dynamic bypass scheme;incremental flush policy;flash memory","","5","","13","","19 Nov 2015","","","IEEE","IEEE Conferences"
"Goal programming to optimize time and cost for each activity in port container handling","A. T. Rahman; R. Sarno; Y. A. Effendi","Department of Informatics, Faculty of Information and Communication Technology, Institut Teknologi Sepuluh Nopember, Surabaya 60111, Indonesia; Department of Informatics, Faculty of Information and Communication Technology, Institut Teknologi Sepuluh Nopember, Surabaya 60111, Indonesia; Department of Informatics, Faculty of Information and Communication Technology, Institut Teknologi Sepuluh Nopember, Surabaya 60111, Indonesia","2018 International Conference on Information and Communications Technology (ICOIACT)","30 Apr 2018","2018","","","866","871","The services of port in Indonesia are increasing from year to year. The traffic of port is increasingly crowded with the number of boats coming to load and unload processes. A lot of ship queues result in delay when exceeding due date from the date of the agreement will cause the higher cost to be issued which is called demurrage. To reduce the costs incurred and the length of queue time on the scheduling at the port, we used Goal Programming (GP). Goal Programming is an algorithm that solves linear programming problems using mathematical formulation to get solutions in getting goals. In this study, optimizing 43 activities and 7 trace variations on loading and unloading activities of container terminal services from events log. The goal programming model from 43 activities has been implemented using Lingo software to obtain objective value in achieving the objectives of each activity used to determine activities that have a major influence on the delay in loading and unloading activities. The result of Goal Programming is that there are two activities which have very high deviation, therefore both of activities are evaluated in performance on container activity.","","978-1-5386-0954-5","10.1109/ICOIACT.2018.8350808","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8350808","event log;goal programming;port container handling;time and cost optimization","Programming;Tools;Containers;Optimization;Mathematical model;Production;Information and communication technology","linear programming;loading;scheduling;sea ports;ships;transportation;unloading","linear programming problems;loading;unloading activities;container terminal services;goal programming model;container activity;port container handling;ship queues result;queue time;demurrage;mathematical formulation","","2","","14","","30 Apr 2018","","","IEEE","IEEE Conferences"
"AppWrapper: Patching Security Functions with Dynamic Policy on Your Insecure Android Apps","S. Lee; S. Kim; S. Kim; S. Jin","Inf. Security Eng., Univ. of Sci. & Technol., Daejeon, South Korea; Inf. Security Res. Inst., Electron. & Telecommun. Res. Inst., Daejeon, South Korea; Inf. Security Res. Inst., Electron. & Telecommun. Res. Inst., Daejeon, South Korea; Inf. Security Res. Inst., Electron. & Telecommun. Res. Inst., Daejeon, South Korea","2018 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)","18 Nov 2018","2018","","","36","41","Android provides a security system with permission control, but there are a number of vulnerabilities that have excessive permission rights and a large number of per-permission related APIs. To address these vulnerabilities, permission control studies have been conducted on APIs that are at risk of compromising user privacy. However, it is impossible to add a new security function to an insecure application, and there is a disadvantage that an overhead occurs in the progress of the app because the user is required to permit permission in real time and the users' convenience is decreased. In this paper, we propose an AppWrapper toolkit. The toolkit can add security functions to the user/administrator's desired locations (method level in activities) of an insecure app using the appwrapping technique. And, using dynamic policy management, it is easy to apply secure policies without adding security functions again. In addition, by providing a real-time app log function that considers the convenience of users, it is possible to confirm the location where the security function is required according to the progress flow of the insecure app, and to create a policy file by setting the policy. Experiments on commercial apps have shown 100% success rate, except for apps with built-in security and Android apps. On the average, it took 1.86 seconds to add the security function through the proposed framework, and the file size increased by about 2.11%, indicating that the security function can be added in a short time with the increase of the minimum file size.","","978-1-5386-9443-5","10.1109/ISSREW.2018.00-34","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8539160","Appwrapping, Dynamic Policy, Mobile Security","Security;Real-time systems;Java;Reflection;Registers;Privacy;Monitoring","Android (operating system);application program interfaces;authorisation;data privacy;mobile computing","insecure Android apps;security system;per-permission related APIs;security function;real-time app log function;permission control;permission rights;user privacy compromise;AppWrapper toolkit","","1","","10","","18 Nov 2018","","","IEEE","IEEE Conferences"
"Pattern detection in unstructured data: An experience for a virtualized IT infrastructure","M. A. Marvasti; A. V. Poghosyan; A. N. Harutyunyan; N. M. Grigoryan",VMware; VMware; VMware; VMware,"2013 IFIP/IEEE International Symposium on Integrated Network Management (IM 2013)","1 Aug 2013","2013","","","1048","1053","Data-agnostic management of today's virtualized and cloud IT infrastructures motivates statistical inference from unstructured or semi-structured data. We introduce a universal approach to the determination of statistically relevant patterns in unstructured data, and then showcase its application to log data of a Virtual Center (VMware's virtualization management software). The premise of this study is that the unstructured data can be converted into events, where an event is defined by time, source, and a series of attributes. Every event can have any number of attributes but all must have a time stamp and optionally a source of origination (be it a server, a location, a business process, etc.) The statistical relevance of the data can then be made clear via determining the joint and prior probabilities of events using a discrete probability computation. From this we construct a Directed Virtual Graph with nodes representing events and the branches representing the conditional probabilities between two events. Employing information-theoretic measures the graphs are reduced to a subset of relevant nodes and connections. Moreover, the information contained in the unstructured data set is extracted from these graphs by detecting particular patterns of interest.","1573-0077","978-3-901882-50-0","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6573128","Fault management;unstructured data;pattern detection;event correlation;directed graph","Correlation;Joints;Data mining;Servers;Probability;Mutual information;Nickel","directed graphs;inference mechanisms;pattern recognition;probability;statistical analysis;virtualisation","virtualized IT infrastructure;data-agnostic management;cloud IT infrastructures;semistructured data;statistical inference;virtual center;VMware virtualization management software;discrete probability computation;directed virtual graph;conditional probability;information-theoretic measures;unstructured data set;pattern detection","","1","1","6","","1 Aug 2013","","","IEEE","IEEE Conferences"
"Secure session on mobile: An exploration on combining biometric, trustzone, and user behavior","T. Feng; N. DeSalvo; L. Xu; X. Zhao; X. Wang; W. Shi","Computer Science Department, University of Houston, USA; Computer Science Department, University of Houston, USA; Computer Science Department, University of Houston, USA; Computer Science Department, University of Houston, USA; Computer Science Department, University of Houston, USA; Computer Science Department, University of Houston, USA","6th International Conference on Mobile Computing, Applications and Services","29 Jan 2015","2014","","","206","215","With the rise of Internet connected mobile devices, applications have migrated from PCs to mobile computing platforms. An important aspect, payment processing, faces new security challenges from these developments. Inasmuch, these advancements demand efforts from researchers and industry to meet increasing security needs. Threats can ensue from data loss, theft from lost, stolen, or decommissioned devices, information-stealing malware, and password peeping. We propose a secure framework for sensitive session driven applications which combines biometric-based continuous and implicit tracking of user identities, and TrustZone. This framework is accomplished through monitoring fingerprint authentication logs as well as detecting events when the phone has left the user's hands, all while in TrustZone, a platform for secure computation and storage on mobile devices. This solution leverages multiple onboard sensors as well as the ARM architecture to accomplish these feats. We conducted two user-studies acquiring smartphone users' usage statistics to investigate security and usability needs of our identity-tracking solution. To monitor these subtle gestures in real-world uncontrolled environments, multi-session data collection has been conducted to iteratively improve system performance. The evaluation results have demonstrated the feasibility of this framework as a secure session-based payment system.","","978-1-63190-024-2","10.4108/icst.mobicase.2014.257767","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7026301","TrustZone;Secure Session;Biometric;Sensor Fusion;User Behavior","Sensors;Monitoring;Authentication;Web services;Smart phones;Switches","fingerprint identification;Internet;invasive software;mobile computing;trusted computing","trustzone;user behavior;Internet connected mobile device;mobile computing platform;payment processing;security need;data loss;decommissioned device;information-stealing malware;password peeping;sensitive session driven application;biometric-based continuous tracking;implicit tracking;user identity;fingerprint authentication log monitoring;event detection;secure computation;secure storage;onboard sensor;ARM architecture;identity-tracking solution;multisession data collection;system performance;secure session-based payment system","","1","2","27","","29 Jan 2015","","","IEEE","IEEE Conferences"
"An IoT Honeynet Based on Multiport Honeypots for Capturing IoT Attacks","W. Zhang; B. Zhang; Y. Zhou; H. He; Z. Ding","School of Science and Technology, Harbin Institute of Technology, Harbin, China; Cyberspace Security Research Center, Peng Cheng Laboratory, Shenzhen, China; Cyberspace Security Research Center, Peng Cheng Laboratory, Shenzhen, China; School of Science and Technology, Harbin Institute of Technology, Harbin, China; School of Science and Technology, Harbin Institute of Technology, Harbin, China","IEEE Internet of Things Journal","13 May 2020","2020","7","5","3991","3999","Internet of Things (IoT) devices are vulnerable against attacks because of their limited network resources and complex operating systems. Thus, a honeypot is a good method of capturing malicious requests and collecting malicious samples but is rarely used on the IoT. Accordingly, this article implements three kinds of honeypots to capture malicious behaviors. First, on the basis of the CVE-2017-17215 vulnerability, we implement a medium-high interaction honeypot that can simulate a specific series of router UPnP services. It has functions, such as service simulation, log recording, malicious sample download, and service self-check. Second, given the limited details available for the simulated UPnP service and to help the honeypot respond to unrecognizable malicious requests, we use the actual IoT device firmware that matches the vulnerability to build a high-interaction honeypot. In addition, we investigate the most exposed SOAP service ports and design corresponding multiport honeypot to improve the capacity of the honeynet, providing a hybrid service from a real device and simulating honeypots. The Docker in the honeynet, which reduces the volume of the honeypot and realizes the rapid deployment of the honeynet, encapsulates all these honeypots. Moreover, the honeynet control center is simultaneously designed to distribute commands and transfer files to each physical node in the honeynet. We implemented the proposed honeynet system and deployed it in practice. We have successfully caught many unknown malicious attacks excluded in the VT, which proved the effectiveness of the proposed framework.","2327-4662","","10.1109/JIOT.2019.2956173","Key Research and Development Program for Guangdong Province; National Key Research and Development Plan; National Natural Science Foundation of China (NSFC); ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8915712","Honeypot;honeynet;Internet of Things (IoT);multiport;SOAP","Internet of Things;Protocols;Simple object access protocol;Computer hacking;Botnet;Computer architecture","computer network security;firmware;Internet of Things;invasive software;operating systems (computers)","IoT honeynet;multiport honeypots;Internet of Things devices;complex operating systems;malicious samples;malicious behaviors;CVE-2017-17215 vulnerability;medium-high interaction honeypot;router UPnP services;service simulation;malicious sample download;service self-check;simulated UPnP service;unrecognizable malicious requests;actual IoT device firmware;high-interaction honeypot;exposed SOAP service ports;hybrid service;honeynet control center;honeynet system;unknown malicious attacks;IoT attacks capturing;honeypots simulation","","3","","37","IEEE","27 Nov 2019","","","IEEE","IEEE Journals"
"An efficient common substrings algorithm for on-the-fly behavior-based malware detection and analysis","J. C. Acosta; H. Mendoza; B. G. Medina","U.S. Army Research Laboratory, White Sands Missile Range, NM 88002-5513; U.S. Army Research Laboratory, White Sands Missile Range, NM 88002-5513; U.S. Army Research Laboratory, White Sands Missile Range, NM 88002-5513","MILCOM 2012 - 2012 IEEE Military Communications Conference","28 Jan 2013","2012","","","1","6","It is well known that malware (worms, botnets, etc...) thrive on communication systems. The process of detecting and analyzing malware is very latent and not well-suited for real-time application, which is critical especially for propagating malware. For this reason, recent methods identify similarities among malware dynamic trace logs to extract malicious behavior snippets. These snippets can then be tagged by a human analyst and be used to identify malware on-the-fly. A major problem with these methods is that they require extensive processing resources. This is especially due to the large amount of malware released each year (upwards of 17 million new instances in 2011). In this paper, we present an efficient algorithm for identifying common substrings in dynamic trace events of malware collections. The algorithm finds common substrings between malware pairs in theoretical linear time by using parallel processing. The algorithm is implemented in the CUDA and results show a performance increase of up to 8 times compared to previous implementations.","2155-7586","978-1-4673-1731-3","10.1109/MILCOM.2012.6415819","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6415819","","Malware;Graphics processing units;Java;Instruction sets;Heuristic algorithms;Algorithm design and analysis;Runtime","invasive software;military communication;telecommunication security","common substrings algorithm;on-the-fly behavior-based malware detection;communication systems;malware dynamic trace logs;malicious behavior snippets;human analyst;processing resources;dynamic trace events;malware collections;malware pairs;parallel processing","","2","","16","","28 Jan 2013","","","IEEE","IEEE Conferences"
"Context-based Multimedia Content Management Framework and Its Location-aware Web Applications","S. Kumamoto; S. Takano; Y. Okada","Grad. Sch. of ISEE, Kyushu Univ., Fukuoka, Japan; Grad. Sch. of ISEE, Kyushu Univ., Fukuoka, Japan; Grad. Sch. of ISEE, Kyushu Univ., Fukuoka, Japan","2010 International Conference on P2P, Parallel, Grid, Cloud and Internet Computing","13 Jan 2011","2010","","","98","104","This paper treats a context-based multimedia content management system (MCMS), whose various types of contents are easily gathered from everywhere at anytime using mobile phones, and stored in a web server as a multimedia database. In this paper, the authors propose the framework of software components for developing location-aware web applications that use multimedia contents stored in the multimedia database on the web server. The authors also introduce several practical location-aware web applications, e.g., Google Maps based Sight-seeing information system, Google Maps based web Natural Science Dictionary, etc. that are already developed using the proposed components. One of the prospective applications of the proposed framework is Google Maps based Life-log system. Data stored by a lot of users using their mobile phones are regarded as their Life-log data because the data includes location data by GPS, date/time data, other related multimedia data such as picture images, movies, recorded sounds and texts. By analyzing them using any data-mining methods, it is possible to extract activity patterns of the users those are very useful for various web services like recommendation systems. As a theoretical aspect of the paper, the authors discuss data analyzing techniques to be used in the proposed framework.","","978-1-4244-8538-3","10.1109/3PGCIC.2010.20","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5664699","Multimedia;Content Management System;Life-log System;Recommendation System;Web Applications","Google;Multimedia communication;Multimedia databases;Mobile handsets;Web server;Global Positioning System","cartography;content management;mobile computing;multimedia databases;recommender systems;Web services","context-based multimedia content management system;location-aware Web applications;mobile phones;Web server;multimedia database;Google Maps based sight-seeing information system;Google Maps based Web natural science dictionary;Google Maps based life-log system;recommendation systems;Web services","","2","","10","","13 Jan 2011","","","IEEE","IEEE Conferences"
"Scalable Routing for Topic-Based Publish/Subscribe Systems Under Fluctuations","V. Turau; G. Siegemund","Inst. of Telematics, Hamburg Univ. of Technol., Hamburg, Germany; Inst. of Telematics, Hamburg Univ. of Technol., Hamburg, Germany","2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS)","17 Jul 2017","2017","","","1608","1617","The loose coupling and the inherent scalability make publish/subscribe systems an ideal candidate for event-driven services for wireless networks using low power protocols such as IEEE 802.15.4. This work introduces a distributed algorithm to build and maintain a routing structure for such networks. The algorithm dynamically maintains a multicast tree for each node. While previous work focused on minimizing these trees we aim to keep the effort to maintain them in case of fluctuations of subscribers low. The multicast trees are implicitly defined by a novel structure called augmented virtual ring. The main contribution is a distributed algorithm to build and maintain this augmented virtual ring. Maintenance operations after sub-and unsubscriptions require message exchange in a limited region only. We compare the average lengths of the constructed forwarding paths with an almost ideal approach. As a result of independent interest we present a distributed algorithm using messages of size O(log n) for constructing virtual rings of graphs that are on average shorter than rings based on depth first search.","1063-6927","978-1-5386-1792-2","10.1109/ICDCS.2017.27","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7980098","Pub/Sub Systems;Virtual ring routing;Fluctuation;Distributed algorithm","Peer-to-peer computing;Routing;Overlay networks;Wireless networks;Distributed algorithms;Maintenance engineering;Fluctuations","computational complexity;distributed algorithms;fluctuations;message passing;middleware;minimisation;network routing;software maintenance;trees (mathematics)","routing structure;topic-based publish/subscribe systems;fluctuations;event-driven services;wireless networks;low power protocols;distributed algorithm;multicast tree;trees minimization;augmented virtual ring;maintenance operations;message exchange;O(log n)","","3","","25","","17 Jul 2017","","","IEEE","IEEE Conferences"
"Scenario-Based Method for Business Process Analysis and Improvement in SOA","J. Wang; L. Jiang; H. Cai","Sch. of Software, Shanghai Jiao Tong Univ., Shanghai, China; Sch. of Software, Shanghai Jiao Tong Univ., Shanghai, China; Sch. of Software, Shanghai Jiao Tong Univ., Shanghai, China","2014 IEEE 11th International Conference on e-Business Engineering","11 Dec 2014","2014","","","19","25","Business Process Management is often associated with software to manage, control, and support operational processes. And in order to meet dynamic and new business needs with flexible information technology solutions, more and more enterprises intend to build their own IT infrastructure under Service Oriented Architecture. Currently analyzing and refining the existing business process models for configuring enterprise information system is a hot area of research. In our approach, we apply the process mining technology to the event logs to discover the scenarios, each of which mainly consists of task originators, sub-process (ordering of service invocations) and business objects. Based on composition of these scenarios, the enterprise is able to redesign the process models and combine closely the business process with the services provided by different application systems in SOA. When new business requirements emerge, solution designers can devise a flexibly composite process that makes the best use of existing scenarios and glue the scenarios together with least augmentation or modification.","","978-1-4799-6563-2","10.1109/ICEBE.2014.16","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6982054","SOA;business process model;process mining;scenario discovery","Business;Unified modeling language;Process control;Data mining;Service-oriented architecture;Information systems;Measurement","business data processing;data mining;information systems;service-oriented architecture","business process management;flexible information technology solutions;IT infrastructure;service oriented architecture;enterprise information system;process mining technology;business objects;SOA;business requirements","","2","","19","","11 Dec 2014","","","IEEE","IEEE Conferences"
"Trrack: A Library for Provenance-Tracking in Web-Based Visualizations","Z. Cutler; K. Gadhave; A. Lex",University of Utah; University of Utah; University of Utah,"2020 IEEE Visualization Conference (VIS)","1 Feb 2021","2020","","","116","120","Provenance-tracking is widely acknowledged as an important feature of visualization systems. By tracking provenance data, visualization designers can provide a wide variety of functionality, ranging from action recovery (undo/redo), reproducibility, collaboration and sharing, to logging in support of quantitative and longitudinal evaluation. However, no widely used library that can provide that functionality is current available. As a consequence, visualization designers either develop ad hoc solutions that are rarely comprehensive, or do not track provenance at all. In this paper, we introduce a web-based software library - Trrack - that is designed for easy integration in existing or future visualization systems. Trrack supports a wide range of use cases, from simple action recovery, to capturing intent and reasoning, and can be used to share states with collaborators and store provenance on a server. Trrack also includes an optional provenance visualization component that supports annotation of states and aggregation of events.","","978-1-7281-8014-4","10.1109/VIS47514.2020.00030","National Science Foundation; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9331264","Human-centered computing;Visualization;Visualization systems and tools","Visualization;Software libraries;Annotations;Data visualization;Collaboration;Tools;Servers","data integrity;data visualisation;Internet;software libraries","Web based visualization;Trrack;provenance visualization;Web based software library;provenance data tracking","","","","20","","1 Feb 2021","","","IEEE","IEEE Conferences"
"Performance Evaluation of Load Balanced Web Proxies","S. Ngamsuriyaroj; P. Rattidham; I. Rassameeroj; P. Wongbuchasin; N. Aramkul; S. Rungmano","Fac. of Inf. & Commun. Technol., Mahidol Univ., Nakhon Pathom, Thailand; Fac. of Inf. & Commun. Technol., Mahidol Univ., Nakhon Pathom, Thailand; Fac. of Inf. & Commun. Technol., Mahidol Univ., Nakhon Pathom, Thailand; Fac. of Inf. & Commun. Technol., Mahidol Univ., Nakhon Pathom, Thailand; Fac. of Inf. & Commun. Technol., Mahidol Univ., Nakhon Pathom, Thailand; Fac. of Inf. & Commun. Technol., Mahidol Univ., Nakhon Pathom, Thailand","2011 IEEE Workshops of International Conference on Advanced Information Networking and Applications","5 May 2011","2011","","","746","750","A large university usually has many units that may locate far apart, and there are many proxy servers installed to give web access services to those units individually. However, some proxies may face the performance bottleneck when many users access the Internet simultaneously since such servers have limited capacity and could not handle high workload whereas the workload cannot be transferred to other proxies. This paper proposes an efficient load balancing technique called Shoot-Lowest Load which simulates the cache size at each proxy by counting the number and the size of log records generated by Squid proxy software in order to approximate the current workload at each proxy. We evaluate our work and compare it with the other two methods: Super Proxy and Time Round-Robin. Two metrics measured are the number of requests and the request size. In addition, the real proxy traffic is simulated as the testing dataset. The experimental results showed that our proposed method gave the best load balancing since it can closely estimate the current workload of each server so that a request will be sent to the proxy server with the lowest load.","","978-1-61284-829-7","10.1109/WAINA.2011.129","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5763593","Web proxy server;Load balancing method;Performance evaluation","Servers;Load management;Internet;Browsers;Software;Monitoring;Measurement","computer network performance evaluation;Internet;network servers;resource allocation;telecommunication traffic","performance evaluation;load balanced Web proxies;proxy servers;Web access services;load balancing technique;shoot-lowest load;Squid proxy software;super proxy;time round-robin;proxy traffic;Internet","","","","12","","5 May 2011","","","IEEE","IEEE Conferences"
"Unprotected Computing: A Large-Scale Study of DRAM Raw Error Rate on a Supercomputer","L. Bautista-Gomez; F. Zyulkyarov; O. Unsal; S. McIntosh-Smith","Barcelona Supercomput. Center, Barcelona, Spain; Barcelona Supercomput. Center, Barcelona, Spain; Barcelona Supercomput. Center, Barcelona, Spain; Univ. of Bristol, Bristol, UK","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis","16 Mar 2017","2016","","","645","655","Supercomputers offer new opportunities for scientific computing as they grow in size. However, their growth also poses new challenges. Resilience has been recognized as one of the most pressing issues to solve for extreme scale computing. Transistor scaling in the single-digit nanometer era and power constraints might dramatically increase the failure rate of next generation machines. DRAM errors have been analyzed in the past for different supercomputers but those studies are usually based on job scheduler logs and counters produced by hardware-level error correcting codes. Consequently, little is known about errors escaping hardware checks, which lead to silent data corruption. This work attempts to fill that gap by analyzing memory errors for over a year on a cluster with about 1000 nodes featuring low-power memory without error correction. The study gathered millions of events recording detailed information of thousands of memory errors, many of them corrupting multiple bits. Several factors are analyzed, such as temporal and spatial correlation between errors, but also the influence of temperature and even the position of the sun in the sky. The study showed that most multi-bit errors corrupted non-adjacent bits in the memory word and that most errors flipped memory bits from 1 to 0. In addition, we observed thousands of cases of multiple single-bit errors occurring simultaneously in different regions of the memory. These new observations would not be possible by simply analyzing error correction counters on classical systems. We propose several directions in which the findings of this study can help the design of more reliable systems in the future.","2167-4337","978-1-4673-8815-3","10.1109/SC.2016.54","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7877133","","Random access memory;Error correction codes;Memory management;Prototypes;Supercomputers;Error analysis;Hardware","DRAM chips;error analysis;fault tolerant computing;parallel machines;software reliability","unprotected computing;DRAM raw error rate;supercomputer;resilience;extreme scale computing;transistor scaling;hardware checks;silent data corruption;memory errors;multibit errors;high-performance computing;fault tolerance research","","31","","33","","16 Mar 2017","","","IEEE","IEEE Conferences"
"Measuring Resiliency through Field Data: Techniques, Tools and Challenges","A. Pecchia; M. Cinque; V. Mendiratta","Critiware S.r.l., Italy; DIETI, Federico II Univ. of Naples, Naples, Italy; Bell Labs., Nokia, Murray Hill, NJ, USA","2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshop (DSN-W)","3 Oct 2016","2016","","","265","265","Data collected under real workload conditions provide valuable information about the stresses the systems encounter and their responses to them. Textual/numeric data and log files produced by applications, operating systems, networks, and other monitoring sources play a key role for assessing system reliability and resiliency properties. Practitioners, academia, and industry strongly recognize the inherent value of log data. Data-driven evaluation deepens our understanding of the system dependability behavior, and enables stronger design and better monitoring strategies.The role of log files and data for measuring the dependability of production systems is recognized since many years. Seminal contributions date back to the late 70s, with studies on VAX mainframes [1]. Today, these studies are assuming particular relevance for failure analysis and prediction in industrial systems and networks [2], [3]; logs are the primary source of data available to gain insight on runtime issues in these systems. The understanding that can be gained from logs on today's systems enables improved design and better monitoring and failure prediction strategies for future systems. However, in spite of recent advances, data-driven reliability evaluation still presents challenges due to the scale, complexity and diversity of applications.","","978-1-5090-3688-2","10.1109/DSN-W.2016.52","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7575394","","Tutorials;Reliability;Monitoring;Data models;Wireless networks;Conferences;Industries","data handling;software reliability","field data;measuring resiliency;workload conditions;operating systems;monitoring sources;resiliency properties;reliability properties;seminal contributions;VAX mainframes;failure prediction;data driven reliability evaluation","","","","3","","3 Oct 2016","","","IEEE","IEEE Conferences"
"A Hybrid Intrusion Detection System for Cloud Computing Environments","M. Jelidi; A. Ghourabi; K. Gasmi","University of Kairouan, Kairouan, Tunisia; University of Sousse, Sousse, Tunisia; Jouf University, Al-Jouf, Saudi Arabia","2019 International Conference on Computer and Information Sciences (ICCIS)","16 May 2019","2019","","","1","6","This article presents a model to protect the cloud by providing a hybrid solution based on the distribution of intrusion detectors and the centralization of alerts for management purposes. The purpose of our approach is to protect the most important layers of the cloud using intrusion detection systems. Each layer (e.g network layer, application layer,...) has its properties that makes it different from other layers. This leads us to use specific intrusion detectors for each layer. The proposed detection model is segmented to two zones. The first zone is equipped with signature-based detectors, and the second zone is equipped with anomaly-based detectors. In the first zone we target previously known attacks, in the second zone we seek to discover previously unknown malicious events on the application layer. This article shows that it is possible to fully protect the cloud by using both signature and anomaly based detection using open source software, and provides the cloud service provider with a complete model to protect, monitor and manage the cloud by visualizing and correlating logs and alerts.","","978-1-5386-8125-1","10.1109/ICCISci.2019.8716422","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8716422","Intrusion Detection System;Cloud Computing;Signature-based detection;Anomaly-based detection","Cloud computing;Intrusion detection;Monitoring;Detectors;Solid modeling;Switches","cloud computing;public domain software;security of data","hybrid intrusion detection system;cloud computing environments;hybrid solution;management purposes;signature-based detectors;anomaly based detection;cloud service provider;malicious events;open source software","","1","","11","","16 May 2019","","","IEEE","IEEE Conferences"
"StraightTaint: Decoupled offline symbolic taint analysis","J. Ming; D. Wu; J. Wang; G. Xiao; P. Liu","College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA 16802, USA; College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA 16802, USA; College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA 16802, USA; College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA 16802, USA; College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA 16802, USA","2016 31st IEEE/ACM International Conference on Automated Software Engineering (ASE)","6 Oct 2016","2016","","","308","319","Taint analysis has been widely applied in ex post facto security applications, such as attack provenance investigation, computer forensic analysis, and reverse engineering. Unfortunately, the high runtime overhead imposed by dynamic taint analysis makes it impractical in many scenarios. The key obstacle is the strict coupling of program execution and taint tracking logic code. To alleviate this performance bottleneck, recent work seeks to offload taint analysis from program execution and run it on a spare core or a different CPU. However, since the taint analysis has heavy data and control dependencies on the program execution, the massive data in recording and transformation overshadow the benefit of decoupling. In this paper, we propose a novel technique to allow very lightweight logging, resulting in much lower execution slowdown, while still permitting us to perform full-featured offline taint analysis. We develop StraightTaint, a hybrid taint analysis tool that completely decouples the program execution and taint analysis. StraightTaint relies on very lightweight logging of the execution information to reconstruct a straight-line code, enabling an offline symbolic taint analysis without frequent data communication with the application. While StraightTaint does not log complete runtime or input values, it is able to precisely identify the causal relationships between sources and sinks, for example. Compared with traditional dynamic taint analysis tools, StraightTaint has much lower application runtime overhead.","","978-1-4503-3845-5","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7582768","Taint analysis;Decoupling;Offline;Symbolic taint analysis","Runtime;Security;Performance analysis;Reverse engineering;Registers;Malware","program diagnostics;security of data","StraightTaint;decoupled offline symbolic taint analysis;ex post facto security applications;attack provenance investigation;computer forensic analysis;reverse engineering;program execution strict coupling;taint tracking logic code;lightweight logging;full-featured offline taint analysis;hybrid taint analysis tool;straight-line code reconstruction","","","","54","","6 Oct 2016","","","IEEE","IEEE Conferences"
"Architectural Support for NVRAM Persistence in GPUs","S. Chen; L. Liu; W. Zhang; L. Peng","Division of Electrical and Computer Engineering, Louisiana State University, Baton Rouge, LA, USA; SKLCA, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Software School, Fudan University, Shanghai, China; Division of Electrical and Computer Engineering, Louisiana State University, Baton Rouge, LA, USA","IEEE Transactions on Parallel and Distributed Systems","22 Jan 2020","2020","31","5","1107","1120","Non-volatile Random Access Memories (NVRAM) have emerged in recent years to bridge the performance gap between the main memory and external storage devices, such as Solid State Drives (SSD). In addition to higher storage density, NVRAM provides byte-addressability, higher bandwidth, near-DRAM latency, and easier access compared to block devices such as traditional SSDs. This enables new programming paradigms taking advantage of durability and larger memory footprint. With the range and size of GPU workloads expanding, NVRAM will present itself as a promising addition to GPU's memory hierarchy. To utilize the nonvolatility of NVRAMs, programs should allow durable stores, maintaining consistency through a power loss event. This is usually done through a logging mechanism that works in tandem with a transaction execution layer which can consist of a transactional memory or a locking mechanism. Together, this results in a transaction processing system that preserves the ACID properties. GPUs are designed with high throughput in mind, leveraging high degrees of parallelism. Transactional memory proposals enable fine-grained transactions at the GPU thread-level. However, with lower write bandwidths compared to that of DRAMs, using NVRAM as-is may yield sub-optimal overall system performance when threads experience long latency. To address this problem, we propose using Helper Warps to move persistence out of the critical path of transaction execution, alleviating the impact of latencies. Our mechanism achieves a speedup of 4.4 and 1.5 under bandwidth limits of 1.6 GB/s and 12 GB/s and is projected to maintain speed advantage even when NVRAM bandwidth gets as high as hundreds of GB/s in certain cases. Due to the speedup, our proposed method also results in reduction in overall energy consumption.","1558-2183","","10.1109/TPDS.2019.2960233","National Science Foundation; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8935351","NVRAM;persistence;GPUs;helper warps","Random access memory;Nonvolatile memory;Bandwidth;Graphics processing units;Hardware;Performance evaluation;Instruction sets","cache storage;DRAM chips;flash memories;graphics processing units;integrated circuit design;transaction processing","ACID properties;DRAMs;Helper Warps;locking mechanism;logging mechanism;power loss event;system performance;GPU memory hierarchy;GPU workloads;memory footprint;programming paradigms;block devices;byte-addressability;storage density;Solid State Drives;external storage devices;performance gap;Nonvolatile Random Access Memories;NVRAM persistence;architectural support;NVRAM bandwidth;GPU thread-level;fine-grained transactions;transactional memory;transaction processing system;transaction execution layer;durable stores;bit rate 1.6 Gbit/s;bit rate 12.0 Gbit/s;SSDs","","2","","49","IEEE","17 Dec 2019","","","IEEE","IEEE Journals"
"An open platform for full body interactive sonification exergames","S. Ghisio; P. Coletta; S. Piana; P. Alborno; G. Volpe; A. Camurri; L. Primavera; C. Ferrari; C. M. Guenza; P. Moretti; V. Bergamaschi; A. Ravaschio","DIBRIS, Casa Paganini-InfoMus, University of Genoa, 16145, Genova, Italy; DIBRIS, Casa Paganini-InfoMus, University of Genoa, 16145, Genova, Italy; DIBRIS, Casa Paganini-InfoMus, University of Genoa, 16145, Genova, Italy; DIBRIS, Casa Paganini-InfoMus, University of Genoa, 16145, Genova, Italy; DIBRIS, Casa Paganini-InfoMus, University of Genoa, 16145, Genova, Italy; DIBRIS, Casa Paganini-InfoMus, University of Genoa, 16145, Genova, Italy; Physical Medicine and, Rehabilitation Unit, Inst. G. Gaslini, 16145, Genova, Italy; Physical Medicine and, Rehabilitation Unit, Inst. G. Gaslini, 16145, Genova, Italy; Physical Medicine and, Rehabilitation Unit, Inst. G. Gaslini, 16145, Genova, Italy; Physical Medicine and, Rehabilitation Unit, Inst. G. Gaslini, 16145, Genova, Italy; Physical Medicine and, Rehabilitation Unit, Inst. G. Gaslini, 16145, Genova, Italy; Physical Medicine and, Rehabilitation Unit, Inst. G. Gaslini, 16145, Genova, Italy","2015 7th International Conference on Intelligent Technologies for Interactive Entertainment (INTETAIN)","12 Nov 2015","2015","","","168","175","This paper addresses the use of a remote interactive platform to support home-based rehabilitation for children with motor and cognitive impairment. The interaction between user and platform is achieved on customizable full-body interactive serious games (exergames). These exergames perform real-time analysis of multimodal signals to quantify movement qualities and postural attitudes. Interactive sonification of movement is then applied for providing a real-time feedback based on “aesthetic resonance” and engagement of the children. The games also provide log file recordings therapists can use to assess the performance of the children and the effectiveness of the games. The platform allows the customization of the games to address the children's needs. The platform is based on the EyesWeb XMI software, and the games are designed for home usage, based on Kinect for Xbox One and simple sensors including 3-axis accelerometers available in low-cost Android smartphones.","","978-1-6319-0061-7","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7325501","rehabilitation;exergames;multimodal analysis","Games;Trajectory;Synchronization;Pediatrics;Libraries;Tracking","interactive systems;medical computing;patient rehabilitation;serious games (computing)","full body interactive sonification exergames;remote interactive platform;home-based rehabilitation;cognitive impairment;real-time feedback;aesthetic resonance;log file recordings;EyesWeb XMI software;Kinect;Xbox;3-axis accelerometers;low-cost Android smartphones","","","","21","","12 Nov 2015","","","IEEE","IEEE Conferences"
"Assessing the Effect of Varying Word Classes on Behavioral Variables in Technology Mediated Vocabulary Learning","P. Supitayakul; Z. Yücel; A. Monden; P. Leelaprute",Okayama university; Okayama University; Okayama university; Kasetsart university,"2019 8th International Congress on Advanced Applied Informatics (IIAI-AAI)","13 Feb 2020","2019","","","226","229","This study focuses on foreign language vocabulary learning in computerized medium and seeks for any possibility of adaptation with respect to background information on word classes. To that end, we employ a spaced repetition flashcard software and display English vocabulary belonging to three word classes as (i) abstract noun, (ii) concrete noun, and (iii) verb. Regarding each word class, we deploy three sets of words with difficulty levels of (i) easy, (ii) medium, and (iii) hard. Through log file analysis, we derive several behavioral variables and examine the polyserial correlation between these variables and difficulty levels across different word classes. It is found that abstract and concrete nouns do not have any significant difference in terms of the correlation for the five kinds of behavioral variables in focus. However, it is noted that front sides of the cards involving verbs are observed relatively longer, while back sides are observed for somewhat shorter duration.","","978-1-7281-2627-2","10.1109/IIAI-AAI.2019.00052","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8992730","e-learning, vocabulary learning, adaptation, behavioral variable","","computer aided instruction;linguistics;vocabulary","behavioral variables;abstract nouns;concrete nouns;word class;technology mediated vocabulary learning;spaced repetition flashcard software;English vocabulary;foreign language vocabulary learning;verb;log file analysis;polyserial correlation","","","","25","","13 Feb 2020","","","IEEE","IEEE Conferences"
"A Television News Graphical Layout Analysis Method Using Eye Tracking","R. Rodrigues; A. Veloso; Ó. Mealha","Dept. of Commun. & Art, Univ. of Aveiro, Aveiro, Portugal; Dept. of Commun. & Art, Univ. of Aveiro, Aveiro, Portugal; Dept. of Commun. & Art, Univ. of Aveiro, Aveiro, Portugal","2012 16th International Conference on Information Visualisation","6 Sep 2012","2012","","","357","362","This paper presents an analysis method used to evaluate the graphical layout of Television (TV) News. With the data gathered from an eye tracker, it is possible to discern viewers' main focus of attention. However, in the case of video media, the visualisation tools of eye tracking software are insufficient for this discernment. This method uses eye tracking log files and combines them with algorithms developed using a spreadsheet application and visual data representation techniques. As a result of this combination, it is possible to identify the graphics which have greater visual attention, and subsequently, these representations supported the analysis of viewers' main focus of attention (areas of interest). A case study was also applied for the validation of this method.","2375-0138","978-1-4673-2260-7","10.1109/IV.2012.66","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6295838","Analysis Method;Television News;Eye Tracking;Graphical Layout;Visual Attention","TV;Data visualization;Visualization;Layout;Algorithm design and analysis;Tracking","data structures;data visualisation;eye;information resources;object tracking;spreadsheet programs;television","television news graphical layout analysis method;TV news;data gathering;video media;visualisation tools;eye tracking software;eye tracking log files;spreadsheet application;visual data representation techniques;visual attention","","3","","20","","6 Sep 2012","","","IEEE","IEEE Conferences"
"Simulation and control integrated framework for modular snake robots locomotion research","K. Melo; J. Leon; J. Monsalve; V. Fernandez; D. Gonzalez","KM-ROBOTA Research Group, KM-ROBOTA S.A.S., Tv 5 No 41-15 Of. 402, Bogota D.C., Colombia; KM-ROBOTA Research Group, KM-ROBOTA S.A.S., Tv 5 No 41-15 Of. 402, Bogota D.C., Colombia; KM-ROBOTA Research Group, KM-ROBOTA S.A.S., Tv 5 No 41-15 Of. 402, Bogota D.C., Colombia; KM-ROBOTA Research Group, KM-ROBOTA S.A.S., Tv 5 No 41-15 Of. 402, Bogota D.C., Colombia; KM-ROBOTA Research Group, KM-ROBOTA S.A.S., Tv 5 No 41-15 Of. 402, Bogota D.C., Colombia","2012 IEEE/SICE International Symposium on System Integration (SII)","4 Feb 2013","2012","","","523","528","Executing challenging tasks in field robotics, commonly depends on the locomotive capabilities of the robot. Particularly, modular snake robots have shown an increasing ability to perform mobility and manipulation tasks in different environments. However, its locomotion strategies are not trivial and the modeling of such type of motion is still an important research goal. Consequently, robust controlling software to command motion and retrieve data is needed. Since the execution of experiments with these robots is expensive and time consuming, computer aided dynamics simulation becomes an indispensable tool to reduce the gap between modeling and real experiments validations. In this paper, a framework architecture to control and simultaneously simulate the modular snake robot locomotion behavior is presented. Using this integrated tool, experiments with real robots can be carried out, simulations of a virtual robot in a user-defined environment can be run and the retrieval of useful data from real or virtual outputs can be performed in a log file.","","978-1-4673-1497-8","10.1109/SII.2012.6427341","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6427341","","Robot sensing systems;Protocols;Physics;Computer architecture;Real-time systems","control engineering computing;digital simulation;information retrieval;mobile robots","control integrated framework;simulation framework;mobility tasks;manipulation tasks;locomotion strategies;motion modeling;robust controlling software;motion command;computer aided dynamics simulation;framework architecture;modular snake robot locomotion behavior;virtual robot simulations;data retrieval;log file","","6","","19","","4 Feb 2013","","","IEEE","IEEE Conferences"
"Why SIEM is Irreplaceable in a Secure IT Environment?","O. Podzins; A. Romanovs","Dept. of Modelling and Simulation, Riga Technical University, Riga, Latvia; Dept. of Modelling and Simulation, Riga Technical University, Riga, Latvia","2019 Open Conference of Electrical, Electronic and Information Sciences (eStream)","6 Jun 2019","2019","","","1","5","The aim of the publication is to brief on the importance of a SIEM (Security Information and Event Management) solution. Its benefits but also taking time to reflect on this system drawbacks. All of which is intended for those who are looking into cybersecurity solution that will learn from entire IT infrastructure and be able to identify anomalies, like cyberattacks. Depending on the region and market, enterprise priorities tends to be different, but all mainly take into consideration TCO (Total Cost of Ownership), which in SIEM case is a key metric. If company/organization is serious about deploying a SIEM, then another key security technology they should think about is SOC. If deployed correctly than SOC (Security Operations Centre) is a full framework of technologies, people and processes to act like a well-oiled machine that identifies, protects, detects, responds and recovers from all security related incidents.","","978-1-7281-2499-5","10.1109/eStream.2019.8732173","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8732173","cyber security;network security;security management;computer security;SIEM;Log analysis;SOC;incidents management","Companies;Computer crime;Software;Firewalls (computing);Monitoring","business data processing;security of data","secure IT environment;total cost of ownership;security operations centre;TCO;event management;security related incidents;SOC;key security technology;SIEM case;enterprise priorities;entire IT infrastructure;cybersecurity solution","","4","","11","","6 Jun 2019","","","IEEE","IEEE Conferences"
"Automated Production of Predetermined Digital Evidence","A. Castiglione; G. Cattaneo; G. De Maio; A. De Santis","Department of Computer Science, University of Salerno, Fisciano, Italy; Department of Computer Science, University of Salerno, Fisciano, Italy; Department of Computer Science, University of Salerno, Fisciano, Italy; Department of Computer Science, University of Salerno, Fisciano, Italy","IEEE Access","10 May 2013","2013","1","","216","231","Digital evidence is increasingly used in juridical proceedings. In some recent legal cases, the verdict has been strongly influenced by the digital evidence proffered by the defense. Digital traces can be left on computers, phones, digital cameras, and also on remote machines belonging to ISPs, telephone providers, companies that provide services via Internet such as YouTube, Facebook, Gmail, and so on. This paper presents a methodology for the automated production of predetermined digital evidence, which can be leveraged to forge a digital alibi. It is based on the use of an automation, a program meant to simulate any common user activity. In addition to wanted traces, the automation may produce a number of unwanted traces, which may be disclosed upon a digital forensic analysis. These include data remanence of suspicious files, as well as any kind of logs generated by the operating system modules and services. The proposed methodology describes a process to design, implement, and execute the automation on a target system, and to properly handle both wanted and unwanted evidence. Many experiments with different combinations of automation tools and operating systems are conducted. This paper presents an implementation of the methodology through VBScript on Windows 7. A forensic analysis on the target system is not sufficient to reveal that the alibi is forged by automation. These considerations emphasize the difference between digital and traditional evidence. Digital evidence is always circumstantial, and therefore it should be considered relevant only if supported by stronger evidence collected through traditional investigation techniques. Thus, a Court verdict should not be based solely on digital evidence.","2169-3536","","10.1109/ACCESS.2013.2260817","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6514969","Antiforensics;automated alibi;counter-forensics;digital alibi;digital evidence;digital forensics;digital investigation;false alibi;false digital alibi","Forensics;Software;Digital forensics;Legal aspects;Microcomputers;Digital systems;Law;Social network services","digital forensics;operating systems (computers)","predetermined digital evidence;juridical proceedings;legal case;digital trace;computers;phones;digital cameras;remote machines;ISP;telephone providers;Internet;YouTube;Facebook;Gmail;digital alibi forging;digital forensic analysis;data remanence;suspicious files;log generation;operating system modules;operating system services;automation tools;VBScript;Windows 7;court verdict","","3","","72","","10 May 2013","","","IEEE","IEEE Journals"
"Enabling re-executable workflows with near-realtime visualization, provenance capture and advanced querying for mass spectrometry data","M. Thomas; J. Laskin; B. Raju; E. Stephan; T. Elsethagen; N. Van; S. Nguyen",Pacific Northwest National Laboratory; Pacific Northwest National Laboratory; Pacific Northwest National Laboratory; Pacific Northwest National Laboratory; Pacific Northwest National Laboratory; Pacific Northwest National Laboratory; Pacific Northwest National Laboratory,"2016 New York Scientific Data Summit (NYSDS)","21 Nov 2016","2016","","","1","10","Mass spectrometry imaging (MSI) enables simultaneous spatially-resolved analysis of numerous ionizable molecules on a sample surface. Ambient ionization techniques are attractive because they enable imaging without sample pretreatment. The current analysis pipeline involves analyzing the data coming off the instrument using a tool called MSI Quickview and saving the results onto a storage drive before moving on to the next experiment. While this works well for single datasets, there is a demand for more scalable, flexible workflows that are re-executable across datasets, support extensive querying and ease collaboration. Here, we present a workflow that moves data analysis from a mere desktop application for single experiments to a more general capability that can be possibly extended to perform multi-modal analysis across datasets. The core components of the workflow include (1) MSI Quickview, a desktop application for the near-real time visualization and analysis of mass spectrometry data; (2) Provenance Environment (ProvEn), a provenance production and collection framework that provides components supporting the production and collection of provenance information for distributed application environments; (3) Elasticsearch, a readily-scalable, broadly-distributable, enterprise-grade search engine that is accessible through an elaborate and extensive API to power extremely fast indexing and searches that support your data discovery applications; (4) Logstash, processing of log files; and (5) Kibana, a platform to visualize, analyze and explore data from multiple sources including Elasticsearch and Logstash.","","978-1-4673-9051-4","10.1109/NYSDS.2016.7747806","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7747806","msi quickview;mass spectrometry;provenance;reexecutable workflows","Ions;Data visualization;Graphical user interfaces;Software;Mass spectroscopy;Imaging;Nanobioscience","biology computing;data analysis;data visualisation;mass spectroscopy;molecular biophysics;physics computing;query processing;search engines","re-executable workflows;near-real-time visualization;provenance capture;advanced querying;mass spectrometry data;MSI Quickview;data analysis;multimodal analysis;Provenance production;Provenance collection framework;Elasticsearch;enterprise-grade search engine;broadly-distributable search engine;API;Logstash;log file processing;Kibana","","","","34","","21 Nov 2016","","","IEEE","IEEE Conferences"
"Coasters: Uniform Resource Provisioning and Access for Clouds and Grids","M. Hategan; J. Wozniak; K. Maheshwari","Comput. Inst., Univ. of Chicago, Chicago, IL, USA; Math. & Comput. Sci. Div., Argonne Nat. Lab., Argonne, IL, USA; Math. & Comput. Sci. Div., Argonne Nat. Lab., Argonne, IL, USA","2011 Fourth IEEE International Conference on Utility and Cloud Computing","9 Jan 2012","2011","","","114","121","In this paper we present the Coaster System. It is an automatically-deployed node provisioning (Pilot Job) system for grids, clouds, and ad-hoc desktop-computer networks supporting file staging, on-demand opportunistic multi-node allocation, remote logging, and remote monitoring. The Coaster System has been previously [32] shown to work at scales of thousands of cores. It has been used since 2009 for applications in fields that include biochemistry, earth systems science, energy modeling, and neuroscience. The system has been used successfully on the Open Science Grid, the TeraGrid [1], supercomputers (IBM Blue Gene/P [15], Cray XT and XE systems [5], and Sun Constellation [26]), a number of smaller clusters, and three cloud infrastructures (BioNimbus [2], Future Grid [20] and Amazon EC2 [16]).","","978-1-4577-2116-8","10.1109/UCC.2011.25","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6123488","","Resource management;Software;Protocols;Libraries;Approximation algorithms;Sockets;Java","cloud computing;Cray computers;grid computing;parallel machines","coaster system;uniform resource provisioning;uniform resource access;cloud infrastructures;node provisioning system;pilot job;ad-hoc desktop-computer networks;file staging;ondemand opportunistic multinode allocation;remote logging;remote monitoring;biochemistry;Earth systems science;energy modeling;neuroscience;open science grid;TeraGrid;supercomputers;IBM Blue Gene/P;Cray XT system;Cray XE system;Sun Constellation;BioNimbus;Future Grid;Amazon EC2","","20","","33","","9 Jan 2012","","","IEEE","IEEE Conferences"
"Online workflow management and performance analysis with Stampede","D. Gunter; E. Deelman; T. Samak; C. H. Brooks; M. Goode; G. Juve; G. Mehta; P. Moraes; F. Silva; M. Swany; K. Vahi","Lawrence Berkeley National Laboratory, Berkeley, CA, USA; University of Southern California Information Science Institute, Marina Del Rey, CA, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; University of San Francisco, San Francisco, CA, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; University of Southern California, Marina Del Rey, CA, USA; University of Southern California Information Science Institute, Marina Del Rey, CA, USA; University of Delaware, Newark, DE, USA; University of Southern California Information Science Institute, Marina Del Rey, CA, USA; University of Delaware, Newark, DE, USA; University of Southern California Information Science Institute, Marina Del Rey, CA, USA","2011 7th International Conference on Network and Service Management","15 Dec 2011","2011","","","1","10","Scientific workflows are an enabler of complex scientific analyses. They provide both a portable representation and a foundation upon which results can be validated and shared. Large-scale scientific workflows are executed on equally complex parallel and distributed resources, where many things can fail. Application scientists need to track the status of their workflows in real time, detect execution anomalies automatically, and perform troubleshooting - without logging into remote nodes or searching through thousands of log files. As part of the NSF Stampede project, we have developed an infrastructure to answer these needs. The infrastructure captures application-level logs and resource information, normalizes these to standard representations, and stores these logs in a centralized general-purpose schema. Higher-level tools mine the logs in real time to determine current status, predict failures, and detect anomalous performance.","2165-963X","978-3-901882-44-9","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6103988","","Databases;Monitoring;Real time systems;Data models;Broadband communication;Servers;USA Councils","parallel processing;scientific information systems;workflow management software","online workflow management;performance analysis;complex scientific analysis;large-scale scientific workflow;parallel resources;distributed resources;execution anomalies;troubleshooting;NSF Stampede project;application-level log;resource information;centralized general-purpose schema;higher-level tool;anomalous performance detection","","4","","52","","15 Dec 2011","","","IEEE","IEEE Conferences"
"An Efficient Online Cache Replacement Algorithm for 5G Networks","A. Gharaibeh; I. Hababeh; M. Alshawaqfeh","School of Electrical Engineering and Information Technology, German Jordanian University, Amman, Jordan; School of Electrical Engineering and Information Technology, German Jordanian University, Amman, Jordan; School of Electrical Engineering and Information Technology, German Jordanian University, Amman, Jordan","IEEE Access","13 Aug 2018","2018","6","","41179","41187","In recent years, 5G cellular networks utilization has rapidly increased and is expected to grow even more in the near future. This will put the current cellular networks operators in a challenge to overcome the network's limits to satisfy the increasing mobile data traffic and the proliferation of user demands in deploying mobile applications. The deployment of cache-enabled small base stations (Femtocells) is a promising solution to reduce the backhaul traffic loading and the file-access latency and therefore decrease the cellular network operational costs. Due to the limited cache capacity when compared with the number of files that can be requested by users, in this paper, we formulate the problem of minimizing the cost paid by the cellular network while satisfying the cache capacity as an integer linear program (ILP). Due to the NP-completeness of the ILP formulation and the difficulty of obtaining the file request sequence apriori in real-life scenarios, we propose an online algorithm that decides which file to remove from cache in order to allocate capacity to the newly-requested file. The algorithm works on a per-request basis and does not require the knowledge of the file request sequence in advance. We prove that for a cache that can store up to k files, the algorithm achieves a competitive ratio of O(log(k)), which is the best competitive ratio achieved by any online algorithm as shown in the literature. The simulations conducted considering a single cache show that while the proposed algorithm achieves a similar hit ratio compared with widely-used replacement schemes, it can reduce the cost of the cellular network by 25%.","2169-3536","","10.1109/ACCESS.2018.2856913","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8412480","5G;cache replacement;competitive ratio;femtocells;integer linear program;online algorithms","Cellular networks;Base stations;Optical character recognition software;5G mobile communication;Internet;Delays;Electrical engineering","5G mobile communication;cache storage;computational complexity;femtocellular radio;integer programming;linear programming;minimisation;telecommunication traffic","mobile applications;cache-enabled small base stations;backhaul traffic loading;cellular network operational costs;cache capacity;ILP formulation;file request sequence apriori;online algorithm;per-request basis;competitive ratio;single cache show;5G cellular networks utilization;mobile data traffic;online cache replacement algorithm","","1","","40","","18 Jul 2018","","","IEEE","IEEE Journals"
"UVisP: User-centric Visualization of Data Provenance with Gestalt Principles","J. Garae; R. K. L. Ko; S. Chaisiri","Cyber Security Lab., Univ. of Waikato, Hamilton, New Zealand; Cyber Security Lab., Univ. of Waikato, Hamilton, New Zealand; Cyber Security Lab., Univ. of Waikato, Hamilton, New Zealand","2016 IEEE Trustcom/BigDataSE/ISPA","9 Feb 2017","2016","","","1923","1930","The need to understand and track files (and inherently, data) in cloud computing systems is in high demand. Over the past years, the use of logs and data representation using graphs have become the main method for tracking and relating information to the cloud users. While being used, tracking related information with 'data provenance' (i.e. series of chronicles and the derivation history of data on metadata) is the new trend for cloud users. However, there is still much room for improving data activity representation in cloud systems for end-users. We propose ""User-centric Visualization of data provenance with Gestalt (UVisP)"", a novel user-centric visualization technique for data provenance. This technique aims to facilitate the missing link between data movements in cloud computing environments and the end-users uncertain queries over their files security and life cycle within cloud systems. The proof of concept for the UVisP technique integrates an open-source visualization API with Gestalt's theory of perception to provide a range of user-centric provenance visualizations. UVisP allows users to transform and visualize provenance (logs) with implicit prior knowledge of 'Gestalt's theory of perception.' We presented the initial development of the UVisP technique and our results show that the integration of Gestalt and 'perceptual key(s)' in provenance visualization allows end-users to enhance their visualizing capabilities, to extract useful knowledge and understand the visualizations better.","2324-9013","978-1-5090-3205-1","10.1109/TrustCom.2016.0294","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7847177","Visualization;Data Provenance;Security;User Centric","Data visualization;Cloud computing;Image color analysis;Visualization;Organizations;Computer security","application program interfaces;cloud computing;data integrity;data structures;data visualisation;public domain software;security of data","cloud computing systems;data activity representation;user-centric visualization of data provenance with Gestalt principles;file security;UVisP technique;open-source visualization API;Gestalt theory of perception","","1","","26","","9 Feb 2017","","","IEEE","IEEE Conferences"
"Random multiple layouts: Keylogger prevention technique","T. O. M. Ali; O. S. A. Awadelseed; A. E. W. Eldewahi","University of Khartoum, Khartoum - Sudan; University of Khartoum, Khartoum - Sudan; University of Khartoum, Khartoum - Sudan","2016 Conference of Basic Sciences and Engineering Studies (SGCAC)","25 Apr 2016","2016","","","1","5","Keylogger is a specific type of spywares, that attempts to steal user information, by keep tracking user keyboard, and log every keystroke in a log file; to be used by a third party. Keylogger is one of the most serious problems which blustering information security in this era. And it still considered an open problem. Most of the keylogger softwares available, intercept the key after it has been translated according to the current language-specific keyboard layout, selected by the user or application. Taking benefit of this characteristic, this paper proposes a new prevention technique. The idea is to use multiple layouts, to make the keyboard layout inconstant to mislead the keylogger. This technique works as follow; with each key press the current keyboard layout is changed, and replaced randomly by one of the multiple predesigned layouts. By this way every keylogger sits after the keyboard driver, and intercept the key after it has been translated by the keyboard layout, will log unreadable information because the keyboard layout is inconstant, and will be misled. After the character is posted to the appropriate window, it should be converted back to the intended language-specific keyboard layout.","","978-1-5090-1812-3","10.1109/SGCAC.2016.7457997","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7457997","spyware;keylogger;user-space keylogger;kernel-space keylogger;keyboard input model","Layout;Keyboards;Kernel;Presses;Cryptography;Computers","computer crime;invasive software;keyboards","random multiple layout;keylogger prevention technique;spyware;information security;language-specific keyboard layout;cybercriminal","","1","","14","","25 Apr 2016","","","IEEE","IEEE Conferences"
"Multilevel Learner Modeling in Training Environments for Complex Decision Making","G. Biswas; R. Rajendran; N. Mohammed; B. S. Goldberg; R. A. Sottilare; K. Brawner; M. Hoffman","Department of EECS, Institute for Software Integrated Systems, Vanderbilt University, Nashville, TN, USA; Department of EECS, Institute for Software Integrated Systems, Vanderbilt University, Nashville, TN, USA; Department of EECS, Institute for Software Integrated Systems, Vanderbilt University, Nashville, TN, USA; US Army Research Laboratory, Human Research and Engineering Directorate, Orlando, FL, USA; US Army Research Laboratory, Human Research and Engineering Directorate, Orlando, FL, USA; US Army Research Laboratory, Human Research and Engineering Directorate, Orlando, FL, USA; Dignitas Technologies, Orlando, FL, USA","IEEE Transactions on Learning Technologies","19 Mar 2020","2020","13","1","172","185","Intelligent learning environments can be designed to support the development of learners' cognitive skills, strategies, and metacognitive processes as they work on complex decision-making and problem-solving tasks. However, the complexity of the tasks may impede the progress of novice learners. Providing adaptive feedback to learners who face difficulties requires learner modeling approaches that can identify learners' proficiencies and the difficulties they face in executing required skills, strategies, and metacognitive processes. This paper discusses a multilevel hierarchical learner modeling scheme that analyzes and captures learners' cognitive processes and problem-solving strategies along with their performance on assigned tasks in a game-based environment called UrbanSim that requires complex decision making for dealing with counterinsurgency scenarios. As the scenario evolves in a turn-by-turn fashion, UrbanSim evaluates the learners' moves using a number of performance measures. Our learner modeling scheme interprets the reported performance values by analyzing the learners' activities captured in log files to derive learners' proficiencies in associated cognitive skills and strategies, and updates the learner model. We discuss the details of the learner modeling algorithms in this paper, and then demonstrate the effectiveness of our approach by presenting results from a study we conducted at Vanderbilt University.","1939-1382","","10.1109/TLT.2019.2923352","Army Research Laboratory; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8737698","Complex decision making;multilevel learner modeling;cognitive strategies;learning analytics;UrbanSim","Task analysis;Problem-solving;Decision making;Sociology;Statistics;Games;Economics","cognition;computer aided instruction;learning (artificial intelligence);serious games (computing)","associated cognitive skills;training environments;intelligent learning environments;metacognitive processes;learner modeling approaches;multilevel hierarchical learner modeling scheme;game-based environment;UrbanSim","","1","","51","IEEE","17 Jun 2019","","","IEEE","IEEE Journals"
"RFID based health assistance & monitoring system through a handmounted embedded device","D. Dixit; A. Kalbande; K. M. Bhurchandi","Electronics Engineering Department, Visvesvaraya National Institute of Technology, Nagpur, India; Electronics Engineering Department, Visvesvaraya National Institute of Technology, Nagpur, India; Electronics Engineering Department, Visvesvaraya National Institute of Technology, Nagpur, India","2013 Fourth International Conference on Computing, Communications and Networking Technologies (ICCCNT)","30 Jan 2014","2013","","","1","9","This paper describes the vision and on-going research in creating ubiquitous computing support for medical work in the hospitals and health assistance technology for the future. In the recent decades health related issues are becoming more and more critical. This highlights the need of continuous monitoring of health parameters of the patient. Today, clinical computer systems seldom play any role in the execution of clinical work as such. Similarly the handy health monitoring electronic equipments do not help in maintaining health logs, analyzing data and assisting depending on the analysis. Electronic Patient Records (EPR) are more confined to the hospital database. A dynamic modification of EPR depending on the real-time vital parameter feed are required for a updated EPR, which would in turn assist the specialist for a better insight of prevailing medical conditions of concerned patient. Extending the accessibility of this updated EPR to the patient could be used to assist the patient in the prevailing health conditions. Following the above line of action and addressing numerous challenges in health monitoring and assistance, hardware and software design of contemporary hand mounted embedded device loaded with biomedical sensors was developed. In this paper we present the design of a Radio Frequency IDentification (RFID) based Health Assistance & Monitoring System on the guidelines of Assistive Technology. The proposed design not only helps the user in monitoring and updating his EPR but also assists the user in organizing his appointments, following the strict dietary plan and assists the user in case of emergency.","","978-1-4799-3926-8","10.1109/ICCCNT.2013.6726620","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6726620","Radio Frequency IDentification (RFID);Wireless System;Ubiquitious Computing;Android;Pulse Oximeter","Radiofrequency identification;Bluetooth;Monitoring;Androids;Humanoid robots;Hospitals","electronic health records;medical computing;patient monitoring;radiofrequency identification;ubiquitous computing","RFID;health assistance;health monitoring system;handmounted embedded device;ubiquitous computing;medical work;clinical computer system;electronic patient record;EPR;hospital database;real-time vital parameter feed;hand mounted embedded device;biomedical sensor;radio frequency identification;assistive technology","","2","","12","","30 Jan 2014","","","IEEE","IEEE Conferences"
"Unlocking the Access to the Effects Induced by IEMI on a Civilian UAV","J. L. Esteves; E. Cottais; C. Kasmi","ANSSI Paris, Wireless Security Lab French Network and Information Security Agency, France; ANSSI Paris, Wireless Security Lab French Network and Information Security Agency, France; Abu Dhabi, TV Labs Dark Matter LLC, UAE","2018 International Symposium on Electromagnetic Compatibility (EMC EUROPE)","8 Oct 2018","2018","","","48","52","Many reports have pointed out the potential use of civilian unmanned aerial vehicles in malicious activities against critical infrastructures. From radioactive material projection in Japan to pictures of classified areas, known events have motivated the development of neutralization techniques. The use of high power directed energy weapons (DEW) is one of them. In order to estimate the efficiency of this solution, it is mandatory to have an in-depth analysis of the effects induced by radiofrequency DEW on targeted devices. Many studies related to the effects of electromagnetic interference on electronic devices have been released. A recently proposed approach has shown to provide insight on the propagation of hardware effects to the software level on tested targets using a health monitoring software. In this paper, we propose to design and run arbitrary software on a locked target to gain access to the internal sensors and logs available in the device. Relevant effects were obtained, opening the possibility to both neutralization and hardening strategies as well as estimating the protection efficiency provided by such solution.","2325-0364","978-1-4673-9698-1","10.1109/EMCEurope.2018.8484990","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8484990","RF DEW;Information Security;Electromagnetic Security;Physical Security;IEMI;UAV","Sensors;Aircraft;Software;Electromagnetic compatibility;Radio frequency;Unmanned aerial vehicles;Payloads","autonomous aerial vehicles;jamming;national security;weapons","electronic devices;civilian UAV;unmanned aerial vehicles;IEMI effects;intentional electromagnetic interference;directed energy weapons;electromagnetic interference;radiofrequency DEW;energy weapons;Japan;radioactive material projection;internal sensors;arbitrary software;health monitoring software","","3","","23","","8 Oct 2018","","","IEEE","IEEE Conferences"
"Use of Data Acquisition for Rapid Feasibily Check of PV Based Illumination Systems","N. E. Mabunda","University of Johannesburg,South Africa","2019 IEEE AFRICON","7 Jul 2020","2019","","","1","8","Data Acquisition Systems (DAS) have been used to acquire fast and slow varying electronics signals. Photovoltaic (PV) produced signals and consumed load energy can therefore be gathered by using DAS systems. In this research: the samples of varying solar irradiance and varying load power are collected by the microcontroller-based DAS and then processed to determine the feasibility of photovoltaic (PV) based illumination systems for the region in question. The microcontroller software algorithm logs the power samples from test PV and the load into a coma separated value file (CSV), which is later transferred to the computer for further analysis. In the computer, tools such as spreadsheets are used to compare the prevailing solar energy with the load conditions to give the feasibility report. This study showed that by physically measuring the ground received solar power and the power that is consumed by the connected load: the surveillance of PV based illumination have been simplified.","2153-0033","978-1-7281-3289-1","10.1109/AFRICON46755.2019.9134011","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9134011","load energy;feasibility;data acquisition;solar irradiance","Temperature sensors;Lighting;Microcontrollers;Energy consumption;Software;Solar energy;Monitoring","data acquisition;lighting;microcontrollers;photovoltaic power systems;power engineering computing;solar power;solar power stations","connected load;solar power;load conditions;solar energy;coma separated value file;test PV;power samples;microcontroller software algorithm;photovoltaic based illumination systems;microcontroller-based DAS;varying load power;solar irradiance;DAS systems;consumed load energy;photovoltaic produced signals;electronics signals;Data Acquisition Systems;PV based illumination Systems","","","","","","7 Jul 2020","","","IEEE","IEEE Conferences"
"Blink: Large-scale P2P network monitoring and visualization system using VM introspection","R. Ando; Y. Kadobayashi; Y. Shinoda","National Institute of Information and Communication Technology, 4-2-1 Nukui-Kitamachi, Koganei, Tokyo 184-8795 Japan; National Institute of Information and Communication Technology, 4-2-1 Nukui-Kitamachi, Koganei, Tokyo 184-8795 Japan; National Institute of Information and Communication Technology, 4-2-1 Nukui-Kitamachi, Koganei, Tokyo 184-8795 Japan","The 6th International Conference on Networked Computing and Advanced Information Management","16 Sep 2010","2010","","","351","358","P2P network is now widely pervasive and increase usability of Internet. However, with the difficulty of tracing flow of P2P traffic, security incident of P2P network has become now serious problem. In this paper we propose Blink, Large-scale P2P network monitoring and visualization system enhanced by VM introspection. We discuss a monitoring and visualizing P2P traffic using the combination of virtualized probe and analyzer on VMM side. In proposed system, probe and monitor are running on guest OS, which is connected to the analyzer and visualizer module on VMM and host OS. Traffic log is transferred to host OS using VM introspection and is analyzed and visualized. Proposed system makes it possible to enhance the analysis and visualization functionality with the least impact of guest OS. Also, proposed system supports large scale traffic log analysis with large amount of disks necessary using storage of host OS. In proposed system we have implemented monitors for two kinds of P2P software: BitTorrent and Winny. Also we have implemented visualization module using Google Earth by translating traffic log file to KML (Keyhole Markup Language). We show system output of visualizing of traffic log of Winny and BitTorrent. We can conclude that proposed system of double-layer architecture can enhance the functionality of analyzing, storing and visualizing P2P traffic logs.","","978-89-88678-26-8","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5572049","P2P network;VM introspection;passive monitor;active monitor;KML","Topology;Probes;Monitoring;Visualization","data visualisation;peer-to-peer computing;system monitoring;telecommunication traffic;virtual machines","Blink;large scale P2P network monitoring;P2P traffic;virtualized probe;virtualized analyzer;BitTorrent;Winny;Google Earth;keyhole markup language;double layer architecture;network visualization system;virtual machine introspection;large scale traffic log analysis","","","1","8","","16 Sep 2010","","","IEEE","IEEE Conferences"
"Improving recovery probability of mobile hosts using secure checkpointing","S. Biswas; S. Neogy","Dept. of Computer Science & Engineering, West Bengal University of Technology, Kolkata, India; Dept. of Computer Science & Engineering, Jadavpur University, Kolkata, India","2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","21 Oct 2013","2013","","","984","989","In this work, we have proposed a mobility based secure checkpointing and log based rollback recovery technique to provide fault tolerance to mobile hosts in infrastructured wireless/mobile computing system, like, wireless cellular network. Mobility based checkpointing limits number of scattered checkpoint or logs in different mobile support stations that remain due to movement of mobile hosts. Secure checkpointing using low overhead elliptic curve cryptography ensures protection of checkpoint against security attack in both nodes and links and restricts access to checkpoint content only to the mobile host that is owner of the checkpoint. Log based rollback recovery ensures optimized recovery from last event using determinants saved in logs. Security attack to checkpoints leads to unsuccessful recovery. In case of security attack to checkpoint, recovery probability of a failed mobile host using secure checkpointing technique is 1 whereas that in checkpointing without security technique is <;1.","","978-1-4673-6217-7","10.1109/ICACCI.2013.6637310","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6637310","handoff;checkpoint;encryption;decryption;recovery","Mobile communication;Checkpointing;Mobile computing;Cryptography;Communication system security;Wireless communication","checkpointing;mobile computing;probability;public key cryptography;software fault tolerance","mobility based secure checkpointing;log based rollback recovery technique;fault tolerance;infrastructured mobile computing system;mobile support stations;low overhead elliptic curve cryptography;checkpoint protection;security attack;checkpoint content;optimized recovery;mobile host recovery probability;infrastructured wireless computing system","","","","14","","21 Oct 2013","","","IEEE","IEEE Conferences"
"ECG classification and prognostic approach towards personalized healthcare","A. Walinjkar; J. Woods","Department of Computer Science and Electronics Engineering University of Essex Colchester, Essex; Department of Computer Science and Electronics Engineering University of Essex Colchester, Essex","2017 International Conference On Social Media, Wearable And Web Analytics (Social Media)","9 Oct 2017","2017","","","1","8","A very important aspect of personalized healthcare is to continuously monitor an individual's health using wearable biomedical devices and essentially to analyse and if possible to predict potential health hazards that may prove fatal if not treated in time. The prediction aspect embedded in the system helps in avoiding delays in providing timely medical treatment, even before an individual reaches a critical condition. Despite of the availability of modern wearable health monitoring devices, the real-time analyses and prediction component seems to be missing with these devices. The research work illustrated in this paper, at an outset, focussed on constantly monitoring an individual's ECG readings using a wearable 3-lead ECG kit and more importantly focussed on performing real-time analyses to detect arrhythmia to be able to identify and predict heart risk. Also, current research shows extensive use of heart rate variability (HRV) analysis and machine learning for arrhythmia classification, which however depends on the morphology of the ECG waveforms and the sensitivity of the ECG equipment. Since a wearable 3-lead ECG kit was used, the accuracy of classification had to be dealt with at the machine learning phase, so a unique feature extraction method was developed to increase the accuracy of classification. As a case study a very widely used Arrhythmia database (MIT-BIH, Physionet) was used to develop learning, classification and prediction models. Neuralnet fitting models on the extracted features showed mean-squared error of as low as 0.0085 and regression value as high as 0.99. Current experiments show 99.4% accuracy using k-NN Classification models and show values of Cross-Entropy Error of 7.6 and misclassification error value of 1.2 on test data using scaled conjugate gradient pattern matching algorithms. Software components were developed for wearable devices that took ECG readings from a 3-Lead ECG data acquisition kit in real time, de-noised, filtered and relayed the sample readings to the tele health analytical server. The analytical server performed the classification and prediction tasks based on the trained classification models and could raise appropriate alarms if ECG abnormalities of V (Premature Ventricular Contraction: PVC), A (Atrial Premature Beat: APB), L (Left bundle branch block beat), R (Right bundle branch block beat) type annotations in MITDB were detected. The instruments were networked using IoT (Internet of Things) devices and abnormal ECG events related to arrhythmia, from analytical server could be logged using an FHIR web service implementation, according to a SNOMED coding system and could be accessed in Electronic Health Record by the concerned medic to take appropriate and timely decisions. The system focused on `preventive care rather than remedial cure' which has become a major focus of all the health care and cure institutions across the globe.","","978-1-5090-5057-4","10.1109/SOCIALMEDIA.2017.8057360","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8057360","ECG classification;Wearable IoT;preventive health-care;real-time ECG;arrhythmia detection;arrhythmia Neural-Net;MITDB;Physionet;GP Connect;HL7;FHIR","Electrocardiography;Heart;Real-time systems;Databases;Biomedical monitoring;Monitoring;Feature extraction","biomedical telemetry;cardiology;data acquisition;electrocardiography;entropy;feature extraction;health care;health hazards;learning (artificial intelligence);medical information systems;medical signal processing;neural nets;patient monitoring;pattern matching;real-time systems;signal classification;telemedicine;Web services","personalized healthcare;wearable systems;real-time ECG Classification;wearable biomedical devices;potential health hazards;timely medical treatment;modern wearable health monitoring devices;real-time analyses;ECG readings;arrhythmia classification;ECG waveforms;ECG equipment;machine learning phase;unique feature extraction method;prediction models;neuralnet fitting models;k-NN Classification models;misclassification error value;wearable devices;3-Lead ECG data acquisition kit;tele health analytical server;prediction tasks;trained classification models;ECG abnormalities;abnormal ECG events;Electronic Health Record;health care;cure institutions","","2","","53","","9 Oct 2017","","","IEEE","IEEE Conferences"
"Balancing scalability, performance and fault tolerance for structured data (BSPF)","A. Khalid; H. Afzal; S. Aftab","Department of Computer Software Engineering, Military College of Signals, National University of Sciences and Technology, Islamabad, Pakistan; Department of Computer Software Engineering, Military College of Signals, National University of Sciences and Technology, Islamabad, Pakistan; Department of Computer Software Engineering, Military College of Signals, National University of Sciences and Technology, Islamabad, Pakistan","16th International Conference on Advanced Communication Technology","27 Mar 2014","2014","","","725","732","Analytical business applications generate reports that give a trend predicting insight into the organization's future, estimating the financial graphs and risk factors. These applications work on huge amounts of data, which comprises of decades of market and company records, and decision logs of an organization. Today, limit of big data is touching zeta-bytes and the structured data makes only 20% of today's data. 20% of a giga-byte can be ignorable in comparison to big data but 20% of big data itself cannot be neglected. Traditional data management tools are like step-dads when it comes to running cross table analytical queries on structured data in distributed processing environment; response time to these data management tools are high because of the ill-aligned data sets and complex hierarchy of distributed computing environment. Data alignment requires a complete shift in data deployment paradigm from row oriented storage layout to column oriented storage layout, and complex hierarchy of distributed computing environment can be handled by keeping metadata of entire data set. Paper proposes an approach to ease the deployment of structured data into the distributed processing environment by arranging data into column-wise combinational entities. Response time to analytical queries can be lowered with the support of two concepts; Shared architecture and Multi path query execution. Highly scalable systems are Shared Nothing architecture based but degradation in performance and fault tolerance are the side effects that came with high scalability. Proposed method is an effort to balance the equation between scalability, performance and fault tolerance. And due to the limited scope of this paper we concentrate on issues and solutions for structured data only. Shared architecture and active backup helps improving the system's performance by sharing the work-load-per-node. BSPF's clustering methodology sheds the data pressure points to minimize the data loss per node crash.","1738-9445","978-89-968650-3-2","10.1109/ICACT.2014.6779058","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6779058","Big data;Distributed and Cloud Computing","Peer-to-peer computing;Distributed databases;Layout;Computer architecture;Indexes;Computer crashes;Information management","Big Data;cloud computing;data structures;fault tolerant computing;pattern clustering","scalability balancing;performance balancing;fault tolerance balancing;analytical business applications;financial graph estimation;risk factor estimation;big data;data management tools;distributed processing environment;ill-aligned data sets;distributed computing environment;data alignment;data deployment paradigm;row oriented storage layout;column oriented storage layout;metadata;structured data deployment;column-wise combinational entities;shared architecture;multipath query execution;shared nothing architecture;active backup;system performance improvement;work-load-per-node sharing;BSPF clustering methodology;data loss minimization;cloud computing","","1","1","9","","27 Mar 2014","","","IEEE","IEEE Conferences"
"Open-source 128-channel bioamplifier module for ambulatory monitoring of gastrointestinal electrical activity","J. C. Erickson; B. Reed; J. Wharton; U. Thapa; J. Robey; R. Shrestha","Washington and Lee University,Department of Physics and Engineering,Lexington,VA,USA,24450; Washington and Lee University,Department of Physics and Engineering,Lexington,VA,USA,24450; Washington and Lee University,Department of Physics and Engineering,Lexington,VA,USA,24450; Washington and Lee University,Department of Physics and Engineering,Lexington,VA,USA,24450; Washington and Lee University,Department of Physics and Engineering,Lexington,VA,USA,24450; Washington and Lee University,Department of Physics and Engineering,Lexington,VA,USA,24450","2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)","27 Aug 2020","2020","","","4429","4432","We present an open-source, low-cost, portable, 128-channel bioamplifier module designed specifically for ambulatory, long-term (≥24 hr) monitoring of gastrointestinal (GI) electrical activity. The electronics hardware integrates stateof-the-art, commercial-off-the-shelf components on a custom PCB. Features include on-board data logging, wireless data streaming, subject motion monitoring, and stable operation up to the maximum 2 kHz/channel sampling rate tested. The new device operates for ≈ 30 hr continuously powered by a single 3.7 V, 2500 mAh LiPo battery. The 3D-printed ABS mechanical enclosure is robust and small (13.1 × 8.8 × 2.5 cm), so that the device can be carried in a standard Holter monitor pouch. Results from initial 128-channel, high spatial resolution body surface colon mapping experiments demonstrate the utility of this new device for GI applications. The new bioamplifier module could also be used for multichannel recording experiments in a variety of biomedical domains to study electrical activity patterns of the neuromuscular system (EMG), uterus (EHG), heart (ECG), and brain (EEG).","2694-0604","978-1-7281-1990-8","10.1109/EMBC44109.2020.9175582","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9175582","","Monitoring;Hardware;Colon;Open source software;Biomedical monitoring;Batteries;Electrodes","biomedical electronics;electrocardiography;electroencephalography;electromyography;medical signal processing;patient monitoring","EEG;brain;ECG;heart;EHG;uterus;EMG;neuromuscular system;multichannel recording;wireless data streaming;3D-printed ABS mechanical enclosure;Holter monitor pouch;high-spatial resolution body surface colon mapping;electrical activity patterns;LiPo battery;subject motion monitoring;on-board data logging;gastrointestinal electrical activity;ambulatory monitoring;open-source 128-channel bioamplifier module;voltage 3.7 V;LiPo","Electrocardiography, Ambulatory;Electronics;Equipment Design;Monitoring, Ambulatory;Wireless Technology","","","13","","27 Aug 2020","","","IEEE","IEEE Conferences"
"Automatic predictor generator & behaviour rule extractor - A system proposal","M. Puheim; J. Paralic; L. Madarasz","Department of Cybernetics and Artificial Intelligence, Technical University of Kosice, Letna 9, 042 00 Kosice, Slovak Republic; Department of Cybernetics and Artificial Intelligence, Technical University of Kosice, Letna 9, 042 00 Kosice, Slovak Republic; Department of Cybernetics and Artificial Intelligence, Technical University of Kosice, Letna 9, 042 00 Kosice, Slovak Republic","2015 16th IEEE International Symposium on Computational Intelligence and Informatics (CINTI)","14 Jan 2016","2015","","","155","159","In this paper we present a proposal for a data-mining system deployed as a cloud service which is supposed to be used for a big data analysis. The main purpose of the system is the analysis of a vast number of event logs using means of data aggregation, clustering, classification and prediction. The system is composed of two components implemented as software services. The Automatic Predictor Generator is supposed to provide a meaningful way to aggregate large amounts of data and the Automatic Behavior Rule Extractor deals with proper analysis of these aggregations. Results of the system are the prediction rules usable for support of decision-making and in areas such as management, marketing, customer segmentation, classification, behavior prediction etc.","","978-1-4673-8520-6","10.1109/CINTI.2015.7382913","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7382913","big data;data analytics;data mining;prediction;data aggregation;rule extraction;cloud services","Decision trees;Big data;Generators;Proposals;Data mining;Correlation;Correlation coefficient","Big Data;cloud computing;data mining;pattern classification;pattern clustering","automatic predictor generator;data-mining system;cloud service;big data analysis;event logs;data aggregation;data clustering;data classification;data prediction;software services;automatic behavior rule extractor;decision-making;customer segmentation;behavior prediction;marketing","","","","11","","14 Jan 2016","","","IEEE","IEEE Conferences"
"Digital Forensic Analysis of Fitbit Wearable Technology: An Investigator’s Guide","A. Almogbil; A. Alghofaili; C. Deane; T. Leschke; A. Almogbil; A. Alghofaili","Information Security Institute Johns Hopkins University,Baltimore,USA; Information Security Institute Johns Hopkins University,Baltimore,USA; Information Security Institute Johns Hopkins University,Baltimore,USA; Information Security Institute Johns Hopkins University,Baltimore,USA; Center for Cybersecurity Technologies King AbdulAziz City for Science & Technology,Riyadh,Saudi Arabia; King Saud University,Information Systems Department,Riyadh,Saudi Arabia","2020 7th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2020 6th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom)","19 Aug 2020","2020","","","44","49","Wearable technology, such as Fitbit devices, log a user's daily activities, heart rate, calories burned, step count, and sleep activity. This information is valuable to digital forensic investigators as it may serve as evidence to a crime, to either support a suspect's innocence or guilt. It is important for an investigator to find and analyze every piece of data for accuracy and integrity; however, there is no standard for conducting a forensic investigation for wearable technology. In this paper, we conduct a forensic analysis of two different Fitbit devices using open-source tools. It is the responsibility of the investigator to show how the data was obtained and to ensure that the data was not modified during the analysis. This paper will guide investigators in understanding what data is collected by a Fitbit device (specifically the Ionic smartwatch and Alta tracker), how to handle Fitbit devices, and how to extract and forensically analyze said devices using open-source tools, Autopsy Sleuth Kit and Bulk Extractor Viewer.","","978-1-7281-6550-9","10.1109/CSCloud-EdgeCom49738.2020.00017","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9170971","digital forensics;wearable technology;digital analysis;application forensics;opensource;Fitbit","","data visualisation;digital forensics;file organisation;public domain software;wearable computers","digital forensic analysis;Fitbit wearable technology;investigator;Fitbit device;user;heart rate;step count;sleep activity;digital forensic investigators;suspect;forensic investigation;different Fitbit devices;open-source tools","","","","9","","19 Aug 2020","","","IEEE","IEEE Conferences"
"Enabling new techniques in environmental assessment through multi-sensor hydrography","E. J. Martin; D. W. Caress; H. Thomas; B. Hobson; R. Henthorn; M. Risi; C. K. Paull; J. P. Barry; G. Troni","Monterey Bay Aquarium Research Institute, Moss Landing, CA 95039, USA; Monterey Bay Aquarium Research Institute, Moss Landing, CA 95039, USA; Monterey Bay Aquarium Research Institute, Moss Landing, CA 95039, USA; Monterey Bay Aquarium Research Institute, Moss Landing, CA 95039, USA; Monterey Bay Aquarium Research Institute, Moss Landing, CA 95039, USA; Monterey Bay Aquarium Research Institute, Moss Landing, CA 95039, USA; Monterey Bay Aquarium Research Institute, Moss Landing, CA 95039, USA; Monterey Bay Aquarium Research Institute, Moss Landing, CA 95039, USA; Pontificia Universidad Católica de Chile, Santiago, Chile","OCEANS 2016 MTS/IEEE Monterey","1 Dec 2016","2016","","","1","7","A suite of complementary survey tools aimed at producing 1-cm resolution bathymetric models co-registered with 2-mm pixel color photography has been assembled. The design goal is to produce quantitative documentation of both geological and biological features that will allow change over time to be assessed at vertical and lateral scales approaching one centimeter. The current suite of tools combines multibeam sonar, stereo cameras with dual xenon strobes, lidar, and an inertial navigation system (INS) aided by Doppler velocity log (DVL). This sensor package is mounted beneath remotely operated vehicles (ROV) and used to map the seafloor from low altitudes. A 100-m by 100-m survey can be accomplished in a single ROV dive. All surveys are conducted with scripted station-keeping control loops operating on the ROV, resulting in more efficient area coverage through tended automation. Fine scale surveys of a chemosynthetic biological community at 2850-m depth show that individual clams can be observed in both lidar bathymetry and photographic imagery. Repeat surveys over multiple years have been conducted in the morphologically active floor of Monterey Canyon. Comparison of these data resolve subtle transitions from depositional to erosional textures, and reveal the changes associated with frequent sediment transport events down the active canyon. The rocky, high relief environment of Sur Ridge offshore California hosts sponge and deep water coral habitats. Here the combination of acoustic and optic sensing proves particularly useful for quantitatively characterizing the benthic community. The multibeam sonar measures bathymetry without sensing soft animals, while the lidar measures a surface that includes these animals. Subtracting the multibeam bathymetry from the lidar bathymetry maps the locations and sizes of soft animals.","","978-1-5090-1537-5","10.1109/OCEANS.2016.7761487","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7761487","","Laser radar;Sonar;Cameras;Software;Animals;Vehicles","bathymetry;oceanographic techniques;remotely operated vehicles;sediments","environmental assessment;multisensor hydrography;survey tools;bathymetric models;color photography;geological feature;biological feature;multibeam sonar;stereo cameras;dual xenon strobes;lidar;inertial navigation system;Doppler velocity log;remotely operated vehicles;seafloor map;single ROV dive;scripted station-keeping control loops;chemosynthetic biological community surveys;lidar bathymetry;photographic imagery;morphologically active floor;Monterey Canyon;depositional texture;erosional texture;sediment transport events;active canyon;Sur Ridge offshore California;sponge coral habitats;deep water coral habitats;acoustic sensing;optic sensing;benthic community;sensing soft animals;multibeam bathymetry;lidar bathymetry maps;soft animal sizes;soft animal locations","","","","14","","1 Dec 2016","","","IEEE","IEEE Conferences"
"A methodology to support load test analysis","H. Malik","Queen's University, Kingston, ON, Canada","2010 ACM/IEEE 32nd International Conference on Software Engineering","27 Oct 2011","2010","2","","421","424","Performance analysts rely heavily on load testing to measure the performance of their applications under a given load. During the load test, analyst strictly monitor and record thousands of performance counters to measure the run time system properties such as CPU utilization, Disk I/O, memory consumption, network traffic etc. The most frustrating problem faced by analysts is the time spent and complexity involved in analysing these huge counter logs and finding relevant information distributed across thousands of counters. We present our methodology to help analysts by automatically identifying important performance counters for load test and comparing them across tests to find performance gain/loss. Further, our methodology help analysts to understand the root cause of a load test failure by finding previously solved problems in test repositories. A case study on load test data of a large enterprise application shows that our methodology can effectively guide performance analysts to identify and compare top performance counters across tests in limited time thereby archiving 88% counter data reduction.","1558-1225","978-1-60558-719-6","10.1145/1810295.1810408","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6062229","automation;counters;load test;performance counters;principal component analysis","Radiation detectors;Testing;Principal component analysis;Monitoring;Servers;Conferences;Electronic mail","business data processing;program testing;software performance evaluation","load test analysis support;performance analysts;run time system properties;relevant information search;automatic performance counter identification;performance gain;performance loss;load test failure;test repositories;large enterprise application","","5","","15","","27 Oct 2011","","","IEEE","IEEE Conferences"
"Generating Complex and Faulty Test Data through Model-Based Mutation Analysis","D. Di Nardo; F. Pastore; L. Briand","Interdiscipl. Centre for Security, Reliability & Trust (SnT Centre), Univ. of Luxembourg, Luxembourg, Luxembourg; Interdiscipl. Centre for Security, Reliability & Trust (SnT Centre), Univ. of Luxembourg, Luxembourg, Luxembourg; Interdiscipl. Centre for Security, Reliability & Trust (SnT Centre), Univ. of Luxembourg, Luxembourg, Luxembourg","2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)","7 May 2015","2015","","","1","10","Testing the correct behaviour of data processing systems in the presence of faulty data is extremely expensive. The data structures processed by these systems are often complex, with many data fields and multiple constraints among them. Software engineers, in charge of testing these systems, have to handcraft complex data files or databases, while ensuring compliance with the multiple constraints to prevent the generation of trivially invalid inputs. In addition, assessing test results often means analysing complex output and log data. Though many techniques have been proposed to automatically test systems based on models, little exists in the literature to support the testing of systems where the complexity is in the data consumed in input or produced in output, with complex constraints between them. In particular, such systems often need to be tested with the presence of faults in the input data, in order to assess the robustness and behaviour of the system in response to such faults. This paper presents an automated test technique that relies upon six generic mutation operators to automatically generate faulty data. The technique receives two inputs: field data and a data model, i.e. a UML class diagram annotated with stereotypes and OCL constraints. The annotated class diagram is used to tailor the behaviour of the generic mutation operators to the fault model that is assumed for the system under test and the environment in which it is deployed. Empirical results obtained with a large data acquisition system in the satellite domain show that our approach can successfully automate the generation of test suites that achieve slightly better instruction coverage than manual testing based on domain expertise.","2159-4848","978-1-4799-7125-1","10.1109/ICST.2015.7102589","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7102589","","Unified modeling language;Data models;Software;Satellites;Testing;Load modeling;Radiation detectors","data structures;program testing;Unified Modeling Language","complex data;faulty test data;model-based mutation analysis;data structure;automated test technique;generic mutation operator;field data;data model;UML class diagram;OCL constraint","","6","","23","","7 May 2015","","","IEEE","IEEE Conferences"
"Scalable Thread Sharing Analysis","J. Huang","Parasol Lab., Texas A&M Univ., College Station, TX, USA","2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)","3 Apr 2017","2016","","","1097","1108","We present two scalable algorithms for identifying program locations that access thread-shared data in concurrent programs. The static algorithm, though simple, and without performing the expensive whole program information flow analysis, is much more efficient, less memory-demanding, and even more precise than the classical escape analysis algorithm. The dynamic algorithm, powered by a location- based approach, achieves significant runtime speedups over a precise dynamic escape analysis. Our evaluation on a set of large real world complex multithreaded systems such as Apache Derby and Eclipse shows that our algorithms achieve unprecedented scalability. Used by client applications, our algorithms reduce the recording overhead of a record-replay system by 9X on average (as much as 16X) and increase the runtime logging speed of a data race detector by 32% on average (as much as 52%).","1558-1225","978-1-4503-3900-1","10.1145/2884781.2884811","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7886983","","Heuristic algorithms;Algorithm design and analysis;Arrays;Instruction sets;Runtime;Detectors;Java","multi-threading;program diagnostics","thread sharing analysis;scalable algorithms;program locations identification;thread-shared data access;concurrent programs;static algorithm;program information flow analysis;location-based approach;dynamic algorithm;multithreaded systems;Apache Derby;Eclipse;record-replay system;data race detector","","5","","42","","3 Apr 2017","","","IEEE","IEEE Conferences"
"Alleviating scalability issues of checkpointing protocols","R. Riesen; K. Ferreira; D. Da Silva; P. Lemarinier; D. Arnold; P. G. Bridges","IBM Res., Dublin, Ireland; Sandia Nat. Labs., Albuquerque, NM, USA; IBM Res., Dublin, Ireland; IBM Res., Dublin, Ireland; Univ. of New Mexico, Albuquerque, NM, USA; Univ. of New Mexico, Albuquerque, NM, USA","SC '12: Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis","25 Feb 2013","2012","","","1","11","Current fault tolerance protocols are not sufficiently scalable for the exascale era. The most-widely used method, coordinated checkpointing, places enormous demands on the I/O subsystem and imposes frequent synchronizations. Uncoordinated protocols use message logging which introduces message rate limitations or undesired memory and storage requirements to hold payload and event logs. In this paper we propose a combination of several techniques, namely coordinated checkpointing, optimistic message logging, and a protocol that glues them together. This combination eliminates some of the drawbacks of each individual approach and proves to be an alternative for many types of exascale applications. We evaluate performance and scaling characteristics of this combination using simulation and a partial implementation. While not a universal solution, the combined protocol is suitable for a large range of existing and future applications that use coordinated checkpointing and enhances their scalability.","2167-4337","978-1-4673-0806-9","10.1109/SC.2012.18","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6468460","","Protocols;Checkpointing;Computational modeling;Payloads;Radiation detectors;Economic indicators;Fault tolerance","checkpointing;digital simulation;input-output programs;parallel processing;protocols;software performance evaluation;storage management;system monitoring","I/O subsystem;optimistic message logging;exascale applications;performance evaluation;scaling characteristics;partial implementation;simulation;scalability;coordinated checkpointing protocol","","10","3","36","","25 Feb 2013","","","IEEE","IEEE Conferences"
"Reliably scalable name prefix lookup","H. Yuan; P. Crowley","Washington University, Department of Computer Science & Engineering, St. Louis, Missouri 63130; Washington University, Department of Computer Science & Engineering, St. Louis, Missouri 63130","2015 ACM/IEEE Symposium on Architectures for Networking and Communications Systems (ANCS)","21 May 2015","2015","","","111","121","Name prefix lookup is a core building block of information-centric networking (ICN). In ICN hierarchical naming schemes, each packet has a name that consists of multiple variable-length name components, and packets are forwarded based on longest name prefix matching (LNPM). LNPM is challenging because names are longer than IP addresses and the namespace is unbounded. Recently proposed solutions have shown encouraging performance, however, most are optimized for or evaluated with a limited number of URL datasets that may not fully characterize the forwarding information base (FIB).What's more, the worst-case scenarios of several schemes require O(k) string lookups, where k is the number of components in each prefix. Thus, the sustained performance of existing solutions is not guaranteed. In this paper, we present a LNPM design based on the binary search of hash tables, which was originally proposed for IP lookup. With this design, the worst-case number of string lookups is O(log(k)) for prefixes with up to k components, regardless of the characteristics of the FIB. We implemented the design in software and demonstrated 10 Gbps throughput with one billion synthetic longest name prefix matching rules, each containing up to seven components. We also propose level pulling to optimize the average LNPM performance based on the observation that some prefixes have large numbers of next-level suffixes in the available URL datasets.","","978-1-4673-6633-5","10.1109/ANCS.2015.7110125","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7110125","Information-centric networking;longest name prefix lookup;binary search of hash tables","IP networks;Uniform resource locators;Memory management;Software;Internet;Multicore processing;Random access memory","file organisation;information networks;IP networks;packet switching;string matching;table lookup","reliably scalable name prefix lookup;information centric networking;ICN hierarchical naming scheme;multiple variable length name component;packet forwarding;longest name prefix matching;O(k) string lookup;LNPM design;binary hash table search;IP lookup;FIB;synthetic longest name prefix matching rules;LNPM performance optimisation;next level suffix;URL datasets","","29","","31","","21 May 2015","","","IEEE","IEEE Conferences"
"Automotive Data Traffic Filtering and Classification with Finding Errors","L. Roguljić; M. Vranjeś; M. Milośević; D. Samardźija","Institute RT-RK Osijek LLC for Information Technology,Osijek,Croatia,Cara Hadrijana 10b; Computer Science and Information Technology Osijek,Faculty of Electrical Engineering,Osijek,Croatia; Faculty of Technical Sciences Novi Sad Trg Dositeja Obradovića 6,Novi Sad,Serbia; Faculty of Technical Sciences Novi Sad Trg Dositeja Obradovića 6,Novi Sad,Serbia","2020 Zooming Innovation in Consumer Technologies Conference (ZINC)","7 Aug 2020","2020","","","201","206","Due to the rapid development of automotive technology and the rapid increase in number of advanced driver assistance systems (ADAS) in today's vehicles, the amount of automotive data traffic is significantly rising day by day. Finding errors and disrupted messages in vehicle communication network is crucial part of vehicle safety system, as well as the safety of the whole its environment. The focus of this paper is analyzing the automotive data traffic transmitted via the CAN bus in the vehicle. The proposed software solution parses the.asc log which contains all the traffic data on the bus. The software solution filters and classifies messages according to their names and routing direction, and according to the routing rules in the network that are described in the routing scenario. Furthermore, proposed solution finds errors by looking for irregularities in data block and checking the response time of the message. As a result of the software solution, a report is given in form of.xlsx file. Based on this report, it is possible to analyze the communication and take steps to improve future communication. The proposed solution is created in Python script language and has features of modularity and upgradability since it is created in modules that can be modified independently of other modules. Verification was carried out within the software solution so that different cases were created by entering anomalies into the.asc file. The results of the verification are positive and meet the requirements set prior to making the solution itself. The proposed solution solves the problem of displaying dry data so that all data read from the bus is filtered, classified and displayed in graphical form for easier analysis of the data itself.","","978-1-7281-8259-9","10.1109/ZINC50678.2020.9161816","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9161816","Automotive data;communication protocols;cAN;error detection;routing","Software;Protocols;Routing;Filtering;Data communication;Advanced driver assistance systems","authoring languages;controller area networks;driver information systems;field buses;pattern classification;Python;road safety;road vehicles;vehicular ad hoc networks","vehicle safety system;traffic data;software solution filters;routing direction;routing rules;data block;dry data;automotive technology;advanced driver assistance systems;disrupted messages;vehicle communication network;automotive data traffic filtering;automotive data traffic classification;CAN bus;Python script language","","","","11","","7 Aug 2020","","","IEEE","IEEE Conferences"
"The development of reducing risk system for running injury","S. Buddhamongkol; W. Suntiamorntut","Department of computer engineering, Faculty of Engineering, Prince of Songkla University, Hadyai Sonkhla 90112 Thailand; Department of computer engineering, Faculty of Engineering, Prince of Songkla University, Hadyai Sonkhla 90112 Thailand","2016 13th International Joint Conference on Computer Science and Software Engineering (JCSSE)","21 Nov 2016","2016","","","1","5","This paper describes the detection and monitoring human running for alert and reduces a risk of injuries. The design divided into two part. There are comprised of the detection of stop suddenly running and the monitoring ground force reaction (GFR) of human running behavior for analysis running gesture in medical. Design and development using the accelerometer based on a smartphone. The recognition using the Support Vector Magnitude (SVM) algorithm for detected stop suddenly running and the third of Newton's laws for calculated GFR. The system alert using the vibrator of smartphone and collected into a log file for monitoring human posture. The experimental results using true classification rate for analyzing the performance of SVM acceleration and represented the corrected of human running stop suddenly. For male runner accuracy about 97 percentage and female runner accuracy about 94 percentage.","","978-1-5090-2033-1","10.1109/JCSSE.2016.7748843","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7748843","Accelerometer;stop sudden;human monitoring","Support vector machines;Acceleration;Force;Injuries;Monitoring;Legged locomotion;Accelerometers","health care;medical computing;Newton method;risk analysis;support vector machines","reducing risk system;running injury;human running monitoring;human running detection;ground force reaction;GFR;human running behavior;support vector magnitude;SVM algorithm;Newton's laws","","","","11","","21 Nov 2016","","","IEEE","IEEE Conferences"
"On the Practicality of a Smart Contract PKI","C. Patsonakis; K. Samari; A. Kiayiasy; M. Roussopoulos",University of Athens; University of Athens; University of Edinburgh and IOHK; University of Athenss,"2019 IEEE International Conference on Decentralized Applications and Infrastructures (DAPPCON)","1 Aug 2019","2019","","","109","118","Public key infrastructures (PKIs) are one of the main building blocks for securing communications over the Internet. Currently, PKIs are under the control of centralized authorities, which is problematic as evidenced by numerous incidents where they have been compromised. The distributed, fault tolerant log of transactions provided by blockchains and more recently, smart contract platforms, constitutes a powerful tool for the decentralization of PKIs. To verify the validity of identity records, blockchain-based identity systems store on chain either all identity records, or, a small (or even constant) sized amount of data for verifying identity records stored off chain. However, as most of these systems have never been implemented, there is little information regarding the practical implications of each design's tradeoffs. In this work, we first implement and evaluate the only provably secure, smart contract based PKI of Patsonakis et al. on top of Ethereum. This construction incurs constant-sized storage at the expense of computational complexity. To explore this tradeoff, we propose and implement a second construction which, eliminates the need for trusted setup, preserves the security properties of Patsonakis et al. and, as illustrated through our evaluation, is the only version with constant-sized state that can be deployed on the live chain of Ethereum. Furthermore, we compare these two systems with the simple approach of most prior works, e.g., the Ethereum Name Service, where all identity records are stored on the smart contract's state, to illustrate several shortcomings of Ethereum and its cost model. We propose several modifications for fine tuning the model, which would be useful to be considered for any smart contract platform like Ethereum so that it reaches its full potential to support arbitrary distributed applications.","","978-1-7281-1264-0","10.1109/DAPPCON.2019.00022","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8783157","blockchain;smart contract;Ethereum;accumulator","Smart contracts;Blockchain;Additives;Internet;Cryptography;Privacy","computational complexity;contracts;formal specification;Internet;public key cryptography;software fault tolerance;system monitoring;telecommunication security","smart contract PKI;public key infrastructures;main building blocks;securing communications;centralized authorities;smart contract platform;blockchain-based identity systems store;provably secure contract;Ethereum;constant-sized storage;security properties;constant-sized state;live chain;Internet;distributed applications;communication security;identity records verification","","3","","49","","1 Aug 2019","","","IEEE","IEEE Conferences"
"Wireless fingerprint attendance management system","M. Kamaraju; P. A. Kumar","Department of E.C.E, Gudlavalleru Engineering College, Andhra Pradesh, India-521 356; Department of E.C.E, Gudlavalleru Engineering College, Andhra Pradesh, India-521 356","2015 IEEE International Conference on Electrical, Computer and Communication Technologies (ICECCT)","27 Aug 2015","2015","","","1","6","This paper presents the design methodology of a simple and high real time Zigbee - biometric system for easy and time saving attendance management using the finger prints of the employees at any organization along with the employee incoming and outgoing log maintenance. Firstly employee's fingerprints are scanned by software and an identity number is allotted as their enrollment. During the attendance time when employees impress their fingerprints, against the scanner, the system compares the new fingerprint patterns and the connection between various points in the fingerprint with the enrollment database. A match is recorded as a knock exercising acquisition, processing, transmission, matching. Through this automatic system, time and manpower is reduced to the great extent.","","978-1-4799-6085-9","10.1109/ICECCT.2015.7226163","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7226163","attendance;zigbee;fingerprint;gabor filter;image enhancement","Zigbee;Digital signal processing;Fingerprint recognition;Image resolution;Transmitters;Computers;Ports (Computers)","business data processing;fingerprint identification;personnel;Zigbee","wireless fingerprint attendance management system;Zigbee-biometric system;time saving attendance management;employee incoming log maintenance;employee outgoing log maintenance;fingerprint scanning;fingerprint patterns;enrollment database;knock exercising acquisition","","10","","11","","27 Aug 2015","","","IEEE","IEEE Conferences"
"Semi-synthetic data set generation for security software evaluation","F. Skopik; G. Settanni; R. Fiedler; I. Friedberg","Safety and Security Department, AIT Austrian Institute of Technology, Austria; Safety and Security Department, AIT Austrian Institute of Technology, Austria; Safety and Security Department, AIT Austrian Institute of Technology, Austria; Safety and Security Department, AIT Austrian Institute of Technology, Austria","2014 Twelfth Annual International Conference on Privacy, Security and Trust","8 Sep 2014","2014","","","156","163","Threats to modern ICT systems are rapidly changing these days. Organizations are not mainly concerned about virus infestation, but increasingly need to deal with targeted attacks. This kind of attacks are specifically designed to stay below the radar of standard ICT security systems. As a consequence, vendors have begun to ship self-learning intrusion detection systems with sophisticated heuristic detection engines. While these approaches are promising to relax the serious security situation, one of the main challenges is the proper evaluation of such systems under realistic conditions during development and before roll-out. Especially the wide variety of configuration settings makes it hard to find the optimal setup for a specific infrastructure. However, extensive testing in a live environment is not only cumbersome but usually also impacts daily business. In this paper, we therefore introduce an approach of an evaluation setup that consists of virtual components, which imitate real systems and human user interactions as close as possible to produce system events, network flows and logging data of complex ICT service environments. This data is a key prerequisite for the evaluation of modern intrusion detection and prevention systems. With these generated data sets, a system's detection performance can be accurately rated and tuned for very specific settings.","","978-1-4799-3503-1","10.1109/PST.2014.6890935","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6890935","anomaly detection evaluation;synthetic data set generation;scalable system behavior model","Virtual machining;Databases;Testing;Complexity theory;Intrusion detection;Data models","data handling;security of data","semisynthetic data set generation;security software evaluation;ICT systems;information and communication technology systems;virus infestation;ICT security systems;self-learning intrusion detection systems;heuristic detection engines;intrusion detection and prevention systems","","12","","14","","8 Sep 2014","","","IEEE","IEEE Conferences"
"Machine learning on merging static and dynamic features to identify malicious mobile apps","M. Su; J. Chang; K. Fung","Department of Computer Science and Information Engineering, Ming Chuan University, Taoyuan, Taiwan; Department of Computer Science and Information Engineering, Ming Chuan University, Taoyuan, Taiwan; Department of Computer Science and Information Engineering, Ming Chuan University, Taoyuan, Taiwan","2017 Ninth International Conference on Ubiquitous and Future Networks (ICUFN)","27 Jul 2017","2017","","","863","867","The amount of Android system-targeted malware has increased dramatically in recent years, and Android has been the focus of far more malware targeting than other mobile operating systems. In order to reduce the hazards of malware, this paper proposes a malware detection system with static and dynamic app features. In terms of the static features, the permissions, native-permissions, function and priority of an app are extracted as the base of analysis. In terms of the dynamic feature, the app is executed in a sandbox emulator, and then log files are analyzed to identify behaviors that help judgment, such as sending short messages without permission, modifying system files or reading personal data. This system extracts the static and dynamic features of an app, which are then merged before the weights are adjusted appropriately. Finally, Weka is used for training to obtain the detection module. According to the experiment, an unknown malicious act is evaluated using tenfold cross validation; the proposed system achieves a 97.4% accuracy.","2165-8536","978-1-5090-4749-9","10.1109/ICUFN.2017.7993923","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7993923","Machine learning;Weka;Android system;static feature;dynamic feature;feature weight;Android emulator;tenfold;malware","Malware;Smart phones;Feature extraction;Training;Mobile communication;Google","Android (operating system);invasive software;learning (artificial intelligence);mobile computing","machine learning;static app features;dynamic app features;malicious mobile apps;Android system-targeted malware;mobile operating systems;malware detection system;native-permissions;sandbox emulator;short messages;system files;personal data;Weka;detection module;malicious act;tenfold cross validation","","2","","27","","27 Jul 2017","","","IEEE","IEEE Conferences"
"Fast Delaunay Triangulation and Voronoi Diagram Generation on the Sphere","Y. Ma; Q. Chen","Center for Digital Media Comput., Shenzhen Institutes of Adv. Technol., Shenzhen, China; Center for Digital Media Comput., Shenzhen Institutes of Adv. Technol., Shenzhen, China","2010 Second World Congress on Software Engineering","22 Feb 2011","2010","1","","187","190","We describe a fast surface reconstruction approach that takes random points distributed near the surface of a sphere as input, and generates as output a Delaunay surface mesh and its dual Voronoi diagram of the sphere. The method starts from dividing the sphere surface into several initial triangles and introduces a concept of index sites in order to employ the randomized incremental algorithm to get the Delaunay triangulation. We develop a heuristic point search method which can locate a random point within the current triangle efficiently. This method is very efficient because no additional storage is needed to record the flip history and a new random point insertion algorithm is used. We test the performance on a collection of point sample sets and demonstrate a 30% performance improvement compared to existing O(n log n) 3D randomized incremental algorithms.","","978-1-4244-9287-9","10.1109/WCSE.2010.136","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5718291","randomized incremental algorithm;Delaunay triangulation;Voronoi diagram","Mathematical model;Equations;Indexes;Three dimensional displays;Surface reconstruction;History;Algorithm design and analysis","computational geometry;mesh generation;random processes;search problems;surface reconstruction","Delaunay triangulation;Voronoi diagram generation;surface reconstruction;Delaunay surface mesh;sphere surface;index site;randomized incremental algorithm;heuristic point search;random point insertion algorithm","","","","12","","22 Feb 2011","","","IEEE","IEEE Conferences"
"Interfacing Computing Platforms for Dynamic Control and Identification of an Industrial KUKA Robot Arm","L. Salameen; A. Estatieh; S. Darbisi; T. A. Tutunji; N. A. Rawashdeh","German Jordanian University,Dept. of Mechatronics Engineering,Amman,Jordan; German Jordanian University,Dept. of Mechatronics Engineering,Amman,Jordan; German Jordanian University,Dept. of Mechatronics Engineering,Amman,Jordan; Philadelphia University,Dept. of Mechatronics Engineering,Amman,Jordan; Michigan Technological University,Dept. of Applied Computing,Houghton,MI,USA","2020 21st International Conference on Research and Education in Mechatronics (REM)","12 Jan 2021","2020","","","1","5","The KUKA Agilus industrial robot arm is a relatively small robot suitable for industrial automation applications requiring a fast light-weight robot. This robot is popular with university research laboratories and educational settings. Several tools for interfacing personal computers and software packages with the robot controller are shared amongst researchers and developers. This paper presents a user interface that was developed as an interface platform between the KUKA Agilus industrial robot arm and personal computers. The developed program is dubbed, the GJU-KUKA Tool Kit, and includes two tools. The first tool uses the JOpenShowVar toolkit to dynamically collect the logging data from the robots six joints to the personal computer, enabling the analysis of robot motion dynamics and real-time control. The second tool uses OpenCV library to draw image files uploaded on the personal computer. The functionality of these tools are described in this paper along with a discussion of sample output and potential research applications.","","978-1-7281-6224-9","10.1109/REM49740.2020.9313878","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9313878","KUKA;industrial robot;interfacing;control;identification;image processing","Robots;Service robots;Tools;Mobile robots;Microcomputers;Libraries;End effectors","control engineering computing;image sensors;industrial manipulators;Java;mobile robots;motion control;production engineering computing;public domain software;robot vision","interfacing personal computers;robot controller;user interface;interface platform;KUKA Agilus industrial robot arm;personal computer;GJU-KUKA Tool Kit;robots six joints;robot motion dynamics;computing platforms;dynamic control;industrial KUKA robot arm;industrial automation applications;fast light-weight robot","","","","26","","12 Jan 2021","","","IEEE","IEEE Conferences"
"MEMS accelerometer based low-cost collision impact analyzer","M. S. A. Rupok; H. K. Patnaik; Xuewen Ding; S. Ganesan","Electrical and Computer Engineering, Oakland University, Rochester hills, Michigan, USA; Electrical and Computer Engineering, Oakland University, Rochester hills, Michigan, USA; Electrical and Computer Engineering, Oakland University, Rochester hills, Michigan, USA; Electrical and Computer Engineering, Oakland University, Rochester hills, Michigan, USA","2016 IEEE International Conference on Electro Information Technology (EIT)","8 Aug 2016","2016","","","0393","0396","Accident impact analysis helps to minimize losses such as death, injuries and property damage due to vehicle collision on road. An efficient impact analyzer is capable of accumulating and processing large amount of data very quickly. Hardware and software both plays important role to collect, process and store large amount of data within a very short period of time. MEMS accelerometers are cost effective and offer better performance with high functionalities compared to conventional piezoelectric accelerometers. The purpose of this paper is to conduct in-depth analyses of low-cost impact analyzer. Hardware and software consideration to implement a low cost impact analyzer (LCIA) are discussed. To simulate the accident, the system has been dropped from a certain height. With time-stamp, the tested data was recorded into microSD card and later the data was analyzed in computer and discussed.","2154-0373","978-1-4673-9985-2","10.1109/EIT.2016.7535272","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7535272","MEMS;Accelelerometer;Real-time data logging;G-force;Collision Impact","Accelerometers;Acceleration;Sensors;Micromechanical devices;Accidents;Vehicles;Hardware","accelerometers;micromechanical devices","MEMS accelerometer;low-cost collision impact analyzer;accident impact analysis;vehicle collision;piezoelectric accelerometer;low cost impact analyzer;LCIA;time-stamp;microSD card;microelectromechanical system","","1","","16","","8 Aug 2016","","","IEEE","IEEE Conferences"
"Rapid shallow coastal coral reef mapping using the teardrop system","F. J. Corpuz; P. Naval; E. Capili; J. Jauod; R. J. Judilla; M. Soriano","Vision and Image Processing Group, National Institute of Physics, University of the Philippines Diliman, Quezon City, Philippines; Computer Vision and Machine, Intelligence Group, Department of Computer Science, University of the Philippines Diliman, Quezon City, Philippines; Mapua Robotics Center, Special Projects Laboratory, Institutional Laboratory Mgmt Office, Mapua Institute of Technology, Intramuros, Manila, Philippines; Mapua Robotics Center, Special Projects Laboratory, Institutional Laboratory Mgmt Office, Mapua Institute of Technology, Intramuros, Manila, Philippines; Mapua Robotics Center, Special Projects Laboratory, Institutional Laboratory Mgmt Office, Mapua Institute of Technology, Intramuros, Manila, Philippines; Vision and Image Processing Group, National Institute of Physics, University of the Philippines Diliman, Quezon City, Philippines","2012 Oceans","10 Jan 2013","2012","","","1","6","We present a rapid shallow coastal reef mapping system using a previously developed inexpensive diver less video capture system coupled with a fast-image mosaicking technique, using an affordable GPS logging device. The system replaces the traditional manta tow system of assessing and monitoring large areas of coastal coral reefs and at the same time provides historical visual record for future references. The system utilizes Teardrop, a hollow, teardrop-shaped that houses a commercial underwater point-and-shoot camera. The camera is set to video mode and captures video of the reef while the Teardrop is being towed by a wooden fishing boat. The resulting video is parsed into frames and mosaicked by using fast image labeling. The approximate location of the mosaicked images is recorded by using an inexpensive GPS logger. The coordinates are embedded on the mosaicked images and overlaid on mapping software such as Google Earth for visual record and spatial information integration. The same points are used to compute for the total distance covered by the capturing process. The whole process from coral reef video capture to mosaicking the images and finally embedding the mosaics in mapping software can be done in as short as 1 day, providing coastal managers and local government units with a tool to rapidly visualize and monitor shallow coral reefs.","0197-7385","978-1-4673-0831-1","10.1109/OCEANS.2012.6405113","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6405113","image stitching;image mosaicing;fast image labelling;underwater imaging;towed-imaging apparatus;Teardrop;coral mapping","Global Positioning System;Monitoring;Cameras;Sea measurements;Earth;Boats;Google","geophysical image processing;image segmentation;oceanographic techniques","rapid shallow coastal coral reef mapping;teardrop system;rapid shallow coastal reef mapping system;video capture system;fast-image mosaicking technique;GPS logging device;traditional manta tow system;commercial underwater point-and-shoot camera;wooden fishing boat;fast image labeling;mosaicked images;inexpensive GPS logger;mapping software;Google Earth","","","","27","","10 Jan 2013","","","IEEE","IEEE Conferences"
"Pymote 2.0: Development of an Interactive Python Framework for Wireless Network Simulations","F. Shahzad","Information and Computer Science Department, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia","IEEE Internet of Things Journal","9 Jan 2017","2016","3","6","1182","1188","Wireless sensor networks (WSNs) are utilized in various applications and are providing the backbone for the new pervasive Internet, or Internet of Things. The development of a reliable and robust large-scale WSN system requires that the design concepts are checked and optimized before they are implemented and tested for a specific hardware platform. Simulation provides a cost effective and feasible method of examining the correctness and scalability of the system before deployment. In this paper, we study the performance of Pymote, a high level Python library for event-based simulation of distributed algorithms in wireless ad-hoc networks. We extended the Pymote framework allowing it to simulate packet level performance. The extension includes radio propagation, energy consumption, mobility and other models. The extended framework also provides interactive plotting, data collection and logging facilities for improved analysis and evaluation of the simulated system.","2327-4662","","10.1109/JIOT.2016.2570220","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7471405","Distributed event modeling;Python;simulation;wireless sensor network (WSN)","Wireless sensor networks;Object oriented modeling;Topology;Algorithm design and analysis;Internet of things;Energy consumption","ad hoc networks;software libraries;telecommunication computing;wireless sensor networks","interactive Python framework;wireless sensor networks;WSN;high level Python library;distributed algorithms;wireless ad-hoc networks;Pymote 2.0 framework","","4","","22","","18 May 2016","","","IEEE","IEEE Journals"
"The Landscape of Media Application Deployment","A. Kovalick","SMPTE Technical Conference, Hollywood, Ca, USA","The 2011 Annual Technical Conference & Exhibition","19 Oct 2015","2011","","","1","18","User software applications (apps) are the lifeblood of a media professional: creating, capturing, scheduling, logging, searching, editing, reviewing, managing, publishing, and more. The common desktop installable program (.exe) is being eclipsed by new methods of application deployment. Web methods, virtualization techniques, and cloud computing are enabling new ways to develop and deploy applications with outstanding value for end users and administrators. Of special importance are advantages gained in the areas of performance, simplicity, access patterns, software distribution, upgrades, and configuration management. This paper will survey the landscape of software application methods including Rich Internet Applications using browsers or sandboxed app players, hosted desktop virtualization, application streaming, and native apps for tablets/phones","","978-1-61482-940-9","10.5594/M001054","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7269271","Software applications;media apps;deployment;RIA;web apps;sandboxed apps;virtualization;VDI;hosted virtual desktop;desktop streaming;AJAX;HTML5","","","","","","","13","","19 Oct 2015","","","SMPTE","SMPTE Conferences"
"Examining Programmer Practices for Locally Handling Exceptions","M. B. Kery; C. Le Goues; B. A. Myers","Sch. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA; Sch. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA; Sch. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA","2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR)","26 Jan 2017","2016","","","484","487","Many have argued that the current try/catch mechanism for handling exceptions in Java is flawed. A major complaint is that programmers often write minimal and low quality handlers. We used the Boa tool to examine a large number of Java projects on GitHub to provide empirical evidence about how programmers currently deal with exceptions. We found that programmers handle exceptions locally in catch blocks much of the time, rather than propagating by throwing an Exception. Programmers make heavy use of actions like Log, Print, Return, or Throw in catch blocks, and also frequently copy code between handlers. We found bad practices like empty catch blocks or catching Exception are indeed widespread. We discuss evidence that programmers may misjudge risk when catching Exception, and face a tension between handlers that directly address local program statement failure and handlers that consider the program-wide implications of an exception. Some of these issues might be ad-dressed by future tools which autocomplete more complete handlers.","","978-1-4503-4186-8","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7832931","Java Exceptions;Boa;GitHub;Error Handlers","Java;Data mining;Face;Software systems;Program processors","exception handling;Java;object-oriented programming","try/catch mechanism;Exception;GitHub;Boa tool;Java;exception handling;programmer practices","","","","12","","26 Jan 2017","","","IEEE","IEEE Conferences"
"Evaluating Virtual Collaborative Learning platforms using Social Network Analysis","W. Tawileh","Wirtschaftsinformatik - Information Management, Technische Universität Dresden, Germany","2016 Sixth International Conference on Digital Information Processing and Communications (ICDIPC)","19 May 2016","2016","","","80","86","During their collaboration in a virtual learning environment, students leave “digital traces” that record their interaction with other students, instructors, and learning objects. These traces allow instructors to monitor students' work and encourage isolated or passive ones to participate actively. They also help course designers to assess students' behavior and experience in the learning environment. This paper aims to explore the effects of design changes in Virtual Collaborative Learning arrangements on the learning experience using Social Network Analysis. The network structures of two international collaborative courses were analyzed, visualized and compared using the Social Network Analysis tool Gephi based on log files of the Social Network software elgg. The results show changes in groups' interaction after adjusting technical functions of the platform, the case study and the deliverables' method. A high potential of Social Network Analysis to understand group dynamics in the virtual classroom is also demonstrated here.","","978-1-4673-7504-7","10.1109/ICDIPC.2016.7470796","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7470796","Virtual Collaborative Learning;Social Network Analysis;Web 2.0;Arab Countries;Jordan","Collaborative work;Social network services;Collaboration;Business;Computers;Blogs;Data visualization","behavioural sciences computing;computer aided instruction;educational administrative data processing;educational courses;social networking (online);virtualisation","virtual classroom;elgg social network software;Gephi social network analysis tool;international collaborative courses;network structures;virtual collaborative learning arrangements;course designers;digital traces;virtual learning environment;virtual collaborative learning platforms","","","","34","","19 May 2016","","","IEEE","IEEE Conferences"
"Implementing a Smart Contract PKI","C. Patsonakis; K. Samari; A. Kiayias; M. Roussopoulos","University of Athens, Athens, Greece; University of Athens, Athens, Greece; University of Edinburgh, Edinburgh, U.K.; University of Athens, Athens, Greece","IEEE Transactions on Engineering Management","16 Oct 2020","2020","67","4","1425","1443","Public key infrastructures (PKIs) provide the foundations for securing Internet communications. Currently, PKIs are operated by centralized authorities, which have been involved in numerous security incidents. Blockchain or smart contract PKIs employ their distributed, fault-tolerant log of transactions to store either all identity records, or, constant-sized data to verify identity records stored off-chain. However, as most of these systems have never been implemented, there is little information regarding their practical implications. In this article, we implement, evaluate, and provide a complete security proof for the smart contract-based PKI of (Patsonakis et al.) on Ethereum. This construction incurs constant-sized storage at the expense of computational complexity. To explore this tradeoff, we propose and implement a second construction which, eliminates the need for trusted setup, preserves its security properties and show that it is the only version with constant-sized state that can be deployed on Ethereum's live chain. We compare these constructions with the simple approach of storing all identity records on the smart contract's state, to illustrate several shortcomings of Ethereum and its cost model. We propose several modifications for fine tuning the model, which should be considered for any smart contract platform like Ethereum so that it may support arbitrary distributed applications.","1558-0040","","10.1109/TEM.2020.2972638","EU Horizon 2020 NEANIAS; ERC Starting; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9057412","Accumulator;blockchain;Ethereum;smart contract","Contracts;Public key;Internet;Additives","computational complexity;contracts;cryptography;formal specification;Internet;public key cryptography;software fault tolerance;system monitoring","constant-sized data;identity records;complete security proof;smart contract-based PKI;constant-sized storage;security properties;constant-sized state;smart contract platform;public key infrastructures;centralized authorities;smart contract PKI;secured Internet communications;Ethereum live chain","","1","","48","IEEE","6 Apr 2020","","","IEEE","IEEE Journals"
"milliScope: A Fine-Grained Monitoring Framework for Performance Debugging of n-Tier Web Services","C. Lai; J. Kimball; T. Zhu; Q. Wang; C. Pu","Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA; Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA; Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA; Comput. Sci. & Eng, Louisiana State Univ., Baton Rouge, LA, USA; Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA","2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS)","17 Jul 2017","2017","","","92","102","Modern distributed systems are often considered to be black boxes that greatly limit the potential to understand behaviors at the level of detail necessary to diagnose some of the most important types of performance problems. Recently researchers have found abnormal response time delays, one to two orders of magnitude longer than the average response time, that exist in short periods and cause economic loss for service providers. These very short bottlenecks are hard to detect due to their short life spans and their variety of possible reasons. In this paper, we propose milliScope (mScope), the first millisecond-granularity software-based resource and event monitoring for distributed systems that achieves both performance, low overhead at high frequency, and high accuracy matched with other firmware monitoring tool. More specifically, milliScope is a fine-grained monitoring framework to collaborate multiple mScopeMonitors for event and resource monitoring to reconstruct the flow of each client request and profile execution performance in a distributed system. We utilize the resource mScopeMonitors for system resource monitoring, and we develop our own event mScopeMonitors to identify the execution boundary in a lightweight, precise and systematic methodology. The semantic and syntactic of these monitoring logs with arbitrary formats are enriched by our multistage data transformation tool, mScopeDataTransformer, which unifies the diverse monitoring logs into a dynamic data warehouse, mScopeDB, for advanced analysis. We conduct several illustrative scenarios in which milliScope successfully diagnoses the response time anomalies caused by very short bottlenecks using a representative web application benchmark (RUBBoS).","1063-6927","978-1-5386-1792-2","10.1109/ICDCS.2017.228","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7979958","","Monitoring;Tools;Time factors;Semantics;Servers;Data warehouses;Frequency measurement","data warehouses;distributed processing;program debugging;system monitoring;Web services","milliScope;fine-grained monitoring framework;performance debugging;n-tier Web services;distributed systems;millisecond-granularity software-based resource;event monitoring;firmware monitoring tool;mScopeMonitors;profile execution performance;system resource monitoring;multistage data transformation tool;mScopeDataTransformer;dynamic data warehouse;mScopeDB;response time anomalies;representative Web application benchmark;RUBBoS","","6","","30","","17 Jul 2017","","","IEEE","IEEE Conferences"
"KPAT: A kernel and protocol analysis tool for embedded networking devices","M. Wang; C. Yu; C. Lin; C. Tseng; L. Yen","Dept. Computer Science, National Chiao Tung University, Hsinchu, Taiwan, R.O.C.; Dept. Computer Science, National Chiao Tung University, Hsinchu, Taiwan, R.O.C.; Dept. Computer Science, National Chiao Tung University, Hsinchu, Taiwan, R.O.C.; Dept. Computer Science, National Chiao Tung University, Hsinchu, Taiwan, R.O.C.; Dept. Computer Science and Information Engineering, National University of Kaohsiung, Kaohsiung, Taiwan, R.O.C.","2014 IEEE International Conference on Communications (ICC)","28 Aug 2014","2014","","","1160","1165","Sniffer tools capture protocol data. Kernel-profiling tools track function calls and events occurring in the kernel. These two types of tools help us observe external and internal behaviors of networking protocols, respectively. We need both types of data for a comprehensive view of protocol behavior. However, none of existing tools performs these two tasks in an integrated way. We developed Kernel and Protocol Analysis Tool (KPAT). KPAT injects software probes into Linux kernel to track interested function calls and event occurrences in the kernel. Probe injection is done systematically and does not require recompiling the kernel. A module in KPAT finds the association between the tracked functions and protocol data captured by an independent sniffer. The result as an integrated log allows users to identify two-way relationship between protocol data and the execution sequence of network functions in the kernel. We successfully used KPAT to identity accurate latency of each handover phase in IEEE 802.11 wireless networks. Experimental results show that KPAT causes light overhead to the patched kernel.","1938-1883","978-1-4799-2003-7","10.1109/ICC.2014.6883478","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6883478","Embedded Devices;Kernel Functions;Kernel Events;Networking Protocols;Packet Sniffers","Kernel;Protocols;Probes;Handover;Linux","Linux;operating system kernels;protocols;wireless LAN","KPAT;kernel and protocol analysis tool;embedded networking devices;protocol data;kernel-profiling tools;function calls;networking protocols;software probes;Linux kernel;probe injection;independent sniffer;IEEE 802.11 wireless networks;network function execution sequence;handover phase","","1","","31","","28 Aug 2014","","","IEEE","IEEE Conferences"
"Design and implementation of search engine on distributed network resources","Xuedou Yu","Department of Computer Science and Technology, Dezhou University, Shandong, China","2010 International Conference on Networking and Digital Society","7 Jun 2010","2010","2","","577","579","Through the research and analysis of search requests to the shared resource on the network, we have designed and implemented a distributed resource search engine that can achieve the following functions: when users search resources on Web, they need only log on a single server and can retrieve the resources exists on multiple servers. Article discusses the background of system design, the main functions of software, getting the state of the server, optimum word segment searching, XML data mode of operation in more detail. System simplifies the process of retrieval of resources, and improves the retrieval efficiency.","","978-1-4244-5162-3","10.1109/ICNDS.2010.5479424","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5479424","Dynamic data;XML;Resource retrieval","Search engines;File servers;Network servers;Information retrieval;XML;System testing;Databases;Computer science;Electronic mail;Telecommunication traffic","information retrieval;Internet;search engines;XML","search engine;distributed network resources;search resources;Web;optimum word segment searching;XML data mode;resource retrieval","","","1","7","","7 Jun 2010","","","IEEE","IEEE Conferences"
"Leveraging the Cloud to Achieve Near Real-time Processing for Drone-Generated Data","S. Sarkar; M. W. Totaro; K. Elgazzar","School of Computing and Informatics University of Louisiana Lafayette,Lafayette,Louisiana,USA; School of Computing and Informatics University of Louisiana Lafayette,Lafayette,Louisiana,USA; School of Computing and Informatics University of Louisiana Lafayette,Lafayette,Louisiana,USA","2019 IEEE Women in Engineering (WIE) Forum USA East","6 Feb 2020","2019","","","1","6","Low-cost drones are an emerging technological area that open the horizon for intelligent new Internet-of-Things (IoT) and a host of other applications. Cloud-based online processing of unmanned aerial vehicle (UAV) captured data is an attractive and interesting option toward real-time data processing, as it accommodates viewing and analyzing data from a variety of sources, thereby making data accessibility across devices significantly smooth and easier. This results in a more effective and seamless professional collaborations. Cloud-based solutions also make information technology (IT) management easier by automatically installing software updates, storing backups and eliminating the need for high-specification and expensive computers for data processing and storage. The ability to upload more than one just data-set at a time also provides greater flexibility, while security matters are handled using log-in credentials. In the present work, we discuss a comparative study between two data processing approaches: in one case the data captured by an UAV has been processed using an on-board low-cost single-board computer; in the other case, the captured data has been off-loaded to the cloud for further processing. Processing times between these two approaches are compared, statistical analysis applied, in order to confirm the superiority of the cloud-based processing approach. Challenges inherent with cloud-based processing have also been identified, which will be the subject of future research work.","","978-1-7281-3318-8","10.1109/WIEForum47344.2019.8981670","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8981670","Drone;real-time surveillance;onboard computer;file transfer protocol;cloud offloading;web application","","autonomous aerial vehicles;cloud computing;distributed control;statistical analysis","real-time processing;drone-generated data;low-cost drones;Internet-of-Things;cloud-based online processing;unmanned aerial vehicle captured data;UAV;real-time data processing;data accessibility;seamless professional collaborations;cloud-based solutions;information technology;data processing approaches;on-board low-cost single-board computer;statistical analysis;log-in credentials","","1","","25","","6 Feb 2020","","","IEEE","IEEE Conferences"
"An Experiment with Conceptual Clustering for the Analysis of Security Alerts","A. Paudice; S. Sarkar; D. Cotroneo","Dipt. di Ing. Elettr. e delle Tecnol. dell'Inf., Univ. degli Studi di Napoli Federico II, Naples, Italy; Dipt. di Ing. Elettr. e delle Tecnol. dell'Inf., Univ. degli Studi di Napoli Federico II, Naples, Italy; Dipt. di Ing. Elettr. e delle Tecnol. dell'Inf., Univ. degli Studi di Napoli Federico II, Naples, Italy","2014 IEEE International Symposium on Software Reliability Engineering Workshops","15 Dec 2014","2014","","","335","340","In response to attack against corporative and enterprise networks, administrators deploy intrusion detection systems, monitors, vulnerability scans and log systems. These systems monitor and record host and network device activities searching for signs of anomalies and security incidents. Doing that, these systems generally produce a huge number of alerts that overwhelms security analysts. This paper proposes the application of a conceptual clustering technique for filtering alerts and shows the results obtained for seven months of security alerts generated in a real large scale SaaS Cloud system. The technique has been useful to support manual analysis activities conducted by the operations team of the reference Cloud system.","","978-1-4799-7377-4","10.1109/ISSREW.2014.82","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6983863","Cloud;security;clustering;filtering;SIEM","Security;Manuals;Monitoring;Robustness;Training;Data analysis;Computer architecture","cloud computing;security of data","conceptual clustering;security alerts;corporative networks;enterprise networks;intrusion detection systems;vulnerability scans;log systems;anomalies;security incidents;real large scale SaaS cloud system","","1","14","18","","15 Dec 2014","","","IEEE","IEEE Conferences"
"An embedded web server based remote motor control system using virtual panel","N. Shalini; S. Siva Sakthi","Embedded System Technologies, Krishnasamy College of Engineering and Technology, Tamilnadu, India; Dept. of EEE, Krishnasamy College of Engineering and Technology, Tamilnadu, India","2014 International Conference on Circuits, Power and Computing Technologies [ICCPCT-2014]","5 Mar 2015","2014","","","1049","1054","In modern industrial field, the requirement for monitoring and controlling system is one of the most important criteria for minimizing the power consumption. In this paper, an effort is made to monitor and control the speed of the motor through TCP/IP connection using virtual instrumentation, labVIEW. A design of PIC microcontroller along with Ethernet module is presented Parameters such as Temperature, voltage, current can be monitored and read out with the help of internal ADC A web server can be embedded into any appliance and connected to the Internet so the appliance can be monitored and controlled from remote places through the browser in a desktop. By typing the IP address on the web browser the user get a web page on the screen; this page contains all the information about the status of the devices. By providing password, we can log on to the system and proceed the monitoring and controlling process. With the help of labVIEW software the system provides high accuracy and flexibility.","","978-1-4799-2397-7","10.1109/ICCPCT.2014.7054986","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7054986","Embedded web server;Ethernet;microcontroller;labVIEW;TCP/IP protocol","Web servers;Protocols;IP networks;Control systems;Browsers","analogue-digital conversion;computerised control;electric motors;embedded systems;file servers;Internet;IP networks;local area networks;machine control;microcontrollers;online front-ends;power consumption;transport protocols;velocity control;virtual instrumentation","embedded Web server;remote motor control system;virtual panel;monitoring system;controlling system;power consumption;speed;TCP-IP connection;virtual instrumentation;PIC microcontroller;Ethernet module;analog-to-digital converter;ADC;Internet;IP address;Web browser;Web page;monitoring process;controlling process;LabVIEW software","","4","","17","","5 Mar 2015","","","IEEE","IEEE Conferences"
"Ransomware detection using process mining and classification algorithms","A. Bahrani; A. J. Bidgly","University of Qom,Qom,Iran; University of Qom (of Affiliation),Department of computer engineering and IT,Qom,Iran","2019 16th International ISC (Iranian Society of Cryptology) Conference on Information Security and Cryptology (ISCISC)","6 Feb 2020","2019","","","73","77","The fast growing of ransomware attacks has become a serious threat for companies, governments and internet users, in recent years. The increasing of computing power, memory and etc. and the advance in cryptography has caused the complicating the ransomware attacks. Therefore, effective methods are required to deal with ransomwares. Although, there are many methods proposed for ransomware detection, but these methods are inefficient in detection ransomwares, and more researches are still required in this field. In this paper, we have proposed a novel method for identify ransomware from benign software using process mining methods. The proposed method uses process mining to discover the process model from the events logs, and then extracts features from this process model and using these features and classification algorithms to classify ransomwares. This paper shows that the use of classification algorithms along with the process mining can be suitable to identify ransomware. The accuracy and performance of our proposed method is evaluated using a study of 21 ransomware families and some benign samples. The results show j48 and random forest algorithms have the best accuracy in our method and can achieve to 95% accuracy in detecting ransomwares.","2475-2371","978-1-7281-4374-3","10.1109/ISCISC48546.2019.8985149","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8985149","Ransomware;Ransomware detection;Process Mining;j48;random forest","","data mining;feature extraction;Internet;invasive software;pattern classification","ransomware detection;classification algorithms;ransomware attacks;Internet users;computing power;process mining methods;process model;random forest algorithms;detection ransomwares;computing power","","2","","13","","6 Feb 2020","","","IEEE","IEEE Conferences"
"Design of a deep water cage culture environment monitoring system based on ZigBee","Yu Hu; Xiaohua Huang; Qiyou Tao; Shaomin Wang; Haiyang Liu; Genxi Guo; Jiaxiang Xue","South China Sea Fisheries Research Institute, CAFS, Guangzhou, China; South China Sea Fisheries Research Institute, CAFS, Guangzhou, China; South China Sea Fisheries Research Institute, CAFS, Guangzhou, China; South China Sea Fisheries Research Institute, CAFS, Guangzhou, China; South China Sea Fisheries Research Institute, CAFS, Guangzhou, China; South China Sea Fisheries Research Institute, CAFS, Guangzhou, China; School of Mechanical and Automotive Engineering, South China University of Technology, Guangzhou, China","2016 2nd IEEE International Conference on Computer and Communications (ICCC)","11 May 2017","2016","","","1545","1549","With the development of deep water cage culture digital management technology and accurate cultivation techniques, monitoring the overall deep water cage culture environment has become extremely important. A deep water cage culture environmental monitoring system is proposed for distant deep water cage culture sites to address the inconvenience of manual monitoring and the low realization rate of automated environmental control. The proposed system integrates a ZigBee wireless communication network, an Ethernet-based server monitoring platform, and a mobile client monitoring platform. Information about the cage culture environment is obtained by logging in to the remote server via the mobile client, and the available information includes water temperature, salinity, dissolved oxygen, and water velocity. This system achieves accurate monitoring of the deep water cage culture environment, and also solves field cabling and low efficiency problems. Additionally, the TCP/IP-based server monitoring platform and the mobile client monitoring platform are fully functional with a friendly user interface, which provides efficient and convenient management for culturists.","","978-1-4673-9026-2","10.1109/CompComm.2016.7924961","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7924961","deep water cage;ZigBee;monitoring","Monitoring;ZigBee;Servers;Mobile communication;Ports (Computers);Radio frequency;Software","aquaculture;computerised monitoring;file servers;mobile computing;transport protocols;Zigbee","deep water cage culture environment monitoring system;deep water cage culture digital management technology;cultivation techniques;distant deep water cage culture sites;ZigBee wireless communication network;Ethernet-based server monitoring platform;mobile client monitoring platform;remote server;water temperature;salinity;dissolved oxygen;water velocity;field cabling;TCP-IP-based server monitoring platform;user interface","","","","11","","11 May 2017","","","IEEE","IEEE Conferences"
"A framework for semi-automated process instance discovery from decorative attributes","A. Burattin; R. Vigo","Department of Pure and Applied Mathematics, University of Padua, Italy; Research and Development Department, SIAV S.p.A., Rubano, Italy","2011 IEEE Symposium on Computational Intelligence and Data Mining (CIDM)","11 Jul 2011","2011","","","176","183","Process mining is a relatively new field of research: its final aim is to bridge the gap between data mining and business process modelling. In particular, the assumption underpinning this discipline is the availability of data coming from business process executions. In business process theory, once the process has been defined, it is possible to have a number of instances of the process running at the same time. Usually, the identification of different instances is referred to a specific “case id” field in the log exploited by process mining techniques. The software systems that support the execution of a business process, however, often do not record explicitly such information. This paper presents an approach that faces the absence of the “case id” information: we have a set of extra fields, decorating each single activity log, that are known to carry the information on the process instance. A framework is addressed, based on simple relational algebra notions, to extract the most promising case ids from the extra fields. The work is a generalization of a real business case.","","978-1-4244-9927-4","10.1109/CIDM.2011.5949450","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5949450","","Business;Data mining;Semantics;Algebra;Buildings;Context;Software systems","business data processing;data mining;identification;information retrieval;relational algebra","semiautomated process instance discovery;decorative attribute;process mining;data mining;business process modelling;business process execution;case id information;activity log;process instance identification;relational algebra","","9","","15","","11 Jul 2011","","","IEEE","IEEE Conferences"
"eLog — Teaching and data management system: Seemless data capture and analysis of education data","C. I. Sheriff; A. Geetha","Department of Computer Science and Engineering, B. S. Abdur Rahman University, Chennai, India; Department of Computer Science and Engineering, B.S. Abdur Rahman University, Chennai, India","2013 IEEE International Conference in MOOC, Innovation and Technology in Education (MITE)","6 Mar 2014","2013","","","310","314","Graduate and Higher Education sector is witnessing increase proliferation of education\campus management software for handling and management of all the data pertaining to the students, right from admissions to final graduation. Traditionally all these data was maintained using manual record\logs and the same was used as supporting documents for ISO and other quality audits\certifications. This transition from paper based data entry to software enabled systems is still in infancy, making the job a faculty members cumbersome as they have to do the job twice, enter into manuals records and then replicate the same in education\campus management software. This paper proposes eLog, a technology enabled solution to address this gap by seamless capturing of education data, reducing faculty workload and focusing on considerable analysis of data which leads to taking remedial actions to improve performance and attendance.","","978-1-4799-1626-9","10.1109/MITE.2013.6756356","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6756356","education management;teaching;attendance","Software;Tablet computers;Smart phones;Educational institutions;Economics;Servers","computer aided instruction;data analysis;further education","data management system;teaching management system;eLog;data capture;data analysis;education data;higher education sector;graduate education sector;education-campus management software;final graduation;quality audits-certification;faculty members","","1","","16","","6 Mar 2014","","","IEEE","IEEE Conferences"
"A research on vulnerability discovering for router protocols based on fuzzing","Zhiqiang Wang; Yuqing Zhang; Qixu Liu","State Key Laboratory of Integrated Services Networks, Xidian University, China; State Key Laboratory of Integrated Services Networks, Xidian University, China; National Computer Network Intrusion Protection Center, Graduate University of Chinese Academy of Sciences, China","7th International Conference on Communications and Networking in China","24 Jan 2013","2012","","","245","250","How to discover router vulnerabilities effectively and automatically is a critical problem to ensure network and information security. Previous research on router security is mostly about the technology of exploiting known flaws of routers. Fuzzing is a famous automated vulnerability finding technology, however, traditional Fuzzing tools are designed for testing network applications or other software. These tools are not or partly not suitable for testing routers. This paper designs a framework of discovering router protocol vulnerabilities, and proposes a mathematical model Two-stage Fuzzing Test Cases Generator (TFTCG) that improves previous methods to generate test cases. We have developed a tool called RPFuzzer based on TFTCG. RPFuzzer monitors routers by sending normal packets, keeping watch on CPU utilization and checking system logs, which can detect DoS, router reboot and so on. RPFuzzer'debugger based on modified Dynamips, which can record register values when an exception occurs. Finally, we experiment on the SNMP protocol, find 8 vulnerabilities,of which there are five unreleased vulnerabilities. The experiment has proved the effectiveness of RPFuzzer.","","978-1-4673-2699-5","10.1109/ChinaCom.2012.6417484","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6417484","router security;fuzzing;TFTCG;protocol vulnerability discovering","Routing protocols;Monitoring;Testing;Security;Generators;Databases","program testing;routing protocols;software tools;telecommunication security","router vulnerability discovering;router protocol vulnerabilities;information security;router security;router testing;two-stage fuzzing test cases generator;TFTCG;RPFuzzer;CPU utilization;system logs;denial of service;DoS;router reboot;Dynamips;SNMP protocol","","3","","25","","24 Jan 2013","","","IEEE","IEEE Conferences"
"Mining temporal intervals from real-time system traces","S. Kauffman; S. Fischmeister","Electrical & Computer Engineering, University of Waterloo, Canada; Electrical & Computer Engineering, University of Waterloo, Canada","2017 6th International Workshop on Software Mining (SoftwareMining)","9 Nov 2017","2017","","","1","8","We introduce a novel algorithm for mining temporal intervals from real-time system traces with linear complexity using passive, black-box learning. Our interest is in mining nfer specifications from spacecraft telemetry to improve human and machine comprehension. Nfer is a recently proposed formalism for inferring event stream abstractions with a rule notation based on Allen Logic. The problem of mining Allen's relations from a multivariate interval series is well studied, but little attention has been paid to generating such a series from symbolic time sequences such as system traces. We propose a method to automatically generate an interval series from real-time system traces so that they may be used as inputs to existing algorithms to mine nfer rules. Our algorithm has linear runtime and constant space complexity in the length of the trace and can mine infrequent intervals of arbitrary length from incomplete traces. The paper includes results from case studies using logs from the Curiosity rover on Mars and two other realistic datasets.","","978-1-5386-1389-4","10.1109/SOFTWAREMINING.2017.8100847","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8100847","","Real-time systems;Measurement;Complexity theory;Telemetry;Software;Data mining;Tools","aerospace computing;data mining;formal specification;learning (artificial intelligence);real-time systems;space vehicles","mining nfer specifications;mining Allen's relations;multivariate interval series;symbolic time sequences;real-time system traces;mine nfer rules;linear runtime;constant space complexity;infrequent intervals;mining temporal intervals;black-box learning","","2","","29","","9 Nov 2017","","","IEEE","IEEE Conferences"
"Characterization of MPI Usage on a Production Supercomputer","S. Chunduri; S. Parker; P. Balaji; K. Harms; K. Kumaran",NA; NA; NA; NA; NA,"SC18: International Conference for High Performance Computing, Networking, Storage and Analysis","14 Mar 2019","2018","","","386","400","MPI is the most prominent programming model used in scientific computing today. Despite the importance of MPI, however, how scientific computing applications use it in production is not well understood. This lack of understanding is attributed primarily to the fact that production systems are often wary of incorporating automatic profiling tools that perform such analysis because of concerns about potential performance over-heads. In this study, we used a lightweight profiling tool, called Autoperf, to log the MPI usage characteristics of production applications on a large IBM BG/Q supercomputing system (Mira) and its corresponding development system (Cetus). Autoperf limits the amount of information that it records, in order to keep the overhead to a minimum while still storing enough data to derive useful insights. MPI usage statistics have been collected for over 100K jobs that were run within a two-year period and are analyzed in this paper. The analysis is intended to provide useful insights for MPI developers and network hardware developers for their next generation of improvements and for supercomputing center operators for their next system procurements.","","978-1-5386-8384-2","10.1109/SC.2018.00033","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8665758","MPI;monitoring;Autoperf;core-hours","Tools;Hardware;Supercomputers;Libraries;Control systems;Production systems","application program interfaces;message passing;parallel machines;program diagnostics;software performance evaluation;software tools","production supercomputer;lightweight profiling tool;MPI usage characteristics;IBM BG/Q supercomputing system;MPI usage statistics;MPI developers;programming model;automatic profiling tools;Autoperf;temperature 100.0 K","","6","","34","","14 Mar 2019","","","IEEE","IEEE Conferences"
"Mobile Application for Adaptive Threshold Hunting in Transcranial Magnetic Stimulation","P. Julkunen","Department of Clinical Neurophysiology, Kuopio University Hospital, Kuopio, Finland","IEEE Transactions on Neural Systems and Rehabilitation Engineering","7 Aug 2019","2019","27","8","1504","1510","Application of transcranial magnetic stimulation (TMS) is expanding with many studies applying adaptive threshold hunting to determine a motor threshold (MT). In addition to being a measure of corticospinal excitability, the MT is used as a baseline stimulation intensity (SI) to which following investigative or modulatory SIs are referenced to. Currently available tools for determining MTs include system-integrated tools and third-party standalone software. System-integrated MT-tools are still rarely available and the stand-alone software usually demand a separate computer, and hence possess additional space-requirements. I introduce and validate a free Android-based mobile application (“ATH-tool”) for adaptive threshold hunting of the MT. The objective is to allow for a simple and validated recording of MTs with sharing capabilities for logs. For comparison, I applied Motor Threshold Assessment Tool 2.0, to compare the MT-values determined with the new application, as it applies closely the same routine. Computational validation with known true threshold values confirmed that the new application captured the true MT at high precision (error ≤ 0.9%). Previously published data on motor evoked potentials (MEPs) were used to simulate realistic response occurrence by considering experimental data from 15 healthy subjects at different stimulation intensities. The MTs of the different methods agreed well (ICC ≥ 0.971, p <; 0.001). There was no significant difference between the MTs determined with the different methods (p ≥ 0.151). The novel mobile application should make it easier for researchers and clinicians to determine MTs and log the results.","1558-0210","","10.1109/TNSRE.2019.2925904","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8752413","Mobile applications;motor threshold (MT);parameter estimation;transcranial magnetic stimulation (TMS);motor evoked potential (MEP)","Mobile applications;Tools;Smart phones;Transcranial magnetic stimulation;Software;Monte Carlo methods;Computational modeling","Android (operating system);bioelectric potentials;biomagnetism;biomedical electrodes;brain;medical signal processing;mobile computing;neuromuscular stimulation;neurophysiology;transcranial magnetic stimulation","adaptive threshold hunting;transcranial magnetic stimulation;baseline stimulation intensity;third-party standalone software;system-integrated MT-tools;additional space-requirements;free Android-based mobile application;ATH-tool;MT-values;known true threshold values;stimulation intensities;motor threshold assessment tool 2;system-integrated tools;TMS","Adult;Algorithms;Evoked Potentials, Motor;Female;Healthy Volunteers;Humans;Male;Mobile Applications;Reproducibility of Results;Transcranial Magnetic Stimulation;Young Adult","","","30","","1 Jul 2019","","","IEEE","IEEE Journals"
"System designed to enable scientific analysis on robot pollination algorithm for orchard robot research","F. Yang; H. S. Ahn; J. Lim; M. Nejati; H. Williams; B. MacDonald","Electrical and Computer Engineering, University of Auckland, Auckland, New Zealand; Electrical and Computer Engineering, University of Auckland, Auckland, New Zealand; Electrical and Computer Engineering, University of Auckland, Auckland, New Zealand; Electrical and Computer Engineering, University of Auckland, Auckland, New Zealand; Electrical and Computer Engineering, University of Auckland, Auckland, New Zealand; Electrical and Computer Engineering, University of Auckland, Auckland, New Zealand","2019 International Conference on Electronics, Information, and Communication (ICEIC)","6 May 2019","2019","","","1","6","For orchard robot research, in order to do scientific analysis on our pollination algorithm used by pollination robot, we developed an artificial canopy system. High speed light sensors are used as fake flowers. Laser pointers are used as fake pollinator nozzles mounted on robot, shooting the fake flowers and collect hit status by software. In this way precise fake flower hitting record can be scientifically analyzed. And the system can solve the following research difficulties in real orchard: (1) hard to count, no record for hitting time, no precise location information on pollinated flower can be found scientifically (2) flower season is too short (less than a month for kiwi fruit) (3) weather and (4) travel problem.","","978-89-950044-4-9","10.23919/ELINFOCOM.2019.8706475","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8706475","Artifical canopy;light sensors;photo diode;ROS;Raspberry pi;PSOC;subscriber;publisher;microcontroller;I2C;ADC;analogue multiplexser;calibration;timing;data logging;orchard robot;pollination;Qt5;custom UI;rqt plugin","Robot sensing systems;Calibration;Microcontrollers;Sensor systems;Graphical user interfaces","agricultural products;agriculture;lasers;mobile robots;nozzles;sensors","robot pollination algorithm;orchard robot research;artificial canopy system;high speed light sensors;fake flowers;fake pollinator nozzles;laser pointers;software;kiwi fruit","","","","16","","6 May 2019","","","IEEE","IEEE Conferences"
"Programmer's Performance with the Keystroke as an Indicator: A Further Study","D. Liu; S. Xu; Z. Cui","Brain Technol., CA, USA; NA; Sch. of Eng. & Comput. Sci., Univ. of the Pacific, Stockton, CA, USA","2012 IEEE/ACIS 11th International Conference on Computer and Information Science","7 Jun 2012","2012","","","577","583","This paper represents a follow-up work that investigates the keystroke pattern of programmers to understand their intellectual behavior while writing code. In this study, we conducted one case study in order to check how students would perform under pressure, both similarity and difference from the previous experiment. The motivation was that observing how programmers corresponded to pressure in terms of performance might help us to better understanding their cognitive process since pressure is indispensable daily programming job in industrial settings. We asked twenty-four second-year undergraduate students to conduct a case study in which they implemented a program. Their numbers of the keystrokes were recorded and their behavior and productivity were analyzed. The case study result demonstrates that there is no close relationship between quality of code and the numbers of keystrokes with programming under pressure. However, it seems that programmers with high performance have high keystroke productivity. Programmers are also more productive under pressure in terms of keystrokes.","","978-1-4673-1536-4","10.1109/ICIS.2012.88","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6211156","Software Development;Programming Performance;Cognitive Activity;Keystroke Logging","Productivity;Java;Programming profession;Educational institutions;Correlation;Programming environments","programming environments","programmer performance;programmer keystroke pattern;intellectual behavior;cognitive process;pressured performance;code quality;keystroke productivity","","","","18","","7 Jun 2012","","","IEEE","IEEE Conferences"
"Analysis of students' behavior in the process of operating system experiments","Lei Wang; Chao Gao; Tianyu Wo; Bin Shi","School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China","2016 IEEE Frontiers in Education Conference (FIE)","1 Dec 2016","2016","","","1","8","Operating system (OS) experiments consolidate the understanding of the OS concepts and cultivate good engineering practices. Major challenges, however, including large class sizes, diverse software versions, and timely identification of difficulties from lab reports, hurt teaching quality. To address these, we designed an integrated environment to support OS experiments and automated the release and testing of lab code. The environment helped students to write, debug, and run code, and it collected their behavior data. These data included login hours and frequency, commands executed, files opened, and the code submission frequency. We found through analyzing the data a lack of preliminary knowledge in some students. Extra instructions and extended deadlines helped them master the subjects. For some students, the files read indicated the lack of attention to some most relevant system architecture code. This finding allowed us to provide targeted and specific help. Other data revealed an interesting “lab 2” phenomenon where the behavioral difference between the competent and average students was much more pronounced for lab 2 than that for lab 1. The data showed that login hours and frequency and log size correlated positively but submission frequency correlated negatively with grades. Analysis of students' behavior data allowed us to realize continuous improvements in the experiment process. 62% of the 152 students completed four labs and 46% all six, a significant improvement over the last semester.","","978-1-5090-1790-4","10.1109/FIE.2016.7757575","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7757575","Student Behavior;Operating System Experiments;Teaching Process","Operating systems;Education;Servers;Programming;Linux;Virtual machining","computer science education;data analysis;operating systems (computers);program debugging;programming environments;student experiments","operating system experiments;engineering practice;software version;teaching quality;lab code;code writing;code debugging;code running;login hours;login frequency;command execution;code submission frequency;student knowledge;system architecture code;student behavioral difference;log size;student behavior data analysis","","","","18","","1 Dec 2016","","","IEEE","IEEE Conferences"
"Stress Detection Through Electrodermal Activity (EDA) and Electrocardiogram (ECG) Analysis in Car Drivers","P. Zontone; A. Affanni; R. Bernardini; A. Piras; R. Rinaldo","University of Udine,Polytechnic Department of Engineering and Architecture,Udine,Italy; University of Udine,Polytechnic Department of Engineering and Architecture,Udine,Italy; University of Udine,Polytechnic Department of Engineering and Architecture,Udine,Italy; University of Udine,Polytechnic Department of Engineering and Architecture,Udine,Italy; University of Udine,Polytechnic Department of Engineering and Architecture,Udine,Italy","2019 27th European Signal Processing Conference (EUSIPCO)","18 Nov 2019","2019","","","1","5","The stress in a driver, happening during unforeseen events or taxing situations, is linked to a subject's sympathetic system response. We present a system which detects the stress presence in car drivers through the analysis of an endosomatic Electrodermal Activity (EDA) signal, namely, Skin Potential Response (SPR), coupled with the analysis of the Electrocardiogram (ECG) signal. To log these signals we utilize a device which records the SPR from each hand of the driver, and the ECG from the chest. In the case of the SPR signal, since the hands movement injects motion artifacts, we also utilize an algorithm that dynamically selects the smoother signal coming from the two hands, and is thus able to output a clean SPR signal. Statistical features are then derived from the ECG and SPR signals, allowing their classification using a Supervised Machine Learning Algorithm. Various subjects were tested in an environment set in a company which develops professional driving simulators, both in hardware and software, and consisted in a motorized platform, a cockpit and a 180° projection screen. The test encompassed driving through a highway, with some unforeseen events happening at some positions. In the end we get a Balanced Accuracy in stress detection of 77.59 % for the considered events.","2076-1465","978-9-0827-9703-9","10.23919/EUSIPCO.2019.8902631","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8902631","Stress Detection;Skin Potential Response;Electrocardiogram;Motion Artifact Removal;Supervised Machine Learning Algorithm","Stress;Electrocardiography;Support vector machines;Skin;Automobiles;Motion artifacts;Heart rate","electrocardiography;learning (artificial intelligence);medical signal processing;skin","endosomatic electrodermal activity signal;skin potential response;stress presence;taxing situations;car drivers;EDA;stress detection;unforeseen events;clean SPR signal;smoother signal;hands movement;ECG;electrocardiogram signal","","5","","31","","18 Nov 2019","","","IEEE","IEEE Conferences"
"Quantitative analysis of GNSS performance under railway obstruction environment","D. Lu; S. Jiang; B. Cai; W. Shangguan; X. Liu; J. Luan","Beijing Engineering Research Center of EMC and GNSS Technology for Rail Transportation, 100044, Beijing, China; Beijing Engineering Research Center of EMC and GNSS Technology for Rail Transportation, 100044, Beijing, China; Beijing Engineering Research Center of EMC and GNSS Technology for Rail Transportation, 100044, Beijing, China; Beijing Engineering Research Center of EMC and GNSS Technology for Rail Transportation, 100044, Beijing, China; CRCC Qingdao Sifang Co., Ltd, 266111, Qingdao, China; CRCC Qingdao Sifang Co., Ltd, 266111, Qingdao, China","2018 IEEE/ION Position, Location and Navigation Symposium (PLANS)","7 Jun 2018","2018","","","1074","1080","GNSS (Global Navigation Satellite Systems) have been widely applied in railway applications as timing, passenger information, track survey, and also safety relevant applications as train localization for train control purposes on lines like Chicago-Detroit Line, Qinghai-Tibet railway line, etc. However, due to the continuous dynamic movement of the locomotive, it goes through various environments as tunnels, bridges, mountainous areas, urban canyons, etc. GNSS performance is affected and thus degraded in some GNSS signal reception constrained environment scenarios, which leads the localization result not meeting the required accuracy and safety requirements. With the dynamic movement of the locomotive, the measurement results are not abundantly enough. It is quite necessary to investigate the methodology for GNSS performance evaluation in signal reception constrained environments using both simulation and field test results together. This will reduce the test complexity and also provide the possibility to compare between different parameters. In this paper, a 3-km-long railway environment scenario in Qinghai-Tibet railway is selected and modelled with signal reception limitations recorded several times in the on-board log files. After an investigation of the terrain structure of the environment, a hill near the railway tracks namely mountainous environment is causing the “half sky” signal blocking scenario. The terrain is modelled using Google SketchUp. Then, the model data is converted into QualiSIM, which is a matlab-based tool to provide the possibility to understand the sky plot of the location according to almanac and relevant information. The satellite visibility, satellite geometry and signal propagation trajectories are calculated and illustrated in QualiSIM. Using the output from QualiSIM, the locomotive operation route, 3D environment “half sky” model and signal propagation trajectory are all configured and simulated using Spirent GSS8000 system. The GNSS signals are repeated using Spirent GSS8000 together with Spirent SimGEN software. A UBLOX M8N and Unicore UB370 GNSS receivers were used to test and compare the performance of both receivers under the modelled mountainous environment. The accuracy levels of both receivers are compared by satellite visible number, HDOP (Horizontal Dilution of Precision) and position error. The measurement errors are illustrated using error ellipse, which shows the different accuracy level and the reliability level for the two receivers. Comparing the recorded results of open sky and different levels of obstruction, GNSS performance can be gradually degraded when the degree of obstruction is increased.","2153-3598","978-1-5386-1647-5","10.1109/PLANS.2018.8373489","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8373489","Satellite navigation;train localization;obstruction environment;positioning error;quantification level","Global navigation satellite system;Rail transportation;Satellites;Safety;Tracking;Receivers;Buildings","Global Positioning System;radio receivers;railways","half sky signal blocking scenario;model data;QualiSIM;satellite visibility;satellite geometry;signal propagation trajectories;locomotive operation route;Spirent GSS8000 system;GNSS signals;modelled mountainous environment;satellite visible number;recorded results;railway obstruction environment;Global Navigation Satellite Systems;railway applications;train localization;train control purposes;Chicago-Detroit Line;Qinghai-Tibet railway line;continuous dynamic movement;GNSS performance evaluation;field test results;signal reception limitations;error ellipse;Unicore UB370 GNSS receivers;railway environment scenario;GNSS signal reception constrained environment scenarios;distance 3 km","","1","","14","","7 Jun 2018","","","IEEE","IEEE Conferences"
"An innovative device for monitoring and controlling vehicular movement in a Smart city","K. Kumarmanas; S. Praveen; V. Neema; S. Devendra","Institute of Engineering and Technology Devi Ahilya Vishwavidyalaya, Indore, India; Institute of Engineering and Technology Devi Ahilya Vishwavidyalaya, Indore, India; Institute of Engineering and Technology Devi Ahilya Vishwavidyalaya, Indore, India; Raja Ramanna Centre for Advanced, Technology, Indore-452013, India","2016 Symposium on Colossal Data Analysis and Networking (CDAN)","19 Sep 2016","2016","","","1","3","An innovative solution is proposed here to monitor, analyze and control vehicular movement in a city and also in sensitive border areas of the country. Various applications of proposed device are discussed in the paper. The method consists of installing a RF transmitter in a clandestine location along with the vehicle's electronics hardware, compulsorily, by road transport office during registration of vehicle. It is suggested that a particular bandwidth is allocated to these signals. Each and every vehicle continuously broadcasts its own identity through RF transmitter on an allotted frequency (registration number and the speed as obtained from speedometer). Low cost small sized transceivers are planted at various undisclosed locations in city to receive these vital information about of every vehicle crossing the particular transceiver. These locations compulsorily include all the exit points of the city. The transceiver is connected to the main server of police control room where the transmitted data is logged at open source database MySQL. This logging provides the informative data over web for authenticated users. The total cost of such a system which enables real time monitoring, recording and analysis of vehicular movements in a city is comparatively very low and the advantages are very significant. The proposed project serves as next generation proposal for the Smart city initiative of the Government of India. The first application is the generation of data which can be analyzed for constructing flyovers, deciding public transport routes and planning of other infrastructure. The second advantage pertains to online help in traffic control for example the system can be used for diverting vehicles away from congestion zones at peak traffic hours. A third application of this system will be in controlling vehicle theft. This solution may effectively utilized for solving cases of vehicular theft and `hit & run' cases. Last but not the least such data is very valuable and can be sold commercially to real estate developers, vehicle manufacturers and infrastructure planners. This paper provides technical details of such a system and the advantages perceived from it.","","978-1-5090-0669-4","10.1109/CDAN.2016.7570882","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7570882","","Vehicles;Transceivers;Real-time systems;Monitoring;Urban areas;Roads;Engines","automotive electronics;Internet;police data processing;public domain software;public transport;radio transceivers;road vehicles;smart cities;SQL","traffic control;Government of India;Web;MySQL;open source database;police control room;transceiver;road transport;vehicle electronics hardware;RF transmitter;smart city;vehicular movement controlling;vehicular movement monitoring;innovative device","","4","","6","","19 Sep 2016","","","IEEE","IEEE Conferences"
"Compressed domain-specific data processing and analysis","D. Dong; J. Herbert","Department of Computer Science, University College Cork, Ireland; Department of Computer Science, University College Cork, Ireland","2017 IEEE International Conference on Big Data (Big Data)","15 Jan 2018","2017","","","325","330","Domain specific data such as sensor outputs and server trace logs have low levels of symbol richness, and so they can be represented in a very compact format. In this paper, we present a bit-oriented compression scheme designed not only to represent the data compactly but also to allow MapReduce programs to perform analysis and processing directly on the compressed data, and to do so in parallel. The core of the compression scheme is a novel hybrid data structure supporting bit pattern searching in constant time, and a scheme for making a block-splittable compressed file. Supporting software allows developers to work transparently with the compressed data. Experimental results demonstrate that the proposed compression scheme can significantly reduce data size and improve MapReduce analysis performance.","","978-1-5386-2715-0","10.1109/BigData.2017.8257941","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8257941","Big Data;Algorithm;Compression;MapReduce","Big Data;Indexing;Synchronization;Data models;Approximation algorithms;Buildings;Probability distribution","data analysis;data compression;data structures","compressed domain-specific data processing;sensor outputs;server trace logs;symbol richness;compact format;compressed data;block-splittable compressed file;MapReduce analysis performance;domain specific data analysis;bit-oriented compression scheme;data representation;hybrid data structure supporting bit pattern searching","","","","10","","15 Jan 2018","","","IEEE","IEEE Conferences"
"Errors Detection Mechanism in Big Data","A. E. M. Eljialy; S. Ahmad","College of Computer Engineering & Sciences, Prince Sattam Bin Abdulaziz University,Department of Information System,P.O.Box 151, Alkharj,Saudi Arabia,11942; College of Computer Engineering & Sciences, Prince Sattam Bin Abdulaziz University,Department of Computer Science,P.O.Box 151, Alkharj,Saudi Arabia,11942","2019 International Conference on Smart Systems and Inventive Technology (ICSSIT)","10 Feb 2020","2019","","","323","328","The rapid growth of information technologies has not been exempt from errors in the transmission and storage of data, which has affected the services, offered to customers on big data platforms. In these platforms, it has been particularly difficult to identify the causes given their complex nature, the large size and the growth of the data set. The detection and correction of errors in the processing of information in big data platforms are essential to guarantee the integrity of the stored data and thus give users confidence that the information is correctly stored without alterations. However, the options that exist to detect errors of this type lack efficiency in the methods used or use third party software that does not always allow determining the cause of the errors. In this research, big data analysis models are used to find bottlenecks of data processing management. Further, it is developed an alternative error detection mechanism that is based on analyzing error-log file with a variation of Fuzzy clustering mechanism. This approach, based on text mining, helps to look for error frequency and entity that had caused it. Moreover, it is shown how the errors that are presented are distributed according to the source.","","978-1-7281-2119-2","10.1109/ICSSIT46314.2019.8987890","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8987890","Errors Detection;Big Data;Text mining;Fuzzy clustering","","Big Data;data analysis;data integrity;data mining;error detection;fuzzy set theory;pattern clustering;storage management;text analysis","big data platforms;big data analysis models;data processing management;error frequency;errors detection mechanism;information technologies;error-log file;data set;stored data integrity;fuzzy clustering mechanism;text mining","","","","16","","10 Feb 2020","","","IEEE","IEEE Conferences"
"Web service-based monitoring system for smart management of the buildings","V. Tanasiev; H. Necula; G. Darie; A. Badea","Department of Energy Production and Use, University POLITEHNICA of Bucharest, Bucharest, Romania; Department of Energy Production and Use, University POLITEHNICA of Bucharest, Bucharest, Romania; Department of Energy Production and Use, University POLITEHNICA of Bucharest, Bucharest, Romania; Academy of Romanian Scientists, Bucharest, Romania","2016 International Conference and Exposition on Electrical and Power Engineering (EPE)","12 Dec 2016","2016","","","025","030","Monitoring the energy use and other important parameters together with third party applications for data access can provide a strong tool for comparative analysis. Easy data interpretation and data maneuverability can bring sufficient information to improve the energy performance of the building. Web services for building monitoring can easily represent the underlying mechanism for decision-making entity in automated or intelligent buildings. The developed software solution is composed of three layers. The first layer processes the data retrieved from sensors through various web services which are lunched by a programmed scheduler. The second layer securely stores the collected data from sensors into database and logging files. The third layer is used by authenticated users to review and export the data in most known formats. The current paper presents how wired sensors' network can easily be used either as a building monitoring solution or as a part of smart management system.","","978-1-5090-6129-7","10.1109/ICEPE.2016.7781296","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7781296","web-service;wired sensors;building monitoring","Monitoring;Web services;Sensors;Buildings;Databases;Protocols","building management systems;decision making;home automation;power consumption;power engineering computing;sensors;Web services","Web service-based monitoring system;smart building management system;energy use monitoring;third party applications;data access;data maneuverability;decision-making entity;intelligent buildings;automated buildings;programmed scheduler;file logging;wired sensor network;building monitoring solution","","4","","12","","12 Dec 2016","","","IEEE","IEEE Conferences"
"Architectural design patterns for adaptable ubiquitous social networking systems","G. Alaa; M. Youssef; R. Hesham","The Egyptian Cabinet's Information & Decision Support Centre 1 Magies El Shaab Street, Cairo, Egypt; Faculty of Informatics & Computer Science, The British University in Egypt, El Sherouk City, Cairo, Egypt; Faculty of Informatics & Computer Science, The British University in Egypt, El Sherouk City, Cairo, Egypt","2012 8th International Conference on Informatics and Systems (INFOS)","12 Jul 2012","2012","","","SE-37","SE-43","Social Networking (SN) systems provide messaging facility as well as the ability to generate and share content in form of user profiles, photos, videos, reviews, ratings, etc. SN architecture is complex, as it covers numerous Web pages with various data types; profiles, instant messaging logs, photos, videos, etc. In addition, ubiquitous SNs enable access from portable devices and therefore require a lightweight, adaptable architecture to meet constraints in screen size, storage capacity and network download time. SN architectural design patterns are proposed to induce reusability and loose coupling, thus improve performance and adaptability. These patterns are derived from conceptual modeling of common SN functionalities found in literature. Five patterns are derived; `user profiles and friend list management', `messaging', `file sharing', `edit & post content' and `semantic services'. The patterns are validated by application on five large-scale SNs targeted at healthcare that showed better reusability, customizability and scalability, and reduced interdependence between SN subcomponents.","","978-977-403-506-7","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6236578","social network;architecture;patterns;ubiquiteous;design;healthcare","","electronic messaging;service-oriented architecture;social networking (online);software reusability;ubiquitous computing","adaptable ubiquitous social networking systems;messaging facility;content sharing;content generation;Web pages;data types;portable devices;screen size;storage capacity;network download time;SN architectural design patterns;performance improvement;adaptability improvement;file sharing;edit & post content;user profiles-and-friend list management;semantic services;healthcare;reusability;customizability;scalability","","","","24","","12 Jul 2012","","","IEEE","IEEE Conferences"
"Intelligent Groundwater Metering Control System","G. Pingxin; L. Qianqian; Z. Shuai","Department of Information Engineering, Weifang Engineering Vocational College, Qingzhou, 262500; Department of Information Engineering, Weifang Engineering Vocational College, Qingzhou, 262500; Department of Information Engineering, Weifang Engineering Vocational College, Qingzhou, 262500","2019 Chinese Control And Decision Conference (CCDC)","12 Sep 2019","2019","","","525","528","This paper introduces an intelligent groundwater metering control system consisting of the lower computer, which takes electromagnetic flowmeter, temperature sensor, power meter as the main data acquisition unit and the development board as the main data processing unit, and the host computer with the corresponding software installed. The development board uses the Modbus Remote Terminal Unit (RTU) protocol as the main data communication format. The digital and analog modules read the data in the acquisition unit in real time, and after recording and analyzing the data, it can independently complete some control and realize some functions, and with the help of General Packet Radio Service (GPRS) Data Transfer Unit's fully transparent transmission mode, relying on GPRS and Internet network, it uploads data and running logs to the management system of the host computer, realizing remote, real-time, wireless processing and recording of data. The actual operation shows that the system can solve the inconvenience of groundwater resource management well and has certain use value and promotion significance.","1948-9447","978-1-7281-0106-4","10.1109/CCDC.2019.8832943","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8832943","intelligent groundwater metering control system;electromagnetic flowmeter;ARM development board;host computer;lower computer","Flowmeters;Ground penetrating radar;Electromagnetics;Protocols;Control systems;Data communication;Real-time systems","data acquisition;flowmeters;groundwater;intelligent control;intelligent sensors;metering;protocols","host computer;development board;main data communication format;running logs;groundwater resource management;intelligent groundwater metering control system;power meter;main data acquisition unit;main data processing unit;general packet radio service data transfer unit;modbus remote terminal unit protocol","","","","4","","12 Sep 2019","","","IEEE","IEEE Conferences"
"An Efficient Cloud-Based Framework for Digital Media Knowledge Extraction","C. Kanchibhotla; P. Venkatesh; D. Somayajulu; P. R. krishna","Infosys Limited,Bangalore,India; Infosys Limited,Bangalore,India; National Institute of Technology,Warangal,India; National Institute of Technology,Warangal,India","2019 IEEE International Conference on Big Data (Big Data)","24 Feb 2020","2019","","","1841","1850","Most of the oil industries have a substantial volume of physical subsurface data generated as part of the exploration study. This data is collected over many decades and exists in various formats such as tapes, cartridges, CDs, DVDs, paper media comprising of maps, technical well reports, and seismic logs. These items are usually stored in large offsite repositories across the globe and is maintained by third-party vendors. Access to this historical data is crucial for oil companies as it helps to find potential prospects for oil extraction which otherwise require an exploratory study by geologists using satellite imagery, surface rocks, terrain, and seismology. Storing large volumes of technical data in offsite repositories also posts many key challenges such as high storage cost, high retrieval time and inaccessibility of information. To address the above challenges, companies are digitizing the physical data and complementing with rich metadata extraction by Optical Character Recognition(OCR). This introduces some more technical challenges while dealing with lower Dots Per Inch (DPI) scans, poor quality scans, and huge file size. Several frameworks are developed which store the data in local repositories but these frameworks have limitations with respect to the number of documents processed, huge file size and storage scalability. To deal with above-mentioned problems, we present a high-performance computing cloud-based framework by storing the digitized data in the cloud, metadata enrichment through OCR along with image enhancement by a series of Image Processing (IP) techniques and provide high data availability to users using cloud-based search. We have tested this framework with big oil and gas company's data on a huge scale and the results are encouraging. Although this paper addresses oil industries domain problem, the proposed framework can be applied to other domains that have huge physical data.","","978-1-7281-0858-2","10.1109/BigData47090.2019.9005480","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9005480","Physical data;oil and gas;Cloud;Azure;Image Processing;Optical Character Recognition","Metadata;Oils;Optical character recognition software;Companies;Geology;Data mining;Image processing","cloud computing;document handling;document image processing;image enhancement;meta data;optical character recognition;petroleum industry;production engineering computing","optical character recognition;oil industries domain problem;big oil;cloud-based search;data availability;image processing techniques;metadata enrichment;digitized data;high-performance computing cloud-based framework;storage scalability;metadata extraction;satellite imagery;oil extraction;oil companies;third-party vendors;offsite repositories;seismic logs;technical well reports;physical subsurface data;substantial volume;digital media knowledge extraction;cloud-based framework","","","","17","","24 Feb 2020","","","IEEE","IEEE Conferences"
"Exploit real-time fine-grained access patterns to partition write buffer to improve SSD performance and life-span","M. Wang; Y. Hu","Department of Electrical Engineering and Computing Systems, University of Cincinnati, OH 45221, USA; Department of Electrical Engineering and Computing Systems, University of Cincinnati, OH 45221, USA","2013 IEEE 32nd International Performance Computing and Communications Conference (IPCCC)","20 Feb 2014","2013","","","1","7","Solid State Drives (SSDs) have become very popular recently due to their high performance and other benefits such as shock-resistance. However, SSDs pose some unique and serious challenges to I/O and file system designers because of flash memory's unique properties, such as out-of-place update, wearing-out, and highly asymmetric performance for read, write and erase operations. Most SSDs employ a log-structured block-based FlashTranslation-Layer (FTL) to solve the out-of-place update problem. The performance of FTLs is often highly sensitive to access patterns, especially the write access patterns. For example, sequential write requests see lower overhead than random writes. Moreover, sequential write requests that are not aligned to the flash page boundary may cause extra write and garbage collection operations, increasing overhead and wear-out. In this paper, we present a novel write buffer design based on sophisticated, fine-grain write access pattern analysis. Our scheme identifies access patterns in a per-process per-stream granularity in the OS buffer cache. These patterns are then used to guide the write buffer to improve the write performance of SSDs that employ a log-structured block-based FTL. Simulation results show that our solutions can improve write performance by up to 38%. Moreover, the schemes reduce SSD erase cycles by up to 56%, which is directly translated to a major improvement on the life-span of SSDs.","2374-9628","978-1-4799-3214-6","10.1109/PCCC.2013.6742772","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6742772","SSD;Access patterns;Write buffer;Flash memory","Blogs;Flash memories;Software;Switches","buffer storage;disc drives;flash memories","fine-grained access pattern;partition write buffer;SSD performance;life-span;solid state drives;shock-resistance;flash memory;out-of-place update;wearing-out;asymmetric performance;log-structured block-based FTL;flasht translation-layer;write buffer design;OS buffer cache","","","2","19","","20 Feb 2014","","","IEEE","IEEE Conferences"
"Introducing Provenance Capture into a Legacy Data System","H. Conover; R. Ramachandran; B. Beaumont; A. Kulkarni; M. McEniry; K. Regner; S. Graves","Information Technology and Systems Center, University of Alabama in Huntsville, Huntsville, AL, USA; Information Technology and Systems Center, University of Alabama in Huntsville, Huntsville, USA; Information Technology and Systems Center, University of Alabama in Huntsville, Huntsville, AL, USA; Information Technology and Systems Center, University of Alabama in Huntsville, Huntsville, AL, USA; Information Technology and Systems Center, University of Alabama in Huntsville, Huntsville, AL, USA; Information Technology and Systems Center, University of Alabama in Huntsville, Huntsville, AL, USA; Information Technology and Systems Center, University of Alabama in Huntsville, Huntsville, AL, USA","IEEE Transactions on Geoscience and Remote Sensing","24 Oct 2013","2013","51","11","5098","5104","Accurate provenance information facilitates improved understanding of Earth science data and scientific reproducibility and can serve as an indicator of data quality. Provenance capture is an integral part of many modern workflow systems but may not have been considered in the design of legacy data production systems. Furthermore, in addition to data lineage, it is also important to capture contextual information needed for understanding how a data set was produced. This paper describes our experience in retrofitting a legacy data system to support capture, storage, and dissemination of provenance. Data inputs and transformations are logged automatically, while broader context information describing science algorithms and ancillary files is manually compiled. Provenance and context information are integrated for interactive user access and embedded into data files as XML documents compliant with the “Lineage” specification for geographic metadata defined by the International Organization for Standardization in the ISO 19115-2 standard. Lessons learned from this approach can inform others who need to incorporate provenance into a data system after the fact.","1558-0644","","10.1109/TGRS.2013.2282817","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6627994","Data management;data processing;geospatial data;metadata standards;provenance;science data systems","Context;Software;Geoscience;Data systems;Standards;Browsers;Communities","geographic information systems;geophysical techniques;geophysics computing;interactive programming;meta data;XML","ISO 19115-2 standard;International Organization for Standardization;metadata;XML documents;data files;interactive user access;science algorithms;provenance storage;provenance dissemination;data set;contextual information;legacy data production system design;modern workflow systems;data quality indicator;Earth science data;provenance information;provenance capture","","2","","14","","10 Oct 2013","","","IEEE","IEEE Journals"
"Organizational Closeness Centralities of Workflow-Supported Performer-to-Activity Affiliation Networks","H. Ahn; K. P. Kim","Data and Process Engineering Research Laboratory, Division of Computer Science and Engineering, Contents Convergence Software Research Institute, Kyonggi University, Suwon, South Korea; Data and Process Engineering Research Laboratory, Division of Computer Science and Engineering, Contents Convergence Software Research Institute, Kyonggi University, Suwon, South Korea","IEEE Access","31 Mar 2021","2021","9","","48555","48582","A workflow model specifies execution sequences of the associated activities and their affiliated relationships with roles, performers, invoked-applications, and relevant data. These affiliated relationships exhibit a series of valuable human-centered organizational knowledge and are utilized for exploring human resource’s work patterns. This paper focuses not only on a specific type of affiliated relationships between performers and activities, in particular, which forms a performer-to-activity affiliation network, but also on a specific type of analysis techniques, which builds a closeness centrality measurement approach for quantifying the degrees of farnesses between performers as well as between activities. In other words, this paper investigates a series of formal approaches for building organizational closeness centrality measurement techniques on the specific type of affiliation networks. The investigation mainly deploys two types of algorithmic formalisms along with an operational example, which are measuring performer-centered organizational closeness centralities and activity-centered organizational closeness centralities, respectively. In order to validate the deployed algorithmic equations, the paper carries out a couple of operational experiments; One is on an ICN-based workflow package and the other is on a discovered workflow model mined from a dataset of workflow event logs. Summarily, this paper devises a series of algorithms and equations for measuring closeness centralities of activities, verify the devised algorithms and their related equations along with operational examples, and discuss the ultimate implications of these analysis techniques of organizational closeness centrality measurements as the performer-to-activity affiliation networking knowledge in workflow-supported organizations.","2169-3536","","10.1109/ACCESS.2021.3065925","Basic Science Research Program through the National Research Foundation of Korea (NRF); Ministry of Education; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9378537","Workflow process;performer-activity affiliations;information control net;organizational social network;workflow-supported affiliation network;closeness centrality;workflow intelligence;human resource management","Social networking (online);Knowledge engineering;Visualization;Organizations;Analytical models;Mathematical model;Particle measurements","","","","","","51","CCBY","15 Mar 2021","","","IEEE","IEEE Journals"
"(MVO) study of antenna and its 3D scale modeling by Finite integration (FIM) method","N. Nasir; N. Yahya; M. N. Akhtar","Electrical and Electronic Engineering Department; Fundamental and Applied Sciences Department, Universiti Teknologi PETRONAS, Bandar Seri Iskandar, 31750 Tronoh, Perak, Malaysia; Electrical and Electronic Engineering Department","2011 Saudi International Electronics, Communications and Photonics Conference (SIECPC)","16 Jun 2011","2011","","","1","5","In sea bed logging detection of hydrocarbon reservoir is a very challenging task for shallow water and deep target. Magnitude of electromagnetic field response is very low and cannot be able to detect the deep target. To detect deep target new aluminium curve antenna is designed by using computer simulation technology (CST) software. 3D scale modeling with and without hydrocarbon was done by Finite integration method (FIM). Straight and curve antenna comparison was done in a 3D scaled marine environment. It was investigated that new design gave 158% higher magnetic field strength than straight antenna. A scale tank with a scale factor of 2000 was built to test the new designed antenna. The series of experiments were done to evaluate the performance of new design antenna with and without the presence of oil in a scale model. Magnitude verses offset (MVO) was done with new design curve antenna with and without oil. The experimental data was recorded with new design antenna with and without oil placed left and right side of receivers (Rx-1, Rx-3) respectively. The magnitude of the EM waves of this new designed antenna increases up to 168% with hydrocarbon. Curve fitting method using MATLAB software was done to validate the MVO data of new designed antenna with and without oil. Correlation value with new design antenna also confirms the presence of oil in a scale model.","","978-1-4577-0069-9","10.1109/SIECPC.2011.5876904","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5876904","Electromagnetic;Hydrocarbon;Antenna;SBL","","antennas;computational electromagnetics;curve fitting;finite element analysis;hydrocarbon reservoirs;oceanographic techniques","MVO;3D scale modeling;finite integration method;antenna;sea bed logging detection;hydrocarbon reservoir;shallow water;electromagnetic field;computer simulation;magnetic field strength;magnitude verses offset;curve fitting","","","","19","","16 Jun 2011","","","IEEE","IEEE Conferences"
"Advanced search system for IT support services","Y. Deng; K. E. Maghraoui; T. D. Griffin; V. Agarwal; S. G. Tamilselvam; R. D. Sharnagat; T. H. Alexander; N. E. Gómez; C. M. Cramer; A. Bivens; D. Jadav; Z. M. Valli-Hasham; K. Wahlmeier",NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA,"IBM Journal of Research and Development","20 May 2017","2017","61","1","3:27","3:40","IBM's Technical Support Services division runs remote support centers, where agents provide phone support for client problems related to IBM and non-IBM hardware and software products. Support center personnel use numerous pieces of information—including many searches, log files, and records of historical support tickets, from disparate data sources—to recommend solutions for customer technical problems. We have built an advanced search system to assist support agents who are resolving customer service requests and improving our client experience. The system has been deployed and used globally by thousands of support center personnel. In this paper, we describe the system's architecture, the technical challenges, and the innovative solution we have built. In addition, we discuss the novel ideas to address the unique requirements and challenges of the support services domain. These ideas include using system logs and domain knowledge to automatically expand agent queries, incorporating implicit agent feedback, and selecting features to extract useful information from highly unstructured and noisy ticket data. Results on the effectiveness of the system are presented. We also discuss future work on enhancing the system's capability to automatically diagnose customer hardware and software problems and remediate them.","0018-8646","","10.1147/JRD.2016.2628658","TSS Advanced Search System; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7877289","","Search problems;Search methods;Knowledge management;Indexing","","","","1","","24","","20 May 2017","","","IBM","IBM Journals"
"Visualizing and Controlling VMI-Based Malware Analysis in IaaS Cloud","N. Rakotondravony; H. P. Reiser","Univ. of Passau, Passau, Germany; Univ. of Passau, Passau, Germany","2016 IEEE 35th Symposium on Reliable Distributed Systems (SRDS)","22 Dec 2016","2016","","","211","212","Security in virtualized environment has known the support of different tools in the low-level detection and analysis of malware. The in-guest tracing mechanisms are now capable of operating at assembly language-, system call-, function call-and instruction-level to detect and classify malicious activities. Therefore, they are producing large amount of data about the state of a target system. However, the integrity of such data becomes questionable whenever the hosting target system is compromised. With virtual machine introspection (VMI), the monitoring tool runs outside the target monitored virtual machine (VM) [1]. Thus, the integrity of retrieved data is ensured even if the target system is compromised. Various works have brought VMI to Infrastructure-as-a-Service (Iaas) cloud environment, allowing the cloud user to run (simultaneous) forensics operations on his production VMs. The associated tracing mechanisms can collect larger amount of data in form of commented behavior traces or unstandardized log records. Thus, a human operator is needed to efficiently parse, represent, visualize and interpret the collected data, to benefit from their security relevance [2]. The use of visualization helps analysts investigate, compare and culster malware samples [3]. Existing visualization tools make use of recorded information to enhance the detection of intrusive behavior or the clustering of malware [4] from the observed system. However, at our knowledge, no existing tools establish a pre-to post-exploitation visualization graphs. We present an approach that enhances the detection and analysis of malware in the cloud by providing the cloud end-users the mean to efficiently visualize the different security relevant data collected through multiple VMI-based mechanisms.","1060-9857","978-1-5090-3513-7","10.1109/SRDS.2016.035","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7794347","","Data visualization;Monitoring;Cloud computing;Security;Malware;Production;Process control","cloud computing;data visualisation;invasive software;virtual machines","IaaS cloud;VMI-based malware analysis;virtualized environment;virtual machine introspection;infrastructure-as-a-service;cloud environment;Iaas;visualization graphs","","2","","5","","22 Dec 2016","","","IEEE","IEEE Conferences"
"A Multilevel Fault-Tolerance Technique for the DAG Data Driven Model","H. Fu; C. Yu; J. Sun; J. Du; M. Wang","Sch. of Comput. Sci. & Technol., Tianjin Univ., Tianjin, China; Sch. of Comput. Sci. & Technol., Tianjin Univ., Tianjin, China; Sch. of Comput. Sci. & Technol., Tianjin Univ., Tianjin, China; Sch. of Comput. Sci. & Technol., Tianjin Univ., Tianjin, China; Sch. of Comput. Sci. & Technol., Tianjin Univ., Tianjin, China","2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing","9 Jul 2015","2015","","","1127","1130","Fault tolerance of hardware failure is a challenging work for parallel programming in massively parallel processing environment. However, traditional rollback-recovery techniques, which an be classified into checkpoint-based and log-based, would introduce extra overhead for recording an overall snapshot of an application. For a specialized programming model, a private recovery technique is valuable and can achieve a better performance.In this paper, a multilevel fault-tolerance technique designed for the DAG data driven model is proposed. It utilized the checkpoint-based fault tolerance technique for system recovery, and timeout to detect and revoery from performance faults. It consists of two kinds of checkpoints: the DAG pattern checkpoint and the intermediate result checkpoint. The DAG pattern checkpoint is designed for tracing the current processing progress of the DAG model, while the intermediate results checkpoint is used to record outputs of compute nodes. Moreover, we also implement this technique in the EasyHPS runtime system. Experimental results show that the check pointing overhead is as low as 2.6%.","","978-1-4799-8006-2","10.1109/CCGrid.2015.89","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7152603","Fault tolerance;Multilevel Fault-tolerance Technique;DAG data driven model","Fault tolerance;Fault tolerant systems;Data models;Computational modeling;Checkpointing;Dynamic programming;Program processors","checkpointing;directed graphs;parallel programming;software fault tolerance","multilevel fault-tolerance technique;DAG data driven model;directed acyclic graph;hardware failure;parallel programming;parallel processing environment;rollback-recovery techniques;private recovery technique;checkpoint-based fault tolerance technique;EasyHPS runtime system","","","","8","","9 Jul 2015","","","IEEE","IEEE Conferences"
"Using MB-System and Matlab to generate geographical mosaics from seafloor images","M. Woolsey; A. Woolsey","National Institute for Undersea Science and Technology (NIUST), University of Southern Mississippi, Abbeville, USA; Mississippi Mineral Resources Institute & NIUST, University of Mississippi, University, USA","OCEANS 2016 MTS/IEEE Monterey","1 Dec 2016","2016","","","1","10","A method of generating photomosaics from AUV-derived seafloor images was produced using principally the software packages MB-System and Matlab. Images are placed based on their geographical positions as logged by the vehicle, however, the raw navigation is refined using acquired bathymetry, reference bathymetry, and the images themselves. During these refinements, care is taken to maintain a physically realizable navigation model. The initial phase of this development was to generate supplementary files to support image placement into world space by GIS utilities. Subsequently, programs were developed in Matlab to assemble image data in a more effective manner than was available from standard utilities, and also to produce an updated navigation model based on visual control points common between crossing and overlapping images. Resulting mosaics have been used for groundtruth points in broader surveys, sensor placement and localization, and general mission planning.","","978-1-5090-1537-5","10.1109/OCEANS.2016.7761283","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7761283","seafloor mapping;AUV navigation;photomosaic","Vehicles;MATLAB;Mathematical model;Cameras;Sonar navigation","autonomous underwater vehicles;bathymetry;geographic information systems;geophysical image processing;mobile robots;software packages","MB-System;Matlab;software package;geographical mosaic generation;seafloor image;autonomous underwater vehicle;AUV;bathymetry;navigation model;image placement;GIS utility","","4","","10","","1 Dec 2016","","","IEEE","IEEE Conferences"
"Aiding Intrusion Analysis Using Machine Learning","L. Zomlot; S. Chandran; D. Caragea; X. Ou","HP Labs., Princeton, NJ, USA; Kansas State Univ., Manhattan, KS, USA; Kansas State Univ., Manhattan, KS, USA; Kansas State Univ., Manhattan, KS, USA","2013 12th International Conference on Machine Learning and Applications","10 Apr 2014","2013","2","","40","47","Intrusion analysis, i.e., the process of combing through IDS alerts and audit logs to identify real successful and attempted attacks, remains a difficult problem in practical network security defense. The major contributing cause to this problem is the high false-positive rate in the sensors used by IDS systems to detect malicious activities. The goal of our work is to examine whether a machine-learned classifier can help a human analyst filter out non-interesting scenarios reported by an IDS alert correlator, so that analysts' time can be saved. This research is conducted in the open-source SnIPS intrusion analysis framework. Throughout observing the output of SnIPS running on our departmental network, we found that an analyst would need to perform repetitive tasks in pruning out the false positives in the correlation graphs produced by it. We hypothesized that such repetitive tasks can yield (limited) labeled data that can enable the use of a machine learning-based approach to prune SnIPS' output based on the human analysts' feedback, much similar to spam filters that can learn from users' past judgment to prune emails. Our goal is to classify the correlation graphs produced from SnIPS into ""interesting"" and ""non-interesting"", where ""interesting"" means that a human analyst would want to conduct further analysis on the events. We spent significant amount of time manually labeling SnIPS' output correlations based on this criterion, and built prediction models using both supervised and semi-supervised learning approaches. Our experiments revealed a number of interesting observations that give insights into the pitfalls and challenges of applying machine learning in intrusion analysis. The experimentation results also indicate that semi-supervised learning is a promising approach towards practical machine learning-based tools that can aid human analysts, when a limited amount of labeled data is available.","","978-0-7695-5144-9","10.1109/ICMLA.2013.103","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6786079","Enterprise Network Security;Intrusion Detection and Analysis;Machine Learning","Correlation;Security;Labeling;Servers;Sensors;Predictive models;Production","computer network security;correlation theory;e-mail filters;learning (artificial intelligence);network theory (graphs);pattern classification;public domain software;unsolicited e-mail","aiding intrusion analysis;audit logs;attack identification;network security defense;false positive rate;sensor;IDS system;malicious activity detection;machine learned classifier;IDS alert correlator;open source SnIPS intrusion analysis;correlation graph;human analyst feedback;spam filters;email pruning;correlation graph classification;manual SnIPS labeling;prediction model;semi-supervised learning approach;machine learning-based tools","","9","","34","","10 Apr 2014","","","IEEE","IEEE Conferences"
"Cost Effective IoT Framework for Connected Sensors and Devices","A. Deb Sarkar; S. Das; S. Singh","TATA Consultancy Services,Kolkata,India; Lovely Professional University,Department of Biosciences,Jalandhar,Punjab,India; Lovely Professional University,Department of Biosciences,Jalandhar,Punjab,India","2020 3rd International Conference on Intelligent Sustainable Systems (ICISS)","18 Jan 2021","2020","","","1267","1273","Internet of Things [IoT]used in manufacturing industry is not very new, however it is evident from different sources, the implemented IoT solution comes with a full suite including cloud and its corresponding ecosystem. Multiple cloud based IoT solutions have been explored during the analysis phase of this project, for example Kosmos Cloud, iMETLand Project etc. After rigorous analysis it has been deduced that majority of the Cloud based IoT solutions are advantageous but costly and coupled closely with their own cloud infrastructure with limited opportunities for the creation of hybrid or multiple cloud model. They also exhibit difficulty for integration with the legacy systems. The pricing models of existing solutions are either based on the number of connected devices, volume of messages or upon more complex calculations. This study has endeavored to state a vision statement for an architectural development of cost effective IoT framework that could connect various types of sensors and devices for an automated unit operation with the help of representational state transfer (REST) API, developed by using windows communication foundation (WCF) framework.","","978-1-7281-7089-3","10.1109/ICISS49785.2020.9316031","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9316031","Bigdata;cloud server;IoT;machine to machine connectivity;NoSQL;REST API;sensor data logging;WCF","Protocols;Cloud computing;Sensors;Servers;Software as a service;Security;Telemetry","application program interfaces;cloud computing;Internet of Things","cloud infrastructure;connected devices;cost effective IoT framework;Internet of Things;manufacturing industry;IoT solution;Kosmos Cloud;iMETLand Project;pricing models;architectural development;automated unit operation;representational state transfer API;REST API;Windows Communication Foundation framework;WCF framework","","","","32","","18 Jan 2021","","","IEEE","IEEE Conferences"
"How aging laws influence parametric and catastrophic reliability distributions","A. Feinberg",DfR Software Company,"2017 Annual Reliability and Maintainability Symposium (RAMS)","30 Mar 2017","2017","","","1","6","In this paper we describe how these physics of failure aging laws influence reliability distributions, not only the type of distribution, but the rate of failure as it relates to the aging rate. We illustrate how one can predict parametric failure rates based on the physics of failure aging laws when known. A number of statements are concluded. We show that when a manufactured part has a key parameter that is distributed normally, and the physics of failure aging for this key parameter ages in log-time, its failure rate is lognormally distributed. When the physics of failure is a power law, we illustrate how the Weibull beta and eta can be obtained from the physics of failure aging law exponent and amplitude in the parametric case. We use the example of creep, and make direct comparisons between the full creep `rate' curve and the bathtub curve. Although the example of creep is used many aging laws have a similar power law forms and can be applied in a similar manner. Although we work though parametric failure rate statistics, one can relate it to the catastrophic case.","","978-1-5090-5284-4","10.1109/RAM.2017.7889749","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7889749","Physics of failure;life distribution models","Aging;Creep;Physics;Mathematical model;Reliability;Weibull distribution;Transistors","ageing;failure analysis;physics;reliability;statistics","aging laws;parametric reliability distributions;catastrophic reliability distributions;parametric failure rate statistics;physics","","1","","9","","30 Mar 2017","","","IEEE","IEEE Conferences"
"Business Process Management with mobile routes","C. Mahmoudi; F. Mourlin","Logics, Algorithm and Complexity Laboratory, UPEC University, Créteil, France; Logics, Algorithm and Complexity Laboratory, UPEC University, Créteil, France","2014 IEEE/ACS 11th International Conference on Computer Systems and Applications (AICCSA)","2 Apr 2015","2014","","","420","427","Business processes are milestone of the information system of any companies. Their availability is a crucial aspect. We provide a solution for the high level of availability of business processes by the use of cluster of enterprise service buses (ESB). Our approach is based on the dynamic creation of the route between the business services and the migration of a runtime context from one ESB to another one. So, we insure the management of business processes over a cluster and measure the impact of such incident. Through the use of log, we also report these events which allow the administrator for preparing updates of the information system. With the use of open source software, we guarantee the reuse of our case study with other kinds of enterprise service bus, which respect open standard exchanges like XML language and REST API.","2161-5330","978-1-4799-7100-8","10.1109/AICCSA.2014.7073229","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7073229","SOA architecture;orchestration;cluster of bus;message routing;web service","Business;Containers;Context;DSL;XML;Servers;Routing","application program interfaces;business data processing;mobile computing;public domain software;service-oriented architecture;Web services;XML","business process management;mobile route;information system;enterprise service bus;ESB;business services;open source software;XML language;REST API","","","","22","","2 Apr 2015","","","IEEE","IEEE Conferences"
"Design of Intelligent Security System Based on ARM Microcontroller","X. Zhao; X. Liu; B. Tan; J. Shen; Y. Wang","Sch. of Electr. & Electron. Eng., Shandong Univ. of Technol., Zibo, China; Sch. of Electr. & Electron. Eng., Shandong Univ. of Technol., Zibo, China; Sch. of Electr. & Electron. Eng., Shandong Univ. of Technol., Zibo, China; Sch. of Electr. & Electron. Eng., Shandong Univ. of Technol., Zibo, China; Sch. of Electr. & Electron. Eng., Shandong Univ. of Technol., Zibo, China","2010 International Conference on E-Product E-Service and E-Entertainment","10 Dec 2010","2010","","","1","4","An intelligent security system based on ARM microcontroller was developed. The system can realize the monitoring of the intrusion signals automatically by using pyroelectric infrared detectors, alarming through the Ethernet, and displaying the invasion information on the PC screen. In order to monitor alarm signals, the PC server is in the state of listening when there is no information arrived, and otherwise display the alarm time and the room number, making the speaker speak to notice the security staff. And then, the reset button could not only stop the speaker's speaking but also stop the ARM microcontroller to continue monitoring invasion signals. The software with a logging feature could record the alarm information and delete it artificially.","","978-1-4244-7161-4","10.1109/ICEEE.2010.5661128","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5661128","","Monitoring;SDRAM;Security;Automation;Circuit synthesis;Crystals;Microcontrollers","alarm systems;computerised monitoring;infrared detectors;local area networks;microcontrollers;pyroelectric detectors","intelligent security system;ARM microcontroller;intrusion signal monitoring;pyroelectric infrared detector;Ethernet;invasion information;PC screen;alarm signal monitoring;PC server;alarm time;room number;security staff;logging feature;alarm information","","","","4","","10 Dec 2010","","","IEEE","IEEE Conferences"
"Development of a GPS-based highway toll collection system","J. Y. Tan; P. J. Ker; D. Mani; P. Arumugam","Centre for Systems and Machines Intelligence, College of Engineering, Universiti Tenaga Nasional, Kajang, Selangor, Malaysia; Centre for Systems and Machines Intelligence, College of Engineering, Universiti Tenaga Nasional, Kajang, Selangor, Malaysia; Centre for Systems and Machines Intelligence, College of Engineering, Universiti Tenaga Nasional, Kajang, Selangor, Malaysia; Centre for Systems and Machines Intelligence, College of Engineering, Universiti Tenaga Nasional, Kajang, Selangor, Malaysia","2016 6th IEEE International Conference on Control System, Computing and Engineering (ICCSCE)","6 Apr 2017","2016","","","125","128","The necessity for vehicles to stop or slow down for toll fee payment results in traffic congestion and reduces fuel efficiency. Hence, a system that enables road users to pay the toll fees without stopping or slowing down was proposed and developed. Hardware and software designs were carried out to develop a Global Positioning System (GPS)-based highway toll collection system. This system was developed using a Raspberry Pi 2 microcontroller. Different modules such as GPS module, Liquid Crystal Display (LCD) module, speaker, wireless Wi-Fi router modem and wireless Wi-Fi adapter were incorporated and integrated with the microcontroller to perform a few specific functions. In general, the system utilized GPS coordinates to detect whether a vehicle passed through predefined locations in the database and the travel details were recorded. The Raspberry Pi 2 microcontroller was configured as a personal cloud server to allow online access of travel logs. This developed system presents a different approach for highway toll collection which eliminates travel delays and construction of expensive gantries or toll booths.","","978-1-5090-1178-0","10.1109/ICCSCE.2016.7893557","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7893557","GPS coordinates;Microcontroller;Raspberry Pi 2;Toll collection","Microcontrollers;Global Positioning System;Databases;Liquid crystal displays;Roads;Wireless communication","cloud computing;Global Positioning System;liquid crystal displays;microcontrollers;modems;road pricing (tolls);telecommunication network routing;traffic engineering computing;wireless LAN","GPS-based highway toll collection system;toll fee payment;traffic congestion;fuel efficiency reduction;road users;Global Positioning System;Raspberry Pi-2 microcontroller;liquid crystal display module;LCD module;speakers;wireless Wi-Fi router modem;wireless Wi-Fi adapter;GPS coordinates;vehicle detection;personal cloud server;online travel log access;travel delay elimination;gantry construction elimination;toll booth construction elimination","","1","","18","","6 Apr 2017","","","IEEE","IEEE Conferences"
"Keeping Authorities ""Honest or Bust"" with Decentralized Witness Cosigning","E. Syta; I. Tamas; D. Visher; D. I. Wolinsky; P. Jovanovic; L. Gasser; N. Gailly; I. Khoffi; B. Ford","Yale Univ., New Haven, CT, USA; Yale Univ., New Haven, CT, USA; Yale Univ., New Haven, CT, USA; Yale Univ., New Haven, CT, USA; Swiss Fed. Inst. of Technol., Lausanne, Switzerland; Swiss Fed. Inst. of Technol., Lausanne, Switzerland; Swiss Fed. Inst. of Technol., Lausanne, Switzerland; Swiss Fed. Inst. of Technol., Lausanne, Switzerland; Swiss Fed. Inst. of Technol., Lausanne, Switzerland","2016 IEEE Symposium on Security and Privacy (SP)","18 Aug 2016","2016","","","526","545","The secret keys of critical network authorities -- such as time, name, certificate, and software update services -- represent high-value targets for hackers, criminals, and spy agencies wishing to use these keys secretly to compromise other hosts. To protect authorities and their clients proactively from undetected exploits and misuse, we introduce CoSi, a scalable witness cosigning protocol ensuring that every authoritative statement is validated and publicly logged by a diverse group of witnesses before any client will accept it. A statement S collectively signed by W witnesses assures clients that S has been seen, and not immediately found erroneous, by those W observers. Even if S is compromised in a fashion not readily detectable by the witnesses, CoSi still guarantees S's exposure to public scrutiny, forcing secrecy-minded attackers to risk that the compromise will soon be detected by one of the W witnesses. Because clients can verify collective signatures efficiently without communication, CoSi protects clients' privacy, and offers the first transparency mechanism effective against persistent man-in-the-middle attackers who control a victim's Internet access, the authority's secret key, and several witnesses' secret keys. CoSi builds on existing cryptographic multisignature methods, scaling them to support thousands of witnesses via signature aggregation over efficient communication trees. A working prototype demonstrates CoSi in the context of timestamping and logging authorities, enabling groups of over 8,000 distributed witnesses to cosign authoritative statements in under two seconds.","2375-1207","978-1-5090-0824-7","10.1109/SP.2016.38","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7546521","transparency;signatures;multisignatures;cryptography;distributed protocols;scalability","Servers;Public key;Protocols;Software;Prototypes;Browsers","computer network security;cryptographic protocols;data protection;digital signatures;Internet;private key cryptography;trees (mathematics)","CoSi protocol;witness cosigning protocol;secret key;privacy protection;transparency mechanism;man-in-the-middle attacker;Internet access;cryptographic multisignature method;communication tree","","64","8","136","","18 Aug 2016","","","IEEE","IEEE Conferences"
"Breaking into social nervous system","D. K. Vashisth","Department of Information Technology, MD University, Rhotak, India","2014 IEEE Canada International Humanitarian Technology Conference - (IHTC)","6 Jul 2015","2014","","","1","5","Gathering and analyzing of real time communicational traces in a society empowers us to formulate behavioral structure of its members. Reality Mining the core concept to support this enables us to collect digital breadcrumbs left by people while they perform their daily activities. Collection of these signals through sociometric badges and then formulating them for a visual view is shown in the further section of this paper. The model proposed in this paper is based on multi-level information gathering and filtration system. In this model society is divided in groups on the basis of their intra-group and inter-group interactions. It determines the sequestered groups and the quickest information distributing group. This filtration is processed on the server and all the data transactions are accomplished with secure protocols. For collection of communicational traces we argue use of mobile devices as sensors, which process data to further server. Further incorporation of influential model and centrality approaches enable us to detect most influential person in the sub-group. Implementation of web based multi-level architecture allows easy extension, wider area coverage, storing and processing large log records and easy integration with preexisting communication network.","","978-1-4799-3996-1","10.1109/IHTC.2014.7147561","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7147561","Sociomertic badges;multilevel-architecture;intermediate server;centrality;clique;styling;SAML","Servers;Communication networks;Education;Predictive models;Lead;Jacobian matrices;Acceleration","behavioural sciences computing;data mining;information filtering;Internet;mobile computing;software architecture","social nervous system;Web based multilevel architecture;reality mining;behavioral structure;sociometric badge;multilevel information gathering system;multilevel information filtration system;mobile device","","","","21","","6 Jul 2015","","","IEEE","IEEE Conferences"
"Post-silicon bug diagnosis with inconsistent executions","A. DeOrio; D. S. Khudia; V. Bertacco",University of Michigan; University of Michigan; University of Michigan,"2011 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)","15 Dec 2011","2011","","","755","761","The complexity of modern chips intensifies verification challenges, and an increasing share of this verification effort is shouldered by post-silicon validation. Focusing on the first silicon prototypes, post-silicon validation poses critical new challenges such as intermittent failures, where multiple executions of a same test do not yield a consistent outcome. These are often due to on-chip asynchronous events and electrical effects, leading to extremely time-consuming, if not unachievable, bug diagnosis and debugging processes. In this work, we propose a methodology called BPS (Bug Positioning System) to support the automatic diagnosis of these difficult bugs. During post-silicon validation, lightweight BPS hardware logs a compact encoding of observed signal activity over multiple executions of the same test: some passing, some failing. Leveraging a novel post-analysis algorithm, BPS uses the logged activity to diagnose the bug, identifying the approximate manifestation time and critical design signals. We found experimentally that BPS can localize most bugs down to the exact root signal and within about 1,000 clock cycles of their occurrence.","1558-2434","978-1-4577-1400-9","10.1109/ICCAD.2011.6105414","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6105414","","Computer bugs;Hardware;Software;Debugging;System-on-a-chip;Manufacturing;Silicon","formal verification;integrated circuit design","post-silicon bug diagnosis;inconsistent execution;verification effort;post-silicon validation;intermittent failures;on-chip asynchronous events;electrical effects;bug positioning system;automatic diagnosis","","15","","25","","15 Dec 2011","","","IEEE","IEEE Conferences"
"Emulation of Anomalies for Wide-Area Monitoring, Protection and Control System Development","M. Stark; C. Clay; A. St. Leger; N. Barry","Department of Electrical Engineering and Computer Science, United States Military Academy, West Point, USA; Department of Electrical Engineering and Computer Science, United States Military Academy, West Point, USA; Department of Electrical Engineering and Computer Science, United States Military Academy, West Point, USA; Department of Electrical Engineering and Computer Science, United States Military Academy, West Point, USA","2018 Clemson University Power Systems Conference (PSC)","10 Mar 2019","2018","","","1","8","For security and stability in smart grid development, identification and reactions to external factors are emphasized through Wide-Area Monitoring Protection and Control (WAMPAC) systems. Anomaly detection and identification are critical to establishing a responsive, controllable grid architecture. In testing anomaly-detection algorithms for WAMPAC applications, there should be a benchmarking process for detection of anomalies and evaluating WAMPAC performance in the presence of anomalies. The ability to create, or emulate, anomalies within a testing environment of WAMPAC systems is a step towards achieving this capability. In this paper, the development and implementation of a software-controlled Anomaly and Fault Generator (AFG) into a 1000:1 scale emulated smart grid testbed is presented. The testbed uses Phasor Measurement Units (PMUs) for high accuracy, fidelity, and time synchronization of measurements. The AFG induces controlled faults and anomalous events into the system to evaluate the performance of anomaly detection systems and WAMPAC applications. System events include stolen breakers, electrical faults with variable fault resistance, and hijacking of transducers. High sample-rate, time-synchronized, measurements are also logged by the AFG to compare to WAMPAC system results.","","978-1-7281-0316-7","10.1109/PSC.2018.8664057","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8664057","Anomaly Detection;Phasor Measurement Units;Power System Faults;Smart Grids;Wide Area Measurements","Circuit faults;Phasor measurement units;Transducers;Generators;Anomaly detection;Optical switches;Monitoring","phasor measurement;power engineering computing;power system control;power system protection;power transmission faults;security of data;smart power grids","AFG;anomaly detection systems;WAMPAC applications;system events;WAMPAC system results;smart grid development;identification;reactions;responsive grid architecture;controllable grid architecture;anomaly-detection algorithms;WAMPAC systems;software-controlled Anomaly;smart grid testbed;phasor measurement units;anomalies detection;control system development;control systems;wide-area monitoring protection","","3","","14","","10 Mar 2019","","","IEEE","IEEE Conferences"
"Integration of a polarity-preserving chirp subbottom profiler into the NIUST AUV Eagle Ray","M. Woolsey; R. Jarnagin; K. Sleeper; L. Macelloni; M. D'Emidio; A. -. Diercks; V. L. Asper","National Institute for Undersea Science and Technology, Undersea Vehicle Technology Center, University of Southern Mississippi, UM Field Station 15 CR 2078, Abbeville, 38601 USA; National Institute for Undersea Science and Technology, Undersea Vehicle Technology Center, University of Southern Mississippi, UM Field Station 15 CR 2078, Abbeville, 38601 USA; Mississippi Mineral Resources Institute, University of Mississippi, 111 Brevard Hall, University, 38677, USA; Mississippi Mineral Resources Institute, University of Mississippi, 111 Brevard Hall, University, 38677, USA; Mississippi Mineral Resources Institute, University of Mississippi, 111 Brevard Hall, University, 38677, USA; National Institute for Undersea Science and Technology, Undersea Vehicle Technology Center, University of Southern Mississippi, UM Field Station 15 CR 2078, Abbeville, 38601 USA; National Institute for Undersea Science and Technology, Undersea Vehicle Technology Center, University of Southern Mississippi, 1020 Balch Blvd, Stennis Space Center, 39529 USA","2013 OCEANS - San Diego","17 Feb 2014","2013","","","1","4","A chirp subbottom profiler was integrated into the NIUST AUV Eagle Ray to expand its mapping capabilities. Previously this vehicle has been used primarily for seafloor mapping using a multibeam sonar. The subbottom profiler provides an additional dimension of data, which allows one to visualize up to 60 meters below the seafloor and correlate surface expressions with subbottom structure. The profiler system was integrated into the vehicle's structure, control software, and operations protocol. The resulting profiler files are merged with the vehicle's logs and processed to infer physical properties of the seafloor and create pseudo 3D volumes.","0197-7385","978-0-933957-40-4","10.23919/OCEANS.2013.6741264","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6741264","subbottom profiler;seafloor survey","Vehicles;Acoustics;Computers;Software;Protocols;Sediments;Chirp","autonomous underwater vehicles;oceanographic equipment;seafloor phenomena","autonomous underwater vehicles;pseudo 3D volumes;multibeam sonar;seafloor mapping;NIUST AUV Eagle Ray;polarity-preserving chirp subbottom profiler","","1","","4","","17 Feb 2014","","","IEEE","IEEE Conferences"
"Blockchain-Enabled Identity Verification for Safe Ridesharing Leveraging Zero-Knowledge Proof","W. Li; C. Meese; H. Guo; M. Nejad","University of Delaware,Department of Civil and Environmental Engineering,U.S.A.; University of Delaware,Department of Civil and Environmental Engineering,U.S.A.; School of Software, Northwestern Polytechnical University,Taicang Campus,China; University of Delaware,Department of Civil and Environmental Engineering,U.S.A.","2020 3rd International Conference on Hot Information-Centric Networking (HotICN)","16 Feb 2021","2020","","","18","24","The on-demand mobility market, including ridesharing, is becoming increasingly important with e-hailing fares growing at a rate of approximately 130% per annum since 2013. By increasing utilization of existing vehicles and empty seats, ridesharing can provide many benefits including reduced traffic congestion and environmental impact from vehicle usage and production. However, the safety of riders and drivers has become of paramount concern and a method for privacy-preserving identity verification between untrusted parties is essential for protecting users. To this end, we propose a novel privacy-preserving identity verification system, extending zero-knowledge proof (ZKP) and blockchain for use in ridesharing applications. We design a permissioned blockchain network to perform the ZKP verification of a driver's identity, which also acts as an immutable ledger to store ride logs and ZKP records. For the ZKP module, we design a protocol to facilitate user verification without requiring the exchange of any private information. We prototype the proposed system on the Hyperledger Fabric platform, with the Hyperledger Ursa cryptography library, and conduct extensive experimentation. To measure the prototype's performance, we utilize the Hyperledger Caliper benchmark tool to perform extensive analysis and the results show that our system is suitable for use in real-world ridesharing applications.","","978-1-7281-9216-1","10.1109/HotICN50779.2020.9350858","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9350858","Blockchain;data privacy;identity verification;ridesharing;zero-knowledge proof","Distributed ledger;Prototypes;Blockchain;Tools;Fabrics;Traffic congestion;Vehicles","cryptography;data privacy;financial data processing","blockchain-enabled identity verification;safe ridesharing leveraging zero-knowledge;on-demand mobility market;e-hailing fares;existing vehicles;traffic congestion;environmental impact;vehicle usage;riders;paramount concern;untrusted parties;novel privacy-preserving identity verification system;permissioned blockchain network;ZKP verification;driver;ZKP records;ZKP module;user verification;Hyperledger Fabric platform;Hyperledger Ursa cryptography library;prototype;real-world ridesharing applications","","","","35","","16 Feb 2021","","","IEEE","IEEE Conferences"
"Embedded realization of a real time Heart Rate Variability logger for at-home sleep studies","B. B. Rekha; A. Kandaswamy; V. M. Mitha","Department of Biomedical Engineering, PSG College of Technology, Coimbatore, India; Department of Biomedical Engineering, PSG College of Technology, Coimbatore, India; Department of Biomedical Engineering, Manipal Institute of Technology, Manipal, India","2015 IEEE Workshop on Computational Intelligence: Theories, Applications and Future Directions (WCI)","23 Jun 2016","2015","","","1","6","Sleep has no substitute and quality of sleep is a major concern for healthy living of human beings. Sleep breathing disorders are events characterized by pauses of breathing during sleep. Sleep breathing disorders like Obstructive Sleep Apnea (OSA) may result in cardiac disorders and fatalities. Though Polysomnography is considered as the gold standard for conducting sleep studies, current research directs that the trend of Heart Rate Variability (HRV) during sleep is indicative of sleep breathing disorder. Hence, reliable HRV recorders with ease of use may contribute to early screening of these disorders. This paper reports the prototype development of an embedded system for logging HRV during sleep for screening during sleep. The system is built with open source Arduino platform consisting of an ATMEGA328 microcontroller along with a provision for storage on a Secure-Digital card. ‘R’ peak detection is carried out using a combination of dynamic threshold and amplitude threshold. The logger is able to work on two modes: (1) plain, long duration logger and (2) HRV Logger. The estimated duration of logging is 72 hours with a +9 V battery supply. The system performance is compared with a commercially available Electrocardiogram (ECG) recorder system and a MATLAB based R peak detection system with real time recordings of 30 healthy adults. The system code is optimized to achieve a logging time of 6.25 milliseconds per sample and 0.98 seconds for each ‘R’ peak detection and storage. The proposed system was also tested with Sleep ECG samples from Physionet database and it achieved a maximum sensitivity of 97.7% and specificity of 95.56%. The maximum recorded percentage error of detection was 2%. The results indicate that the proposed system and software design can be developed as a compact, economical and portable device for early detection of sleep breathing disorders.","","978-1-4673-8215-1","10.1109/WCI.2015.7495532","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7495532","Heart Rate Variability;Obstructive Sleep Apnea;Arduino platform;Secure Digital card;HRV Logger","Sleep apnea;Heart rate variability;Electrocardiography;Real-time systems;Clocks","","","","1","","20","","23 Jun 2016","","","IEEE","IEEE Conferences"
"A Regression-based Hybrid Machine Learning Technique to Enhance the Database Migration in Cloud Environment","A. Kumar; M. Sivak Kumar; V. Namdeo","Sarvepalli Radha Krishnan University,RKDF Institute of Science & Technology,Department of Computer Science & Engineering,Bhopal,India; Sarvepalli Radha Krishnan University,RKDF Institute of Science & Technology,Department of Computer Science & Engineering,Bhopal,India; Sarvepalli Radha Krishnan University,RKDF Institute of Science & Technology,Department of Computer Science & Engineering,Bhopal,India","2021 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS)","12 Apr 2021","2021","","","149","155","The report of cloud computing in recent years has prompted circumstances that usually has lead to numerous advancements & novel mechanisms. The technologies available in the cloud have been prevalent for businesses as well as people who understand that cloud computing is a significant problem, even though they don't know why. We present a methodology that accurately assesses the migration cost, relocation length with cloud operating cost of the social databases, and upgraded the execution. The first step in our approach is to acquire workloads and structure models for moving the database from the database logs as well as from schemes. The second step uses these models to perform a discrete form of event simulation for estimated costs and time. We have implemented the software tools that simplify our approach in both phases. A comprehensive review contrasts our approach to the effects of real-world cloud data migration.","","978-1-7281-8529-3","10.1109/ICCCIS51004.2021.9397123","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9397123","Cloud Computing;Database Migration;Hybrid HGSA;Enterprise Systems","Cloud computing;Databases;Computational modeling;Machine learning;Tools;Software reliability;Software tools","","","","","","11","","12 Apr 2021","","","IEEE","IEEE Conferences"
"Strain gauge measurement system with Wi-Fi data transfer in LabVIEW","J. Hnidka; D. Rozehnal","University of Defence, Kounicova 65, 662 10 Brno, Czech Republic; University of Defence, Kounicova 65, 662 10 Brno, Czech Republic","2017 International Conference on Military Technologies (ICMT)","24 Jul 2017","2017","","","520","525","A small strain gage measurement system was developed at the University of Defence in Brno. The system relays on N1 CompactDAQ hardware and LabVIEW software. The system serves as a functional demonstrator for larger systems, which will be designed in the future. The wiring can cause significant problems as the physical dimensions expand and, therefore, it was decided to solve the data transfer wirelessly. Two system architectures are introduced, which are then stress-tested and compared with the reference measurement performed via Ethernet. During the test, both architectures had to handle the same signal generated by arbitrary signal generator. Both of them managed the same task with significant differences. Based on the results of the test a system architecture was chosen. The program was written in LabVIEW and not only it displays data, it also logs them into TDMS file for further processing.","","978-1-5090-5666-8","10.1109/MILTECHS.2017.7988813","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7988813","LabVIEW;National Instruments;cDAQ;consumer-producer architecture;Wi-Fi;time synchronization","Nickel;Sensors;Wireless fidelity;Strain measurement;Systems architecture;Force measurement;Data acquisition","data acquisition;signal generators;strain gauges;strain measurement;virtual instrumentation;wireless LAN","strain gauge measurement system;Wi-Fi data transfer;University of Defence;Brno;relay system architecture;NI CompactDAQ hardware;LabVIEW software;wiring;wireless data transfer;stress-testing;Ethernet;arbitrary signal generator;TDMS file","","1","","9","","24 Jul 2017","","","IEEE","IEEE Conferences"
"Advil: A Pain Reliever for the Storage Performance of Mobile Devices","J. Kim; J. Kim","Coll. of Inf. & Commun. Eng., Sungkyunkwan Univ., Suwon, South Korea; Coll. of Inf. & Commun. Eng., Sungkyunkwan Univ., Suwon, South Korea","2012 IEEE 15th International Conference on Computational Science and Engineering","24 Jan 2013","2012","","","429","436","Recently, mobile devices are demanding more performance in computing power, network, and storage. Among these components, storage is one of the most important components which directly influence end-user experience. The poor random write performance is particularly painful to mobile devices, but this situation is expected to continue due to limited cost and power budget in embedded flash-based storage. This paper proposes a novel software layer called Advil to relieve the random write performance of mobile devices. Advil filters out small random writes and logs them sequentially into a small buffer space (called reserved area), in a transparent way to file systems and flash-based storage devices. To take advantage of the fact that the data invalidated in the reserved area does not have to be synchronized to the original location, Advil identifies hot data and keeps them in the reserved area. The amount of hot data is dynamically adjusted according to the change in the workload characteristics. In addition, Advil selectively performs page padding and block padding when the data is moved to the original location to increase the efficiency in the underlying flash-based storage. Our evaluation results show that Advil improves the storage write performance of realistic smart phone workloads up to three times.","","978-0-7695-4914-9","10.1109/ICCSE.2012.66","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6417325","Smartphone;eMMC;Mobile Storage","Ash;Performance evaluation;Mobile handsets;Mobile communication;Computer architecture;System-on-a-chip;Random access memory","buffer storage;flash memories;smart phones","mobile devices;computing power;end-user experience;random write performance;mobile devices;power budget;embedded flash-based storage;software layer;Advil filters;file systems;flash-based storage devices;workload characteristics;page padding;block padding;storage write performance;realistic smartphone workloads","","5","","18","","24 Jan 2013","","","IEEE","IEEE Conferences"
"Dual Clustering for Clinical Care Construction","H. Iwata; S. Tsumoto; S. Hirano","Div. of Nursing, Shimane Univ., Izumo, Japan; Dept. of Med. Inf., Shimane Univ., Izumo, Japan; Dept. of Med. Inf., Shimane Univ., Izumo, Japan","2016 IEEE International Conference on Healthcare Informatics (ICHI)","8 Dec 2016","2016","","","314","314","A hospital information system (HIS) was introduced about twenty years ago and all the clinical environment has been dramatically changed [1]-[3]. HIS stores all the histories of clinical activities in a hospital, such as electronic patient records, laboratory data, x-ray photos, and so on. The advantage of HIS is that all the data are input through the network service and they can retrieve from the terminals inside the hospital [3], [4]. Data stored in HIS can be viewed as histories of clinical actions, described as the results of clinical actions with time stamp. Thus, data mining techniques, explored in web mining or network analysis can be applied to the HIS data. Data mining in HIS may become an important tool for hospital management in which spatiotemporal data mining, social network analysis and other new data mining methods may play central roles [2], [5]. 1 The method consists of the following five steps (Fig. 2): first, histories of nursing orders are extracted from hospital information system. Second, orders are classified into several groups by using clustering on the pricipal components (sample clustering, Fig 1). Third, attributes clustering is applied to the data. Fourth, the method compares between generated functions for sample and attribute clustering which relate the number of clusters and calculated similarities. Fifth, if attribute clustering gives better performance with respect to the function, the dataset is decomposed into subtables by using the grouping of attribute clustering. Then, the first step will be repeated in a recursive way. After the grouping results are stable, a new pathway will be constructed from all the induced results. The results show that the proposed method is useful for construction of a clinical pathway Clinical environment is very complex, and flexible and adaptive service improvement is crucial in maintaining quality of medical care. Thus, incremental software development in hospital information system and its evaluation is important. This paper introduces a statistical estimation method of an embedded software in which service logs are used to measure the differences between responsive time before and after a new interface has been introduced. The empirical results show that statistical methods are useful to evaluate the system performance in a real clinical environment.","","978-1-5090-6117-4","10.1109/ICHI.2016.56","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7776372","","Hospitals;Data mining;History;Informatics;Statistical analysis","data mining;health care;medical information systems;pattern clustering;statistical analysis","statistical estimation method;incremental software development;medical care quality;adaptive service improvement;clinical pathway;attribute clustering;data mining methods;HIS;hospital information system;clinical care construction;dual clustering","","","","7","","8 Dec 2016","","","IEEE","IEEE Conferences"
"Sensor-Based Arm Skill Training in Chronic Stroke Patients: Results on Treatment Outcome, Patient Motivation, and System Usability","A. A. A. Timmermans; H. A. M. Seelen; R. P. J. Geers; P. K. Saini; S. Winter; J. te Vrugt; H. Kingma","Eindhoven University of Technology (Biomedical Technology) and Adelante Centre of Expertise in Rehabilitation and Audiology, Hoensbroek, The Netherlands; Adelante Centre of Expertise in Rehabilitation and Audiology, Hoensbroek; Adelante Centre of Expertise in Rehabilitation and Audiology, Hoensbroek; Philips Research Europe (User Experiences), Eindhoven, The Netherlands; Philips Research Europe (Medical Signal Processing), Aachen; Philips Research Europe (Medical Signal Processing), Aachen; (Department of ORL-HNS), Maastricht University Medical Centre, Maastricht, De","IEEE Transactions on Neural Systems and Rehabilitation Engineering","7 Jun 2010","2010","18","3","284","292","As stroke incidence increases, therapists' time is under pressure. Technology-supported rehabilitation may offer new opportunities. The objective of this study was to evaluate patient motivation for and the feasibility and effects of a new technology-supported task-oriented arm training regime (T-TOAT). Nine chronic stroke patients performed T-TOAT (2 × 30 min/day, four days/week) during eight weeks. A system including movement tracking sensors, exercise board, and software-based toolkit was used for skill training. Measures were recorded at baseline, after four and eight weeks of training, and six months posttraining. T-TOAT improved arm-hand performance significantly on Fugl-Meyer, Action Research Arm Test, and Motor Activity Log. Training effects lasted at least six months posttraining. Health-related-quality-of-life had improved significantly after eight weeks of T-TOAT with regard to perceived physical health, but not to perceived mental health (SF-36). None of the EuroQol-5D components showed significant differences before and after training. Participants were intrinsically motivated and felt competent to use the system. Furthermore, system usability was rated very good. However, exercise challenge as perceived by participants decreased significantly over eight weeks of training. The results of this study indicate that T-TOAT is feasible. Despite the small number of stroke patients tested, significant and clinically relevant improvements in skilled arm-hand performance were found.","1558-0210","","10.1109/TNSRE.2010.2047608","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5446388","Clinical trial;motor learning;occupational and physical therapy;rehabilitation;stroke;wireless devices","Medical treatment;Usability;Management training;Testing;Europe;Tellurium;Tracking;Sensor systems;Biomedical measurements;Wireless sensor networks","biomechanics;biomedical measurement;brain;motion measurement;patient rehabilitation;patient treatment;sensors","sensor based arm skill training;chronic stroke patients;treatment outcome;patient motivation;system usability;patient therapy;technology supported rehabilitation;technology supported task oriented arm training regime;T-TOAT;movement tracking sensors;exercise board;software based toolkit;arm-hand performance;Fugl-Meyer scale;action research arm test;motor activity log;health related quality of life","Arm;Biomechanics;Chronic Disease;Data Interpretation, Statistical;Female;Hand;Humans;Learning;Male;Middle Aged;Motivation;Motor Skills;Neuropsychological Tests;Psychomotor Performance;Quality of Life;Stroke;Stroke;Treatment Outcome","43","","71","","12 Apr 2010","","","IEEE","IEEE Journals"
"Designing a GENI Experimenter Tool to Support the Choice Net Internet Architecture","D. Brown; O. Ascigil; H. Nasir; C. Carpenter; J. Griffioen; K. Calvert","Lab. for Adv. Networking, Univ. of Kentucky, Lexington, KY, USA; Lab. for Adv. Networking, Univ. of Kentucky, Lexington, KY, USA; Lab. for Adv. Networking, Univ. of Kentucky, Lexington, KY, USA; Lab. for Adv. Networking, Univ. of Kentucky, Lexington, KY, USA; Lab. for Adv. Networking, Univ. of Kentucky, Lexington, KY, USA; Lab. for Adv. Networking, Univ. of Kentucky, Lexington, KY, USA","2014 IEEE 22nd International Conference on Network Protocols","11 Dec 2014","2014","","","548","554","Test beds such as GENI provide an ideal environment for experimenting with future internet architectures such as Choice Net. Unlike the narrow waist of the current Internet (IP), Choice Net encourages alternatives and competition at the network layer via an economic plane that allows users to choose and purchase precisely the services they need. In this paper we describe our experiences implementing the Choice Net architecture on GENI. Some features of GENI, such as the ability to program the network layer, to leverage existing protocols and software, to run real applications generating realistic traffic, and the ability to perform long-running experiments made GENI an ideal platform for Choice Net experimentation. However, we found that GENI currently lacks the tools needed to make it easy to use these features. To address this issue, we designed and implemented a GENI Experimenter Tool specifically designed and tailored to perform tasks commonly needed by experimenters such as dynamically configuring nodes, loading and compiling node-specific code, executing Click modules, running commands on sets of nodes, accessing the local file system on nodes, and dynamically logging into nodes.","1092-1648","978-1-4799-6204-4","10.1109/ICNP.2014.88","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6980427","","Topology;Computer architecture;Software;Prototypes;Routing;Internet;Libraries","Internet;protocols","GENI experimenter tool;software;protocols;network layer;Choice Net architecture;test beds;Internet architecture","","1","","25","","11 Dec 2014","","","IEEE","IEEE Conferences"
"Data Provenance in the Cloud: A Blockchain-Based Approach","D. Tosh; S. Shetty; X. Liang; C. Kamhoua; L. L. Njilla","Computer Science, University of Texas at El Paso, United States; Virginia Modeling, Analysis and Simulation, Old Dominion University, Norfolk, Virginia United States; Virginia Modeling, Analysis and Simulation, Old Dominion University, Norfolk, Virginia United States; Network Security Branch, U.S. Army Research Laboratory, Adelphi, Maryland United States; Cyber Assurance Branch, U.S. Air Force Research Laboratory, Rome, New York United States","IEEE Consumer Electronics Magazine","6 Jun 2019","2019","8","4","38","44","Ubiquitous adoption of cloud computing and virtualization technology has necessitated the need for strong security mechanisms. Multiple entities are involved in creating, exchanging, and altering data objects in the cloud environment, making it challenging to track malicious activities and security violations. To address these issues, there is a need for a data provenance framework, with which each data object in the federated cloud environment can be tracked and recorded. Although log-based provenance provides the ability to track operations conducted on digital assets, the provenance data are not transparent and immutable. Blockchain technology offers a promising mechanism for building a tamper-proof information system backed by strong cryptographic primitives. In this article, we propose BlockCloud, a blockchain-empowered data provenance architecture for the cloud computing platform. In addition, we present a proof-of-stake (PoS) consensus mechanism for BlockCloud to alleviate the overhead of computational requirements that the traditional proof-of-work (PoW) consensus needs. Finally, we discuss several research challenges and vulnerabilities that need to be addressed to realize BlockCloud.","2162-2256","","10.1109/MCE.2019.2892222","Air Force Materiel Command; Assistant Secretary of Defense for Research and Engineering; U.S. Department of Energy; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8732702","","Cloud computing;Blockchain;Software development;Security;Virtualization","cloud computing;cryptography","ubiquitous adoption;virtualization technology;strong security mechanisms;data objects;malicious activities;security violations;data provenance framework;federated cloud environment;log-based provenance;digital assets;provenance data;blockchain technology;tamper-proof information system;strong cryptographic primitives;BlockCloud;blockchain-empowered data provenance architecture;cloud computing platform;proof-of-stake consensus mechanism;computational requirements;vulnerabilities;blockchain-based approach","","6","","15","","6 Jun 2019","","","IEEE","IEEE Magazines"
"PGIS: Electronic diary data integration with GPS data initial application in substance-abuse patients","M. Vahabzadeh; M. Mezghanni; J. Lin; D. H. Epstein; K. L. Preston","DHHS, National Institutes of Health, National Institute on Drug Abuse, Intramural Research Program, Baltimore, MD 21224; Johns Hopkins Bayview Medical Center, Baltimore, MD 21224; DHHS, National Institutes of Health, National Institute on Drug Abuse, Intramural Research Program, Baltimore, MD 21224; DHHS, National Institutes of Health, National Institute on Drug Abuse, Intramural Research Program, Baltimore, MD 21224; DHHS, National Institutes of Health, National Institute on Drug Abuse, Intramural Research Program, Baltimore, MD 21224","2010 IEEE 23rd International Symposium on Computer-Based Medical Systems (CBMS)","13 Oct 2011","2010","","","474","479","Quantification of exposure to psychosocial stressors and drug availability might assist in the prevention and treatment of substance-use problems. The core of such interventions lies in combining real-time self-report data (via Ecological Momentary Assessment, EMA) with real-time geolocation data (via GPS logging). Combining these types of data and linking the result with patients' clinical research records has inherent technical challenges. In this paper, we describe how we have addressed those challenges with our Psychosocial Geolocation Integration System (PGIS), which we successfully used in two clinical studies involving polydrug-abusing participants.","1063-7125","978-1-4244-9168-1","10.1109/CBMS.2010.6042691","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6042691","","Global Positioning System;Batteries;Software;Geology;Drugs;Real time systems;Satellites","data handling;Global Positioning System;medical computing;personal computing;psychology;social sciences computing","PGIS;electronic diary data integration;GPS data;substance-abuse patients;psychosocial stressors;drug availability;substance-use problems;ecological momentary assessment;real-time geolocation data;GPS logging;psychosocial geolocation integration system;polydrug-abusing participants","","7","","12","","13 Oct 2011","","","IEEE","IEEE Conferences"
"A detection method for malicious codes in Android apps","Jinxin Liu; Hao Wu; Huabin Wang","State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, China; State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, China; State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, China","10th International Conference on Wireless Communications, Networking and Mobile Computing (WiCOM 2014)","25 Jun 2015","2014","","","514","519","In recent years, the Android operating system for mobile terminals has developed very quickly. A variety of mobile devices which are using Android operating system are more than 60% in the domestic market share. With the number of Android application raising fast, a variety of information leakage, malicious chargeback, failure of operating system events occurred frequently; the safety of Android system also attracts wide attention of researchers. In this paper, combining static analysis and dynamic analysis, we present a malicious code detection method and implementation. Through the statistics of the sensitive API functions and tracking the flow of sensitive information, static analysis module uses the static analysis reverse technology to achieve the detection of malicious behaviors. And dynamic analysis module mainly uses system log analysis and records a variety of sensitive behaviors generated during the operation to discover intrusions. Furthermore, the ultimate combination of static analysis and dynamic analysis will determine whether the target software contains malicious codes.","","978-1-84919-845-5","10.1049/ic.2014.0154","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7129682","Android;Static Analysis;Dynamic Analysis","","Android (operating system);application program interfaces;mobile computing;program diagnostics;security of data","malicious codes;Android apps;Android operating system;mobile terminals;information leakage;malicious chargeback;static analysis;API functions;reverse technology","","","","","","25 Jun 2015","","","IET","IET Conferences"
"“B here” Class Attendance Tracking System with Gamification","R. Pinter; S. M. Čisar","Subotica Tech,Department of Informatics,Subotica,Serbia; Subotica Tech,Department of Informatics,Subotica,Serbia","2019 10th IEEE International Conference on Cognitive Infocommunications (CogInfoCom)","11 May 2020","2019","","","271","276","“B here” student attendance tracking software system allows instructors to accurately verify whether a student is physically present in class. The majority of students have some kind of smart device and these can be used to make an application so that students can log in their attendance. The system presented in the paper consists of two parts. The first part is a hardware software system that automates the recording of attendees and provides a detailed overview of the student's presence during the school year. The second part of the application is a system that uses gamification. Gamification is seen as a way to improve student engagement, motivation, attendance, and academic performance.","2380-7350","978-1-7281-4793-2","10.1109/CogInfoCom47531.2019.9089936","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9089936","class attendance;tracking system;gamification;motivation","Correlation;Task analysis;Servers;Microprocessors;Bluetooth;Smart phones","computer aided instruction;educational administrative data processing;educational institutions;serious games (computing)","B here class attendance tracking system;gamification;smart device;hardware software system;student engagement;student attendance tracking software system","","","","40","","11 May 2020","","","IEEE","IEEE Conferences"
"A cloud you can trust","C. Cachin; M. Schunter",NA; NA,"IEEE Spectrum","28 Nov 2011","2011","48","12","28","51","This past April, Amazon's Elastic Compute Cloud service crashed during a system upgrade, knocking customers' websites off-line for anywhere from several hours to several days. That same month, hackers broke into the Sony PlayStation Network, exposing the personal information of 77 million people around the world. And in June a software glitch at cloud-storage provider Dropbox temporarily allowed visitors to log in to any of its 25 million customers' accounts using any password-or none at all. As a company blogger drily noted: ""This should never have happened."" And yet it did, and it does, with astonishing regularity. The Privacy Rights Clearinghouse has logged 175 data breaches this year in the United States alone, involving more than 13 million records.","1939-9340","","10.1109/MSPEC.2011.6085778","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6085778","","Cloud computing;Computer security;Virtual machining;Privacy","cloud computing;computer crime;data privacy;Web sites","Amazon elastic compute cloud service;customer Websites;hackers;Sony PlayStation network;software glitch;cloud storage provider;Dropbox;privacy rights clearinghouse","","14","1","","","28 Nov 2011","","","IEEE","IEEE Magazines"
"Large-scale Detection of Privacy Leaks for BAT Browsers Extensions in China","Y. Zhao; L. He; Z. Li; L. Yang; H. Dong; C. Li; Y. Wang",Beihang University; National Computer Network Emergency Response Technical Team/Coordination Center of China; Beihang University; Beihang University; Beihang University; Beihang University; Beihang University,"2019 International Symposium on Theoretical Aspects of Software Engineering (TASE)","28 Nov 2019","2019","","","57","64","Although browser extensions bring users a better experience, it creates a hidden danger of privacy leakage. A common privacy leakage detection method is realized through detecting private data transmission. However, only the unintended transmission is considered to be a privacy leak. Therefore, the real challenge is to determine whether or not the transmission is user intended. In order to address this problem, we check the rationality of private data transmission by establishing a privacy model based on classification for extensions to confirm the scope of private data that can be uploaded and domains that can be sent to. Furthermore, we present BEDS (Browser Extension Detection System), a Chromium based extension dynamic detection system. BEDS first builds a privacy model for each extension and then records the extension's network logs and browser API logs when accessing specified pages. Finally, BEDS determines whether there exists a privacy leak according to the strict privacy leakage judgment rules. We test our implementation in large scale on extensions in browsers developed by China's three major Internet companies and complete 15 months of continuous tracking. After examining a total of 14,487 extensions, 1,897 privacy leaks are identified, all results have been inspected by manual and the accuracy of BEDS is over 97%. A number of domains that illegally collect private user data are discovered and tracked. Our results show that about 47,000 Chinese IPs upload private information to suspicious servers every day.","","978-1-7281-3342-3","10.1109/TASE.2019.00-19","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8914061","privacy leaks, BAT browsers extensions, large-scale detection","Browsers;Data privacy;Privacy;Chromium;Data communication;Servers;History","data privacy;online front-ends","privacy leak;BAT browsers extensions;browser extensions;common privacy leakage detection method;private data transmission;privacy model;BEDS;Chromium based extension dynamic detection system;browser API;privacy leaks;private user data;privacy leakage judgment rules;browser extension detection system","","1","","32","","28 Nov 2019","","","IEEE","IEEE Conferences"
"Design and Development of an Autonomous Surface Vessel for Inland Water Depth Monitoring","M. H. Bin Mat Idris; M. Al Azhari Bin Che Kamarudin; M. I. Sahalan; Z. Bin Zainal Abidin; M. M. Rashid","Dept. of Mechatron., IIUM, Kuala Lumpur, Malaysia; Dept. of Mechatron., IIUM, Kuala Lumpur, Malaysia; Dept. of Mechatron., IIUM, Kuala Lumpur, Malaysia; Dept. of Mechatron., IIUM, Kuala Lumpur, Malaysia; Dept. of Mechatron., IIUM, Kuala Lumpur, Malaysia","2016 International Conference on Computer and Communication Engineering (ICCCE)","9 Jan 2017","2016","","","177","182","This paper is about design and development of an Autonomous Surface Vessel (ASV) for inland water depth monitoring. Water depth measurement a.k.a bathymetry is important for critical areas such as river and water reservoir (dam) to estimate the volume and surface area for environment conservation and safety purpose. Conventionally, in inland water monitoring, the data obtained from the measurements are recorded in the log book and have to go through some screening process before plotting contour. This procedure is not only slow but also requires a lot of logistic equipment and labor forces. Therefore, an Autonomous Surface Vessel equipped with an echo sounder and data telemetry is proposed. It can be either remotely controlled or run automatically by autopilot navigation software. It is also equipped with GPS and compass for the navigation sensor feedback. The ASV was tested at several lakes for the performance of data collection and navigation.","","978-1-5090-2427-8","10.1109/ICCCE.2016.47","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7808305","unmanned surface vessel;autonomous surface vessel;autopilot;ardupilot;bathymetry;waypoint tracking","Global Positioning System;Sea surface;Telemetry;Software;Brushless motors;Monitoring","autonomous underwater vehicles;bathymetry;computerised monitoring;echo;Global Positioning System;logistics;oceanographic equipment;spatial variables measurement;telemetry","autonomous surface vessel design;inland water depth monitoring;autonomous surface vessel development;ASV;GPS;water depth measurement;a.k.a bathymetry;screening process;logistic equipment;labor forces;data telemetry;autopilot navigation software;navigation sensor feedback;data collection","","4","","11","","9 Jan 2017","","","IEEE","IEEE Conferences"
"On the functionalities of an information-based management system of community health service in China","H. Hu; Y. Zhang; F. Yu; Y. He; Y. Wan","Institute of Medical Information, Chinese Academy of Medical Sciences, Beijing, China; School of Management, Beijing University of Chinese Medicine, Beijing, China; Institute of Medical Information, Chinese Academy of Medical Sciences, Beijing, China; Institute of Medical Information, Chinese Academy of Medical Sciences, Beijing, China; Institute of Medical Information, Chinese Academy of Medical Sciences, Beijing, China","2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)","24 Oct 2016","2016","","","1414","1420","To increase work efficiency of administration transaction, we have designed the Information-based management system of CHS. The system includes four subsidiary systems: The administration management handing system, evaluating system, searching system and statistics analytical system. The administration management handling system includes the application system of the CHS station(center), the examine system of the CHS center, the examination and approval system of district public health bureau and the record system of municipal public health bureau. The contents include installation, registering, checking-up, alteration and logging-off. The system of automatic evaluation can compute automated synthetically index numbers of evaluated units and display the best and the worst items according to the evaluation system which are set up in the computer in advance; it could rank the order of units of the same kind according to the synthesized index numbers and the order of the individual items. The searching system included basic data searching, personnel searching, administrations results searching. The system of statistics and analysis can make statistics and analyze the basic circumstance about CHS, by which researchers can accurately master the condition of the CHS resources, and can constitute statistics charts meeting different requests. It could provide basis decision for the administrative departments so as to allot hygiene resources reasonably. By carrying out the system, the compliance management definitely be strengthened so that CHS sector will enter the information age of standardized and automated operation, complying with laws and regulation. CHS can be improved by management.","","978-1-5090-4093-3","10.1109/FSKD.2016.7603385","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7603385","Community health service;Information-based;Management","Organizations;Registers;Medical diagnostic imaging;Software;Standards organizations;Indexes;Medical services","health care;information management;medical information systems","information-based management system;community health service;China;administration management handing system;evaluating system;searching system;statistics analytical system;CHS station;district public health bureau;record system;examine system;approval system;municipal public health bureau","","","","26","","24 Oct 2016","","","IEEE","IEEE Conferences"
"Graph-Aided Directed Testing of Android Applications for Checking Runtime Privacy Behaviours","J. C. J. Keng; L. Jiang; T. K. Wee; R. K. Balan",NA; NA; NA; NA,"2016 IEEE/ACM 11th International Workshop in Automation of Software Test (AST)","9 Jan 2017","2016","","","57","63","While automated testing of mobile applications is very useful for checking run-time behaviours and specifications, its capability in discovering issues in apps is often limited in practice due to long testing time. A common practice is to randomly and exhaustively explore the whole app test space, which takes a lot of time and resource to achieve good coverage and reach targeted parts of the apps.In this paper, we present MAMBA1, a directed testing system for checking privacy in Android apps. MAMBA performs path searches of user events in control-flow graphs of callbacks generated from static analysis of app bytecode. Based on the paths found, it builds test cases comprised of user events that can trigger the executions of the apps and quickly direct the apps' activity transitions from the starting activity towards target activities of interest, revealing potential accesses to privacy-sensitive data in the apps. MAMBA's backend testing engine then simulates the executions of the apps following the generated test cases to check actual runtime behavior of the apps that may leak users' private data. We evaluated MAMBA against another automated testing approach that exhaustively searches for target activities in 24 apps, and found that our graph-aided directed testing achieves the same coverage of target activities 6.1 times faster on average, including the time required for bytecode analysis and test case generation. By instrumenting privacy access/leak detectors during testing, we were able to verify from test logs that almost half of target activities accessed user privacy data, and 26.7% of target activities leaked privacy data to the network.","","978-1-4503-4151-6","10.1109/AST.2016.017","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7809823","Automated Mobile App Testing;Mobile Privacy;Static/Dynamic Analysis;Run-time Verification","Testing;Androids;Humanoid robots;Privacy;Data privacy;Mobile applications;Detectors","Android (operating system);data privacy;formal specification;graph theory;mobile computing;program diagnostics","graph aided directed testing;Android applications;runtime privacy behaviour checking;mobile applications;specification checking;MAMBA1;callback control flow graphs;static analysis;app bytecode;data privacy sensitivity","","","","27","","9 Jan 2017","","","IEEE","IEEE Conferences"
"Stride: Search-based deterministic replay in polynomial time via bounded linkage","J. Zhou; X. Xiao; C. Zhang","The Prism Research Group, Department of Computer Science and Engineering, The Hong Kong University of Science and Technology; The Prism Research Group, Department of Computer Science and Engineering, The Hong Kong University of Science and Technology; The Prism Research Group, Department of Computer Science and Engineering, The Hong Kong University of Science and Technology","2012 34th International Conference on Software Engineering (ICSE)","28 Jun 2012","2012","","","892","902","Deterministic replay remains as one of the most effective ways to comprehend concurrent bugs. Existing approaches either maintain the exact shared read-write linkages with a large runtime overhead or use exponential off-line algorithms to search for a feasible interleaved execution. In this paper, we propose Stride, a hybrid solution that records the bounded shared memory access linkages at runtime and infers an equivalent interleaving in polynomial time, under the sequential consistency assumption. The recording scheme eliminates the need for synchronizing the shared read operations, which results in a significant overhead reduction. Comparing to the previous state-of-the-art approach of deterministic replay, Stride reduces, on average, 2.5 times of runtime overhead and produces, on average, 3.88 times smaller logs.","1558-1225","978-1-4673-1067-3","10.1109/ICSE.2012.6227130","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6227130","Concurrency;Replaying;Debugging","Couplings;Instruction sets;Schedules;Law;Runtime;Instruments","polynomials;program debugging;search problems;shared memory systems","Stride;search based deterministic replay;polynomial time;bounded linkage;concurrent bugs;read write linkages;exponential offline algorithms;shared memory access;sequential consistency assumption","","12","","34","","28 Jun 2012","","","IEEE","IEEE Conferences"
"Continuous queries over distributed streams of heterogeneous monitoring data in cloud datacenters","D. Tovarňák; T. Pitner","Masaryk University, Faculty of Informatics, Botanická 68a, 60200 Brno, Czech Republic; Masaryk University, Faculty of Informatics, Botanická 68a, 60200 Brno, Czech Republic","2014 9th International Conference on Software Engineering and Applications (ICSOFT-EA)","8 Oct 2015","2014","","","470","481","The use of stream processing for state monitoring of distributed infrastructures has been advocated by some in order to overcome the issues of traditional monitoring solutions when tasked with complex continuous queries. However, in the domain of behavior monitoring the situation gets more complicated. It is mainly because of the low-quality source of behavior-related monitoring information (natural language computer logs). Existing approaches prevalently rely on indexing and real-time data-mining of the behavior-related data rather than on using event/stream processing techniques and the many corresponding benefits. The goal of this paper is to present a general notion of Distributed Event-Driven Monitoring Architecture that will enable an easy definition of expressive continuous queries over many distributed and heterogeneous streams of behavior-related (and state-related) monitoring data.","","978-9-8975-8124-3","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7293903","Stream Processing;Distributed Architectures;Monitoring;Cloud","Monitoring;Distributed databases;Hardware;Computer architecture;Natural languages;Security;Virtualization","","","","","","27","","8 Oct 2015","","","IEEE","IEEE Conferences"
"Tracking attack sources based on traceback honeypot for ICS network","S. Abe; Y. Tanaka; Y. Uchida; S. Horata","ICS security Response Group, JPCERT/CC, Tokyo, Japan; ICS security Response Group, JPCERT/CC, Tokyo, Japan; ICS security Response Group, JPCERT/CC, Tokyo, Japan; ICS security Response Group, JPCERT/CC, Tokyo, Japan","2017 56th Annual Conference of the Society of Instrument and Control Engineers of Japan (SICE)","13 Nov 2017","2017","","","717","723","In Industrial Control System (ICS) networks, it is generally difficult to discover security threats from logs recorded in ICS devices, such as packets originated from malware-infected devices and evidence of the devices remotely controlled by attackers. This is due to the fact that ICS devices output logs not for detecting security threats but simply for recording operation history. Some researchers have suggested placing honeypots within ICS networks to observe packets from attackers in order to detect threats. In the research for this paper, the suggested method was further improved so that it responds to packets reaching the honeypots and collects information of the attack sources. Previous analysis has revealed that machines infected with some known malware (e.g. Havex RAT - a Remote Access Tool) in ICS networks conduct scan activities against certain devices, and therefore the traceback honeypots are expected to identify infected devices out of such scans in an effective manner. Information about attack sources collected from the analysis can be utilised for proactive purposes, which could be useful in detecting or blocking certain communication to prevent further infection. This paper discusses methods of tracking attack sources using traceback honeypots.","","978-4-907764-57-9","10.23919/SICE.2017.8105603","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8105603","ICS Security;Honeypot;Malware","Malware;Protocols;Computer security;Ports (Computers)","control engineering computing;industrial control;invasive software;production engineering computing","attack sources;traceback honeypot;ICS network;Industrial Control System networks;security threats;malware-infected devices;honeypots;ICS devices","","3","","20","","13 Nov 2017","","","IEEE","IEEE Conferences"
"Distributed Platform for the Analysis of Cryptographic Algorithms","V. Ozunu; C. Pîrvu; C. Leordeanu; V. Cristea","Fac. of Autom. Control & Comput., Univ. `Politeh.` of Bucharest, Bucharest, Romania; Fac. of Autom. Control & Comput., Univ. `Politeh.` of Bucharest, Bucharest, Romania; Fac. of Autom. Control & Comput., Univ. `Politeh.` of Bucharest, Bucharest, Romania; Fac. of Autom. Control & Comput., Univ. `Politeh.` of Bucharest, Bucharest, Romania","2016 10th International Conference on Complex, Intelligent, and Software Intensive Systems (CISIS)","22 Dec 2016","2016","","","296","301","Data protection and information security have always been intricate problems of the majority of software applications which have been deployed throughout the Internet. Consequently, a substantial effort has been put into the creation and development of a wide variety of solutions to tackle this very issue. The aim of this paper is to offer a means of performance measurement and security validation for some of the encryption algorithms which are extensively used in today's industry (DES, 3DES, AES, etc.), as well as some hash functions. Therefore, the evaluation platform takes on the above mentioned algorithms from two very divergent perspectives: one of them focuses mainly on CPU vs. GPU performance issues, whereas the latter tackles the problem of randomness of the encrypted results by comparison to several strict criteria. In order to achieve these goals, the platform provides a graphical user interface which eases interactions such as: tests selection, worker attachment or removal, events logging, input provision and output analysis. The proposed solution represents an evaluation platform that performs a wide range of tests on hash and symmetric key algorithms in order to deduce their performance and behavior on multiple architectures, as well as various NIST tests, on different environments.","","978-1-5090-0987-9","10.1109/CISIS.2016.139","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7791898","Distributed Platform;Cryptographic algorithms;AES;3DES;Performance evaluation","Servers;Cryptography;Computer architecture;Graphical user interfaces;Industries;Context;Protocols","cryptography;data protection;graphical user interfaces;Internet","cryptographic algorithms;data protection;information security;Internet;encryption algorithms;hash functions;CPU performance issues;GPU performance issues;graphical user interface;symmetric key algorithms;NIST tests","","","","9","","22 Dec 2016","","","IEEE","IEEE Conferences"
"Development of a Content Rating System Using a Pressure Sensor and Its Application to a Comic Dialogue","H. Yoshida; J. Itou; J. Munemori","Wakayama Univ., Wakayama, Japan; Wakayama Univ., Wakayama, Japan; Wakayama Univ., Wakayama, Japan","2013 International Conference on Culture and Computing","12 Dec 2013","2013","","","163","164","We have developed a content rating system based on the evaluations of two or more audiences. The audiences watch the content using this system, and they input emoticons using a pressure sensor if they feel that the content is interesting. The system displays these emoticons on a chat screen, and records them in a log. We applied this system to a Japanese form of comic dialogue, and performed a series of experiments. The comic dialogue involves two people, one in the role of ""boke,"" and the other as ""tsukkomi."" We found that the appearance of emoticons and the laughter recorded in the content do not necessarily occur at the same time.","","978-0-7695-5047-3","10.1109/CultureComputing.2013.47","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6680360","pressure sensor;emoticon;chat","Robot sensing systems;Educational institutions;Software;Electronic mail;Pressure measurement;Mice","entertainment;pressure sensors;text analysis","content rating system;pressure sensor;emoticons;chat screen;boke;tsukkomi;Japanese comic dialogue;laughter;audience evaluation","","","","3","","12 Dec 2013","","","IEEE","IEEE Conferences"
"Web Application Vulnerabilities - The Hacker's Treasure","K. Nirmal; B. Janet; R. Kumar","NIT, Trichy, India; NIT, Trichy, India; Wipro Technologies, India","2018 International Conference on Inventive Research in Computing Applications (ICIRCA)","3 Jan 2019","2018","","","58","62","In today's online era, a web application is an integral part of every business. A web application may be a single page HTML website or a large web portal that offers various services on a web browser. There are many tools and methodologies that are relied upon to develop a web application. The development methodologies incorporate specialized frameworks, libraries in order to have the application more standardized and have it developed at a rapid pace to meet market demands. Web applications (web app) are hardened to mitigate security issues which are commonly referred as web application vulnerabilities. A web app security vulnerability is any kind of loop hole that allows an attacker to break into the web application to perform undesired actions on the target website. This may range from a cross site scripting (XSS) to vulnerabilities like Server Side Request Forger (SSRF) and its implications like XML External Entity (XXE). Though web applications are hardened to mitigate vulnerabilities, large scale web applications are still vulnerable post release in most cases. As a part of security research, critical vulnerabilities on large scale web applications were identified and the same were reported to the concerned security research team. The reporting was acknowledged and mitigated through appropriate channels. Common Vulnerabilities and Exposures (CVEs) were filed on Microsoft and CISCO products and the same were logged in National Vulnerability Database (NVD). Insights and tenets regarding web application and its vulnerabilities are highlighted in this manuscript.","","978-1-5386-2456-2","10.1109/ICIRCA.2018.8597221","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8597221","Online Security;Web application vulnerability;Phishing;Pharming","Software;Computer hacking;Portals;SQL injection;Penetration testing","online front-ends;security of data","web application vulnerabilities;web portal;web browser;web app security vulnerability;cross site scripting;XSS;server side request forger;SSRF;XML external entity;XXE;common vulnerabilities and exposures;National Vulnerability Database;Microsoft;CISCO products","","1","","11","","3 Jan 2019","","","IEEE","IEEE Conferences"
"IMS Based Session Initiation Protocol in Robot Framework for Telephony Services","S. Thejashwini; M. S. Kumar; S. A. Alex","Ramaiah Institute of Technology, Bangalore; Nokia Solutions & Networks, Bangalore; Ramaiah Institute of Technology, Bangalore","2018 International Conference on Inventive Research in Computing Applications (ICIRCA)","3 Jan 2019","2018","","","1218","1223","IMS is a standard architectural framework which is used for the purpose of delivering multimedia communications services such as voice, video and text messaging over IP networks. IMS architecture has been sub-divided into number of network components or elements which each does its own distinct job in the network, standardized interfaces it follows to promote scalability, flexibility and extensibility. The Session Initiation Protocol or it can be called as SIP is used for establishing the sessions, modifying the sessions, and termination the sessions. The SIP protocol is designed for the purpose of providing the independent services for the underlying transport protocol, so SIP applications can run over TCP, UDP, or TLS or any other network protocols. The paper introduces with what is robot framework basically and how it is connected to IMS and how it uses an standard protocol Session Initiation Protocol for telephony services. Robot framework is used for functional testing building the devices to work end-to-end. The framework uses an approach keyword driven for the purpose of writing the scripts and after the execution of the scripts the it will be stored in log files and displayed in from html page.","","978-1-5386-2456-2","10.1109/ICIRCA.2018.8597315","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8597315","SIP;IMS;ROBOT FRAMEWORK","Protocols;Robots;Testing;Servers;Libraries;Standards;Software","Internet telephony;IP networks;multimedia communication;signalling protocols;transport protocols","robot framework;telephony services;standard architectural framework;multimedia communications services;text messaging;IP networks;IMS architecture;SIP protocol;independent services;SIP applications;network protocols;transport protocol;session initiation protocol","","","","10","","3 Jan 2019","","","IEEE","IEEE Conferences"
"Effects of Digital Textbook-Based Nursing Education in Professional School","M. Tanaka; A. Uchida; A. Kanda; T. Matsuo",NA; NA; NA; NA,"2015 International Conference on Computer Application Technologies","7 Jan 2016","2015","","","205","206","In this paper, we introduce a case study of digital textbook-based education in professional school to educate nursing and care. Digital textbook is utilized on digital textbook viewer software installed on the table computer. In professional education, using digital textbook is one of promising field to enhance its quality since the detailed description can be easily understood. Further, instructors can know students' condition of learning viewing recorded log of their learning and users history. The latter half of this paper discusses our challenges of use of digital textbook in actual professional education. In nursing education, digital textbook-based instruction and study is not familiar with instructor and students. However, we achieve a measure of our goal of instruction using digital textbook in nursing school.","","978-1-4673-8211-3","10.1109/CCATS.2015.56","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7372346","Digital Text;Nurse training;Registered dietitian training","Education;Portable document format;Medical services;Computers;Smart phones;Three-dimensional displays;Solid modeling","biomedical education;computer aided instruction;patient care","digital textbook-based nursing education;professional school;digital textbook-based education;nursing and care;digital textbook viewer software;table computer;professional education;digital textbook-based instruction","","1","","4","","7 Jan 2016","","","IEEE","IEEE Conferences"
"Biometric Class Attendance Register","A. A. Effah; C. C. Ackatiah; F. N. Oppong; E. A. Frimpong","Kwame Nkrumah University of Science and Technology,Department of Electrical and Electronic Engineering,Kumasi,Ghana; Kwame Nkrumah University of Science and Technology,Department of Electrical and Electronic Engineering,Kumasi,Ghana; Kwame Nkrumah University of Science and Technology,Department of Electrical and Electronic Engineering,Kumasi,Ghana; Kwame Nkrumah University of Science and Technology,Department of Electrical and Electronic Engineering,Kumasi,Ghana","2020 IEEE PES/IAS PowerAfrica","12 Oct 2020","2020","","","1","5","The paper presents an on-going work to design an automatic class attendance register. The proposed design uses a student's index finger to register his/her attendance, with the use of radio frequency identification (RFID) card as a backup. The attendance register is uploaded onto a web-based database for recording and analysis. The hardware and software components of the design have been largely done. Preliminary test results show that the design successfully enrolls, registers, logs and stores attendance data.","","978-1-7281-6746-6","10.1109/PowerAfrica49420.2020.9219846","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9219846","Biometric;database;fingerprint;register","","biometrics (access control);educational administrative data processing;radiofrequency identification","biometric class attendance register;automatic class attendance register;student;radio frequency identification card;Web-based database;software components;attendance data;RFID card","","","","8","","12 Oct 2020","","","IEEE","IEEE Conferences"
"Visual Drift Detection for Sequence Data Analysis of Business Processes","A. Yeshchenko; C. Di Ciccio; J. Mendling; A. Polyvyanyy","Information Systems and Operations, Vienna University of Economics and Business, 27254 Wien, Wien, Austria, (e-mail: anton.yeshchenko@wu.ac.at); Department of Computer Science, Sapienza University of Rome, 9311 Rome, Rome, Italy, (e-mail: diciccio@di.uniroma1.it); Information Systems and Operations, Vienna University of Economics and Business, 27254 Wien, Wien, Austria, (e-mail: jan.mendling@wu.ac.at); School of Computing and Information Systems, The University of Melbourne, 2281 Melbourne, Victoria, Australia, (e-mail: artem.polyvyanyy@unimelb.edu.au)","IEEE Transactions on Visualization and Computer Graphics","","2021","PP","99","1","1","Event sequence data is increasingly available in various application domains, such as business process management, software engineering, or medical pathways. Processes in these domains are typically represented as process diagrams or flow charts. So far, various techniques have been developed for automatically generating such diagrams from event sequence data. An open challenge is the visual analysis of drift phenomena when processes change over time. In this paper, we address this research gap. Our contribution is a system for fine-granular process drift detection and corresponding visualizations for event logs of executed business processes. We evaluated our system both on synthetic and real-world data. On synthetic logs, we achieved an average F-score of 0.96 and outperformed all the state-of-the-art methods. On real-world logs, we identified all types of process drifts in a comprehensive manner. Finally, we conducted a user study highlighting that our visualizations are easy to use and useful as perceived by process mining experts. In this way, our work contributes to research on process mining, event sequence analysis, and visualization of temporal data.","1941-0506","","10.1109/TVCG.2021.3050071","H2020 Marie Skodowska-Curie Actions; MIUR grant Dipartimenti di eccellenza 2018-2022; Australian Respiratory Council; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9316994","Sequence data;Visualization;Temporal data;Process mining;Process drifts;Declarative process models","Business;Data visualization;Data mining;Visualization;Erbium;Antibiotics;Guidelines","","","","","","","IEEE","8 Jan 2021","","","IEEE","IEEE Early Access Articles"
"Identifying and Understanding Stakeholders using Process Mining: Case Study on Discovering Business Processes that Involve Organizational Entities","S. Saito",NTT,"2019 IEEE 27th International Requirements Engineering Conference Workshops (REW)","19 Dec 2019","2019","","","216","219","To successfully define a system and context boundary, requirements engineers must identify the stakeholders related to the system and understand them. Typically, to make an industrial system development project, it is necessary for requirement engineers to know which organizational entity in the industry, as a stakeholder, has an influence on the requirements of the system. Models of business processes that including swim lanes are very helpful for requirements engineers to identify the organizational entity executing the activities related to the system. However, such business processes are often undocumented, and discovering them is difficult and time-consuming. In this report, we introduce a novel approach for discovering business processes that involve swim lanes. Based on process mining technique, our approach generates the business process from an event log and user information extracted from an existing system.","","978-1-7281-5165-6","10.1109/REW.2019.00045","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8933622","Stakeholders, Business Process, Process Mining","Unified modeling language;Stakeholders;Data mining;Tools;Industries;Software","business data processing;data mining;organisational aspects","industrial system development project;organizational entity;business process;process mining technique;organizational entities","","","","9","","19 Dec 2019","","","IEEE","IEEE Conferences"
"An energy management led approach to configuration and deployment of energy harvesting data loggers to monitor trackside assets","R. J. Preece; T. M. Hanif; R. J. Amos; E. J. C. Stewart","Arrowvale Electronics, UK, Shawbank Road, Reddich, Worcestershire, B98 8YN; University of Birmingham, UK, Edgbaston, Birmingham, B15 2TT; Arrowvale Electronics, UK, Shawbank Road, Reddich, Worcestershire, B98 8YN; Arrowvale Electronics, UK, Shawbank Road, Reddich, Worcestershire, B98 8YN; University of Birmingham, UK, Edgbaston, Birmingham, B15 2TT","6th IET Conference on Railway Condition Monitoring (RCM 2014)","14 May 2015","2014","","","1","6","The monitoring of remote assets or earthworks on Network Rail property can be prohibitive for a multitude of reasons not only limited to power, access and deployment complexities. This problem has led to the development of an Energy Harvesting Data Logger or EHDL, this product has been designed to be a low powered, generic data logger, powered from harvested energy and configurable to multiple applications. Currently proposed applications include embankment slippage monitoring, water levels and orientation of temporary speed restriction boards. This information can then be fed back into server based land-side system, this system aims to allow for rapid deployment and remote configuration of loggers. This paper covers an energy management approach to remote data logging based around the EHDL; the hardware has been designed for the purpose of extracting maximum data logging potential from limited harvested energy. Companioned to the EHDL is a graphical utility used for configuration and programming of the device, this tool enables the user to develop application specific configuration files. By selection of various parameters such as energy source, required number of inputs and geographical location, the utility allows the user to make informed decisions on how regularly the hardware can perform channel scans, server updates and other functions. Further to this, the internal software of the EHDL is subsequently configured by the utility to reserve power when necessary, when in operation the hardware monitors its own power usage when performing various functions, its remaining battery level and an estimated battery life. By feeding this information back into the utility, a more accurate power model can be produced, allowing for more precise estimations.","","978-1-84919-913-1","10.1049/cp.2014.0990","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7105031","Condition Monitoring;Energy Harvesting","","computerised monitoring;condition monitoring;data loggers;energy harvesting;energy management systems;light emitting diodes;railway engineering","earthworks;network rail;energy management LED approach;energy harvesting data loggers;trackside remote assets monitoring;embankment slippage monitoring;water level monitoring;temporary speed restriction board;landside system;rapid deployment;remote configuration;remote data logging;EHDL;decision making","","2","","","","14 May 2015","","","IET","IET Conferences"
"RATFAT: Real-Time FAT for Cooperative Multitasking Environments in WSNs","S. Schildt; W. Pöttner; F. Büsching; L. Wolf","Inst. of Oper. Syst. & Comput. Networks, Tech. Univ. Braunschweig, Braunschweig, Germany; Inst. of Oper. Syst. & Comput. Networks, Tech. Univ. Braunschweig, Braunschweig, Germany; Inst. of Oper. Syst. & Comput. Networks, Tech. Univ. Braunschweig, Braunschweig, Germany; Inst. of Oper. Syst. & Comput. Networks, Tech. Univ. Braunschweig, Braunschweig, Germany","2013 IEEE International Conference on Distributed Computing in Sensor Systems","29 Jul 2013","2013","","","388","393","Today, many sensor nodes are equipped with a microSD slot to provide a cost effective way to store large amounts of data. When using the FAT file system, data collected by a node can be easily read by a PC without the need for any special software or communication protocol. While several FAT implementations for microcontrollers do exist, they are not suited for real-time applications. For a WSN in an industrial scenario where a node needs to run a closed loop control program, logging to a non-real-time capable persistent storage system is not an option. In this paper we present RATFAT, an efficient implementation of a flexible real-time capable FAT file system for Contiki, that can be used for applications requiring real-time guarantees.","2325-2944","978-0-7695-5041-1","10.1109/DCOSS.2013.70","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6569461","","Real-time systems;Writing;Wireless sensor networks;Microcontrollers;Multitasking;Distributed databases;Ash","real-time systems;storage allocation;wireless sensor networks","wireless sensor networks;nonreal-time capable persistent storage system;Contiki;RATFAT;closed loop control program;WSN;real-time applications;FAT file system;microSD slot","","2","","7","","29 Jul 2013","","","IEEE","IEEE Conferences"
"Billions and billions of constraints: Whitebox fuzz testing in production","E. Bounimova; P. Godefroid; D. Molnar","Microsoft Research, USA; Microsoft Research, USA; Microsoft Research, USA","2013 35th International Conference on Software Engineering (ICSE)","26 Sep 2013","2013","","","122","131","We report experiences with constraint-based whitebox fuzz testing in production across hundreds of large Windows applications and over 500 machine years of computation from 2007 to 2013. Whitebox fuzzing leverages symbolic execution on binary traces and constraint solving to construct new inputs to a program. These inputs execute previously uncovered paths or trigger security vulnerabilities. Whitebox fuzzing has found one-third of all file fuzzing bugs during the development of Windows 7, saving millions of dollars in potential security vulnerabilities. The technique is in use today across multiple products at Microsoft. We describe key challenges with running whitebox fuzzing in production. We give principles for addressing these challenges and describe two new systems built from these principles: SAGAN, which collects data from every fuzzing run for further analysis, and JobCenter, which controls deployment of our whitebox fuzzing infrastructure across commodity virtual machines. Since June 2010, SAGAN has logged over 3.4 billion constraints solved, millions of symbolic executions, and tens of millions of test cases generated. Our work represents the largest scale deployment of whitebox fuzzing to date, including the largest usage ever for a Satisfiability Modulo Theories (SMT) solver. We present specific data analyses that improved our production use of whitebox fuzzing. Finally we report data on the performance of constraint solving and dynamic test generation that points toward future research problems.","1558-1225","978-1-4673-3076-3","10.1109/ICSE.2013.6606558","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6606558","","Security;Testing;Computer bugs;Production;Servers;Monitoring","computability;data analysis;program diagnostics;program testing;security of data;virtual machines","binary traces;constraint solving performance;symbolic execution;program testing;security vulnerability;Windows 7;SAGAN system;JobCenter;commodity virtual machines;satisfiability modulo theories;SMT solver;data analysis;dynamic test generation;constraint-based whitebox fuzz testing","","45","2","23","","26 Sep 2013","","","IEEE","IEEE Conferences"
"Debugging Data Flows in Reactive Programs","H. Banken; E. Meijer; G. Gousios","Delft Univ. of Technol., Delft, Netherlands; Delft Univ. of Technol., Delft, Netherlands; Delft Univ. of Technol., Delft, Netherlands","2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)","2 Sep 2018","2018","","","752","763","Reactive Programming is a style of programming that provides developers with a set of abstractions that facilitate event handling and stream processing. Traditional debug tools lack support for Reactive Programming, leading developers to fallback to the most rudimentary debug tool available: logging to the console. In this paper, we present the design and implementation of RxFiddle, a visualization and debugging tool targeted to Rx, the most popular form of Reactive Programming. RxFiddle visualizes the dependencies and structure of the data flow, as well as the data inside the flow. We evaluate RxFiddle with an experiment involving 111 developers. The results show that RxFiddle can help developers finish debugging tasks faster than with traditional debugging tools.","1558-1225","978-1-4503-5638-1","10.1145/3180155.3180156","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8453148","reactive programming;debugging;visualization;program comprehension","Debugging;Programming;Tools;Observers;Libraries;Companies;Interviews","program debugging;program visualisation","debugging tool;RxFiddle;data flow;debugging tasks;traditional debugging tools;stream processing;traditional debug tools;rudimentary debug tool;reactive programming","","3","","52","","2 Sep 2018","","","IEEE","IEEE Conferences"
"Efficient Large-Scale Trace Checking Using MapReduce","M. M. Bersani; D. Bianculli; C. Ghezzi; S. Krstic; P. San Pietro","DEEPSE group - DEIB, Politec. di Milano, Milan, Italy; SnT Centre, Univ. of Luxembourg, Luxembourg, Luxembourg; DEEPSE group - DEIB, Politec. di Milano, Milan, Italy; DEEPSE group - DEIB, Politec. di Milano, Milan, Italy; DEEPSE group - DEIB, Politec. di Milano, Milan, Italy","2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)","3 Apr 2017","2016","","","888","898","The problem of checking a logged event trace against a temporal logic specification arises in many practical cases. Unfortunately, known algorithms for an expressive logic like MTL (Metric Temporal Logic) do not scale with respect to two crucial dimensions: the length of the trace and the size of the time interval of the formula to be checked. The former issue can be addressed by distributed and parallel trace checking algorithms that can take advantage of modern cloud computing and programming frameworks like MapReduce. Still, the latter issue remains open with current state-of-the-art approaches. In this paper we address this memory scalability issue by proposing a new semantics for MTL, called lazy semantics. This semantics can evaluate temporal formulae and boolean combinations of temporal-only formulae at any arbitrary time instant. We prove that lazy semantics is more expressive than point-based semantics and that it can be used as a basis for a correct parametric decomposition of any MTL formula into an equivalent one with smaller, bounded time intervals. We use lazy semantics to extend our previous distributed trace checking algorithm for MTL. The evaluation shows that the proposed algorithm can check formulae with large intervals, on large traces, in a memory-efficient way.","1558-1225","978-1-4503-3900-1","10.1145/2884781.2884832","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7886965","metric temporal logic;trace checking;MapReduce","Semantics;Programming;Scalability;Clustering algorithms;Measurement;Data models;Heuristic algorithms","formal specification;parallel algorithms;temporal logic","large-scale trace checking;MapReduce;temporal logic specification;MTL logic;metric temporal logic;parallel trace checking algorithms;distributed trace checking algorithms;memory scalability issue;lazy semantics;Boolean combination;point-based semantics","","1","","29","","3 Apr 2017","","","IEEE","IEEE Conferences"
"Using Unstructured Data to Improve the Continuous Planning of Critical Processes Involving Humans","C. Paterson; R. Calinescu; S. Manandhar; D. Wang",University of York; University of York; University of York; University of York,"2019 IEEE/ACM 14th International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS)","5 Aug 2019","2019","","","25","31","The success of processes executed in uncertain and changing environments is reliant on the dependable use of relevant information to support continuous planning at runtime. At the core of this planning is a model which, if incorrect, can lead to failures and, in critical processes such as evacuation and disaster relief operations, to harm to humans. Obtaining reliable and timely estimations of model parameters is often difficult, and considerable research effort has been expended to derive methods for updating models at run-time. Typically, these methods use data sources such as system logs, run-time events and sensor readings, which are well structured. However, in many critical processes, the most relevant data are produced by human participants to, and observers of, the process and its environment (e.g., through social media) and is unstructured. For such scenarios we propose COPE, a work-in-progress method for the continuous planning of critical processes involving humans and carried out in uncertain, changing environments. COPE uses a combination of runtime natural-language processing (to update a stochastic model of the target process based on unstructured data) and stochastic model synthesis (to generate Pareto-optimal plans for the process). Preliminary experiments indicate that COPE can support continuous planning effectively for a simulated evacuation operation after a natural disaster.","2157-2321","978-1-7281-3368-3","10.1109/SEAMS.2019.00013","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8787082","natural-language processing;stochastic model synthesis;probabilistic model checking","Planning;Stochastic processes;Optimization;Data models;Computational modeling;Twitter;Probabilistic logic","disasters;emergency management;natural language processing;Pareto optimisation;planning;stochastic processes","human participants;continuous planning;runtime natural-language processing;stochastic model;target process;unstructured data;Pareto-optimal plans;uncertain environments;disaster relief operations;simulated evacuation operation","","1","","40","","5 Aug 2019","","","IEEE","IEEE Conferences"
"Workload characterization and optimization of TPC-H queries on Apache Spark","T. Chiba; T. Onodera","IBM Research - Tokyo, 19-21, Nihonbashi Hakozaki-cho, Chuo-ku, 103-8510, Japan; IBM Research - Tokyo, 19-21, Nihonbashi Hakozaki-cho, Chuo-ku, 103-8510, Japan","2016 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)","2 Jun 2016","2016","","","112","121","Besides being an in-memory-oriented computing framework, Spark runs on top of Java Virtual Machines (JVMs), so JVM parameters must be tuned to improve Spark application performance. Misconfigured parameters and settings degrade performance. For example, using Java heaps that are too large often causes a long garbage collection pause time, which accounts for over 10-20% of application execution time. Moreover, recent computing nodes have many cores with simultaneous multi-threading technology and the processors on the node are connected via NUMA, so it is difficult to exploit best performance without taking into account of these hardware features. Thus, optimization in a full stack is also important. Not only JVM parameters but also OS parameters, Spark configuration, and application code based on CPU characteristics need to be optimized to take full advantage of underlying computing resources. In this paper, we used the TPC-H benchmark as our optimization case study and gathered many perspective logs such as application, JVM (e.g. GC and JIT), system utilization, and hardware events from a performance monitoring unit. We discuss current problems and introduce several JVM and OS parameter optimization approaches for accelerating Spark performance. As a result, our optimization exhibits 30-40% increase in speed on average and is up to 5x faster than the naive configuration.","","978-1-5090-1953-3","10.1109/ISPASS.2016.7482079","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7482079","","Sparks;Optimization;Java;Benchmark testing;Measurement;Runtime;Hardware","Java;multi-threading;optimisation;query processing;storage management;virtual machines","workload characterization;optimization;TPC-H queries;Apache Spark;in-memory-oriented computing;Java virtual machines;JVM;simultaneous multithreading technology;NUMA;OS parameters","","24","","28","","2 Jun 2016","","","IEEE","IEEE Conferences"
"Securing Car Data and Analytics using Blockchain","G. Saldamli; K. Karunakaran; V. K. Vijaykumar; W. Pan; S. Puttarevaiah; L. Ertaul","SanJose State University,Computer Engineering Department,San Jose,CA,USA; SanJose State University,Computer Engineering Department,San Jose,CA,USA; SanJose State University,Computer Engineering Department,San Jose,CA,USA; SanJose State University,Computer Engineering Department,San Jose,CA,USA; SanJose State University,Computer Engineering Department,San Jose,CA,USA; California State University,Department of Computer Science,East Bay Hayward,CA,USA","2020 Seventh International Conference on Software Defined Systems (SDS)","20 Jul 2020","2020","","","153","159","Automakers in collaboration with technology industries are swiftly innovating and transforming the automobile industry. The current trend of connected cars relies on retrieving various kinds of vehicular data since there is a huge demand from associated entities included insurance companies, vehicle buyers/sellers and government authorities. Currently, the data collection is done either manual or unsupervised that poses trust, legitimacy and accuracy issues such as duplicated or falsified vehicular data records, tampered safety checks and meddled driving history, etc. Hence, a strong tool that can protect the vehicular data; log the changes for audit purposes and eventually build the trust in the system is necessary. We propose the use of blockchain technology for these needs. The proposed solution involves connecting an IoT module to a car data port to collect rich telemetric data; analyze the driving behaviors on various fronts and storing the outcomes to a blockchain. For various good reasons we use the Ethereum blockchain in this study. However, other blockchain deployments can also be utilized. If adopted by the stakeholders, the proposed solution can provide a trusted, transparent and easily accessible platform to auto buyers/sellers, insurance agencies, vehicle dealers, law enforcements and vehicle history providers.","","978-1-7281-7219-4","10.1109/SDS49854.2020.9143914","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9143914","Car data;Blockchain;Ethereum;Smart contracts;OpenXC device","Automobiles;Maintenance engineering;Contracts;Industries;Libraries","automobile industry;automobiles;data analysis;data protection;distributed databases;driver information systems;insurance data processing;Internet of Things;production engineering computing;trusted computing","data collection;car data port;Ethereum blockchain technology;vehicular data;safety checks;audit purposes;IoT module;vehicle dealers;law enforcements;government authorities;insurance companies;automobile industry","","","","17","","20 Jul 2020","","","IEEE","IEEE Conferences"
"Extracting workflow structures through Bayesian learning and provenance data","M. Naseri; S. A. Ludwig","Department of Computer Science, University of Saskatchewan, Saskatoon, Canada; Department of Computer Science, North Dakota State University, Fargo, USA","2013 13th International Conference on Intellient Systems Design and Applications","13 Oct 2014","2013","","","319","324","Mining workflow models has been a problem of interest for the past few years. Event logs have been the main source of data for the mining process. Previous workflow mining approaches mostly focused on mining control flows that were based on data mining methods, as well as exploited time constraints of events to discover the workflow models. In this work, we present a mining approach which not only takes the behaviourial aspect of workflows into account, but also takes advantage of their informational perspective. Provenance information is a source of reasoning, learning, and analysis since it provides information regarding the service inputs, outputs and quality of service values. Therefore, provenance information along with Bayesian structure-learning methods are exploited for this purpose. Two constraint-based Bayesian structure-learning algorithms are investigated and modified in order to make use of additional provenance information. We will show that this leads to better mining results based on three common mining scenarios.","2164-7151","978-1-4799-3516-1","10.1109/ISDA.2013.6920756","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6920756","","Quality of service;Meteorology","belief networks;data mining;learning (artificial intelligence);quality of service;workflow management software","workflow structures;Bayesian learning;provenance data;mining workflow models;workflow mining;data mining method;time constraint;mining approach;provenance information;quality of service;Bayesian structure-learning method;constraint-based Bayesian structure-learning algorithm","","1","","20","","13 Oct 2014","","","IEEE","IEEE Conferences"
"Visual machinery surveillance for high-speed periodic operations","I. Ishii; Y. Wang; T. Takaki","Hiroshima University, 739-8527, Japan; Hiroshima University, 739-8527, Japan; Hiroshima University, 739-8527, Japan","2011 IEEE/RSJ International Conference on Intelligent Robots and Systems","5 Dec 2011","2011","","","1208","1213","Abnormal behavior detection in periodic operations and high-frame-rate (HFR) video logging were realized by introducing a visual machinery surveillance algorithm, which includes a phase-encoding process for periodic operations to match input images with pre-stored reference images efficientl. Assuming that the periodic machinery operations to be monitored are perfectly regular, the surveillance algorithm can encode the phase of a periodic operation just by inspecting temporal changes in the brightnesses at several significan pixels in an input image; abnormal behavior in the periodic operation can be detected by comparing the input image with a reference image synchronized with its encoded phase without the heavy computation needed to search all the reference images. This algorithm was software-implemented on a high-speed vision platform, IDP express, which can record input images of 512 × 512 pixels and process the results at 1000 fps. An experiment was performed using a sewing machine with periodic operation at a frequency of 12 Hz, and a vidio of the abnormal behavior was automatically recorded at 1000 fps to verify the effectiveness of HFR-video-based machinery surveillance.","2153-0866","978-1-61284-456-5","10.1109/IROS.2011.6094407","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6094407","","USA Councils;Intelligent robots;Conferences","factory automation;image matching;machinery;maintenance engineering;video surveillance","visual machinery surveillance;high speed periodic operations;abnormal behavior detection;high frame rate video logging;phase encoding process;image matching;prestored reference images;high speed vision platform;IDP express","","1","","9","","5 Dec 2011","","","IEEE","IEEE Conferences"
"Reducing Waste in Extreme Scale Systems through Introspective Analysis","L. Bautista-Gomez; A. Gainaru; S. Perarnau; D. Tiwari; S. Gupta; C. Engelmann; F. Cappello; M. Snir","Argonne Nat. Lab., Argonne, IL, USA; Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Argonne Nat. Lab., Argonne, IL, USA; Oak Ridge Nat. Lab., Oak Ridge, TN, USA; Oak Ridge Nat. Lab., Oak Ridge, TN, USA; Oak Ridge Nat. Lab., Oak Ridge, TN, USA; Argonne Nat. Lab., Argonne, IL, USA; Argonne Nat. Lab., Argonne, IL, USA","2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","21 Jul 2016","2016","","","212","221","Resilience is an important challenge for extreme-scale supercomputers. Today, failures in supercomputers are assumed to be uniformly distributed in time. However, recent studies show that failures in high-performance computing systems are partially correlated in time, generating periods of higher failure density. Our study of the failure logs of multiple supercomputers show that periods of higher failure density occur with up to three times more than the average. We design a monitoring system that listens to hardware events and forwards important events to the runtime to detect those regime changes. We implement a runtime capable of receiving notifications and adapt dynamically. In addition, we build an analytical model to predict the gains that such dynamic approach could achieve. We demonstrate that in some systems, our approach can reduce the wasted time by over 30%.","1530-2075","978-1-5090-2140-6","10.1109/IPDPS.2016.100","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7516017","Supercomputers;Fault Tolerance;Resilience;Silent Data Corruption;Soft Errors;Introspective Systems","Supercomputers;Runtime;Program processors;Hardware;Standards;Monitoring;Analytical models","data flow analysis;parallel processing;software fault tolerance","waste reduction;extreme scale system;introspective analysis;supercomputer failure;high-performance computing system;HPC system;monitoring system design;runtime implementation","","17","","34","","21 Jul 2016","","","IEEE","IEEE Conferences"
"A Model-Driven Approach to Trace Checking of Pattern-Based Temporal Properties","W. Dou; D. Bianculli; L. Briand","SnT - Univ. of Luxembourg, Luxembourg, Luxembourg; SnT - Univ. of Luxembourg, Luxembourg, Luxembourg; SnT - Univ. of Luxembourg, Luxembourg, Luxembourg","2017 ACM/IEEE 20th International Conference on Model Driven Engineering Languages and Systems (MODELS)","9 Nov 2017","2017","","","323","333","Trace checking is a procedure for evaluating requirements over a log of events produced by a system. This paper deals with the problem of performing trace checking of temporal properties expressed in TemPsy, a pattern-based specification language. The goal of the paper is to present a scalable and practical solution for trace checking, which can be used in contexts where relying on model-driven engineering standards and tools for property checking is a fundamental prerequisite.The main contributions of the paper are: a model-driven trace checking procedure, which relies on the efficient mapping of temporal requirements written in TemPsy into OCL constraints on a conceptual model of execution traces; the implementation of this trace checking procedure in the TEMPSY-CHECK tool; the evaluation of the scalability of TEMPSY-CHECK, applied to the verification of real properties derived from a case study of our industrial partner, including a comparison with a state-of-the-art alternative technology based on temporal logic. The results of the evaluation show the feasibility of applying our model-driven approach for trace checking in realistic settings: TEMPSY-CHECK scales linearly with respect to the length of the input trace and can analyze traces with one million events in about two seconds.","","978-1-5386-3492-9","10.1109/MODELS.2017.9","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8101278","Trace checking;temporal properties;model-driven engineering;OCL","Standards;Tools;Business;Scalability;Monitoring;Model driven engineering;Syntactics","formal specification;formal verification;software tools;specification languages;temporal logic","model-driven engineering standards;property checking;trace checking procedure;TEMPSY-CHECK tool;pattern-based temporal properties;pattern-based specification language;temporal logic","","2","","37","","9 Nov 2017","","","IEEE","IEEE Conferences"
"Autonomous dynamic soaring","M. Boslough","Sandia National Laboratories, PO Box 5800 Albuquerque, NM 87185","2017 IEEE Aerospace Conference","8 Jun 2017","2017","","","1","20","This project makes use of “biomimetic behavioral engineering” in which adaptive strategies used by animals in the real world are applied to the development of autonomous robots. The key elements of the biomimetic approach are to observe and understand a survival behavior exhibited in nature, to create a mathematical model and simulation capability for that behavior, to modify and optimize the behavior for a desired robotics application, and to implement it. The application described in this report is dynamic soaring, a behavior that certain sea birds use to extract flight energy from laminar wind velocity gradients in the shallow atmospheric boundary layer directly above the ocean surface. Theoretical calculations, computational proof-of-principle demonstrations, and the first instrumented experimental flight test data for dynamic soaring are presented to address the feasibility of developing dynamic soaring flight control algorithms to sustain the flight of unmanned airborne vehicles (UAVs). Both hardware and software were developed for this application. Eight-foot custom foam sailplanes were built and flown in a steep shear gradient. A logging device was designed and constructed with custom software to record flight data during dynamic soaring maneuvers. A computational toolkit was developed to simulate dynamic soaring in special cases and with a full 6-degree of freedom flight dynamics model in a generalized time-dependent wind field. Several 3-dimensional visualization tools were built to replay the flight simulations. A realistic aerodynamics model of an eight-foot sailplane was developed using measured aerodynamic derivatives. Genetic programming methods were developed and linked to the simulations and visualization tools. These tools can now be generalized for other biomimetic behavior applications. This work was carried out in 2000 and 2001, and until now its results have only been available in an internal Sandia report.","","978-1-5090-1613-6","10.1109/AERO.2017.7943967","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7943967","","Birds;Aerodynamics;Vehicle dynamics;Wind energy;Atmospheric modeling;Sea surface;Ocean temperature","aerodynamics;aerospace control;aerospace simulation;atmospheric boundary layer;autonomous aerial vehicles;biomimetics;genetic algorithms","autonomous dynamic soaring;biomimetic behavioral engineering;adaptive strategies;autonomous robots. development;biomimetic approach;mathematical model;mathematical simulation;sea birds;flight energy extraction;laminar wind velocity gradients;shallow atmospheric boundary layer;computational proof-of-principle demonstrations;flight test data;dynamic soaring flight control algorithms;unmanned airborne vehicle flight;UAV flight;Eight-foot custom foam sailplanes;steep shear gradient;logging device;computational toolkit;6-degree of freedom flight dynamics model;time-dependent wind field;3-dimensional visualization tools;flight simulation;aerodynamic derivatives;genetic programming methods;internal Sandia report","","","","26","","8 Jun 2017","","","IEEE","IEEE Conferences"
"Revamped Market-Basket Analysis using In-Memory Computation framework","Thanmayee; H. R. M. Prasad","Department of ISE, Vivekananda College of Engineering and Technology, Puttur, Karnataka, India; Department of CSE NMAMIT, Nitte, Karnataka, India","2017 11th International Conference on Intelligent Systems and Control (ISCO)","16 Feb 2017","2017","","","65","70","Data sets are growing day by day as they are being captured by information sensing devices such as mobiles, computers, wireless sensor networks, cameras, software logs, weblogs, remote sensing in various fields such as medical, engineering, science and many more. These large data sets are now called Big Data. Working with Big Data is not a common task. As this large data set has information hidden within them, researchers cannot and they have not ignored the large data set. Data mining is an interdisciplinary field in Computer Science which extracts information or the hidden patterns from data. Association rule mining and frequent itemset mining are popular data mining techniques that requires entire data to be in main memory. But large datasets does not fit into main memory. To handle this drawback, Hadoop MapReduce approach is used which has scalability and robustness features to handle large datasets. Apriori, Eclat and FP Growth are well known Frequent Itemset Mining algorithms. These algorithms are revised to work with Big Data using Hadoop MapReduce. But MapReduce framework has problems such as it stores the intermediate data in local disk. So the data needs to be accessed from the local disk which results in high latency problem. To address this issue Spark follows a general execution model that helps in in-memory computing and optimization of arbitrary operator graphs so that querying data becomes much faster when compared to the disk based engines like MapReduce. Thus the paper focuses on enhancing the performance of Frequent Itemset Mining using Apache Spark architecture and study the performance of this Revamped Market Basket Analysis based on FP-Growth by comparing it with Hadoop MapReduce implementation of Frequent Itemset Mining task, BigFIM and also with different datasets.","","978-1-5090-2717-0","10.1109/ISCO.2017.7855955","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7855955","Frequent Itemset Mining;Hadoop MapReduce;Apache Spark","Itemsets;Data mining;Sparks;Big data;File systems;Scalability","Big Data;data mining;graph theory;marketing data processing","revamped market-basket analysis;in-memory computation;information sensing devices;Big Data;large data set;data mining;association rule mining;frequent itemset mining;Hadoop MapReduce;Apriori;Eclat;FP growth;in-memory computing;optimization;arbitrary operator graphs;Apache Spark architecture;BigFIM","","","","20","","16 Feb 2017","","","IEEE","IEEE Conferences"
"Design flow of wearable heart monitoring and fall detection system using wireless intelligent personal communication node","W. Yi; O. Sarkar; S. Mathavan; J. Saniie","Department of Electrical and Computer Engineering, Illinois Institute of Technology, Chicago, Illinois, United States; Department of Electrical and Computer Engineering, Illinois Institute of Technology, Chicago, Illinois, United States; Department of Electrical and Computer Engineering, Illinois Institute of Technology, Chicago, Illinois, United States; Department of Electrical and Computer Engineering, Illinois Institute of Technology, Chicago, Illinois, United States","2015 IEEE International Conference on Electro/Information Technology (EIT)","8 Oct 2015","2015","","","314","319","Most current remote health monitoring systems provide limited physiological information on the user end. Raw physiological data collected from the on-body sensors are sent to a distant location for data storage and analysis without being analyzed on the user end. In this paper, we present the design flow of a system utilizing the Wireless Intelligent Personal Communication Node (W-iPCN) for analyzing heart activities and detecting sudden fall situations of a remote patient. The W-iPCN is a small, compact and lightweight system which has the ability to process and analyze body sensor data. Furthermore, this system is capable of communicating with other devices through wireless protocols. The purpose of having the W-iPCN for wireless body sensor network system is to provide in-depth biomedical data to the user in real-time. As an example, we emphasize on electrocardiography (ECG), accelerometer and gyroscope data analysis to show the feasibility and capability of the W-iPCN. The ECG sensor data is useful to determine current heart conditions of the user. Accelerometer and gyroscope data are useful to detect sudden collapse events. In this paper, we expand our system design to an Android smartphone to present acquired analyzed sensor data to the user. In addition, the Android smartphone transmits the data to the distant server for data logging and history keeping through the Internet in realtime. Our system targets standard Android smartphone users to be able to install and run our Android application software without requiring expert software knowledge.","2154-0373","978-1-4799-8802-0","10.1109/EIT.2015.7293360","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7293360","Wireless Body Sensor Network;Heart Monitoring;Sudden Collapse;Fall Detection;Android Smartphone","Sensors;Androids;Humanoid robots;Electrocardiography;Accelerometers;Heart;Gyroscopes","accelerometers;bioelectric potentials;biomedical telemetry;body sensor networks;data analysis;data loggers;electrocardiography;gyroscopes;Internet;mechanoception;medical signal processing;smart phones;telemedicine","wearable heart monitoring system;fall detection system;wireless intelligent personal communication node;remote health monitoring systems;physiological information;wireless body sensor network system;electrocardiography data analysis;accelerometer data analysis;gyroscope data analysis;data logging;Internet;Android smartphone users","","7","","15","","8 Oct 2015","","","IEEE","IEEE Conferences"
"A Resilient Framework for Fault Handling in Web Service Oriented Systems","W. Wang; L. Wang; W. Lu","Dept. of Comput. Sci., Univ. of Wyoming, Laramie, WY, USA; Dept. of Comput. Sci., Univ. of Wyoming, Laramie, WY, USA; Sch. of Software Eng., Beijing Jiaotong Univ., Beijing, China","2015 IEEE International Conference on Web Services","17 Aug 2015","2015","","","663","670","Resilience is an important factor in designing web service oriented systems due to frequent failures arising in runtime. These failures derive from the stochastic and uncertainty nature of a composite web service. Service providers need to rapidly address issue when a fault occurs in system running. But it is not easy to locate and fix the faults only using the log generated by the system. In this paper, we propose a resilient framework to automatically generate a fault handling strategy for each failed service to improve the efficiency of fault handling. In the framework, we design and implement three components including exception analyzer, decision maker, and strategy selector. First, The exception analyzer builds a record, derived from the system log generated by an application, for each failed service. Next, the decision maker adopts a k-means clustering approach to construct a decision including the fault handling to each failed service in a scope. Then, the strategy selector uses an integer program solver to generate the solution to strategy selection problem that is boiled down to the optimization problem. The experiment shows that the framework can improve resilience of Web service-oriented systems under acceptable overheads, and meanwhile the accuracy of fault handling strategy is over 95%.","","978-1-4673-7272-5","10.1109/ICWS.2015.93","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7195628","Web service;Resilience;Fault handling","Web services;Resilience;Time factors;Quality of service;Computers;Databases;Runtime","decision making;integer programming;pattern clustering;Web services","resilient framework;fault handling;Web service oriented systems;composite Web service;exception analyzer;decision maker;strategy selector;k-means clustering approach;integer program solver;optimization problem","","5","","20","","17 Aug 2015","","","IEEE","IEEE Conferences"
"Malware task identification: A data driven approach","E. Nunes; C. Buto; P. Shakarian; C. Lebiere; S. Bennati; R. Thomson; H. Jaenisch","School of Computing, Informatics and Decision Systems Engineering, Arizona State University, Tempe, AZ 85281, USA; School of Computing, Informatics and Decision Systems Engineering, Arizona State University, Tempe, AZ 85281, USA; School of Computing, Informatics and Decision Systems Engineering, Arizona State University, Tempe, AZ 85281, USA; Carnegie Mellon University, Pittsburgh, PA 15218; Carnegie Mellon University, Pittsburgh, PA 15218; Carnegie Mellon University, Pittsburgh, PA 15218; Sentar Inc., Huntsville, AL 35805","2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)","11 Feb 2016","2015","","","978","985","Identifying the tasks a given piece of malware was designed to perform (e.g. logging keystrokes, recording video, establishing remote access, etc.) is a difficult and time-consuming operation that is largely human-driven in practice. In this paper, we present an automated method to identify malware tasks. Using two different malware collections, we explore various circumstances for each - including cases where the training data differs significantly from test; where the malware being evaluated employs packing to thwart analytical techniques; and conditions with sparse training data. We find that this approach consistently out-performs the current state-of-the art software for malware task identification as well as standard machine learning approaches - often achieving an unbiased F1 score of over 0.9. In the near future, we look to deploy our approach for use by analysts in an operational cyber-security environment.","","978-1-4503-3854-7","10.1145/2808797.2808894","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7403665","","Malware;Mathematical model;Computational modeling;Analytical models;Training;Social network services;Electronic mail","invasive software;learning (artificial intelligence)","operational cyber-security environment;unbiased F1 score;machine learning approach;sparse training data;thwart analytical techniques;malware collections;data driven approach;malware task identification","","1","1","28","","11 Feb 2016","","","IEEE","IEEE Conferences"
"New novel idea for Cloud Computing: How can we use Kalman filter in security of Cloud Computing","M. Darbandi; P. Shahbazi; S. Setayesh; O. Granmo","Iran University of Science and Technology (IUST); Tehran, Iran; University of Agder (UiA); Grimstad, Norway; International Branch of Ferdowsi University of Mashhad, Iran; University of Agder (UiA); Grimstad, Norway","2012 6th International Conference on Application of Information and Communication Technologies (AICT)","31 Dec 2012","2012","","","1","5","Cloud is a virtual image about some amount of undefined powers, that is widespread and had unknown power and inexact amount of hardware and software configurations, and because of we have not any information about clouds location and time dimensions and also the amounts of its sources we tell that Cloud Computing. This technology presents lots of abilities and opportunities such as processing power, storage and accessing it from everywhere, supporting, working - team group - with the latest versions of software and etc., by the means of internet. On the other hand, in such a large scale networks we should consider the reliability and powerfulness of such networks in facing with events such as high amount of users that may login to their profiles simultaneously, or for example if we have the ability to predict about what times that we would have the most crowd in network, or even users prefer to use which part of the Cloud Computing more than other parts - which software or hardware configuration. With knowing such information, we can avoid accidental crashing or hanging of the network that may be cause by logging of too much users. In this paper we propose Kalman Filter that can be used for estimating the amounts of users and software's that run on cloud computing or other similar platforms at a certain time. After introducing this filter, at the end of paper, we talk about some potentials of this filter in cloud computing platform. In this paper we demonstrate about how we can use Kalman filter in estimating and predicting of our target, by the means of several examples on Kalman filter.","","978-1-4673-1740-5","10.1109/ICAICT.2012.6398466","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6398466","Cloud computing and its influences;Security;Kalman Filter;Estimation and prediction","Cloud computing;Kalman filters;Companies;Educational institutions;Computers;Hardware","cloud computing;Kalman filters;security of data","Kalman filter;cloud computing security;virtual image;hardware configurations;software configurations;Internet;network crashing;network hanging","","1","","15","","31 Dec 2012","","","IEEE","IEEE Conferences"
"A Cost-Effective Audio-Visual Summarizer for Summarization of Presentations and Seminars","A. Bhat; A. C. Rao; A. Bhaskar; V. Adithya; D. Pratiba","B. E. Computer Science and Engineering, RVCE, Bangalore; B. E. Computer Science and Engineering, RVCE, Bangalore; B. E. Computer Science and Engineering, RVCE, Bangalore; B. E. Computer Science and Engineering, RVCE, Bangalore; RVCE, Bangalore, India","2018 3rd International Conference on Computational Systems and Information Technology for Sustainable Solutions (CSITSS)","25 Jul 2019","2018","","","271","276","Nowadays, more than half the world's population live hand-in-hand with technology. From smart watches to smart phones to smart cities, the embodiment of technology in all of our day-to-day activities no longer looks like a distant dream. Lectures in classrooms have also advanced to the extent of using smart-boards and smart-classrooms; the developments in jotting down notes in such scenarios has, however, not advanced at the same pace. In the problem statement being tackled, based on similar lines, the target audience includes the attendees of any seminar, presentation or lecture, be it students, or the public in general, attending important conferences and talks. More often than not, complete undivided attention proves to be difficult at these seminars as the attendee may be preoccupied with the objective of jotting down pointers and making notes for future reference. It is during this process that several essentials in the speaker's delivery are missed out. Keeping this forethought in mind, this paper delves into the implementation of an audio-visual summarizer that achieves the aforementioned motive. With audio evidence on the speaker's delivery, paired with visual images of PowerPoint slides or handwritten material that is presented in the seminars, this device provides a smart solution of summarizing the entire presentation and logging the summary to a remote database server from where it is accessed through a user-end software application. The prototype comprises a Raspberry Pi coupled with a camera and a microphone. The prototype uses a fast RCNN model for text detection, Open Source Computer Vision (OpenCV) for text extraction, Google Speech Recognition and Natural Language Processing concepts for generating the summarized data. The proposed solution is very effective, in terms of feasibility and cost cutting factors. The novelty aspect of the proposed solution lies in the consolidation of ideas of Internet of Things (IOT) and machine learning, to deliver a product capable of providing a smooth and potent solution to the problem statement.","","978-1-5386-6078-2","10.1109/CSITSS.2018.8768740","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8768740","Raspberry Pi;Object detection;Text extraction;Speech-to-text;Natural Language Processing","Training;Object detection;Seminars;Servers;Microphones;Cameras;Google","audio-visual systems;Internet of Things;learning (artificial intelligence);natural language processing;smart phones;speech recognition;text analysis;user interfaces","cost-effective audio-visual summarizer;smart watches;smart phones;smart cities;distant dream;smart-boards;smart-classrooms;audio evidence;visual images;remote database server;user-end software application;summarized data;natural language processing concepts","","","","15","","25 Jul 2019","","","IEEE","IEEE Conferences"
"Efficient Algorithms for Global Snapshots in Large Distributed Systems","R. Garg; V. K. Garg; Y. Sabharwal","IBM T.J. Watson Research Center, Yorktown Heights; University of Texas at Austin, Austin; IBM India Research Laboratory, New Delhi","IEEE Transactions on Parallel and Distributed Systems","29 Mar 2010","2010","21","5","620","630","Existing algorithms for global snapshots in distributed systems are not scalable when the underlying topology is complete. There are primarily two classes of existing algorithms for computing a global snapshot. Algorithms in the first class use control messages of size 0(1) but require O(N) space and O(N) messages per processor in a network with JV processors. Algorithms in the second class use control messages (such as rotating tokens with vector counter method) of size O(N), use multiple control messages per channel, or require recording of message history. As a result, algorithms in both of these classes are not efficient in large systems when the logical topology of the communication layer such as MPI is complete. In this paper, we propose three scalable algorithms for global snapshots: a grid-based, a tree-based, and a centralized algorithm. The grid-based algorithm uses O(N) space but only O(¿(N)) messages per processor each of size O(¿(N)). The tree-based and centralized algorithms use only O(1) size messages. The tree-based algorithm requires O(1) space and O(log N log(W/N)) messages per processor where W is the total number of messages in transit. The centralized algorithm requires O(1) space and O(log(W/N)) messages per processor. We also have a matching lower bound for this problem. We also present hybrid of centralized and tree-based algorithms that allow trade-off between the decentralization and the message complexity. Our algorithms have applications in checkpointing, detecting stable predicates, and implementing synchronizers.","1558-2183","","10.1109/TPDS.2009.108","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5128903","Checkpointing;global snapshots;stable predicates.","Size control;Network topology;Counting circuits;Communication system control;History;Application software;Checkpointing;Distributed computing;Fault tolerant systems;Condition monitoring","checkpointing;communication complexity;fault tolerant computing;message passing","efficient algorithms;global snapshots;large distributed systems;multiple control messages;message history;communication topology;MPI;centralized algorithm;grid-based algorithm;messages per processor;tree-based algorithms;message complexity decentralization","","17","","16","","26 Jun 2009","","","IEEE","IEEE Journals"
"Building Information Modelling and the documentation of architectural heritage: Between the ‘typical’ and the ‘specific’","S. Fai; M. Sydor","Carleton Immersive Media Studio (CIMS), Carleton University, Ottawa, Canada; Carleton Immersive Media Studio (CIMS), Carleton University, Ottawa, Canada","2013 Digital Heritage International Congress (DigitalHeritage)","20 Feb 2014","2013","1","","731","734","One of the greatest challenges to using Building Information Modelling (BIM) for the documentation of architectural heritage is in overcoming the propensity of the software toward standardization. Most BIM applications are optimized for industrialized building systems where even a minor deviation in geometry or dimension between like elements is considered problematic. Heritage buildings, on the other hand, are more typically constructed of unique elements that, while sometimes similar, can never be assumed to be identical. For example, two Corinthian capitals from the Temple of Mars Ultor may be similar, but they are not the same. In this paper, we discuss a novel method for developing a BIM for a unique vernacular building in eastern Ontario, Canada. Constructed anonymously in two discrete stages during the last half of the 19C, the builders employed both stacked log and an idiosyncratic balloon frame construction. Both types of construction are far from the standard assemblies found in commercial BIM software. In discussing the construction of the model, we will outline the integration of detailed survey data, including pointcloud, with a library of `typical', but parametric, construction details under development by our research group. While the survey provides an accurate geometrical record of the building under discussion - including structural deformations - the library is used to develop the specific assemblies and is based on, and fully indexed to, `typical' details culled from construction manuals available in Canada during the late 19C.","","978-1-4799-3170-5","10.1109/DigitalHeritage.2013.6743828","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6743828","architectural heritage;parametric modelling;building information modelling;Canadian Architecture;laser scanning","Buildings;Libraries;Assembly;Documentation;Architecture;Materials;Software","buildings (structures);digital libraries;history;information dissemination;structural engineering computing","Corinthian capitals;structural deformations;Temple of Mars Ultor;heritage buildings;industrialized building systems;architectural heritage documentation;building information modelling","","6","","29","","20 Feb 2014","","","IEEE","IEEE Conferences"
"Real-Time Analysis of a Novel Approach for Localization and Tracking of a Mobile Node","M. H. Siddiqui; M. R. Khalid","Electr. Eng. Dept., Nat. Univ. of Sci. & Technol., Islamabad, Pakistan; Electr. Eng. Dept., Nat. Univ. of Sci. & Technol., Islamabad, Pakistan","2012 8th International Conference on Wireless Communications, Networking and Mobile Computing","14 Mar 2013","2012","","","1","4","This paper investigates the performance of a modified particle filter algorithm for tracking and localization of a mobile node. A modified particle filter algorithm is used to track the mobile node, with the aim to reduce the execution time. Tracking is done through trilateration as well as particle filter algorithm. Wi-Fi signal is used to get initial guess of the position of mobile node in x-y coordinates system. Performance is evaluated interms of execution time and root mean square error (RMSE). Commercially available software ""Wireless Mon"" was used to read the WiFi signal strength from the WiFi card. Visual C++ version 6 was used to interact with this software to read only the required data from the log-file generated by ""Wireless Mon"" software. This work does not rely on only the simulation model to evaluate the performance of the proposed technique. Experimental real-time analysis are carried out to identify practical limitations of the proposed scheme and to ensure a fair evaluation under realistic operating conditions.","2161-9654","978-1-61284-683-5","10.1109/WiCOM.2012.6478636","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6478636","","IEEE 802.11 Standards;Mobile nodes;Particle filters;Wireless sensor networks;Wireless communication;Real-time systems","C++ language;geometry;mean square error methods;mobile computing;particle filtering (numerical methods);visual programming;wireless LAN","mobile node localization;mobile node tracking;modified particle filter algorithm;trilateration;x-y coordinates system;root mean square error;RMSE;wireless mon software;WiFi signal strength;WiFi card;Visual C++ version 6;real-time analysis","","","","14","","14 Mar 2013","","","IEEE","IEEE Conferences"
"NEPTUNE Canada: Data integrity from the seafloor to your (Virtual) Door","R. Jenkyns","NEPTUNE Canada, PO Box 1700 STN CSC, Victoria, BC V8W 2Y2 Canada","OCEANS 2010 MTS/IEEE SEATTLE","10 Dec 2010","2010","","","1","7","In December 2009, the NEPTUNE Canada ocean observatory successfully launched to the public, allowing anyone with an Internet connection access to observatory data. This event also officially marked a transition from infrastructure development to a fully operational observatory. Given that this occurred only a few months after the instrument platforms were deployed at four nodes, this transition was a considerable achievement. Notably, this quick turnaround presented a significant change for principal investigators who normally have much more time, often years, to conduct their science before releasing data to the public. In this paper, commissioning processes, operational challenges, and future plans are described as they relate to observatory data integrity. The data commissioning phase, beginning even before an instrument enters the water, is a team effort that requires NEPTUNE Canada staff, the principal investigator (PI) and sometimes the manufacturer. For each instrument, there are three qualification stages. First and foremost, the systems team checks that instruments can be powered and perform within acceptable limits (e.g., no ground faults). Most instruments pass these requirements. The next level involves a daily review by NEPTUNE Canada staff of data from each instrument. The main objectives are to ensure raw data meets manufacturer specifications, sensor values are appropriately parsed and calibrated, and basic metadata are appropriately recorded in the database. Most issues at this stage can be relatively easily fixed. The third level focuses on the quality of the data itself. Pis are provided both raw data and derived data products since they are expected to be involved in the verification process. Various routines and visualizations plots are used to ensure values make sense. Anomalies and data gaps are noted and investigated. Instrument configurations may require numerous iterations to conquer challenges like interference between acoustic instruments and to better meet scientific objectives. Online tools (e.g., a screen to view and change configuration parameters) and specialized software (e.g., for interaction between instruments) have been created to support these adjustments. Data products are also being continually developed as a response to PI feedback and requests. As a result of this continual examination of the data, about 30 instruments have successfully passed commissioning phase. These instruments include Acoustic Doppler Current Profilers, Bottom Pressure Recorders, CTDs, fluorometers, a gravimeter, video cameras, hydrophones, methane and oxygen sensors. Post-launch, in addition to ongoing instrument commissioning and system maintenance, there were new operational challenges. The first challenge has been meeting the demands for data access. As of May 28, 2010, there have been over 20,000 data requests (1463 within the first two days) from over 1000 registered users via the Data Search tool in Oceans 2.0 (an online collaborative workspace for interacting with data and metadata and remotely controlling instruments). In response to user demand, more data product options continue to be offered. Web services are being developed as an alternative data access method. The general public has responded with an enormous appetite for video data, which has necessitated instructive tutorials for watching video data and explanations as to why cameras cannot be operated 24/7. A second challenge has been communicating important information to the end-user. Feedback is helping us determine how and what information to convey with regards to data interruptions/delays, instrument malfunctions, metadata and quality. Maintenance cruises present a third significant challenge, as there are significant metadata updates to make and new instruments to commission. Metadata must be quickly obtained, verified and entered, particularly for instruments that are already 'live'. A combination of technologies facilitate necessary communications with the crew at sea: live and archived video from ROPOS (including logs) via our website, Twitter, an installation blog, Skype and email. The first successful maintenance cruise occurred in May, and another deployment at Folger Shallow is scheduled for this summer. Going forward, there are further plans to enhance data integrity on the system, such as: developing automated quality control routines to detect anomalous data, exposing more metadata about instruments (history, documentation, annotations, quality flags, etc.), providing a notification system for events affecting data (e.g., instrument configuration changes and power outages), improving help resources and information within Oceans 2.0, increasing staff (two Scientific Data Specialists start in June). At the time of this conference, NEPTUNE Canada will be actively deploying more instruments including Wally II (an improved version of the crawler Wally), a repaired VPS, and Endeavour Ridge instrument platforms. A new wave of commissioning begins.","0197-7385","978-1-4244-4332-1","10.1109/OCEANS.2010.5664290","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5664290","","Instruments;Oceans;Acoustics;Observatories;Interference;Web services;Maintenance engineering","data acquisition;data integrity;geophysics computing;meta data;oceanographic equipment;oceanographic techniques;seafloor phenomena","NEPTUNE Canada;data integrity;commissioning processes;operational challenges;principal investigator;sensor values;metadata;data products;Acoustic Doppler Current Profilers;Bottom Pressure Recorders;video cameras;hydrophones;oxygen sensors;automated quality control;Endeavour Ridge instrument platforms;seafloor data","","4","","5","","10 Dec 2010","","","IEEE","IEEE Conferences"
"Identifying and Generating Missing Tests using Machine Learning on Execution Traces","M. Utting; B. Legeard; F. Dadeau; F. Tamagnan; F. Bouquet","USC Business School University of the Sunshine Coast; University of Bourgogne Franche-Comté,Dept. DISC - FEMTO-ST; University of Bourgogne Franche-Comté,Dept. DISC - FEMTO-ST; Orange Labs Services,Pessac,France; University of Bourgogne Franche-Comté,Dept. DISC - FEMTO-ST","2020 IEEE International Conference On Artificial Intelligence Testing (AITest)","25 Aug 2020","2020","","","83","90","Testing IT systems has become a major bottleneck for many companies. Besides the growing complexity of such systems, shorter release cycles and increasing quality requirements have led to increased verification and validation costs. However, analysis of existing testing procedures reveals that not all artifacts are exploited to tame this cost increase. In particular, customer traces are usually ignored by validation engineers. In this paper, we use machine learning from execution traces (both customer traces and test execution traces) to identify test needs and to generate new tests in the context of web services and API testing. Log files of customer traces are split into smaller traces (user sessions) then encoded into Pandas DataFrames for data analysis and machine learning. Clustering algorithms are used to analyse the customer traces and compare them with existing system tests, and machine learning models are used to generate missing tests in the desired clusters. The tool-set is implemented in an open-source library called Agilkia.","","978-1-7281-6984-2","10.1109/AITEST49225.2020.00020","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9176745","automated regression testing;machine learning;customer traces;clustering;test generation","Machine learning;Test pattern generators;Web services;Clustering algorithms","application program interfaces;data analysis;learning (artificial intelligence);pattern clustering;program testing;program verification;public domain software;Web services","customer traces;system tests;machine learning;missing tests;execution traces;quality requirements;API testing;clustering algorithm;Pandas DataFrames;data analysis;open-source library;Agilkia;Web services","","","","25","","25 Aug 2020","","","IEEE","IEEE Conferences"
"Efficient fault-tolerance for iterative graph processing on distributed dataflow systems","C. Xu; M. Holzemer; M. Kaul; V. Markl","Technische Universität Berlin, Germany; Technische Universität Berlin, Germany; IIT Hyderabad, Germany; Technische Universität Berlin, Germany","2016 IEEE 32nd International Conference on Data Engineering (ICDE)","23 Jun 2016","2016","","","613","624","Real-world graph processing applications often require combining the graph data with tabular data. Moreover, graph processing usually is part of a larger analytics workflow consiting of data preparation, analysis and model building, and model application. General-purpose distributed dataflow frameworks execute all steps of such workflows holistically. This holistic view enables these systems to reason about and automatically optimize the processing. Most big graph processing algorithms are iterative and incur a long runtime, as they require multiple passes over the data until convergence. Thus, fault tolerance and quick recovery from any intermittent failure at any step of the workflow are crucial for effective and efficient analysis. In this work, we propose a novel fault-tolerance mechanism for iterative graph processing on distributed data-flow systems with the objective to reduce the checkpointing cost and failure recovery time. Rather than writing checkpoints that block downstream operators, our mechanism writes checkpoints in an unblocking manner, without breaking pipelined tasks. In contrast to the typical unblocking checkpointing approaches (i.e., managing checkpoints independently for immutable datasets), we inject the checkpoints of mutable datasets into the iterative dataflow itself. Hence, our mechanism is iteration-aware by design. This simplifies the system architecture and facilitates coordinating the checkpoint creation during iterative graph processing. We achieve speedier recovery, i.e., confined recovery, by using the local log files on each node to avoid a complete re-computation from scratch. Our theoretical studies as well as our experimental analysis on Flink give further insight into our fault-tolerance strategies and show that they are more efficient than blocking checkpointing and complete recovery for iterative graph processing on dataflow systems.","","978-1-5090-2020-1","10.1109/ICDE.2016.7498275","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7498275","","Checkpointing;Writing;Data models;Sparks;Fault tolerance;Fault tolerant systems;Analytical models","checkpointing;data analysis;data flow analysis;distributed processing;fault tolerant computing;graph theory;iterative methods;workflow management software","fault-tolerance;iterative graph processing;tabular data;analytics workflow;data preparation;data analysis;model building;general-purpose distributed dataflow frameworks;big graph processing algorithms;quick recovery;checkpointing cost;failure recovery time","","7","","28","","23 Jun 2016","","","IEEE","IEEE Conferences"
"Automated Nonintrusive Analysis of Electronic System Level Designs","M. Goli; J. Stoppe; R. Drechsler","Institute of Computer Science, German Research Center for Artificial Intelligence (DFKI GmbH), Bremen, Germany; Institute of Computer Science, German Research Center for Artificial Intelligence (DFKI GmbH), Bremen, Germany; Institute of Computer Science, German Research Center for Artificial Intelligence (DFKI GmbH), Bremen, Germany","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","20 Jan 2020","2020","39","2","492","505","Due to the ever increasing complexity of hardware systems, designers strive for higher levels of abstractions in the early stages of the design process. Modeling hardware at the electronic system level (ESL) is one way to address this demand, with the C++-based system modeling framework SystemC and its abstract communication library transaction level modeling (TLM) having become de-facto standards for ESL system design. While the C++ compiler is sufficient to compile and simulate a given ESL design, for tasks of design understanding, debugging, or validation (where access to the details of design's structure and behavior is necessarily required), design needs to be processed by an appropriate tool. This problem is often solved by adding instrumentation code to either the design or the library, usually resulting in incomplete logs, work overhead and/or incompatibilities. This paper introduces an approach that automatically extracts information about both, structure and behavior of SystemC designs and TLM transactions, nonintrusively. The information is retrieved from a given design by running it in debug mode while being connected to a preprogrammed debugger, thus leaving the existing sources and workflows untouched while collecting a vast amount of data without user intervention. Illustrating use cases, value change dump files of the SystemC models' behavior and unified modeling language activity diagrams of transaction protocols are created automatically from simulation runs.","1937-4151","","10.1109/TCAD.2018.2889665","German Federal Ministry of Education and Research (BMBF) within the Project SecRec; German Research Foundation within the Subproject P01 “Predictive function” of the Collaborative Research Center; University of Bremen’s Graduate School SyDe through the German Excellence Initiative; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8588352","Data extraction;GNU debugger (GDB);nonintrusive;SystemC;transaction level modeling (TLM);transaction","Unified modeling language;Data mining;Time-varying systems;Time-domain analysis;Integrated circuit modeling;Libraries;C++ languages","C++ language;electronic design automation;hardware-software codesign;program compilers;program debugging;Unified Modeling Language","automated nonintrusive analysis;electronic system level designs;system modeling framework SystemC;abstract communication library transaction level modeling;ESL system design;SystemC designs;TLM transactions;unified modeling language activity diagrams;C++ compiler;preprogrammed debugger","","5","","41","IEEE","25 Dec 2018","","","IEEE","IEEE Journals"
"Precise and Scalable Querying of Syntactical Source Code Patterns Using Sample Code Snippets and a Database","O. Panchenko; J. Karstens; H. Plattner; A. Zeier","Hasso Plattner Inst., Potsdam, Germany; SAP AG, Walldorf, Germany; Hasso Plattner Inst., Potsdam, Germany; Hasso Plattner Inst., Potsdam, Germany","2011 IEEE 19th International Conference on Program Comprehension","1 Aug 2011","2011","","","41","50","While analyzing a log file of a text-based source code search engine we discovered that developers search for fine-grained syntactical patterns in 36% of queries. Currently, to cope with queries of this kind developers need to use regular expressions, to add redundant terms to the query or to combine searching with other tools provided by the development environment. To improve the expressiveness of the queries, these can be formulated as tree patterns of abstract syntax trees. These search patterns can be expressed by using query languages, such as XPath. However, developers usually do not work with either XPath or with AST. To shield developers from the complexity of query formulation we propose using sample code snippets as queries. The novelty of our approach is the combination of a query language that is very close to the surface programming language and a special database technology to store a large amount of abstract syntax trees. The advantage of this approach over existing source code query languages and search engines is the performance of both query formulation and query execution. This paper describes the technical details of the method and illustrates the value of this approach with performance measures and an industrial controlled experiment. All developers were able to complete the tasks of the experiment faster and more accurately by using our tool (ACS) than by using a text-based search engine. The number of false positives in the result lists was significantly decreased.","1092-8138","978-0-7695-4398-7","10.1109/ICPC.2011.31","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5970162","source code search;query-by-example;source code query language;abstract syntax trees;XPath;database","Databases;Database languages;Search engines;XML;Data models;Syntactics","computational linguistics;database management systems;query formulation;query languages;query processing;search engines;software tools;source coding","syntactical source code pattern querying;sample code snippets;abstract syntax tree;search pattern;query formulation;surface programming language;database technology;source code query language;source code search engine;query execution;ACS tool","","5","","27","","1 Aug 2011","","","IEEE","IEEE Conferences"
"Visualizing BFT SMR Distributed Systems - Example of BFT-SMaRt","N. Rakotondravony; H. P. Reiser","Univ. of Passau, Passau, Germany; Univ. of Passau, Passau, Germany","2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W)","23 Jul 2018","2018","","","152","157","Byzantine Fault Tolerant (BFT) State Machine Replication (SMR) distributed systems are very complex systems whose optimization traditionally relies on the analysis of log files or the output of debugging processes. In this work in progress, we use visualization techniques to support human users in the task of analyzing, understand, and optimizing a BFT SMR distributed system in both online and offline modes. The visualization of the execution of BFT SMR distributed systems features with the possibility to correlate information from different levels of abstraction. Our design methodologies also allow us to address the different challenges accompanying the visualization of distributed systems in general. We illustrate our study with an example usage scenario in which the execution of a BFT-SMaRt-based distributed system is visualized.","2325-6664","978-1-5386-6553-4","10.1109/DSN-W.2018.00055","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8416240","distributed systems, visualization, BFT-SMaRt","Data visualization;Servers;Libraries;Task analysis;Data acquisition;Protocols;Monitoring","data visualisation;fault tolerance;finite state machines;optimisation;software fault tolerance","design methodologies;correlate information;offline modes;online modes;optimization;state machine replication;Byzantine fault tolerant;BFT SMR distributed systems;complex systems;BFT-SMaRt-based distributed system;visualization techniques","","","","16","","23 Jul 2018","","","IEEE","IEEE Conferences"
"M3-Driven smart space creation using a DD-WRT-Based device","S. Mikhailov; A. Kashevnik","ITMO University, Saint Petersburg, Russian Federation; ITMO University, Saint Petersburg, Russian Federation","2017 20th Conference of Open Innovations Association (FRUCT)","19 Oct 2017","2017","","","275","283","The paper describes the process of smart space creation based on integration of Smart-M3 platform with a DD-WRT-based device. Smart-M3 is an open source platform which implements concept of smart space. Wi-Fi router is used as platform hosting which reduces the number of devices participating in smart space-based scenarios. The article covers a process of compilation and installation of Smart-M3 platform on DD-WRT-based Wi-Fi router. Evaluation shows that smart space organized this way can be used for scenarios with few participants. The authors developed “Smart-M3 Control Panel” web-service which allows users to control Smart-M3 platform by a graphical web interface. “Smart-M3 Control Panel” user can view the current status of platform; launch, stop, and reload it; view information storage content and change it; download log files; and change startup options. SocketIO interface was used for the user interaction with web service.","2305-7254","978-9-5268-6530-0","10.23919/FRUCT.2017.8071323","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8071323","","Universal Serial Bus;Operating systems;Semantics;Program processors;Servers;Wireless fidelity","authorisation;Internet;public domain software;Web services;wireless LAN","smart space creation;DD-WRT-based device;Smart-M3 platform;open source platform;Wi-Fi router;smart space-based scenarios;Smart-M3 Control Panel web-service;Smart-M3 Control Panel user","","","","25","","19 Oct 2017","","","IEEE","IEEE Conferences"
"On-Board Short-Circuit Detection of Li-ion Batteries Undergoing Fixed Charging Profile as in Smartphone Applications","A. Naha; A. Khandelwal; K. S. Hariharan; A. Kaushik; A. Yadu; S. M. Kolake","Samsung R&D Institute India-Bangalore, Bangalore, India; Samsung R&D Institute India-Bangalore, Bangalore, India; Samsung R&D Institute India-Bangalore, Bangalore, India; Samsung R&D Institute India-Bangalore, Bangalore, India; Samsung R&D Institute India-Bangalore, Bangalore, India; Samsung R&D Institute India-Bangalore, Bangalore, India","IEEE Transactions on Industrial Electronics","1 Jul 2019","2019","66","11","8782","8791","In this paper, an effective and robust algorithm is developed for on-board detection of battery anomaly caused by short circuit (SC). The proposed algorithm uses the battery-terminal voltage and current information measured using battery-management system during standard uses. The proposed method is purely a noninvasive, software-based solution and does not need any additional hardware. It extracts a set of designed features from the recorded data and stores their statistics as normal operating behavior for the initial five charge-discharge cycles. After the initial characterization of the battery, the likelihood of the features of being normal are evaluated for the subsequent cycles. If a feature is likely to be healthy, then the statistics for that feature get updated with the current value. However, if the feature is not likely to be healthy, then the battery is considered to be faulty with respect to that particular feature. If more than 50% of the features identify the battery as faulty, then the overall battery status is classified as faulty. A mobile application is developed to log the smartphone's battery data during charging and discharging for testing. The faulty data are generated by connecting SC resistances of various values to the battery terminals, which emulates the SC condition in the battery. The data logged in the smartphones are transferred to a PC for analysis using the proposed algorithm. 100% detection accuracy is achieved for SC resistance values less than 200 Ω.","1557-9948","","10.1109/TIE.2018.2889623","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8600739","Anomaly detection;battery short circuit (SC);Li-ion battery;mobile phone;on-board fault diagnosis;rechargeable battery","Circuit faults;Smart phones;Feature extraction;Lithium-ion batteries;Voltage measurement;Temperature measurement","battery management systems;lithium compounds;secondary cells;short-circuit currents;smart phones","fixed charging profile;robust algorithm;battery-terminal voltage;battery-management system;software-based solution;charge-discharge cycles;Li-ion batteries;on-board short-circuit detection;battery anomaly;smartphone battery data;Li","","8","","46","","3 Jan 2019","","","IEEE","IEEE Journals"
"High Assurance Smart Metering","S. Cleemput; M. A. Mustafa; B. Preneel","Dept. Electr. Eng., KU Leuven, Leuven, Belgium; Dept. Electr. Eng., KU Leuven, Leuven, Belgium; Dept. Electr. Eng., KU Leuven, Leuven, Belgium","2016 IEEE 17th International Symposium on High Assurance Systems Engineering (HASE)","3 Mar 2016","2016","","","294","297","This paper describes a high assurance architecture for smart metering. Hacking the smart metering infrastructure can have an enormous physical impact, therefore, it is essential that the components in this architecture are proven to be secure. In order for components to be verifiable, however, they need to be sufficiently simple. In this paper, we map the functionalities and different software modules of a smart meter to a minimal number of physical components in order to obtain a cost-effective and secure smart meter. The resulting smart meter contains seven physical components: a clock, a metrology component, a display, an off-switch, memory and two processors. It contains six main software modules: a communications module, a computations module, a credit balance module and three separate security modules, one of which is implemented on the second processor. Finally, there are six strongly separated memory segments: three for log files, one for the tariffs, one for the credit balance and one containing the operational parameters.","1530-2059","978-1-4673-9913-5","10.1109/HASE.2016.41","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7423169","","Smart meters;Switches;Security;Computer architecture;Logic gates;Power grids;Program processors","computer crime;power system security;smart meters;tariffs","high assurance smart metering;high assurance architecture;software modules;physical components;secure smart meter;metrology component;clocks;communications module;computations module;credit balance module;security modules;tariffs","","4","","10","","3 Mar 2016","","","IEEE","IEEE Conferences"
"Characterizing the Topology of Probabilistic Biological Networks","A. Todor; A. Dobra; T. Kahveci","University of Florida, Gainesville; University of Florida, Gainesville; University of Florida, Gainesville","IEEE/ACM Transactions on Computational Biology and Bioinformatics","9 Dec 2013","2013","10","4","970","983","Biological interactions are often uncertain events, that may or may not take place with some probability. This uncertainty leads to a massive number of alternative interaction topologies for each such network. The existing studies analyze the degree distribution of biological networks by assuming that all the given interactions take place under all circumstances. This strong and often incorrect assumption can lead to misleading results. In this paper, we address this problem and develop a sound mathematical basis to characterize networks in the presence of uncertain interactions. Using our mathematical representation, we develop a method that can accurately describe the degree distribution of such networks. We also take one more step and extend our method to accurately compute the joint-degree distributions of node pairs connected by edges. The number of possible network topologies grows exponentially with the number of uncertain interactions. However, the mathematical model we develop allows us to compute these degree distributions in polynomial time in the number of interactions. Our method works quickly even for entire protein-protein interaction (PPI) networks. It also helps us find an adequate mathematical model using MLE. We perform a comparative study of node-degree and joint-degree distributions in two types of biological networks: the classical deterministic networks and the more flexible probabilistic networks. Our results confirm that power-law and log-normal models best describe degree distributions for both probabilistic and deterministic networks. Moreover, the inverse correlation of degrees of neighboring nodes shows that, in probabilistic networks, nodes with large number of interactions prefer to interact with those with small number of interactions more frequently than expected. We also show that probabilistic networks are more robust for node-degree distribution computation than the deterministic ones. Availability: all the data sets used, the software implemented and the alignments found in this paper are available at >http://bioinformatics.cise.ufl.edu/projects/probNet/.","1557-9964","","10.1109/TCBB.2013.108","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6594743","Probabilistic biological networks;network topology;degree distribution;random graphs","Probabilistic logic;Random variables;Maximum likelihood estimation;Joints;Network topology;Mathematical model","biochemistry;molecular biophysics;polynomials;probability;proteins;topology","probabilistic biological network topology;biological interactions;alternative interaction topologies;joint-degree distributions;node pairs;mathematical model;polynomial time;protein-protein interaction networks;PPI networks;biological networks;classical deterministic networks;flexible probabilistic networks;power-law models;log-normal models;deterministic networks;node-degree distribution computation","Animals;Computational Biology;Gene Regulatory Networks;Metabolic Networks and Pathways;Models, Biological;Models, Statistical;Protein Interaction Maps;Signal Transduction","6","","47","","9 Sep 2013","","","IEEE","IEEE Journals"
"RAMP: Real-Time Anomaly Detection in Scientific Workflows","J. Dinal Herath; C. Bai; G. Yan; P. Yang; S. Lu","State University of New York at Binghamton,Binghamton,NY,USA; Wayne State University,Detroit,MI,USA; State University of New York at Binghamton,Binghamton,NY,USA; State University of New York at Binghamton,Binghamton,NY,USA; Wayne State University,Detroit,MI,USA","2019 IEEE International Conference on Big Data (Big Data)","24 Feb 2020","2019","","","1367","1374","Research integrity is crucial to ensuring the trustworthiness of scientific discoveries. This work is aimed at detecting misbehaviors targeting scientific workflows, which are computing paradigms widely used to facilitate scientific collaborations across multiple geographically distributed research sites. We develop a new system called RAMP(Real-Time Aggregated Matrix Profile) for real-time anomaly detection in scientific workflow systems. RAMP builds upon an existing time series data analysis technique called Matrix Profile to detect anomalous distances among subsequences of event streams collected from scientific workflows in an online manner. Using an adaptive uncertainty function, the anomaly detection model is dynamically adjusted to prevent high false alarm rates. RAMP can incorporate user feedback on reported anomalies and modify model parameters to improve anomaly detection accuracy. Our experimental results from applying RAMP to the logs generated by DATAVIEW, a scientific workflow platform, show that RAMP is able to identify a varied range of anomalies with high accuracy for both interleaved and non-interleaved workflow executions in real time.","","978-1-7281-0858-2","10.1109/BigData47090.2019.9005653","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9005653","","Task analysis;Anomaly detection;Real-time systems;Time series analysis;Adaptation models;Training;Uncertainty","data analysis;data mining;natural sciences computing;security of data;time series;workflow management software","scientific collaborations;multiple geographically distributed research sites;RAMP;Matrix Profile;real-time anomaly detection;scientific workflow systems;anomaly detection model;anomaly detection accuracy;scientific workflow platform;noninterleaved workflow executions;research integrity;scientific discoveries;time series data analysis technique","","","","29","","24 Feb 2020","","","IEEE","IEEE Conferences"
"Collaborative navigation with ground vehicles and personal navigators","A. Kealy; N. Alam; C. Toth; T. Moore; V. Gikas; C. Danezis; G. W. Roberts; G. Retscher; A. Hasnur-Rabiain; D. A. Grejner-Brzezinska; C. Hill; C. Hide; L. Bonenberg","Department of Infrastructure Engineering, University of Melbourne, Australia; School of Surveying and Geospatial Engineering, University of New South Wales, Sydney, Australia; Center for Mapping, The Ohio State University, Columbus, USA; Nottingham Geospatial Institute, Faculty of Engineering, University of Nottingham, UK; School of Rural and Surveying Engineering, National Technical University of Athens, Greece; School of Rural and Surveying Engineering, National Technical University of Athens, Greece; Faculty of Science and Engineering, University of Nottingham Ningbo, China; Department of Geodesy and Geoinformation, Vienna University of Technology, Austria; Department of Infrastructure Engineering, University of Melbourne, Australia; Department of Civil and Environmental Engineering and Geodetic Science, The Ohio State University, Columbus, USA; Nottingham Geospatial Institute, Faculty of Engineering, University of Nottingham, UK; Nottingham Geospatial Institute, Faculty of Engineering, University of Nottingham, UK; Nottingham Geospatial Institute, Faculty of Engineering, University of Nottingham, UK","2012 International Conference on Indoor Positioning and Indoor Navigation (IPIN)","24 Jan 2013","2012","","","1","8","An integrated positioning solution termed `collaborative positioning' employs multiple location sensors with different accuracy on different platforms for sharing of their absolute and relative localizations. Typical application scenarios are dismounted soldiers, swarms of UAV's, team of robots, emergency crews and first responders. The stakeholders of the solution (i.e., mobile sensors, users, fixed stations and external databases) are involved in an iterative algorithm to estimate or improve the accuracy of each node's position based on statistical models. This paper studies the challenges to realize a public and low-cost solution, based on mass users of multiple-sensor platforms. For the investigation field experiments revolved around the concept of collaborative navigation, and partially indoor navigation. For this purpose different sensor platforms have been fitted with similar type of sensors, such as geodetic and low-cost high-sensitivity GNSS receivers, tactical grade IMU's, MEMS-based IMU's, miscellaneous sensors, including magnetometers, barometric pressure and step sensors, as well as image sensors, such as digital cameras and Flash LiDAR, and ultra-wide band (UWB) receivers. The employed platforms in the tests include a train on a building roof, mobile mapping vans, a personal navigator and a foot tracker unit. In terms of the tests, the data from the different platforms are recorded simultaneously. Several field experiments conducted in a week at the University of Nottingham are described and investigated in the paper. The personal navigator and a foot tracker unit moved on the building roof, then trough the building down to where it logged data simultaneously with the vans, all of them moving together and relative to each other. The platforms then logged data simultaneously covering various accelerations, dynamics, etc. over longer trajectories. Promising preliminary results of the field experiments showed that a positioning accuracy on the few meter level can be achieved for the navigation of the different platforms.","","978-1-4673-1954-6","10.1109/IPIN.2012.6418893","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6418893","collaborative navigation;ubiquitous positioning;seamless indoor/outdoor positioning;GNSS;INS;MEMS-based sensors;UWB","Universal Serial Bus;Software;Global Positioning System;Geospatial analysis;Position measurement;Foot","atmospheric pressure;image sensors;indoor radio;magnetometers;radio receivers;satellite navigation;ultra wideband communication","collaborative navigation;ground vehicle;personal navigator;integrated positioning solution;collaborative positioning;location sensor;absolute localization sharing;relative localization sharing;dismounted soldier;UAV;robot;iterative algorithm;position estimation;statistical model;multiple-sensor platform;partial indoor navigation;geodetic sensor;GNSS receiver;tactical grade IMU;MEMS-based IMU;magnetometer;barometric pressure sensor;step sensor;image sensor;digital camera;flash LiDAR;ultra-wide band receiver;UWB receiver;train;building roof;mobile mapping vans;foot tracker unit","","13","","23","","24 Jan 2013","","","IEEE","IEEE Conferences"
"An analysis model of botnet tracking based on ant colony optimization algorithm","Ping Wang; Tzu Chia Wang; Pu-Tsun Kuo; Chin Pin Wang","Department of Information Management, Kun Shan University, Taiwan; Department of Information Management, Kun Shan University, Taiwan; Department of Information Management, Kun Shan University, Taiwan; Department of Information Management, Kun Shan University, Taiwan","The 6th International Conference on Networked Computing and Advanced Information Management","16 Sep 2010","2010","","","606","611","Available botnet detection schemes all supposed that ISPs would be cooperative to record or generate the necessary routing information for path reconstruction. In practice, ISP's service constantly is a mutual benefit for intelligence exchange. Therefore the constraint, require cooperation between ISPs, ought to be relaxed. A new IP traceback scheme based on ant colony optimization (ACO) algorithm is proposed for incomplete routing logs are provided. The aim of our work is to develop an analysis model for reconstruction of attack paths to traceback the botnet C&C via ant-inspired collective intelligence by calculating the pheromone to find possible routes with support and confidence degree. The validation of model uses NS2 (Network Simulator, version2) complied by dark IP map, to simulate the scenario of fake IP attack, to test the effectiveness of model. Furthermore, sensitivity analysis is conducted to investigate significant parameters' effect on the output of attack paths. Experimental results show that the proposed approach effectively suggests the best attack path of botnet in a dynamic network environment.","","978-89-88678-26-8","","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5573232","Botnet;attack path;ant colony optimization","Nickel;Facsimile","optimisation;software agents","ant colony optimization algorithm;botnet detection;routing information;ISP service;sensitivity analysis","","1","","6","","16 Sep 2010","","","IEEE","IEEE Conferences"
"Grey Fault Detection Method Based on Application Interference Model in Cloud Storage","B. Liang; N. Chen; Y. Xie; Y. Chen",Guangxi University; Guangxi University; Guangxi University; Guangxi University,"2019 IEEE International Conference on Smart Internet of Things (SmartIoT)","14 Nov 2019","2019","","","36","43","The existing failure detection technology in cloud storage system mainly used to identify abnormal states of nodes by analyzing log records and the usage trend of CPU, memory, disk space and other physical resources of nodes. Despite great efforts made by researchers in the field of failure detection, there are still the gray faults have not been detected, such as the effect of memory jitter, and the system does not consider it an abnormal fault. We found that the shortcoming of current fault detection techniques was that they do not consider the performance impact between different applications in the same node. The performance interference between different applications is due to the limitation of virtualization technology for physical resource isolation. Therefore, we propose a gray failure detection method based on application scenario modeling. This method automatically analyze the performance interference between applications, and establish the relationship model between application performance interference and gray fault for the application scenarios. And then it uses the relational model to perceive environment changes of performance interference, so as to detect the node's failure location by itself. The accuracy and timeliness of the proposed method is verified in the data collected in the docker-based virtual storage cluster environment, as well as in the Google cluster data. And the method can detect a gray fault in 6.4 seconds, and have high precision.","","978-1-7281-3488-8","10.1109/SmartIoT.2019.00015","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8896696","Failure detection;Situational awareness;Grey fault;Application performance","","cloud computing;software fault tolerance;storage management;system recovery;virtual machines;virtual storage;virtualisation","relational model;failure location;docker-based virtual storage cluster environment;grey fault detection method;application interference model;cloud storage system;disk space;memory jitter;fault detection techniques;virtualization technology;physical resource isolation;gray failure detection method;application scenario modeling;relationship model;application performance interference;failure detection technology;Google cluster data","","","","22","","14 Nov 2019","","","IEEE","IEEE Conferences"
"Dynalog: an automated dynamic analysis framework for characterizing android applications","M. K. Alzaylaee; S. Y. Yerima; S. Sezer","Centre for Secure Inf. Technol., Queen's Univ. Belfast, Belfast, UK; Centre for Secure Inf. Technol., Queen's Univ. Belfast, Belfast, UK; Centre for Secure Inf. Technol., Queen's Univ. Belfast, Belfast, UK","2016 International Conference On Cyber Security And Protection Of Digital Services (Cyber Security)","9 Jul 2016","2016","","","1","8","Android is becoming ubiquitous and currently has the largest share of the mobile OS market with billions of application downloads from the official app market. It has also become the platform most targeted by mobile malware that are becoming more sophisticated to evade state-of-the-art detection approaches. Many Android malware families employ obfuscation techniques in order to avoid detection and this may defeat static analysis based approaches. Dynamic analysis on the other hand may be used to overcome this limitation. Hence in this paper we propose DynaLog, a dynamic analysis based framework for characterizing Android applications. The framework provides the capability to analyse the behaviour of applications based on an extensive number of dynamic features. It provides an automated platform for mass analysis and characterization of apps that is useful for quickly identifying and isolating malicious applications. The DynaLog framework leverages existing open source tools to extract and log high level behaviours, API calls, and critical events that can be used to explore the characteristics of an application, thus providing an extensible dynamic analysis platform for detecting Android malware. DynaLog is evaluated using real malware samples and clean applications demonstrating its capabilities for effective analysis and detection of malicious applications.","","978-1-5090-0709-7","10.1109/CyberSecPODS.2016.7502337","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7502337","","Malware;Androids;Humanoid robots;Feature extraction;Smart phones;Instruments;Mobile communication","Android (operating system);invasive software;mobile computing;system monitoring","DynaLog;dynamic analysis framework;Android malware;mobile OS;Android operating system","","1","","37","","9 Jul 2016","","","IEEE","IEEE Conferences"
"Targeted Attack Prevention at Early Stage","C. Chen; P. Yang; Y. Ou; H. Hsiao","Dept. of Inf. Manage., Nat. Sun Yet-sen Univ., Kaohsiung, Taiwan; Dept. of Inf. Manage., Nat. Sun Yet-sen Univ., Kaohsiung, Taiwan; Dept. of Inf. Manage., Nat. Sun Yet-sen Univ., Kaohsiung, Taiwan; Dept. of Inf. Manage., Nat. Univ. of Kaohsiung, Kaohsiung, Taiwan","2014 28th International Conference on Advanced Information Networking and Applications Workshops","26 Jun 2014","2014","","","866","870","Targeted cyber attacks play a critical role in disrupting network infrastructure and information privacy. Based on the incident investigation, Intelligence gathering is the first phase of such attacks. To evade detection, hacker may make use of botnet, a set of zombie machines, to gain the access of a target and the zombies send the collected results back to the hacker. Even though the zombies would be blocked by detection system, the hacker, using the access information obtained from the botnet, would login the target from another machine without being noticed by the detection system. Such information gathering tactic can evade detection and the hacker grants the initial access to the target. The proposed defense system analyzes multiple logs from the network and extracts the reconnaissance attack sequences related to targeted attacks. State-based model is adopted to model the steps of the above early phase attack performed by multiple scouts and an intruder and such attack events in a long time frame becomes significant in the state-aware model. The results show that the proposed system can identify the attacks at the early stage efficiently to prevent further damage in the networks.","","978-1-4799-2653-4","10.1109/WAINA.2014.134","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6844748","pervasive computing;targeted attacks;intrusion detection","Hidden Markov models;Joints;Servers;Computer hacking;Reconnaissance;IP networks","authorisation;data privacy;invasive software;ubiquitous computing","targeted attack prevention;cyber attack;network infrastructure;information privacy;intelligence gathering;botnet;state-based model","","2","","18","","26 Jun 2014","","","IEEE","IEEE Conferences"
"Honware: A Virtual Honeypot Framework for Capturing CPE and IoT Zero Days","A. Vetterl; R. Clayton","Computer Laboratory, University of Cambridge,Cambridge,UK; Computer Laboratory, University of Cambridge,Cambridge,UK","2019 APWG Symposium on Electronic Crime Research (eCrime)","19 Mar 2020","2019","","","1","13","Existing solutions are ineffective in detecting zero day exploits targeting Customer Premise Equipment (CPE) and Internet of Things (IoT) devices. We present honware, a high-interaction honeypot framework which can emulate a wide range of devices without any access to the manufacturers' hardware. Honware automatically processes a standard firmware image (as is commonly provided for updates), customises the filesystem and runs the system with a special pre-built Linux kernel. It then logs attacker traffic and records which of their actions led to a compromise. We provide an extensive evaluation and show that our framework improves upon existing emulation strategies which are limited in their scalability, and that it is significantly better both in providing network functionality and in emulating the devices' firmware applications - a crucial aspect as vulnerabilities are frequently exploited by attackers in `front-end' functionalities such as web interfaces. Honware's design precludes most honeypot fingerprinting attacks, and as its performance is comparable to that of real devices, fingerprinting with timing attacks can be made far from trivial. We provide four case studies in which we demonstrate that honware is capable of rapid deployment to capture the exact details of attacks along with malware samples. In particular we identified a previously unknown attack in which the default DNS for an ipTIME N604R wireless router was changed. We believe that honware is a major contribution towards re-balancing the economics of attackers and defenders by reducing the period in which attackers can exploit zero days at Internet scale.","2159-1245","978-1-7281-6383-3","10.1109/eCrime47957.2019.9037501","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9037501","","","computer network security;firmware;Internet;invasive software;Linux;operating system kernels;system monitoring","high-interaction honeypot framework;standard firmware image;attacker traffic;emulation strategies;honeypot fingerprinting attacks;timing attacks;unknown attack;virtual honeypot framework;IoT zero days;customer premise equipment;Internet of Things devices;pre-built Linux kernel;honware design;ipTIME N604R wireless router;Internet scale","","2","","47","","19 Mar 2020","","","IEEE","IEEE Conferences"
"Improving big data on research and education networks using future internet approach: A case study of networks analysis","P. Tantatsanawong; S. Dontongdang; P. U-Aroon","Department of Computing, Faculty of Science, Silpakorn University, Nakorn Phathom, Thailand; Department of Computing, Faculty of Science, Silpakorn University, Nakorn Phathom, Thailand; Department of Mathematics Statistics and Computer, Faculty of Liberal Arts and Science, Kasetsart University Kamphang Sean Campus, Nakorn Phathom, Thailand","2015 12th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON)","20 Aug 2015","2015","","","1","5","Big Data has emerged as an important platform for data intensive computing. In the complex network infrastructure like UniNet, which is one of the biggest research and education network in Thailand, the network analysis is crucial to operate and maintain the network. Netflow is one of the useful tools for collect data from routers to log files for analyzing the problems. Unfortunately, Netflow generates large amount of data. Therefore, Hadoop platform is selected to implement because it can both store data and process the analysis jobs. Typical Hadoop system operates in a simple network topology and difficult to work in complex network topology like UniNet network, which has two layers: Backbone and Distribution layers. In this case, it needs more than on Hadoop systems working together in this network. Content Centric Network (CCN) is applied to discover network data in the whole network and send the analysis jobs to the particular Hadoop nodes. In principle, OpenFlow enables an application to adjust topology as required by the computation, providing additional network bandwidth to those resources requiring it and also support fault-tolerance when fail-over has happened.","","978-1-4799-7961-5","10.1109/ECTICon.2015.7207133","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7207133","Future Internet;Big Data;Networks Analysis;Content Centric Network;Software Define Network","Big data;Education;Internet;Fault tolerance;Fault tolerant systems;Bandwidth;Computer architecture","Big Data;computer aided instruction;educational institutions;Internet","Big Data;future Internet approach;networks analysis;data intensive computing;UniNet;research and education network;Thailand;Hadoop platform;content centric network;CCN","","3","2","12","","20 Aug 2015","","","IEEE","IEEE Conferences"
"KVSSD: Close integration of LSM trees and flash translation layer for write-efficient KV store","S. Wu; K. Lin; L. Chang","Department of Computer Science, National Chiao-Tung University, Hsinchu, Taiwan, R.O.C.; Department of Computer Science, National Chiao-Tung University, Hsinchu, Taiwan, R.O.C.; Department of Computer Science, National Chiao-Tung University, Hsinchu, Taiwan, R.O.C.","2018 Design, Automation & Test in Europe Conference & Exhibition (DATE)","23 Apr 2018","2018","","","563","568","Log-Structured-Merge (LSM) trees are a write-optimized data structure for lightweight, high-performance Key-Value (KV) store. Solid State Disks (SSDs) provide acceleration of KV operations on LSM trees. However, this hierarchical design involves multiple software layers, including the LSM tree, host file system, and Flash Translation Layer (FTL), causing cascading write amplifications. We propose KVSSD, a close integration of LSM trees and the FTL, to manage write amplifications from different layers. KVSSD exploits the FTL mapping mechanism to implement copy-free compaction of LSM trees, and it enables direct data allocation in flash memory for efficient garbage collection. In our experiments, compared to the hierarchical design, our KVSSD reduced the write amplification by 88% and improved the throughput by 347%.","1558-1101","978-3-9819263-0-9","10.23919/DATE.2018.8342070","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8342070","","Compaction;Metadata;Software;Writing;Resource management;Random access memory;Throughput","data structures;flash memories;merging;storage management;tree data structures","lightweight high-performance key-value store;garbage collection;FTL mapping mechanism;write amplifications;write-optimized data structure;write-efficient KV store;LSM tree;KVSSD;flash translation layer","","7","","16","","23 Apr 2018","","","IEEE","IEEE Conferences"
"Free the Bugs: Disclosing Blocking Violations in Reactive Programming","F. Dobslaw; M. Vallin; R. Sundström","Mid Sweden University,Dept. of Computer Sciences,Östersund,Sweden; Mid Sweden University,Dept. of Computer Sciences,Östersund,Sweden; Mid Sweden University,Dept. of Computer Sciences,Östersund,Sweden","2020 IEEE 20th International Working Conference on Source Code Analysis and Manipulation (SCAM)","11 Nov 2020","2020","","","177","186","In programming, concurrency allows threads to share processing units interleaving and seemingly simultaneous to improve resource utilization and performance. Previous research has found that concurrency faults are hard to avoid, hard to find, often leading to undesired and unpredictable behavior. Further, with the growing availability of multi-core devices and adaptation of concurrency features in high-level languages, concurrency faults occur reportedly often, which is why countermeasures must be investigated to limit harm. Reactive programming provides an abstraction to simplify complex concurrent and asynchronous tasks through reactive language extensions such as the RxJava and Project Reactor libraries for Java. Still, blocking violations are possibly resulting in concurrency faults with no Java compiler warnings. BlockHound is a tool that detects incorrect blocking by wrapping the original code and intercepting blocking calls to provide appropriate runtime errors. In this study, we seek an understanding of how common blocking violations are and whether a tool such as BlockHound can give us insight into the root-causes to highlight them as pitfalls to developers. The investigated Softwares are Java-based open-source projects using reactive frameworks selected based on high star ratings and large fork quantities that indicate high adoption. We activated BlockHound in the project's test-suites and analyzed log files for common patterns to reveal blocking violations in 7/29 investigated open-source projects with 5024 stars and 1437 forks. A small number of system calls could be identified as root-causes. We here present countermeasures that successfully removed the uncertainty of blocking violations. The code's intentional logic was retained in all validated projects through passing unit-tests.","2470-6892","978-1-7281-9248-2","10.1109/SCAM51674.2020.00025","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9252028","Reactive-Programming;Blocking-Violations;Concurrency;BlockHound","Concurrent computing;Java;Tools;Programming;Task analysis;Inductors;Open source software","Java;object-oriented programming;program compilers;program debugging;program testing","high-level languages;concurrency faults;reactive programming;complex concurrent tasks;asynchronous tasks;reactive language extensions;blocking violations;intercepting blocking calls;Java-based open-source projects;reactive frameworks;share processing units;concurrency features","","","","35","","11 Nov 2020","","","IEEE","IEEE Conferences"
"UStore: A Low Cost Cold and Archival Data Storage System for Data Centers","Q. Zhang; Y. Dai; F. Li; L. Zhang","Peking Univ., Beijing, China; Peking Univ., Beijing, China; Shanghai Jiao Tong Univ., Shanghai, China; NA","2015 IEEE 35th International Conference on Distributed Computing Systems","23 Jul 2015","2015","","","431","441","Recent trend in cloud computing demands vast and ever increasing storage capacity for data centers. For many cloud service providers, much of the storage capacity demand is driven by cold and archival data, such as user uploaded contents, system logs, and backups. In this paper, we describe UStore, a hard disk based storage system designed for such workloads. We make the assumption that most data centers are already populated with computer servers and networking gears, and propose a solution to attach additional disks to these servers reliably at extremely low cost. The main component of UStore is a novel fat tree interconnect fabric to connect hard disks to existing servers and network infrastructure. To reduce cost, UStore leverages the mature commodity USB 3.0 technology to build the fabric, which has extremely low amortized cost per disk while still providing sufficient throughput to satisfy cold and archival workload. The software of the UStore system abstracts the system's physical topology and provides a consistent view of the storage capacity to the upper layer services such as distributed file systems or backup services. In a sense, UStore can be regarded as external USB hard disks designed for data centers.","1063-6927","978-1-4673-7214-5","10.1109/ICDCS.2015.51","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7164929","archival storage;cold data;USB interconnect","Universal Serial Bus;Fabrics;Servers;Control systems;Hard disks;Software;Topology","cloud computing;computer centres;hard discs;storage management","hard disk based storage system;data centers;fat tree interconnect fabric;USB 3.0 technology;UStore system;external USB hard disks;cloud computing","","1","","52","","23 Jul 2015","","","IEEE","IEEE Conferences"
"Analysis on Online Learning Environment using Process Mining Technique for Personal Knowledge Management Mapping","S. Ismail; F. Tumin","Malaysian Institute of Information Technology, Universiti Kuala Lumpur,Process Mining Research Cluster (ProMineRCE),Kuala Lumpur,Malaysia; Malaysian Institute of Information Technology, Universiti Kuala Lumpur,Process Mining Research Cluster (ProMineRCE),Kuala Lumpur,Malaysia","2019 6th International Conference on Research and Innovation in Information Systems (ICRIIS)","23 Apr 2020","2019","","","1","6","As proven in recent research on personal knowledge management (PKM) processes in online learning environment, this study took the initiative to prove that these processes happen in the said platform by reducing the biasness in respondents' feedbacks. This paper proposes the usage of process mining technique to discover the PKM model in real online learning platform, by analysing the event logs data retrieved from two-semester duration of academic calendar. The focus of this paper is on how the online activities being mapped to the PKM processes, in which the GUSC Model has been used as the appropriate model to represent PKM. GUSC Model is based on the common processes of get knowledge, understand knowledge, share knowledge and connect to knowledge source, as proven to be construed at granular level in intelligent multi-agent system. Since there was a gap between the unconfirmed reality of the GUSC processes in online platform and the construed GUSC processes at intelligent software level, this study significantly contributes to the closure of this gap with its results and findings. It is discovered that PKM model does exist in online learning environment, with the right mapping of the online activities to the GUSC processes.","2324-8157","978-1-7281-6726-8","10.1109/ICRIIS48246.2019.9073269","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9073269","process mining;process model discovery;personal knowledge management;online learning environment","Data mining;Knowledge management;Software;Internet;Collaboration;Learning management systems","computer aided instruction;data mining;knowledge management","online learning environment;process mining technique;personal knowledge management processes;PKM model;online learning platform;PKM processes;GUSC Model;knowledge source","","","","24","","23 Apr 2020","","","IEEE","IEEE Conferences"
"Educational Process Mining: A Systematic Literature Review","M. A. Ghazal; O. Ibrahim; M. A. Salama","Dept. of Comput. Sci., British Univ. in Egypt, Cairo, Egypt; Dept. of Software Eng., British Univ. in Egypt Cairo, Cairo, Egypt; Dept. of Comput. Sci., British Univ. in Egypt, Cairo, Egypt","2017 European Conference on Electrical Engineering and Computer Science (EECS)","19 Jul 2018","2017","","","198","203","Process mining (PM) is an emerging research discipline that is concerned with obtaining fact-based insights on how business processes are executed within an organization. It combines model-based and data-oriented analysis techniques to discover, monitor and improve executed processes by extracting knowledge from event logs generated and stored in information systems. When process mining techniques are applied to educational data, they are often referred to as Educational Process Mining (EPM), and in different case studies EPM results were promising. This paper presents a systematic review of the current status of research in this important area. We examined the literature on research with associated case studies conducted in the domain during the past eight years (2009-2016). Search terms identified 59 papers, but inclusion and exclusion criteria limited the key studies to 37. The research questions, methodologies and findings of these published papers were analysed carefully. This paper aims to highlight the use of process mining to improve educational processes, and to provide researchers with a comprehensive background for understanding the current work being undertaken in this field.","","978-1-5386-2085-4","10.1109/EECS.2017.45","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8412021","Process Mining (PM);Educational Process Mining (EPM);Educational Data Mining(EDM);Literature review;Case studies","Data mining;Bibliographies;Tools;Process control;Systematics;Computer science;Data models","data mining;educational administrative data processing;information systems","data-oriented analysis techniques;process mining techniques;educational data;associated case studies;educational processes;educational process mining;business processes","","2","","50","","19 Jul 2018","","","IEEE","IEEE Conferences"
"Process Mining of Duplicate Tasks: A Systematic Literature Review","C. Duan; Q. Wei","College of Software Engineering, Chongqing University of Posts and Telecommunication,Chongqing,China; College of Computer Science and Technology, Chongqing University of Posts and Telecommunication,Chongqing,China","2020 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)","1 Sep 2020","2020","","","778","784","Process mining improves and provides insights for business processes, which are information related to process execution. In general, process mining can be separated into three classes: process discovery, conformance checking and process enhancement. In order to simplify the process model, we make an assumption that both events in the log and tasks in the model have an injective relation in process mining, i.e., do not allow two tasks to share the same label (thus duplicates task). In addition, Duplicate tasks have some issues concerning the quality of process model discovered and the potential indeterminism in conformance checking. In this paper, we perform a systematic literature review of process discovery and conformance checking metrics for duplicate tasks. This review can: (1) provide a comprehensive review of the current work of duplicate tasks in process discovery and conformance checking; (2) help researchers choose proper process mining approach, tools, and metrics; (3) identify research opportunities in duplicate tasks.","","978-1-7281-7005-3","10.1109/ICAICA50127.2020.9182667","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9182667","process mining;duplicate tasks;systematic literature reviewe;process discovery;conformance checking","Task analysis;Measurement;Data mining;Tools;Databases;Systematics;Bibliographies","business data processing;data mining","duplicate tasks;systematic literature review;business processes;process discovery;conformance checking;process enhancement;process model;process mining approach","","","","58","","1 Sep 2020","","","IEEE","IEEE Conferences"
"Cluster system management","N. Besaw; L. Scheidenbach; J. Dunham; S. Kaur; A. Ohmacht; F. Pizzano; Y. Park",NA; NA; NA; NA; NA; NA; NA,"IBM Journal of Research and Development","13 May 2020","2020","64","3/4","7:1","7:9","Cluster system management (CSM) was co-designed with the Department of Energy Labs to provide the support necessary to effectively manage the Summit and Sierra supercomputers. The CSM system administration tools provide a unified view of a large-scale cluster and the ability to examine and understand data from multiple sources. CSM consists of five components: 1) application programming interfaces (APIs) and infrastructure; 2) Big Data Store; 3) support for reliability, availability, and serviceability (RAS); 4) Diagnostic and Health Check; and 5) support for job management. APIs and infrastructure provide lightweight daemons for compute nodes, hardware and software inventory collection, job accounting, and RAS. Logs, environmental data, and performance data are collected in the Big Data Store for analysis. RAS events can trigger corrective actions by CSM. Diagnostic and Health Check are provided through a diagnostic framework and test results collection. To support job management, CSM coordinates with the Job Step Manager to provide an overlay network of JSM daemons. CSM is an open source and available at https://github.com/IBM/CAST. Documentation can be found at https://cast.readthedocs.io.","0018-8646","","10.1147/JRD.2020.2967309","CORAL NRE; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8961133","","Hardware;Software;Peer-to-peer computing;Jitter;Big Data;Resource management;History","","","","1","","11","IBM","16 Jan 2020","","","IBM","IBM Journals"
"The Design of Real-Time Adaptive Forensically Sound Secure Critical Infrastructure","R. Hunt; J. Slay","Dept. of Comput. Sci. & Software Eng., Univ. of Canterbury, Christchurch, New Zealand; Dept. of Eng. & the Environ., Univ. of South Australia, Adelaide, SA, Australia","2010 Fourth International Conference on Network and System Security","15 Nov 2010","2010","","","328","333","Network security design has seen significant advances in recent years. This has been demonstrated by a growing number of new encryption algorithms, more intelligent firewall and intrusion detection techniques, new developments in multifactor authentication, advances in malware protection and many more. During a similar period of time the industry has seen the need for network infrastructure which provides a greater degree of trust which has resulted in the development of forensic analysis tools which meet the requirements of law enforcement agencies. Such tools must provide for commercial intelligence and national security. This paper proposes that application of the common ground between security and forensics has great potential to provide for improvements in the effort to achieve real-time adaptive security. This implies an architecture which can detect security breaches and in real-time record and analyse traffic logs in a forensically sound manner, provide corrective feedback to security devices and attempt to trace back to the source of the attack. In addressing computer security and forensic analysis from a real-time perspective, this paper recognises that some of these processes already exist, but proposes methods whereby the ongoing damage and potential risk to critical infrastructure can be reduced. This requires the implementation of a highly integrated approach to security and forensics such that they can inter-work in real-time in order to address the significant security issues which currently face the industry.","","978-1-4244-8484-3","10.1109/NSS.2010.38","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=5635616","critical infrastructure;network forensics;real-time adaptive security","Security;Forensics;Fires;Real time systems;Servers;Computer architecture;Adaptive systems","authorisation;computer forensics;real-time systems","network security design;intrusion detection techniques;firewall;multifactor authentication;forensic analysis tools;real-time adaptive security","","3","","11","","15 Nov 2010","","","IEEE","IEEE Conferences"
"Association Rule Mining from large datasets of clinical invoices document","G. Agapito; B. Calabrese; P. H. Guzzi; S. Graziano; M. Cannataro","Data Analytics Research Center, University of Catanzaro,Dept Surgical and Medical Sciences,Catanzaro,Italy; Data Analytics Research Center, University of Catanzaro,Dept Surgical and Medical Sciences,Catanzaro,Italy; Data Analytics Research Center, University of Catanzaro,Dept Surgical and Medical Sciences,Catanzaro,Italy; Open Knowledge Technologies,Rende,Italy; Data Analytics Research Center, University of Catanzaro,Dept Surgical and Medical Sciences,Catanzaro,Italy","2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","6 Feb 2020","2019","","","2232","2238","The concept of massive data generation nowadays affects several domains such as marketing including electronic invoices of large retailers, web access log files, healthcare, life sciences and so on. All these web activities introduced a new way to pay through the concept of electronic invoices (eInvoice), replacing the paper invoices. For these reasons, eInvoicing can be thought of as an innovative digital infrastructure for the issue, transmission, and storage of invoices. The availability of large volumes of eInvoices allows the discovery of new knowledge through data mining in these domains. Thus, users by using data mining can extract knowledge from large invoices documents. In this paper, we present a software tool for mining association rules from invoices produced in healthcare centers. In particular, the tool adopt a novel preprocessing methodology that provides merging, cleaning, formatting and summarization of eInvocies. The methodology can improve the quality of a huge amount of clinical invoices reducing the quantity of irrelevant data, making the remaining data suitable to mine information in form of association rules. The core of the tool allows to extract association rules from eInvoices; as a case study, we discuss the mined rules, highlighting the relationships among the purchased goods.","","978-1-7281-1867-3","10.1109/BIBM47256.2019.8982934","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8982934","Data Mining;Association Rules;Electronic Invoices;Knowledge Discovery from Databases","","data mining;document handling;health care;information retrieval;Internet;invoicing;medical information systems","association rule mining;clinical invoices document;massive data generation;electronic invoices;Web activities;eInvoice;eInvoicing;innovative digital infrastructure;data mining;software tool;healthcare centers;knowledge extraction","","","","13","","6 Feb 2020","","","IEEE","IEEE Conferences"
"Hardware Support for Safety Interlocks and Introspection","U. Dhawan; A. Kwon; E. Kadric; C. Hritcu; B. C. Pierce; J. M. Smith; A. DeHon; G. Malecha; G. Morrisett; T. F. Knight; A. Sutherland; T. Hawkins; A. Zyxnfryx; D. Wittenberg; P. Trei; S. Ray; G. Sullivan","Dept. of Electr. & Syst. Eng., Univ. of Pennsylvania, Philadelphia, PA, USA; Dept. of Electr. & Syst. Eng., Univ. of Pennsylvania, Philadelphia, PA, USA; Dept. of Electr. & Syst. Eng., Univ. of Pennsylvania, Philadelphia, PA, USA; Dept. of Electr. & Syst. Eng., Univ. of Pennsylvania, Philadelphia, PA, USA; Dept. of Electr. & Syst. Eng., Univ. of Pennsylvania, Philadelphia, PA, USA; Dept. of Electr. & Syst. Eng., Univ. of Pennsylvania, Philadelphia, PA, USA; Dept. of Electr. & Syst. Eng., Univ. of Pennsylvania, Philadelphia, PA, USA; Dept. of Comput. Sci., Harvard Univ., Cambridge, MA, USA; Dept. of Comput. Sci., Harvard Univ., Cambridge, MA, USA; Adv. Inf. Technol., BAE Syst., Burlington, MA, USA; Adv. Inf. Technol., BAE Syst., Burlington, MA, USA; Adv. Inf. Technol., BAE Syst., Burlington, MA, USA; Adv. Inf. Technol., BAE Syst., Burlington, MA, USA; Adv. Inf. Technol., BAE Syst., Burlington, MA, USA; Adv. Inf. Technol., BAE Syst., Burlington, MA, USA; Adv. Inf. Technol., BAE Syst., Burlington, MA, USA; Adv. Inf. Technol., BAE Syst., Burlington, MA, USA","2012 IEEE Sixth International Conference on Self-Adaptive and Self-Organizing Systems Workshops","15 Apr 2013","2012","","","1","8","Hardware interlocks that enforce semantic invariants and allow fine-grained privilege separation can be built with reasonable costs given modern semiconductor technology. In the common error-free case, these mechanisms operate largely in parallel with the intended computation, monitoring the semantic intent of the computation on an operation-by-operation basis without sacrificing cycles to perform security checks. We specifically explore five mechanisms: (1) pointers with manifest bounds (fat pointers), (2) hardware types (atomic groups), (3) processor-supported authority, (4)authority-changing procedure calls (gates), and (5) programmable metadata validation and propagation (tags and dynamic tag management). These mechanisms allow the processor to continuously introspect on its operation, efficiently triggering software handlers on events that require logging, merit sophisticated inspection, or prompt adaptation. We present results from our prototype FPGA implementation of a processor that incorporates these mechanisms, quantifying the logic, memory, and latency requirements. We show that the dominant cost is the wider memory necessary to hold our metadata (the atomic groups and programmable tags), that the added logic resources make up less than 20% of the area of the processor, that the concurrent checks do not degrade processor cycle time, and that the tag cache is comparable to a small L1 data cache.","","978-0-7695-4895-1","10.1109/SASOW.2012.11","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6498372","Processor;security;least privilege;separation of privilege;complete mediation;hardware interlocks","","field programmable gate arrays;meta data;security of data","hardware support;safety hardware interlock;semantic invariant;fine-grained privilege separation;semiconductor technology;security check;hardware introspection;pointer mechanism;hardware type mechanism;processor-supported authority mechanism;authority-changing procedure call mechanism;programmable metadata validation-and-propagation mechanism;FPGA;field programmable gate array;logic requirement;memory requirement;latency requirement;tag cache;processor cycle time;concurrent check;software handler","","8","2","31","","15 Apr 2013","","","IEEE","IEEE Conferences"
"Monitoring and control of real time simulated microgrid with renewable energy sources","Y. J. Reddy; S. Dash; A. Ramsesh; Y. V. P. Kumar; K. P. Raju","Honeywell Technology Solution Lab Hyderabad, Andhra Pradesh, India; Honeywell Technology Solution Lab Hyderabad, Andhra Pradesh, India; Honeywell Technology Solution Lab Hyderabad, Andhra Pradesh, India; Honeywell Technology Solution Lab Hyderabad, Andhra Pradesh, India; Department of Electronics and Communication Engineering, JNTU, Kakinada, India","2012 IEEE Fifth Power India Conference","14 Mar 2013","2012","","","1","6","The objective of this experimental research paper is to explore a new methodology to simulate, manage and monitor a renewable energy based hybrid power system model. The model is developed using SIMULINK and the simulation of the model is carried out in real time by means of XPC target software. The energy management and control action for the system is achieved using a PLC, which interfaces the model with the help of the Data acquisition board. The real time monitoring, alarm handling, human machine interfacing, event and data logging is accomplished using the SCADA. Simulation is conducted for the meteorological and load variation to demonstrate various state of the hybrid power system. This type of simulation allows the user to run the model in real time for longer durations, analyze the dynamics of the system and hence designing the proper control system, and to test the accuracy of the control algorithm developed for the live microgrid.","","978-1-4673-0766-6","10.1109/PowerI.2012.6479480","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6479480","Energy management;Microgrid;Hybrid Power System;Real time simulation;PLC;SCADA;XPC target","Microgrids;Real-time systems;Mathematical model;Load modeling;Batteries;Computational modeling;Laboratories","data acquisition;distributed power generation;energy management systems;power generation control;programmable controllers;renewable energy sources","real time simulated microgrid control;real time microgrid monitoring;renewable energy sources;hybrid power system model;SIMULINK;XPC target software;energy management;control action;PLC;data acquisition board;human machine interfacing;SCADA;meteorological variation;load variation;control algorithm","","3","","22","","14 Mar 2013","","","IEEE","IEEE Conferences"
"Predictive Monitoring of Business Processes: A Survey","A. E. Márquez-Chamorro; M. Resinas; A. Ruiz-Cortés","Departamento de Lenguajes y Sistemas Informáticos, University of Seville, Sevilla, Spain; Universidad de Sevilla, Sevilla, Spain; University of Seville, Sevilla, Spain","IEEE Transactions on Services Computing","7 Dec 2018","2018","11","6","962","977","Nowadays, process mining is becoming a growing area of interest in business process management (BPM). Process mining consists in the extraction of information from the event logs of a business process. From this information, we can discover process models, monitor and improve our processes. One of the applications of process mining, is the predictive monitoring of business process. The aim of these techniques is the prediction of quantifiable metrics of a running process instance with the generation of predictive models. The most representative approaches for the runtime prediction of business process are summarized in this paper. The different types of computational predictive methods, such as statistical techniques or machine learning approaches, and certain aspects as the type of predicted values and quality evaluation metrics, have been considered for the categorization of these methods. This paper also includes a summary of the basic concepts, as well as a global overview of the process predictive monitoring area, that can be used to support future efforts of researchers and practitioners in this research field.","1939-1374","","10.1109/TSC.2017.2772256","European Commission (FEDER); Spanish and the Andalusian R&D&I programmes; Juan de la Cierva; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8103817","Business process management;process mining;predictive monitoring;process indicators","Process planning;Business process management;Predictive models;Data mining;Measurement;Runtime;Software systems","business data processing;data mining;learning (artificial intelligence)","computational predictive methods;process mining;business process management;process models;business processes;BPM","","12","","69","","10 Nov 2017","","","IEEE","IEEE Journals"
"Forensic analysis of Windows 10 Sandbox","A. Đuranec; S. Gruičić; M. Žagar","INsig2 d.o.o.,Zagreb,Croatia; INsig2 d.o.o.,Zagreb,Croatia; Zagreb University of Applied Sciences,Zagreb,Croatia","2020 43rd International Convention on Information, Communication and Electronic Technology (MIPRO)","6 Nov 2020","2020","","","1224","1229","With each Windows operating system Microsoft introduces new features to its users. Newly added features present a challenge to digital forensics examiners as they are not analyzed or tested enough. One of the latest features, introduced in Windows 10 version 1909 is Windows Sandbox; a lightweight, temporary, environment for running untrusted applications. Because of the temporary nature of the Sandbox and insufficient documentation, digital forensic examiners are facing new challenges when examining this newly added feature which can be used to hide different illegal activities. Throughout this paper, the focus will be on analyzing different Windows artifacts and event logs, with various tools, left behind as a result of the user interaction with the Sandbox feature on a clear virtual environment. Additionally, the setup of testing environment will be explained, the results of testing and interpretation of the findings will be presented, as well as open-source tools used for the analysis.","2623-8764","978-953-233-099-1","10.23919/MIPRO48935.2020.9245226","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9245226","Windows 10;Sandbox;digital forensics","Operating systems;Prefetching;Digital forensics;Virtual environments;Tools;Open source software;Testing","digital forensics;Microsoft Windows (operating systems)","Sandbox feature;clear virtual environment;testing environment;forensic analysis;Windows 10 Sandbox;Windows operating system Microsoft;digital forensics examiners;Windows 10 version 1909;lightweight environment;temporary environment;Windows artifacts","","","","19","","6 Nov 2020","","","IEEE","IEEE Conferences"
"Using metrics and cluster analysis for analyzing learner video viewing behaviours in educational videos","A. Kleftodimos; G. Evangelidis","Dept of Digital Media and Communication, Technological Education Institute of Western Macedonia, Kastoria, Greece; Dept of Applied Informatics, School of Information Sciences, University of Macedonia, Thessaloniki, Greece","2014 IEEE/ACS 11th International Conference on Computer Systems and Applications (AICCSA)","2 Apr 2015","2014","","","280","287","On line video is a powerful tool for e-learning and this is evident from a number of reports, research papers and university initiatives, which portray that online video is becoming an important medium for delivering educational content. Therefore, research that focuses on how students view educational videos becomes of particular interest and in previous work we argued that in order to efficiently analyze learner viewing behavior we should deploy tools that log the learner activity and assist usage analysis and data mining. Working towards this direction, a framework for recording and analyzing learner behavior was presented together with findings of applying the framework into educational settings. In this paper, we continue this work by presenting a set of metrics that can be derived from the framework and be used to measure learner engagement and video popularity. These metrics in conjunction with the data mining method of clustering are then used to gain insights into learner viewing behavior.","2161-5330","978-1-4799-7100-8","10.1109/AICCSA.2014.7073210","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7073210","video in education;viewing behavior;video usage analysis;cluster analysis;metrics","Measurement;Data mining;Education;Software;Databases;Communications technology;Media","behavioural sciences computing;data mining;educational administrative data processing;pattern clustering","cluster analysis;metrics analysis;learner video viewing behaviours;educational videos;learner engagement measurement;video popularity;data mining method","","5","","18","","2 Apr 2015","","","IEEE","IEEE Conferences"
"Face Morphing Attack Generation & Detection: A Comprehensive Survey","S. Venkatesh; R. Ramachandra; K. Raja; C. Busch","Norwegian University of Science and Technology (NTNU), Norway. (e-mail: sushma.venkatesh@ntnu.no); Norwegian University of Science and Technology (NTNU), Norway.; Norwegian University of Science and Technology (NTNU), Norway.; Norwegian University of Science and Technology (NTNU), Norway.","IEEE Transactions on Technology and Society","","2021","PP","99","1","1","Face recognition has been successfully deployed in real-time applications, including secure applications such as border control. The vulnerability of face recognition systems (FRSs) to various kinds of attacks (both direct and indirect attacks) and face morphing attacks has received great interest from the biometric community. The goal of a morphing attack is to subvert an FRS at an automatic border control (ABC) gate by presenting an electronic machine-readable travel document (eMRTD) or e-passport that is obtained based on a morphed face image. Since the application process for an e-passport in the majority of countries requires a passport photo to be presented by the applicant, a malicious actor and an accomplice can generate a morphed face image to obtain the e-passport. An e-passport with a morphed face image can be used by both the malicious actor and the accomplice to cross a border, as the morphed face image can be verified against both of them. This can result in a significant threat, as a malicious actor can cross the border without revealing the trace of his/her criminal background, while the details of the accomplice are recorded in the log of the access control system. This survey aims to present a systematic overview of the progress made in the area of face morphing in terms of both morph generation and morph detection. In this paper, we describe and illustrate various aspects of face morphing attacks, including different techniques for generating morphed face images and state-of-the-art morph attack detection (MAD) algorithms based on a stringent taxonomy as well as the availability of public databases, which allow us to benchmark new MAD algorithms in a reproducible manner. The outcomes of competitions and benchmarking, vulnerability assessments and performance evaluation metrics are also provided in a comprehensive manner. Furthermore, we discuss the open challenges and potential future areas that need to be addressed in the evolving field of biometrics.","2637-6415","","10.1109/TTS.2021.3066254","iMARS; ","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=9380153","","Face recognition;Faces;Benchmark testing;Open source software;Tools;Taxonomy;Strain","","","","","","","CCBYNCND","17 Mar 2021","","","IEEE","IEEE Early Access Articles"
"Structured and Unstructured Big Data Analytics","S. Mishra; A. Misra",Noida Inter National University (U.P); Noida Inter National University (U.P),"2017 International Conference on Current Trends in Computer, Electrical, Electronics and Communication (CTCEEC)","6 Sep 2018","2017","","","740","746","The volume of data in the world is growing very fast and generated from verity of sources like social media, sensors airline industry or scientific data in different formats. Biggest challenge is how to infer meaningful insights from such a varietyful and big data along with concern of data storage and management of fast growing data. The size of the databases used in today's enterprises has been growing at exponential rates day by day. Hence, industries requirement to quickly process and analyze the big data volumes for business decision making and customer insights has also grown exponentially. Data pouring from various sources may be can be structured or unstructured in nature. Structured data refers to a relatively well-organized information, which can be further inserted into traditional RDBMS. As Traditional RDBMS are efficient and easy queries by simple, straightforward search algorithms or SQL queries. In contrast to structured data, unstructured data can be considered as information, which does not, comes in a pre-defined data format, well organized data storage model, or cannot be stored well into relational tables. It is assumed to be fastest growing type of data, e.g. image, sensors data, web chats, social networking messaging data, video, documents, log files, and email data. There are many techniques and software available, which can process and provide efficient storage of unstructured data and help organization to perform analytics on unstructured data. Unstructured data does not well-organized and not stored in predefined manner e.g. logs, web chats. The variety and on ordered nature of data makes storage methods and structure makes execution a time and resource-consuming affair. Advancement into technology has open floodgates to push huge volume of unstructured type of data. Multimedia data is one of the example of unstructured big data, which spans all over the Internet. This needs high execution capability to extract useful information. Rapid processing of multimedia data such as video is important for e.g. criminal investigations, surveillance monitoring, news analysis, sports analytics domain, emotion extraction, etc. Hence, analysis of multimedia data in minimum timeframe is one of the latest research areas. Therefore, we have researched techniques for analyzing unstructured data to extract meaningful information hidden in the big data. In addition, we will describe about various techniques and software used to Manage, process unstructured big data in efficient manner, and increases the performance of complexity analysis.","","978-1-5386-3243-7","10.1109/CTCEEC.2017.8454999","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8454999","Big data;HDFS;Hadoop;Map-Reduce;sentiment analysis;emoticon based clustering;MongodB;Couchdb","Big Data;Distributed databases;Industries;Data mining;Task analysis","Big Data;data analysis;data mining;decision making;Internet;query processing;relational databases;social networking (online);sport;SQL","complexity analysis;emotion extraction;sports analytics domain;Internet;surveillance monitoring;SQL queries;search algorithms;RDBMS;business decision making;sensors airline industry;social media;structured big data analytics;unstructured big data analytics;data storage model;scientific data;multimedia data;email data;social networking messaging data;sensors data;pre-defined data format;big data volumes","","1","","16","","6 Sep 2018","","","IEEE","IEEE Conferences"
"Prototype development of a Low Cost data logger for PV based LED Street Lighting System","A. Purwadi; Y. Haroen; Farianza Yahya Ali; N. Heryana; D. Nurafiat; A. Assegaf","School of Electrical Engineering and Informatics, Institute of Technology Bandung, Jalan Ganesha No 10, 40132, Indonesia; School of Electrical Engineering and Informatics, Institute of Technology Bandung, Jalan Ganesha No 10, 40132, Indonesia; School of Electrical Engineering and Informatics, Institute of Technology Bandung, Jalan Ganesha No 10, 40132, Indonesia; School of Electrical Engineering and Informatics, Institute of Technology Bandung, Jalan Ganesha No 10, 40132, Indonesia; School of Electrical Engineering and Informatics, Institute of Technology Bandung, Jalan Ganesha No 10, 40132, Indonesia; School of Electrical Engineering and Informatics, Institute of Technology Bandung, Jalan Ganesha No 10, 40132, Indonesia","Proceedings of the 2011 International Conference on Electrical Engineering and Informatics","19 Sep 2011","2011","","","1","5","The data logger is an electronic device that records data over time or in relation to location with a built in sensor. The need of data logger is to accomplish monitoring task of street lighting system based on PV Solar Cell in Jakarta province, especially in Thousand Island Regency. Existing PV LED street lighting has not equipped by data logger, therefore monitoring task is done by manually. Prototype developed based on a microcontroller, battery powered, and equipped with internal memory for data storage, and sensors. The system developed should be “Low Power Consumption”, “Low Cost”, portable and easily embedded inside the PV LED Street Lighting controller. One of the primary benefits of using data loggers is the ability to automatically collect data on a 24-hour basis, and time stamp of logging can be set (five minutes to one hour basis). Upon activation, data logger is left unattended to measure and record information for the duration of the monitoring period. This allows for a comprehensive, accurate picture of sub-system conditions being monitored, such as PV Solar Cell, Battery, Charger/Controller, LED Lamp and ambient temperature. Data loggers interface with a personal computer and utilize software to activate the data logger view and analyze the collected data to verify performance of PV Based LED Street Lighting System. The data logger can be used as a stand-alone device. The reports generated are including daily energy used, total energy used, charging or discharging status and failure condition. The power consumption measured of data logger is about 0.24W (20mA at 12V).","2155-6830","978-1-4577-0752-0","10.1109/ICEEI.2011.6021693","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6021693","data logger;low cost;led;street lighting system;photovoltaic cell","Lighting;Power measurement;Monitoring;Batteries;Photovoltaic cells;Prototypes;Light emitting diodes","data acquisition;LED lamps;lighting control;microcontrollers;photovoltaic cells;solar cells","prototype development;data logger;LED street lighting system;photovoltaic solar cell;lighting controller;charger-controller;ambient temperature;personal computer;stand-alone microcontroller condition;time 24 hr;power 0.24 W;current 20 mA;voltage 12 V","","17","","8","","19 Sep 2011","","","IEEE","IEEE Conferences"
"Data analytics and web insights in area of data mining and analytics","A. Adamov","School of Information Technology and Engineering, ADA University, Azerbaijan","2017 International Conference on Innovations in Electrical Engineering and Computational Technologies (ICIEECT)","4 May 2017","2017","","","1","1","Summary form only given. The extremely fast grow of Internet Services, Web and Mobile Applications and advance of the related Pervasive, Ubiquity and Cloud Computing concepts have stimulated production of tremendous amounts of data partially available online (call metadata, texts, emails, social media updates, photos, videos, location, etc.). Even with the power of today's modern computers it still big challenge for business and government organizations to manage, search, analyze, and visualize this vast amount of data as information. Data-Intensive computing which is intended to address this problems become quite intense during the last few years yielding strong results. Data intensive computing framework is a complex system which includes hardware, software, communications, and Distributed File System (DFS) architecture. Just small part of this huge amount is structured (Databases, XML, logs) or semi-structured (web pages, email), over 90% of this information is unstructured, what means data does not have predefined structure and model. Generally, unstructured data is useless unless applying data mining and analysis techniques. At the same time, just in case if you can process and understand your data, this data worth anything, otherwise it becomes useless. Two key components of any Data-intensive system are: Data Storage and Data Processing. So, which technologies, techniques, platforms, tools are best for Big Data storing and processing? How Big Data Era effect technological landscape? These and many other questions will be answered during a speech.","","978-1-5090-3310-2","10.1109/ICIEECT.2017.7916571","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7916571","","Data mining;Electronic mail;Big Data;Data analysis;Information technology;Web and internet services;Mobile applications","Big Data;data analysis;data mining","data analytics;data mining;data-intensive computing;data storage;data processing;Big Data","","1","","","","4 May 2017","","","IEEE","IEEE Conferences"
"V-Buddy: A Learning Management System","K. Wadkar; P. Koshti; D. Parab; S. Tamboli","Department of Infromation Technology, Vidyalankar Institute of Technology, Wadala(E), Mumbai, 400037; Department of Infromation Technology, Vidyalankar Institute of Technology, Wadala(E), Mumbai, 400037; Department of Infromation Technology, Vidyalankar Institute of Technology, Wadala(E), Mumbai, 400037; Department of Infromation Technology, Vidyalankar Institute of Technology, Wadala(E), Mumbai, 400037","2018 Second International Conference on Electronics, Communication and Aerospace Technology (ICECA)","30 Sep 2018","2018","","","539","541","The software we are going to build is useful for different users such as students, placement coordinator, academic coordinator, exam coordinator, faculties, etc. for fulfilling their requirements. The user who accesses the system through his unique log in id and password and then gets information according to the access rights established in the system. Students will be able to get information related to placements, holidays, announcements from faculties, events, timetable, etc. through android app and faculties will have access to the system through a web portal from which information will be shared.","","978-1-5386-0965-1","10.1109/ICECA.2018.8474743","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8474743","Learning Management System;Student;Faculty;Android;Laravel;Notifications","Androids;Humanoid robots;Conferences;Servers;Computer architecture;Smart phones;Aerospace electronics","Android (operating system);authorisation;learning management systems;portals","learning management system;password;access rights;V-Buddy;unique login ID;Android app;Web portal","","1","","13","","30 Sep 2018","","","IEEE","IEEE Conferences"
"Augmented reality services of photos and videos from filming sites using their shooting locations and attitudes","Y. Chang; S. Lee; J. Cho","ETRI (Electronics and Telecommunications Research Institute), Korea; ETRI (Electronics and Telecommunications Research Institute), Korea; ETRI (Electronics and Telecommunications Research Institute), Korea","2018 20th International Conference on Advanced Communication Technology (ICACT)","26 Mar 2018","2018","","","1","1","Usually lots of photos and videos are recorded additionally to capture the scenes of many filming sites of dramas and movies. When people visit the sites later, however, they can rarely experience these contents again. The objective of this study is to exploit these photos and videos from filming sites together with their shooting locations and attitudes in order to provide users with AR (Augmented Reality) tourism services. Users will be able to experience as if they are at the site just at the time of filming. We designed and made an external attachable device for DSLR (Digital Single-Lens Reflex) cameras to log shooting locations and attitudes. We also implemented syncing software and defined data formats to integrate these data with photos and videos. We implemented mobile AR tourism apps for iOS and Android platforms. We adopted FAST (Features from Accelerated Segment Test) algorithm with additional modification for AR visualization of photos in mobile environment. We adopted Ferns algorithm, histogram specification and OpenGL texture mapping for AR visualization of videos with better performance. These apps and the results of this study were evaluated at actual filming sites of several dramas, and the survey of many filming staffs was conducted for feedbacks. The results of this study could be applied many other fields concerned with multimedia contents.","","979-11-88428-01-4","10.23919/ICACT.2018.8323871","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8323871","Photo;Video;Location;Attitude;Filming Site;Augmented Reality","","","","","","","","","26 Mar 2018","","","IEEE","IEEE Conferences"
"Cost and power optimized electrical drive train system design for an electric three-wheel vehicle based on field test data acquisition and offline simulations","I. Saegesser; A. Vezzini; B. Galliker","Laboratory for Industrial Electronics, Bern UAS, CH-2501 Biel/Bienne, Switzerland; Laboratory for Industrial Electronics, Bern UAS, CH-2501 Biel/Bienne, Switzerland; S.A.M. Group AGCH-4702 Oensingen, Switzerland","2011 IEEE Vehicle Power and Propulsion Conference","13 Oct 2011","2011","","","1","5","One of the first pure electric vehicles with Lithium Ion Battery running on the European market is the SAM EVII developed by S A.M. Group AG. It is a light weight three wheel vehicle with automotive comfort, sold to customers since November 2009. To gather information about its daily usage and therefore to be able to design the drive train of the follow-up model, SAM 3, a field test has been started. In already sold customer cars used in different daily conditions, CAN data loggers have been recording data for several months. They log around 50 values in periodic time steps of 110 ms. In order to analyze this flood of information, software has been developed. The data is compared with simulations results based on static and dynamic requirements for the future model. Together the analysis allows a cost and power optimized choice of the electric drive train parameters. In this paper, the way to get and analyze the data is presented, as well as a small part of the first results.","1938-8756","978-1-61284-247-9","10.1109/VPPC.2011.6042984","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6042984","Electric Vehicle;Electric Motor optimization;EV Field Tests;Sizing of Drive Train","Batteries;Torque;Hybrid electric vehicles;Vehicle dynamics;Inverters;Data acquisition","data acquisition;electric drives;electric vehicles;secondary cells","electrical drive train system design;electric three wheel vehicle;field test data acquisition;offline simulations;electric vehicles;lithium ion battery","","1","","1","","13 Oct 2011","","","IEEE","IEEE Conferences"
"An autonomous system for high-resolution mapping of indoor wireless coverage","R. Wakim; J. A. Weitzen","University of Massachusetts Lowell, Lowell MA 01854; ECE at University of Massachusetts Lowell, Lowell Ma 01854 and Commscope, Chelmsford MA 01824","2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)","15 Feb 2018","2017","","","1","5","This paper describes an autonomous robotic system for characterizing and mapping indoor wireless coverage. Most current-generation systems for measuring wireless coverage require human testers to walk around a prescribed path and manually record their position. This time-consuming process limits measurement resolution and its accuracy is subject to human error. As indoor coverage solutions become more widespread, techniques for efficiently measuring coverage quality are growing in importance. The autonomous robotic system described automatically explores an area and accurately determines its position relative to the building map. Handset logging equipment onboard the robot determines physical layer data rates and signal quality as measured by metrics such as SNR, RSRP, and CQI. Following the robotic test, postprocessing software overlays the test metrics on the building map to create significantly higher resolution coverage maps than have been previously possible. This system allows for verification of wireless coverage against service level agreements, and for repetitive testing of in-development indoor solutions.","2166-9589","978-1-5386-3531-5","10.1109/PIMRC.2017.8292327","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8292327","Wireless Coverage;Robotics;Smallcells;femtocells","Robots;Buildings;Manuals;Telephone sets;Wireless communication;Signal to noise ratio;Portable computers","indoor radio;radiowave propagation;robots","high-resolution mapping;autonomous system;in-development indoor solutions;higher resolution coverage maps;robotic test;autonomous robotic system;indoor coverage solutions;time-consuming process;indoor wireless coverage","","1","","7","","15 Feb 2018","","","IEEE","IEEE Conferences"
"808nm High-Power Semiconductor Laser Therapeutic Apparatus Based on LPC2138","B. Wang","Changchun Inst. of Opt. Fine Mech. & Phys., Changchun, China","2011 International Conference on Intelligent Computation and Bio-Medical Instrumentation","16 Jan 2012","2011","","","163","165","In this paper, we designed an intelligent 808nm high-power semiconductor laser therapeutic apparatus with the LPC2138 ARM processor. The laser is driven by the digital constant current source module, and the apparatus has good human-machine interaction performance on account of set the parameters intelligently. The function such as operation mode, output power, output pulse width and pulse cycle could be easily set through the TFT touch screen. The application software could transmission the log data to the computer by the USB interface in order to check and study the records later.","","978-1-4577-1151-0","10.1109/ICBMI.2011.2","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6131737","LPC2138;808nm;High-Power;Semiconductor Laser Therapeutic Apparatus","Semiconductor lasers;Laser modes;Power lasers;Laser applications;Fiber lasers;Power generation;Voltage control","biomedical equipment;laser applications in medicine;medical computing;microprocessor chips;patient treatment;semiconductor lasers;touch sensitive screens;user interfaces","808nm high-power semiconductor laser therapeutic apparatus;LPC2138 ARM processor;digital constant current source module;human-machine interaction performance;operation mode;output power;output pulse width;pulse cycle;TFT touch screen;USB interface;wavelength 808 nm","","","","8","","16 Jan 2012","","","IEEE","IEEE Conferences"
"New Systems, New Behaviors, New Patterns: Monitoring Insights from System Standup","J. Brandt; A. Gentile; C. Martin; J. Repik; N. Taerat","Sandia Nat. Labs., Albuquerque, NM, USA; Sandia Nat. Labs., Albuquerque, NM, USA; Los Alamos Nat. Lab., Los Alamos, NM, USA; NA; Open Grid Comput., Austin, TX, USA","2015 IEEE International Conference on Cluster Computing","29 Oct 2015","2015","","","658","665","Disentangling significant and important log messages from those that are routine and unimportant can be a difficult task. Further, on a new system, understanding correlations between significant and possibly new types of messages and conditions that cause them can require significant effort and time. The initial standup of a machine can provide opportunities for investigating the parameter space of events and operations and thus for gaining insight into the events of interest. In particular, failure inducement and investigation of corner case conditions can provide knowledge of system behavior for significant issues that will enable easier diagnosis and mitigation of such issues for when they may actually occur during the platform lifetime. In this work, we describe the testing process and monitoring results from a testbed system in preparation for the ACES Trinity system. We describe how events in the initial standup including changes in configuration and software and corner case testing has provided insights that can inform future monitoring and operating conditions, both of our test systems and the eventual large-scale Trinity system.","2168-9253","978-1-4673-6598-7","10.1109/CLUSTER.2015.116","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7307665","","Blades;Testing;Monitoring;Program processors;Cooling;Layout;Temperature","parallel processing;program testing;system monitoring","system standup;failure inducement;system behavior;testing process;testbed system;ACES Trinity system;monitoring insights;corner case testing;eventual large-scale Trinity system;high performance computer;HPC systems","","3","","10","","29 Oct 2015","","","IEEE","IEEE Conferences"
"Augmented reality services of photos and videos from filming sites using their shooting locations and attitudes","Y. Chang; S. Lee; J. Cho","ETRI (Electronics and Telecommunications Research Institute), Korea; ETRI (Electronics and Telecommunications Research Institute), Korea; ETRI (Electronics and Telecommunications Research Institute), Korea","2018 20th International Conference on Advanced Communication Technology (ICACT)","26 Mar 2018","2018","","","656","660","Usually lots of photos and videos are recorded additionally to capture the scenes of many filming sites of dramas and movies. When people visit the sites later, however, they can rarely experience these contents again. The objective of this study is to exploit these photos and videos from filming sites together with their shooting locations and attitudes in order to provide users with AR (Augmented Reality) tourism services. Users will be able to experience as if they are at the site just at the time of filming. We designed and made an external attachable device for DSLR (Digital Single-Lens Reflex) cameras to log shooting locations and attitudes. We also implemented syncing software and defined data formats to integrate these data with photos and videos. We implemented mobile AR tourism apps for iOS and Android platforms. We adopted FAST (Features from Accelerated Segment Test) algorithm with additional modification for AR visualization of photos in mobile environment. We adopted Ferns algorithm, histogram specification and OpenGL texture mapping for AR visualization of videos with better performance. These apps and the results of this study were evaluated at actual filming sites of several dramas, and the survey of many filming staffs was conducted for feedbacks. The results of this study could be applied many other fields concerned with multimedia contents.","","979-11-88428-01-4","10.23919/ICACT.2018.8323872","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8323872","Photo;Video;Location;Attitude;Filming Site;Augmented Reality","Videos;Cameras;Mobile handsets;Data visualization;Histograms;Feature extraction;Classification algorithms","augmented reality;cameras;feature extraction;image segmentation;image texture;mobile computing;travel industry;video signal processing","Augmented reality services;shooting locations;AR tourism services;DSLR cameras;actual filming sites;filming staffs;digital single-lens reflex camera;features from accelerated segment test algorithm;FAST algorithm;mobile AR tourism apps;iOS platforms;Android platforms;OpenGL texture mapping;multimedia contents;Ferns algorithm","","1","","13","","26 Mar 2018","","","IEEE","IEEE Conferences"
"Email trouble: Secrets of spoofing, the dangers of social engineering, and how we can help","B. Opazo; D. Whitteker; C. Shing","Department of Information Technology, Radford University, Radford, VA, USA; Department of Information Technology, Radford University, Radford, VA, USA; Department of Information Technology, Radford University, Radford, VA, USA","2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)","25 Jun 2018","2017","","","2812","2817","Email spoofing is a method of scamming individuals by impersonating a trusted correspondent via email. Incidences of successful Business Email Compromise (BEC) implemented by email spoofing are rising astronomically. Existing security systems are not widely implemented and cannot provide perfect protection against a technological threat that relies on social engineering for success. When existing security systems are implemented the settings are generally not restrictive enough to catch the more sophisticated email attacks. Businesses are not comfortable with legitimate emails being lost due to security false positives. Our idea for a solution would add a layer to existing precautions that would permit looser server-side security settings but would warn the user when discrepancies occur in the header source code that could result from a spoofed email. We suggest a client-side sentinel to vet email header source code and alert the user to potential problems. This software could log alerts, notify company officials, remind users of company policies to be followed in the event of suspicious email, and could increase user accountability by logging incidents. Users could have the option of white-listing frequently flagged trusted correspondents which would decrease the annoyance of false positives.","","978-1-5386-2165-3","10.1109/FSKD.2017.8393226","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8393226","email spoofing;phishing;SPF;DKIM;DMARC","Electronic mail;Servers;Companies;Phishing;Wires","computer crime;electronic mail;security of data;unsolicited e-mail","vet email header source code;suspicious email;Email trouble;social engineering;email spoofing;sophisticated email attacks;legitimate emails;security false positives;looser server-side security settings;successful business email;BEC","","1","","18","","25 Jun 2018","","","IEEE","IEEE Conferences"
"IoT Device Fingerprint using Deep Learning","S. Aneja; N. Aneja; M. S. Islam","Faculty of Integrated Technologies, Universiti Brunei Darussalam, Brunei Darussalam; Institute of Applied Data Analytics, Universiti Brunei Darussalam, Brunei Darussalam; Faculty of Integrated Technologies, Universiti Brunei Darussalam, Brunei Darussalam","2018 IEEE International Conference on Internet of Things and Intelligence System (IOTAIS)","6 Jan 2019","2018","","","174","179","Device Fingerprinting (DFP) is the identification of a device without using its network or other assigned identities including IP address, Medium Access Control (MAC) address, or International Mobile Equipment Identity (IMEI) number. DFP identifies a device using information from the packets which the device uses to communicate over the network. Packets are received at a router and processed to extract the information. In this paper, we worked on the DFP using Inter Arrival Time (IAT). IAT is the time interval between the two consecutive packets received. This has been observed that the IAT is unique for a device because of different hardware and the software used for the device. The existing work on the DFP uses the statistical techniques to analyze the IAT and to further generate the information using which a device can be identified uniquely. This work presents a novel idea of DFP by plotting graphs of IAT for packets with each graph plotting 100 IATs and subsequently processing the resulting graphs for the identification of the device. This approach improves the efficiency to identify a device DFP due to achieved benchmark of the deep learning libraries in the image processing. We configured Raspberry Pi to work as a router and installed our packet sniffer application on the Raspberry Pi. The packet sniffer application captured the packet information from the connected devices in a log file. We connected two Apple devices iPad4 and iPhone 7 Plus to the router and created IAT graphs for these two devices. We used Convolution Neural Network (CNN) to identify the devices and observed the accuracy of 86.7%.","","978-1-5386-7358-4","10.1109/IOTAIS.2018.8600824","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8600824","IoT;Fingerprint;Signature;Image Processing;Deep Learning;Raspberry Pi;Device identification","IP networks;Protocols;Deep learning;Object recognition;Internet of Things;Wireless LAN;Wireless sensor networks","access protocols;convolutional neural nets;Internet of Things;IP networks;learning (artificial intelligence);radio networks;smart phones;telecommunication security","IoT device fingerprint;Device Fingerprinting;Medium Access Control address;International Mobile Equipment Identity number;Inter Arrival Time;device DFP;deep learning libraries;packet sniffer application;packet information;Apple devices;IAT graphs","","7","","15","","6 Jan 2019","","","IEEE","IEEE Conferences"
"RSDA 2019 Workshop Keynote","",,"2019 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)","13 Feb 2020","2019","","","39","39","This talk will cover research and insights from the speaker's last 7 years of research on two topics of relevance to the RSDA workshop. The first part will be about behavioural data analytics for reliability and dependability purposes, i.e., with a focus on sequences of events, rather than static structures or the presence of certain events in a log. Specifically, the approach of using Process Mining techniques to gain insights into traces of logged events will be presented. The second part of the talk will be centred around a summary of insights into security, dependability, and reliability aspects of blockchain applications. With the introduction of smart contracts, blockchain technology has become a general-purpose code execution framework, and architects and developers need to understand the specific implications on security and dependability when building applications on blockchain.","","978-1-7281-5138-0","10.1109/ISSREW.2019.00021","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=8990313","","","data mining","RSDA workshop;behavioural data analytics;static structures;process mining techniques;logged events;blockchain technology;general-purpose code execution framework","","","","","","13 Feb 2020","","","IEEE","IEEE Conferences"
"Message from the BigDataMR2012 Workshop Chairs","",,"2012 Second International Conference on Cloud and Green Computing","14 Feb 2013","2012","","","xxix","xxix","The First International Symposium on Big Data and MapReduce (BigDataMR2012) is co-located with the Second International Conference on Cloud and Green Computing (CGC2012) held on November 1-3, 2012, Xiangtan, Hunan, China. Big data is an emerging paradigm applied to datasets whose size is beyond the ability of commonly used software tools to capture, manage, and process the data within a tolerable elapsed time. Such datasets are often from various sources (Variety) yet unstructured such as social media, sensors, scientific applications, surveillance, video and image archives, Internet texts and documents, Internet search indexing, medical records, business transactions and web logs; and are of large size (Volume) with fast data in/out (Velocity). Various technologies are being discussed to support the handling of big data such as massively parallel processing databases, scalable storage systems, cloud computing platforms, and MapReduce. MapReduce is a distributed programming paradigm and an associated implementation to support distributed computing over large datasets on cloud. This symposium aims at providing a forum for researchers, practitioners and developers from different background areas such as cloud computing, distributed computing and database area to exchange the latest experience, research ideas and synergic research and development on fundamental issues and applications about big data and MapReduce in cloud environments. BigDataMR2012 contains 9 papers. Each of them was peer reviewed by at least three program committee members. The symposium covers a broad range of topics in the field of Big Data and MapReduce.","","978-1-4673-3027-5","10.1109/CGC.2012.136","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=6382926","","","","","","","","","","14 Feb 2013","","","IEEE","IEEE Conferences"
"Software Engineering for Big Data Systems","I. Gorton; A. B. Bener; A. Mockus","Northeastern University; Ryerson University; University of Tennessee, Knoxville","IEEE Software","26 Feb 2016","2016","33","2","32","35","Software engineering for big data systems is complex and faces challenges including pervasive distribution, write-heavy workloads, variable request loads, computation-intensive analytics, and high availability. The articles in this theme issue examine several facets of this complicated puzzle. The Web extra at https://youtu.be/YKBGf9EOBUo is an audio recording of Davide Falessi speaking with Ayse Basar Bener and Audris Mockus about the authors, articles, and discussions that went into the IEEE Software March/April 2016 theme issue on software engineering for big data systems.","1937-4194","","10.1109/MS.2016.47","","https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/stamp/stamp.jsp?arnumber=7420514","strategic prototyping;online video processing;operational-log analysis;data analytics;big data;software engineering;software development","","","","","15","","5","","26 Feb 2016","","","IEEE","IEEE Magazines"

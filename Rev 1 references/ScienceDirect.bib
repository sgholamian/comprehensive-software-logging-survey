@article{ZHANG2018335,
title = {SEG - A Software Program for Finding Somatic Copy Number Alterations in Whole Genome Sequencing Data of Cancer},
journal = {Computational and Structural Biotechnology Journal},
volume = {16},
pages = {335-341},
year = {2018},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2018.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S2001037018300333},
author = {Mucheng Zhang and Deli Liu and Jie Tang and Yuan Feng and Tianfang Wang and Kevin K. Dobbin and Paul Schliekelman and Shaying Zhao},
keywords = {SEG, Somatic Copy Number Alteration, Whole Genome Sequencing, Cancer},
abstract = {As next-generation sequencing technology advances and the cost decreases, whole genome sequencing (WGS) has become the preferred platform for the identification of somatic copy number alteration (CNA) events in cancer genomes. To more effectively decipher these massive sequencing data, we developed a software program named SEG, shortened from the word “segment”. SEG utilizes mapped read or fragment density for CNA discovery. To reduce CNA artifacts arisen from sequencing and mapping biases, SEG first normalizes the data by taking the log2-ratio of each tumor density against its matching normal density. SEG then uses dynamic programming to find change-points among a contiguous log2-ratio data series along a chromosome, dividing the chromosome into different segments. SEG finally identifies those segments having CNA. Our analyses with both simulated and real sequencing data indicate that SEG finds more small CNAs than other published software tools.}
}
@article{PAYEN20131826,
title = {Echo-Power Estimation from Log-Compressed Video Data in Dynamic Contrast-Enhanced Ultrasound Imaging},
journal = {Ultrasound in Medicine & Biology},
volume = {39},
number = {10},
pages = {1826-1837},
year = {2013},
issn = {0301-5629},
doi = {https://doi.org/10.1016/j.ultrasmedbio.2013.03.022},
url = {https://www.sciencedirect.com/science/article/pii/S0301562913006236},
author = {Thomas Payen and Alain Coron and Michele Lamuraglia and Delphine {Le Guillou-Buffello} and Emmanuel Gaud and Marcel Arditi and Olivier Lucidarme and S. Lori Bridal},
keywords = {Contrast-enhanced ultrasound, Echo-power, Linearization, Log compression, Microbubbles, Perfusion quantification, Video compression},
abstract = {Ultrasound (US) scanners typically apply lossy, non-linear modifications to the US data for visualization purposes. The resulting images are then stored as compressed video data. Some system manufacturers provide dedicated software for quantification purposes to eliminate such processing distortions, at least partially. This is currently the recommended approach for quantitatively assessing changes in contrast-agent concentration from clinical data. However, the machine-specific access to US data and the limited set of analysis functionalities offered by each dedicated-software package make it difficult to perform comparable analyses with different US systems. The objective of this work was to establish if linearization of compressed video images obtained with an arbitrary US system can provide an alternative to dedicated-software analysis of machine-specific files for the estimation of echo-power. For this purpose, an Aplio 50 system (Toshiba Medical Systems, Tochigi, Japan), coupled with dedicated CHI-Q (Contrast Harmonic Imaging Quantification) software by Toshiba Medical Systems, was used. Results were compared with two approaches that apply algorithms to estimate relative echo-power from compressed video images: commercially available VueBox software by Bracco Suisse SA (Geneva, Switzerland) and in-laboratory software called PixPower. The echo-power estimated by CHI-Q analysis indicated a strong linear relationship versus agent concentration in vitro (R2 ≥ 0.9996) for dynamic range (DR) settings of DR60 and DR80, with slopes between 9.22 and 9.57 dB/decade (p = 0.05). These values approach the theoretically predicted dependence of 10.0 dB/decade (equivalent to 3 dB for each concentration doubling). Echo-power estimations obtained from compressed video images with VueBox and PixPower also exhibited strong linear proportionality with concentration (R2 ≥ 0.9996), with slopes between 9.30 and 9.68 dB/decade (p = 0.05). On an independent in vivo data set (N = 24), the difference in echo-power estimation between CHI-Q and each of the other two approaches was calculated after excluding regions that contain pixels affected by saturated or thresholded pixel values. The mean difference in estimates (expressed in decibels) was −0.25 dB between VueBox and CHI-Q (95% confidence interval: −0.75 to 0.26 dB) and −0.17 dB between PixPower and CHI-Q (95% confidence interval: −0.67 to 0.13 dB). To achieve linearization of data, one of the approaches (VueBox) requires calibration files provided by the software manufacturer for each machine type and setting. The other (PixPower) requires empirical correction of the imaging dynamic range based on ground truth data. These requirements could potentially be removed if US system manufacturers were willing to make relevant information on the applied processing publically available. Reliable echo-power estimation from linearized data would facilitate inclusion of different US systems in multicentric studies and more widespread implementation of emerging techniques for quantitative analysis of contrast ultrasound.}
}
@article{GOEL2019103959,
title = {A data-driven alarm and event management framework},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {62},
pages = {103959},
year = {2019},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2019.103959},
url = {https://www.sciencedirect.com/science/article/pii/S0950423019303638},
author = {Pankaj Goel and E.N. Pistikopoulos and M.S. Mannan and Aniruddha Datta},
keywords = {Alarm management, Data mining and analysis, Abnormal situation management (ASM), Visualization},
abstract = {Industrial systems are monitored and controlled by sensors and actuators. The signals from these instruments are mostly configured as alarms in the control system which alerts the plant operators in case of an abnormal process event. A higher number of alarms appearing on the operator screen signifies poor system performance and results in additional workload on the operators and may lead to abnormal situations. These situations can further escalate to a catastrophic incident if not managed properly. There are several solutions available to address the challenges related to the alarms and their management as a part of alarm management life-cycle process defined in industrial standards and guidelines. With the advent of Open Process Automation (OPA) concept and requirement of Real-time Operational Technology (OT) services there is a need to develop solutions based on open source software platforms. To address this, an alarm management framework is proposed, that integrates the alarm management life-cycle concept provided in ANSI/ISA-18.2 with data mining and analysis methods applied on alarm and event logs generated from a control system. The framework involves four distinct levels - design, rationalize, advance, and intelligent. A methodology to benchmark an alarm system by calculating Key Performance Indicators (KPIs) is formulated, results are obtained and shown visually by designing a tool using an open source software platform (R and Python). With the use of data mining and analysis, the methodology/tool benchmarks the system for the defined metrics for alarm systems as KPIs, identifies the bad actors, provides insights about the alarm system to manage the alarm flooding. We use the results to address the key issues and thereby improve the overall efficiency of the alarm system. A historical real industrial Alarm and Event (A&E) log data-set is used as an example to demonstrate the potential application of the proposed alarm management tool, to arrive at reliable decision, contributing to better alarm management and safer operations.}
}
@article{TSIATSIKAS201550,
title = {An efficient and easily deployable method for dealing with DoS in SIP services},
journal = {Computer Communications},
volume = {57},
pages = {50-63},
year = {2015},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2014.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S014036641400348X},
author = {Zisis Tsiatsikas and Dimitris Geneiatakis and Georgios Kambourakis and Angelos D. Keromytis},
keywords = {Session Initiation Protocol, Entropy, Abnormal traffic, Denial of Service, Anonymity},
abstract = {Voice over IP (VoIP) architecture and services consist of different software and hardware components that may be susceptible to a plethora of attacks. Among them, Denial of Service (DoS) is perhaps the most powerful one, as it aims to drain the underlying resources of a service and make it inaccessible to the legitimate users. So far, various detection and prevention schemes have been deployed to detect, deter and eliminate DoS occurrences. However, none of them seems to be complete in assessing in both realtime and offline modes if a system remains free of such types of attacks. To this end, in the context of this paper, we assert that audit trails in VoIP can be a rich source of information toward flushing out DoS incidents and evaluating the security level of a given system. Specifically, we introduce a privacy-friendly service to assess whether or not a SIP service provider suffers a DoS by examining either the recorded audit trails (in a forensic-like manner) or the realtime traffic. Our solution relies solely on the already received network logistic files, making it simple, easy to deploy, and fully compatible with existing SIP installations. It also allows for the exchange of log files between different providers for cross-analysis or its submission to a single analysis center (as an outsourced service) in an opt-in basis. Through extensive evaluation involving both offline and online executions and a variety of DoS scenarios, it is argued that our detection scheme is efficient enough, while its realtime operation introduces negligible overhead.}
}
@article{YOO201882,
title = {LongLine: Visual Analytics System for Large-scale Audit Logs},
journal = {Visual Informatics},
volume = {2},
number = {1},
pages = {82-97},
year = {2018},
note = {Proceedings of PacificVAST 2018},
issn = {2468-502X},
doi = {https://doi.org/10.1016/j.visinf.2018.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S2468502X18300159},
author = {Seunghoon Yoo and Jaemin Jo and Bohyoung Kim and Jinwook Seo},
abstract = {Audit logs are different from other software logs in that they record the most primitive events (i.e., system calls) in modern operating systems. Audit logs contain a detailed trace of an operating system, and thus have received great attention from security experts and system administrators. However, the complexity and size of audit logs, which increase in real time, have hindered analysts from understanding and analyzing them. In this paper, we present a novel visual analytics system, LongLine, which enables interactive visual analyses of large-scale audit logs. LongLine lowers the interpretation barrier of audit logs by employing human-understandable representations (e.g., file paths and commands) instead of abstract indicators of operating systems (e.g., file descriptors) as well as revealing the temporal patterns of the logs in a multi-scale fashion with meaningful granularity of time in mind (e.g., hourly, daily, and weekly). LongLine also streamlines comparative analysis between interesting subsets of logs, which is essential in detecting anomalous behaviors of systems. In addition, LongLine allows analysts to monitor the system state in a streaming fashion, keeping the latency between log creation and visualization less than one minute. Finally, we evaluate our system through a case study and a scenario analysis with security experts.}
}
@article{MURASE201473,
title = {A Tool for Parameter-space Explorations},
journal = {Physics Procedia},
volume = {57},
pages = {73-76},
year = {2014},
note = {Proceedings of the 27th Workshop on Computer Simulation Studies in Condensed Matter Physics (CSP2014)},
issn = {1875-3892},
doi = {https://doi.org/10.1016/j.phpro.2014.08.134},
url = {https://www.sciencedirect.com/science/article/pii/S187538921400279X},
author = {Yohsuke Murase and Takeshi Uchitane and Nobuyasu Ito},
keywords = {parallel computing, parameter search, comprehensive simulation},
abstract = {A software for managing simulation jobs and results, named “OACIS”, is presented. It controls a large number of simulation jobs executed in various remote servers, keeps these results in an organized way, and manages the analyses on these results. The software has a web browser front end, and users can submit various jobs to appropriate remote hosts from a web browser easily. After these jobs are finished, all the result files are automatically downloaded from the computational hosts and stored in a traceable way together with the logs of the date, host, and elapsed time of the jobs. Some visualization functions are also provided so that users can easily grasp the overview of the results distributed in a high-dimensional parameter space. Thus, OACIS is especially beneficial for the complex simulation models having many parameters for which a lot of parameter searches are required. By using API of OACIS, it is easy to write a code that automates parameter selection depending on the previous simulation results. A few examples of the automated parameter selection are also demonstrated.}
}
@article{DALTRO2021104428,
title = {Heterosis effects on 305-day milk yield in a Girolando dairy cattle population in different lactation orders},
journal = {Livestock Science},
volume = {245},
pages = {104428},
year = {2021},
issn = {1871-1413},
doi = {https://doi.org/10.1016/j.livsci.2021.104428},
url = {https://www.sciencedirect.com/science/article/pii/S1871141321000366},
author = {Darlene dos Santos Daltro and Renata Negri and Jaime Araújo Cobuci},
keywords = {Crossbreeding, dairy cows, Heritability, Mathematical model},
abstract = {The aim of this study was to evaluate the heterosis effects for 305-day milk yield (305MY) in different lactation orders of Girolando cows, by fitting test-day milk yield records to Wood's, Mixed Log, and Wilmink's non-linear models. A total of 400,328 test-day milk yield records of 39,077 Holstein (H), Gyr (G), and Girolando (1/2 H, 1/4 H, 3/4 H, 3/8 H, 5/8 H, and 7/8 H genetic groups) cows from 822 herds were collected in the state of Minas Gerais, Brazil, from 1998 to 2014. The pedigree file comprised 36,640 animals, including 3677 sires and 24,472 cows. Genetic parameters were obtained by single-trait analysis using the AIREMLF90 software. Heritability estimates of 305MY and of 305MY fitted to the different non-linear models were higher in the first lactation and ranged from 0.10 to 0.29. We observed a greater heterosis effect for 305MY (and 305MY fitted) verified in cows of genetic group 1/2H, which presented average production superior to the average of their parents (of pure breeds) in the first (1034.33 kg) and second (1303.94 kg) lactation, respectively. Both Wood's and Wilmink's models stood out in quality of fit, presenting the best estimates for 305MY adjusted for different lactation orders. Wood's model is the most suitable to estimate 305MY of cows with a higher proportion of H genes. However, Wilmink's model allows estimating heterosis effects for 305MY and components of the lactation curve more similar to those observed in 305MY in the different genetic groups. The results indicate that local breeders can promote an improvement in milk production through exploitation of the heterosis effect, making choices of genetic combinations (breeders) more suitable for mating to obtain greater economic and productive efficiency of their herds during lactations.}
}
@article{LOPEZ2011151,
title = {A framework for building mobile single and multi-robot applications},
journal = {Robotics and Autonomous Systems},
volume = {59},
number = {3},
pages = {151-162},
year = {2011},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2011.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S092188901100011X},
author = {Joaquin López and Diego Pérez and Eduardo Zalama},
keywords = {Mobile robot, Robot control architecture, Robot programming framework, Petri net},
abstract = {The complexity of robot software systems calls for the use of a well-conceived architecture together with programming tools to support it. One common feature of robot architectures is the modular decomposition of systems into simpler and largely independent components. These components implement primitive actions and report events about their state. The robot programming framework proposed here includes a tool (RoboGraph) to program and coordinate the activity (tasks) of these middleware modules. Project developers use the same task programming IDE (RoboGraph) on two different levels. The first is to program tasks that must be executed autonomously by one robot and the second is to program tasks that can include several robots and building elements. Tasks are described using a Signal Interpreted Petri Net (SIPN) editor and stored in an xml file. A dispatcher loads these files and executes the different Petri nets as needed. A monitor that shows the state of all the running nets is very useful for debugging and tracing purposes. The whole system has been used in several applications: A tour-guide robot (GuideBot), a multi-robot surveillance project (WatchBot) and a hospital food and laundry transportation system based on mobile robots.}
}
@article{ERSSON2013662,
title = {Botnet Detection with Event-Driven Analysis},
journal = {Procedia Computer Science},
volume = {22},
pages = {662-671},
year = {2013},
note = {17th International Conference in Knowledge Based and Intelligent Information and Engineering Systems - KES2013},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.09.147},
url = {https://www.sciencedirect.com/science/article/pii/S187705091300940X},
author = {Joakim Ersson and Esmiralda Moradian},
keywords = {Log analysis, botnet, firewall log,network analysis},
abstract = {Due to the huge impact on businesses, botnets are recognized as one of the most serious security threats. Malicious entities use various techniques to conceal and keep themselves undetected durin the proliferation of malware from computer to computer. Detection of a botnet is commonly performed in two ways either by using antivirus software or by analysing logged network data. However antivirus software usually detects malware that is already known and has been analysed, which is a main drawback of such approach due to the constant evolving of malware. The approach of analysis of logged network data do not reveals botnet activities and requires knowledge about botnets and type of data to look for within the collected log. Thus, the significant information can be overlooked and missed. In this paper, we propose event-driven log analysis software that enables detection of botnet activities and indicates whether the end-users machines have become a member of a botnet. Moreover, to optimize software functionality we performed an experiment that demonstrates how botnet communicates between itself and the command and control. Experiment along with the result is presented in this research.}
}
@article{FRONZA20132,
title = {Failure prediction based on log files using Random Indexing and Support Vector Machines},
journal = {Journal of Systems and Software},
volume = {86},
number = {1},
pages = {2-11},
year = {2013},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2012.06.025},
url = {https://www.sciencedirect.com/science/article/pii/S0164121212001732},
author = {Ilenia Fronza and Alberto Sillitti and Giancarlo Succi and Mikko Terho and Jelena Vlasenko},
keywords = {Failure prediction, Random Indexing, Support Vector Machine (SVM), Event sequence data, Log files},
abstract = {Research problem
The impact of failures on software systems can be substantial since the recovery process can require unexpected amounts of time and resources. Accurate failure predictions can help in mitigating the impact of failures. Resources, applications, and services can be scheduled to limit the impact of failures. However, providing accurate predictions sufficiently ahead is challenging. Log files contain messages that represent a change of system state. A sequence or a pattern of messages may be used to predict failures.
Contribution
We describe an approach to predict failures based on log files using Random Indexing (RI) and Support Vector Machines (SVMs).
Method
RI is applied to represent sequences: each operation is characterized in terms of its context. SVMs associate sequences to a class of failures or non-failures. Weighted SVMs are applied to deal with imbalanced datasets and to improve the true positive rate. We apply our approach to log files collected during approximately three months of work in a large European manufacturing company.
Results
According to our results, weighted SVMs sacrifice some specificity to improve sensitivity. Specificity remains higher than 0.80 in four out of six analyzed applications.
Conclusions
Overall, our approach is very reliable in predicting both failures and non-failures.}
}
@article{MAVRIDIS2017133,
title = {Performance evaluation of cloud-based log file analysis with Apache Hadoop and Apache Spark},
journal = {Journal of Systems and Software},
volume = {125},
pages = {133-151},
year = {2017},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2016.11.037},
url = {https://www.sciencedirect.com/science/article/pii/S0164121216302370},
author = {Ilias Mavridis and Helen Karatza},
keywords = {Log Analysis, Cloud, Apache Hadoop, Apache Spark, Performance Evaluation},
abstract = {Log files are generated in many different formats by a plethora of devices and software. The proper analysis of these files can lead to useful information about various aspects of each system. Cloud computing appears to be suitable for this type of analysis, as it is capable to manage the high production rate, the large size and the diversity of log files. In this paper we investigated log file analysis with the cloud computational frameworks Apache™Hadoop® and Apache Spark™. We developed realistic log file analysis applications in both frameworks and we performed SQL-type queries in real Apache Web Server log files. Various experiments were performed with different parameters in order to study and compare the performance of the two frameworks.}
}
@article{MARRINGTON2011S52,
title = {CAT Detect (Computer Activity Timeline Detection): A tool for detecting inconsistency in computer activity timelines},
journal = {Digital Investigation},
volume = {8},
pages = {S52-S61},
year = {2011},
note = {The Proceedings of the Eleventh Annual DFRWS Conference},
issn = {1742-2876},
doi = {https://doi.org/10.1016/j.diin.2011.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S1742287611000314},
author = {Andrew Marrington and Ibrahim Baggili and George Mohay and Andrew Clark},
keywords = {Timeline inconsistency, Event correlation, Precondition event, Happened-before, CAT detect},
abstract = {The construction of timelines of computer activity is a part of many digital investigations. These timelines of events are composed of traces of historical activity drawn from system logs and potentially from evidence of events found in the computer file system. A potential problem with the use of such information is that some of it may be inconsistent and contradictory thus compromising its value. This work introduces a software tool (CAT Detect) for the detection of inconsistency within timelines of computer activity. We examine the impact of deliberate tampering through experiments conducted with our prototype software tool. Based on the results of these experiments, we discuss techniques which can be employed to deal with such temporal inconsistencies.}
}
@article{BAEK201528,
title = {Parity Resynchronization using a Block-level Journaling for Software RAID},
journal = {Information Systems},
volume = {54},
pages = {28-42},
year = {2015},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2015.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0306437915000952},
author = {Sung Hoon Baek and Ki-Woong Park},
keywords = {Secondary storage, Fault tolerance},
abstract = {Software redundant arrays of independent disks (RAID) suffer from several hours of resynchronization time after a sudden power-off. Data blocks and a parity block in a stripe must be updated in a consistent manner. However, a data block may be updated without a parity update if power goes off. Such a partially modified stripe must be updated with a correct parity block after a reboot. It is difficult, however, to find which stripe is partially updated. The widely used traditional parity resynchronization approach entails a very long process that scans the entire volume to find and fix partially updated stripes. As a remedy to this problem, this paper presents a parity resynchronization scheme that exhibits a small overhead for a wide range of workloads, finishes parity resynchronization within several minutes, and is transparent to file systems, thanks to a new seamless block-level journaling. The proposed scheme is integrated into a software RAID driver in a Linux system. A performance evaluation demonstrates that the proposed scheme shortens the resynchronization process from 200min to 30s with 1% overhead, compared to 51% overhead for the prior scheme.}
}
@article{WURZENBERGER201613,
title = {Complex log file synthesis for rapid sandbox-benchmarking of security- and computer network analysis tools},
journal = {Information Systems},
volume = {60},
pages = {13-33},
year = {2016},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2016.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S030643791530212X},
author = {Markus Wurzenberger and Florian Skopik and Giuseppe Settanni and Wolfgang Scherrer},
keywords = {Log line clustering, Markov chains, Log file analysis, Log data modeling, IDS deployment optimization},
abstract = {Today Information and Communications Technology (ICT) networks are a dominating component of our daily life. Centralized logging allows keeping track of events occurring in ICT networks. Therefore a central log store is essential for timely detection of problems such as service quality degradations, performance issues or especially security-relevant cyber attacks. There exist various software tools such as security information and event management (SIEM) systems, log analysis tools and anomaly detection systems, which exploit log data to achieve this. While there are many products on the market, based on different approaches, the identification of the most efficient solution for a specific infrastructure, and the optimal configuration is still an unsolved problem. Today׳s general test environments do not sufficiently account for the specific properties of individual infrastructure setups. Thus, tests in these environments are usually not representative. However, testing on the real running productive systems exposes the network infrastructure to dangerous or unstable situations. The solution to this dilemma is the design and implementation of a highly realistic test environment, i.e. sandbox solution, that follows a different – novel – approach. The idea is to generate realistic network event sequence (NES) data that reflects the actual system behavior and which is then used to challenge network analysis software tools with varying configurations safely and realistically offline. In this paper we define a model, based on log line clustering and Markov chain simulation to create this synthetic log data. The presented model requires only a small set of real network data as an input to understand the complex real system behavior. Based on the input׳s characteristics highly realistic customer specified NES data is generated. To prove the applicability of the concept developed in this work, we conclude the paper with an illustrative example of evaluation and test of an existing anomaly detection system by using generated NES data.}
}
@article{LAL2019481,
title = {Three-level learning for improving cross-project logging prediction for if-blocks},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {31},
number = {4},
pages = {481-496},
year = {2019},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2017.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S1319157817300307},
author = {Sangeeta Lal and Neetu Sardana and Ashish Sureka},
abstract = {Log statements present in the source code provide important execution information to the software developers while debugging. Predicting source code constructs that must be logged is a technically challenging task. Machine learning models are popularly used in literature for logging prediction. These models may not be suitable for small or new projects as these projects do not have sufficient prior data to train the prediction model. For such scenarios, cross-project logging prediction can be useful, in which knowledge gained from the standard project(s) is used to build the model. This paper proposes a three-level model, CLIF, for Cross-project if-blocks logging prediction. CLIF first converts textual features into numeric-textual features(Level-1). CLIF then combines numeric-textual features with Boolean and numeric features(Level-2). Since only few code constructs are logged, CLIF finally learns an effective imbalance decision threshold boundary to predict logged/non-logged code constructs(Level-3). We use 2 text mining classifiers(Level-1) and 8 machine learning classifiers(Level-2) and generate a total of 16 CLIF classifiers. We evaluate performance of CLIF classifiers on three open source projects(six source & target project pair). Experimental results show that CLIF outperforms the existing LogOpt-Plus model and give an improvement of up to 8.21% in average F1-score, and 4.89% in average ROC-AUC.}
}
@article{OZKAYA20189,
title = {FRACOR-software toolbox for deterministic mapping of fracture corridors in oil fields on AutoCAD platform},
journal = {Computers & Geosciences},
volume = {112},
pages = {9-22},
year = {2018},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2017.11.016},
url = {https://www.sciencedirect.com/science/article/pii/S0098300417308269},
author = {Sait I. Ozkaya},
keywords = {Fracture corridors, Software toolbox data integration, Oil fields, AutoCAD platform},
abstract = {Fracture corridors are interconnected large fractures in a narrow sub vertical tabular array, which usually traverse entire reservoir vertically and extended for several hundreds of meters laterally. Fracture corridors with their huge conductivities constitute an important element of many fractured reservoirs. Unlike small diffuse fractures, actual fracture corridors must be mapped deterministically for simulation or field development purposes. Fracture corridors can be identified and quantified definitely with borehole image logs and well testing. However, there are rarely sufficient image logs or well tests, and it is necessary to utilize various fracture corridor indicators with varying degrees of reliability. Integration of data from many different sources, in turn, requires a platform with powerful editing and layering capability. Available commercial reservoir characterization software packages, with layering and editing capabilities, can be cost intensive. CAD packages are far more affordable and may easily acquire the versatility and power of commercial software packages with addition of a small software toolbox. The objective of this communication is to present FRACOR, a software toolbox which enables deterministic 2D fracture corridor mapping and modeling on AutoCAD platform. The FRACOR toolbox is written in AutoLISPand contains several independent routines to import and integrate available fracture corridor data from an oil field, and export results as text files. The resulting fracture corridor maps consists mainly of fracture corridors with different confidence levels from combination of static and dynamic data and exclusion zones where no fracture corridor can exist. The exported text file of fracture corridors from FRACOR can be imported into an upscaling programs to generate fracture grid for dual porosity simulation or used for field development and well planning.}
}
@article{PETERSON201524,
title = {Event-based processing of neutron scattering data},
journal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
volume = {803},
pages = {24-28},
year = {2015},
issn = {0168-9002},
doi = {https://doi.org/10.1016/j.nima.2015.09.016},
url = {https://www.sciencedirect.com/science/article/pii/S0168900215010682},
author = {Peter F. Peterson and Stuart I. Campbell and Michael A. Reuter and Russell J. Taylor and Janik Zikovsky},
keywords = {Neutron, Software, Data Analysis},
abstract = {Many of the world׳s time-of-flight spallation neutrons sources are migrating to recording individual neutron events. This provides for new opportunities in data processing, the least of which is to filter the events based on correlating them with logs of sample environment and other ancillary equipment. This paper will describe techniques for processing neutron scattering data acquired in event mode which preserve event information all the way to a final spectrum, including any necessary corrections or normalizations. This results in smaller final uncertainties compared to traditional methods, while significantly reducing processing time and memory requirements in typical experiments. Results with traditional histogramming techniques will be shown for comparison.}
}
@article{HNATYSHIN2014103,
title = {Paleomagnetic dating: Methods, MATLAB software, example},
journal = {Tectonophysics},
volume = {630},
pages = {103-112},
year = {2014},
issn = {0040-1951},
doi = {https://doi.org/10.1016/j.tecto.2014.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0040195114002613},
author = {Danny Hnatyshin and Vadim A. Kravchinsky},
keywords = {Age dating software, Geological dating, Gold mineralization, Ore deposit formation, Paleomagnetism, Plotting software},
abstract = {A MATLAB software tool has been developed to provide an easy to use graphical interface for the plotting and interpretation of paleomagnetic data. The tool takes either paleomagnetic directions or paleopoles and compares them to a user defined apparent polar wander path or secular variation curve to determine the age of a paleomagnetic sample. Ages can be determined in two ways, either by translating the data onto the reference curve, or by rotating it about a set location (e.g. sampling location). The results are then compiled in data tables which can be exported as an excel file. This data can also be plotted using variety of built-in stereographic projections, which can then be exported as an image file. This software was used to date the giant Sukhoi Log gold deposit in Russia. Sukhoi Log has undergone a complicated history of faulting, folding, metamorphism, and is the vicinity of many granitic bodies. Paleomagnetic analysis of Sukhoi Log allowed for the timing of large scale thermal or chemical events to be determined. Paleomagnetic analysis from gold mineralized black shales was used to define the natural remanent magnetization recorded at Sukhoi Log. The obtained paleomagnetic direction from thermal demagnetization produced a paleopole at 61.3°N, 155.9°E, with the semi-major axis and semi-minor axis of the 95% confidence ellipse being 16.6° and 15.9° respectively. This paleopole is compared to the Siberian apparent polar wander path (APWP) by translating the paleopole to the nearest location on the APWP. This produced an age of 255.2−31.0+32.0Ma and is the youngest well defined age known for Sukhoi Log. We propose that this is the last major stage of activity at Sukhoi Log, and likely had a role in determining the present day state of mineralization seen at the deposit.}
}
@article{CAMINO20172,
title = {A software for managing chemical processes in a multi-user laboratory},
journal = {Journal of Chemical Health and Safety},
volume = {24},
number = {4},
pages = {2-5},
year = {2017},
issn = {1871-5532},
doi = {https://doi.org/10.1016/j.jchas.2016.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1871553216300810},
author = {F.E. Camino},
abstract = {We report a software for logging chemical processes in a multi-user laboratory, which implements a workflow designed to reduce hazardous situations associated with the disposal of chemicals in incompatible waste containers. The software allows users to perform only those processes displayed in their list of authorized chemical processes and provides the location and label code of waste containers, among other useful information. The software has been used for six years in the cleanroom of the Center for Functional Nanomaterials at Brookhaven National Laboratory and has been an important factor for the excellent safety record of the Center.}
}
@article{ROHRER2019104847,
title = {Tractor CAN bus interface tools and application development for real-time data analysis},
journal = {Computers and Electronics in Agriculture},
volume = {163},
pages = {104847},
year = {2019},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2019.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S016816991831888X},
author = {R.A. Rohrer and S.K. Pitla and J.D. Luck},
keywords = {Agricultural machinery, Controller area network, CAN bus, Data acquisition, Diesel engine, SAE J1939},
abstract = {Embedded electronic controllers connected via controller area network (CAN) are used extensively in modern off-road machinery to broadcast machine information for on-board processes and diagnostics. A variety of tools are commercially available to monitor and record CAN data for research and commercial uses. Some plug-and-play options are easy to set up but have limited customization. Others are more complex to configure but can be used to create fully customized and deployable real-time software applications. Several CAN data tools and software packages were evaluated to monitor and record machine and engine data. A proof-of-concept application was created with Matlab Vehicle Network Toolbox for real-time monitoring of diesel engine speed and load, and generation of a cumulative speed/load histogram based on SAE J1939 parameters. This study investigated four hardware and software CAN tool configurations that could be used for tractor CAN bus data acquisition. This study is not intended to be an exhaustive evaluation of all available CAN tools or their capabilities, rather it was intended to present information on several readily available hardware and software platforms and a method for creating a stand-alone custom application as a proof-of-concept for tractor CAN data analysis. The custom application developed will facilitate in-cab decision making for improved tractor management during field operations.}
}
@article{RAHMAN2018101,
title = {QALMA: A computational toolkit for the analysis of quality protocols for medical linear accelerators in radiation therapy},
journal = {SoftwareX},
volume = {7},
pages = {101-106},
year = {2018},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2018.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S2352711018300311},
author = {Md Mushfiqur Rahman and Yu Lei and Georgios Kalantzis},
keywords = {Star shot, Picket fence, Winston–Lutz, Light radiation coincidence, Log file analysis, Quality assurance},
abstract = {Quality Assurance (QA) for medical linear accelerator (linac) is one of the primary concerns in external beam radiation Therapy. Continued advancements in clinical accelerators and computer control technology make the QA procedures more complex and time consuming which often, adequate software accompanied with specific phantoms is required. To ameliorate that matter, we introduce QALMA (Quality Assurance for Linac with MATLAB), a MALAB toolkit which aims to simplify the quantitative analysis of QA for linac which includes Star-Shot analysis, Picket Fence test, Winston–Lutz test, Multileaf Collimator (MLC) log file analysis and verification of light & radiation field coincidence test.}
}
@incollection{BOURNE201441,
title = {Chapter 4 - Features Common to Many Applications},
editor = {Kelly C. Bourne},
booktitle = {Application Administrators Handbook},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {41-62},
year = {2014},
isbn = {978-0-12-398545-3},
doi = {https://doi.org/10.1016/B978-0-12-398545-3.00004-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780123985453000042},
author = {Kelly C. Bourne},
keywords = {COTS, modules, customizations, configuration changes, user preferences, data types, databases, imports, admin consoles, log files, breadcrumbs, dashboards, workflow, cookies},
abstract = {This chapter describes features that are common to many third-party (COTS—commercial off the shelf software) applications. Some of these features are reporting, user preferences, data, methods of importing data, application administration tools, log files, navigation, menus, breadcrumbs, dashboards, and workflow.}
}
@article{LI201018,
title = {Multi-faceted quality and defect measurement for web software and source contents},
journal = {Journal of Systems and Software},
volume = {83},
number = {1},
pages = {18-28},
year = {2010},
note = {SI: Top Scholars},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2009.04.055},
url = {https://www.sciencedirect.com/science/article/pii/S0164121209001149},
author = {Zhao Li and Nasser Alaeddine and Jeff Tian},
keywords = {Quality and reliability, Defect measurement and analysis, Web software and contents, Dynamic web applications, Web application development and maintenance},
abstract = {In this paper, we examine external failures and internal faults traceable to web software and source contents. We develop related defect and quality measurements based on different perspectives of customers, users, information or service hosts, maintainers, developers, integrators, and managers. These measurements can help web information and service providers with their quality assessment and improvement activities to meet the quality expectations of their customers and users. The different usages of our measurement framework by different stakeholders of web sites and web applications are also outlined and discussed. The data sources include existing web server logs and statistics reports, defect repositories from web application development and maintenance activities, and source files. We applied our approach to four diverse websites: one educational website, one open source software project website, one online catalog showroom for a small company, and one e-Commerce website for a large company. The results demonstrated the viability and effectiveness of our approach.}
}
@article{SANGA2019849,
title = {SlimDna – An in-house expert system for STR profiling},
journal = {Forensic Science International: Genetics Supplement Series},
volume = {7},
number = {1},
pages = {849-850},
year = {2019},
note = {The 28th Congress of the International Society for Forensic Genetics},
issn = {1875-1768},
doi = {https://doi.org/10.1016/j.fsigss.2019.10.200},
url = {https://www.sciencedirect.com/science/article/pii/S1875176819303609},
author = {Malin Sanga and Linus Ek and Samuel Boiso},
keywords = {DNA analysis, Expert system, LIMS},
abstract = {At NFC the internally developed software for DNA analysis, SlimDna is used. It is complementary to the LIMS and keeps track of samples, sample batches, instrument files and generated data. The system communicates with the LIMS, instruments and other software. SlimDna includes four modules (Analysis, Control, Administration and Frequency calculation) and covers the entire analysis process from DNA extraction to reporting approved STR profiles to the LIMS. SlimDna is built with Microsoft Internet Information Services (IIS) as application server and the user interface consists of Windows clients. For data storage MySQL is used. The software has been implemented in different packages, STR typing being the latest. The system has streamlined the STR typing by providing, among other functions, automated evaluation of STR profiles, databases for observed variant alleles as well as tracking and logging of negative and positive controls. STR electropherograms from analysis of crime scene samples (around 30 000 per year) are evaluated by SlimDna according to set rules concerning for example stutter ratio, heterozygote balance and number of alleles per marker in order to define the profile as a single source genotype or a mixture. The SlimDna output is compared to a manual evaluation before the results are reported to the LIMS. This process saves nearly one minute hands-on time per sample compared to the previous Excel-based solution. For reference samples, SlimDna evaluates the majority (55%) of the samples without manual revision. SlimDna’s automated interpretation has proven to be very accurate and will soon replace more of the manual revisions. In the near future, we aim to develop SlimDna further, with for example a mixture module supporting the reporting officers in analysis.}
}
@article{HASSINE201878,
title = {A framework for the recovery and visualization of system availability scenarios from execution traces},
journal = {Information and Software Technology},
volume = {96},
pages = {78-93},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917303166},
author = {Jameleddine Hassine and Abdelwahab Hamou-Lhadj and Luay Alawneh},
keywords = {Non-functional requirements, Dynamic analysis, Availability, Use case maps, Log filtering, Log segmentation},
abstract = {Context
Dynamic analysis is typically concerned with the analysis of system functional aspects at run time. However, less work has been devoted to the dynamic analysis of software quality attributes. The recovery of availability scenarios from system execution traces is particularly important for critical systems to verify that the running implementation supports and complies with availability requirements, especially if the source code is not available (e.g., in legacy systems) and after the system has undergone several ad-hoc maintenance tasks.
Objective
Propose a dynamic analysis approach, along with tool support, to recover availability scenarios, from system execution traces running high availability features.
Method
Native execution traces, collected from systems running high availability features, are pre-processed, filtered, merged, and segmented into execution phases. The segmented scenarios are then visualized, at a high level of abstraction, using the ITU-T standard Use Case Maps (UCM) language extended with availability annotations.
Results
The applicability of our proposed approach has been demonstrated by implementing it as a prototype feature within the jUCMNav tool and by applying it to four real-world systems running high availability features. Furthermore, we have conducted an empirical study to prove that resulting UCM models improve the understandability of log files that contain high availability features.
Conclusion
We have proposed a framework to filter, merge, segment, and visualize native log traces. The framework presents the following benefits: (1) it offers analysts the flexibility to specify what to include/exclude from an execution trace, (2) it provides a log segmentation method based on the type of information reported in the execution trace, (3) it uses the UCM language to visually describe availability scenarios at a high level of abstraction, and (4) it offers a scalable solution for the visualization problem through the use of the UCM stub-plug-in concept.}
}
@article{MUHAMMAD201714,
title = {Visualizing trace of Java collection APIs by dynamic bytecode instrumentation},
journal = {Journal of Visual Languages & Computing},
volume = {43},
pages = {14-29},
year = {2017},
issn = {1045-926X},
doi = {https://doi.org/10.1016/j.jvlc.2017.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X17300770},
author = {Tufail Muhammad and Zahid Halim and Majid Ali Khan},
keywords = {Software visualization tool, Program comprehension, Treemap, Java API usage, Dynamic analysis},
abstract = {Object-oriented languages are widely used in software development to help the developer in using dynamic data structures which evolve during program execution. However, the task of program comprehension and performance analysis necessitates the understanding of data structures used in a program. Particularly, in understanding which application programming interface (API) objects are used during runtime of a program. The objective of this work is to give a compact view of the complete program code information at a single glance and to provide the user with an interactive environment to explore details of a given program. This work presents a novel interactive visualization tool for collection framework usage, in a Java program, based on hierarchical treemap. A given program is instrumented during execution time and data recorded into a log file. The log file is then converted to extensible markup language (XML)-based tree format which proceeds to the visualization component. The visualization provides a global view to the usage of collection API objects at different locations during program execution. We conduct an empirical study to evaluate the impact of the proposed visualization in program comprehension. The experimental group (having the proposed tool support), on average, completes the tasks in 45% less time as compared to the control group (not provided with the proposed tool). Results show that the proposed tool enables to comprehend more information with less effort and time. We have also evaluated the performance of the proposed tool using 20 benchmark software tools. The proposed tool is anticipated to help the developer in understanding Java programs and assist in program comprehension and maintenance by identifying APIs usage and their patterns.}
}
@article{HADEM2021108015,
title = {An SDN-based Intrusion Detection System using SVM with Selective Logging for IP Traceback},
journal = {Computer Networks},
volume = {191},
pages = {108015},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108015},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621001274},
author = {Pynbianglut Hadem and Dilip Kumar Saikia and Soumen Moulik},
keywords = {Network security, Software-defined network, Intrusion detection, Machine learning, IP traceback, SDN, SVM, IDS, ML},
abstract = {In this paper we introduce a Software Defined Networking (SDN) based Intrusion Detection System (IDS) using the Support Vector Machines (SVM) along with Selective Logging for IP Traceback. We achieve a detection accuracy of 95.98% on the full NSL-KDD dataset and 87.74% on the selected sub-features of the dataset. Detection of anomalous traffic and network intrusion is done during the PACKET_IN event at the controller and then again by fetching the flow statistics from the OpenFlow switches at regular intervals. Selective logging of suspicious packets/flows during a PACKET_IN event enables an IP traceback to be performed in the eventuality of an attack which can be initiated by a network admin using an HTTP-based web console. This approach gains significance given that it is practically impossible to achieve 100% attack detection accuracy. Moreover, it is not always correct to take punitive action against packets of a traffic flow, solely based on a detection of a possible threat which may result in blocking or dropping of genuine packets. In the proposed scheme, logging is performed selectively at the controller and not at the switches, achieving significant savings in terms of overall memory resources. Moreover logging is performed using the in-memory structure at the controller thereby enhances the performance of the logging operation over traditional file-based database by 9.76%. Finally, we have chosen this approach because (i) SDN provides a centralized architecture for detection analysis and logging (ii) SVM provides decent detection accuracy without much computation overhead (iii) Selective Logging provides about 90% to 95% savings in terms of overall memory resources and (iv) IP traceback provides the ability to track the actual source of the packets in the eventuality of an attack.}
}
@article{YODER2018184,
title = {Characterization of topology optimized Ti-6Al-4V components using electron beam powder bed fusion},
journal = {Additive Manufacturing},
volume = {19},
pages = {184-196},
year = {2018},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2017.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2214860417302348},
author = {S. Yoder and S. Morgan and C. Kinzy and E. Barnes and M. Kirka and V. Paquit and P. Nandwana and A Plotkowski and R.R. Dehoff and S.S. Babu},
keywords = {Additive manufacturing, Electron beam powder bed fusion, Ti64, Topology optimization},
abstract = {The use of manufacturing to generate topology optimized components shows promise for designers. However, designers who assume that additive manufacturing follows traditional manufacturing techniques may be misled due to the nuances in specific techniques. Since commercial topology optimization software tools are neither designed to consider orientation of the parts nor large variations in properties, the goal of this research is to evaluate the limitations of an existing commercial topology optimization software (i.e. Inspire®) using electron beam powder bed fusion (i.e. Arcam®) to produce optimized Ti-6Al-4V alloy components. Emerging qualification tools from Oak Ridge National Laboratory including in-situ near-infrared imaging and log file data analysis were used to rationalize the final performance of components. While the weight savings of each optimized part exceeded the initial criteria, the failure loads and locations proved instrumental in providing insight to additive manufacturing with topology optimization. This research has shown the need for a comprehensive understanding of correlations between geometry, additive manufacturing processing conditions, defect generation, and microstructure for characterization of complex components such as those designed by topology optimization.}
}
@article{VIDAL2016160,
title = {Recompiling learning processes from event logs},
journal = {Knowledge-Based Systems},
volume = {100},
pages = {160-174},
year = {2016},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2016.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0950705116001167},
author = {Juan C. Vidal and Borja Vázquez-Barreiros and Manuel Lama and Manuel Mucientes},
keywords = {Learning flows discovery, Process mining, Adaptive rules mining, IMS Learning Design},
abstract = {In this paper a novel approach to reuse units of learning (UoLs) – such as courses, seminars, workshops, and so on – is presented. Virtual learning environments (VLEs) do not usually provide the tools to export in a standardized format the designed UoLs, making thus more challenging their reuse in a different platform. Taking into account that many of these VLEs are legacy or proprietary systems, the implementation of a specific software is usually out of place. However, these systems have in common that they record the events of students and teachers during the learning process. The approach presented in this paper makes use of these logs (i) to extract the learning flow structure using process mining, and (ii) to obtain the underlying rules that control the adaptive learning of students by means of decision tree learning. Finally, (iii) the process structure and the adaptive rules are recompiled in IMS Learning Design (IMS LD) – the de facto educational modeling language standard. The three steps of our approach have been validated with UoLs from different domains.}
}
@article{NICULESCU2010238,
title = {Forward modeling of ultra-long spacing electrical logs},
journal = {Journal of Petroleum Science and Engineering},
volume = {73},
number = {3},
pages = {238-247},
year = {2010},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2010.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0920410510001415},
author = {Bogdan Mihai Niculescu},
keywords = {ULSEL, Forward modeling, Blowout well, Relief well},
abstract = {Ultra-Long Spacing Electrical Logs (ULSEL) are Normal type logging devices with very large electrode spacings, used to investigate resistive or conductive anomalies located far away from a borehole. Such kind of electrical investigations are also used to control the drilling of relief wells, by estimating the distance between them and a cased blowout well, in order to intercept it and terminate the eruption. In this respect, the conductive casing in the target well affects the ULSEL apparent resistivity response recorded in a relief well if the distance between the wells is comparable with the investigation radius of the logging device. Qualitative and quantitative interpretations of ULSEL results, such as trajectory corrections and distance estimations, may be accomplished by comparing the apparent resistivity log actually recorded with the theoretical response of a multilayered geoelectrical model, which is representative for the formations crossed by the borehole. The paper presents a forward modeling algorithm and software that can be used to simulate this response, not only for ULSEL but also for conventional investigation devices, such as the Normal (Potential) and Lateral (Gradient) electrode arrays. The simulation is carried out by solving in cylindrical coordinates the Laplace differential equation for the electrical field's potentials, using a matrix representation and a numerical solution for the fundamental system of boundary conditions.}
}
@article{CHEN20169,
title = {A time-driven and event-driven approach for substation feeder incident analysis},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {74},
pages = {9-15},
year = {2016},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2015.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S0142061515003038},
author = {Chao-Rong Chen and Chi-Juin Chang and Cheng-Hung Lee},
keywords = {Time-driven, Event-driven, SCADA, Intelligent alarm processor},
abstract = {Feeder incident information is quite useful for evaluating the quality of the electricity distribution system. The improving scheme can be taken based on this information as well. In this paper, an adequate way for gathering this information was developed and the related messages from SCADA historical event log were analyzed by a novel methodology. For correctly and rapidly analyzing the collected messages purpose, this paper developed a time-driven and event-driven approach in order to express the exact feeder incident affairs. The feeder fault information can be fetched precisely and efficiently according to the result of this paper. Furthermore, the outage duration of the fault feeder, excluding the load transferring condition, was computed as well. For the sake of cyber security purpose, this paper developed a low cost and secured file transfer mechanism for transferring the event log from the SCADA system to an intranet web server. Corresponding to the web-based application software based on the result of this paper was consequently developed and installed on the sever of information management department of Taiwan Power Company (Taipower) for all SCADA control centers, and the software is running successfully.}
}
@article{JIANG201990,
title = {TreeRing: A GameSafe parser for z-Tree},
journal = {Journal of Behavioral and Experimental Finance},
volume = {22},
pages = {90-92},
year = {2019},
issn = {2214-6350},
doi = {https://doi.org/10.1016/j.jbef.2019.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S2214635018302624},
author = {Ming Jiang and Jingchao Li},
keywords = {Experimental economics, Software, z-Tree},
abstract = {This article presents a software that parses the server–clientcommunication log files (called GameSafe) generated by the experimental economics software z-Tree (Fischbacher, 2007). The program parses the file and writes to a spreadsheet that includes the experimental parameters and a complete timeline of subjects’ decisions. It can be used to recover data from an experimental session in case of a computer crash, as well as to reconstruct experimental datasets any point in time during the experiment even if variables are overwritten later.}
}
@article{KALAMATIANOS20171,
title = {Distributed analysis and filtering of application event streams},
journal = {Journal of Systems and Software},
volume = {129},
pages = {1-25},
year = {2017},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.03.057},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217300687},
author = {Theodoros Kalamatianos and Kostas Kontogiannis},
keywords = {Software engineering, System understanding, Dynamic analysis, Event stream filtering, Information theory},
abstract = {Large information systems comprise different interconnected hardware and software components, that collectively generate large volumes of data. Furthermore, the run-time analysis of such data involves computationally expensive algorithms, and is pivotal to a number of software engineering activities such as, system understanding, diagnostics, and root cause analysis. In a quest to increase the performance of run-time analysis for large sets of logged data, we present an approach that allows for the real time reduction of one or more event streams by utilizing a set of filtering criteria. More specifically, the approach employs a similarity measure that is based on information theory principles, and is applied between the features of the incoming events, and the features of a set of retrieved or constructed events, that we refer to as beacons. The proposed approach is domain and event schema agnostic, can handle infinite data streams using a caching algorithm, and can be parallelized in order to tractably process high volume, high frequency, and high variability data. Experimental results obtained using the KDD’99 and CTU-13 labeled data sets, indicate that the approach is scalable, and can yield highly reduced sets with high recall values with respect to a use case.}
}
@incollection{BOLT201191,
title = {Chapter 6 - Xbox 360–Specific File Types},
editor = {Steven Bolt},
booktitle = {XBOX 360 Forensics},
publisher = {Syngress},
address = {Boston},
pages = {91-103},
year = {2011},
isbn = {978-1-59749-623-0},
doi = {https://doi.org/10.1016/B978-1-59749-623-0.00006-1},
url = {https://www.sciencedirect.com/science/article/pii/B9781597496230000061},
author = {Steven Bolt},
keywords = {CON, PIRS, LIVE},
abstract = {Publisher Summary
The XBOX 360 console is provided through downloads through the XBOX Live service. There are three types or general classifications of downloads that are pushed to the machine during the course of online activities. These classifications can either be system software updates, game updates, including map updates, or XBOX firmware updates. There is a great deal of data that is present on the hard drive of the XBOX 360 upon shipping. This information includes preloaded videos, games, and trailers for both movies and games, as well as a wide variety of text messages that are streamed to the end user, during game play or as part of the log-in process to the XBOX Live service. The three XBOX 360–specific file types are the CON, PIRS, and LIVE files. Each one of these files populates the drive through specific actions that an end user must initiate in order to obtain the files. Research indicates that the PIRS files are files that are signed and delivered by Microsoft but not delivered through the XBOX Live services, including such items as system updates.}
}
@article{AHUJA2021103108,
title = {Automated DDOS attack detection in software defined networking},
journal = {Journal of Network and Computer Applications},
volume = {187},
pages = {103108},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103108},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521001296},
author = {Nisha Ahuja and Gaurav Singal and Debajyoti Mukhopadhyay and Neeraj Kumar},
keywords = {Machine learning, Software-defined-networking, DDOS Attack detection, Traffic protocol, SDN Traffic classification, Mininet},
abstract = {Software-Defined Networking (SDN) is a networking paradigm that has redefined the term network by making the network devices programmable. SDN helps network engineers to monitor the network expeditely, control the network from a central point, identify malicious traffic and link failure in easy and efficient manner. Besides such flexibility provided by SDN, it is also vulnerable to attacks such as DDoS which can halt the complete network. To mitigate this attack, the paper proposes to classify the benign traffic from DDoS attack traffic by using machine learning technique. The major contribution of this paper is identification of novel features for DDoS attack detections. Novel features are logged into CSV file to create the dataset and machine learning algorithms are trained on the created SDN dataset. Various work which has already been done for DDoS attack detection either used a non-SDN dataset or the research data is not made public. A novel hybrid machine learning model is utilized to perform the classification. Results show that the hybrid model of Support Vector classifier with Random Forest (SVC-RF) classifies the traffic with the highest testing accuracy of 98.8% with a very low false alarm rate.}
}
@article{PISSARENKO20101766,
title = {3D finite-difference synthetic acoustic log in cylindrical coordinates},
journal = {Journal of Computational and Applied Mathematics},
volume = {234},
number = {6},
pages = {1766-1772},
year = {2010},
note = {Eighth International Conference on Mathematical and Numerical Aspects of Waves (Waves 2007)},
issn = {0377-0427},
doi = {https://doi.org/10.1016/j.cam.2009.08.026},
url = {https://www.sciencedirect.com/science/article/pii/S0377042709005019},
author = {D. Pissarenko and G. Reshetova and V. Tcheverda},
keywords = {Sonic logging, Staggered grid, Azimuth refinement, Grid coupling, Perfectly matched layer, Tube waves, Parallel implementation},
abstract = {A finite-difference method of numerical simulation of sonic logging has been developed and implemented. The very general statement is considered: the surrounding medium is allowed to be 3D heterogeneous and a source can be located at any point inside or outside the well. To provide a maximally precise description of the sharpest interface of the problem, the interface between the well and surrounding formations, we use cylindrical coordinates with the axis directed along the well. In order to avoid an excessive azimuth inflation of grid cells with an increase of radius we perform periodical refinement of the grid step in the azimuth direction. In order to truncate the area of computations, Perfectly Matched Layer (PML) for cylindrical coordinates is developed and implemented. Its main advantage in comparison with other approaches is an extremely low level of artificial reflections and the absence of necessity to perform splitting of variables in the azimuth direction. Based on this numerical method, a software oriented for the use of parallel computations is developed and implemented under Message Passing Interface Library. Results of numerical experiments for a well with completion embedded within layered elastic background with a vertical planar crack are presented and discussed.}
}
@article{KHOVRICHEV2019134,
title = {Intelligent Approach for Heterogeneous Data Integration: Information Processes Analysis Engine in Clinical Remote Monitoring Systems},
journal = {Procedia Computer Science},
volume = {156},
pages = {134-141},
year = {2019},
note = {8th International Young Scientists Conference on Computational Science, YSC2019, 24-28 June 2019, Heraklion, Greece},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.188},
url = {https://www.sciencedirect.com/science/article/pii/S187705091931107X},
author = {Mikhail Khovrichev and Liubov Elkhovskaya and Vladimir Fonin and Marina Balakhontceva},
keywords = {intelligent integration, remote monitoring, heterogeneous data, value-based healthcare},
abstract = {The paper presents a research project which aimed to design and develop an intelligent approach for the integration of heterogeneous data and knowledge sources in personalized healthcare. The integration profile we propose is mainly focused on remote monitoring of patients with arterial hypertension and chronic heart failure. This clarifies main temporal data used by event detection systems and limits it to systolic/diastolic pressure and heart rate. Our aim is to design and propose software component for such a system which makes it possible to reconstruct the whole remote monitoring and treatment process by connecting event logs. The analysis of the reconstructed process done by comparing with an expert system which contains different rules for monitoring/treatment process optimization.}
}
@article{TAK2018330,
title = {Conceptual design of new data integration and process system for KSTAR data scheduling},
journal = {Fusion Engineering and Design},
volume = {129},
pages = {330-333},
year = {2018},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2018.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S0920379618300334},
author = {Taehyun Tak and Jaesic Hong and Kaprai Park and Woongryul Lee and Taegu Lee and Hyunsun Han and Giil Kwon and Jinseop Park},
keywords = {KSTAR, Fusion, Dataflow, Automation, Many-task, Big Data},
abstract = {The KSTAR control and data acquisition systems mainly use data storage layer of MDSPlus for diagnostic data and channel archiver for EPICS-based control system data. In addition to these storage systems, KSTAR has various types of data such as user logs from Relational Database (RDB) and various types of logs from the control system. A large scientific machine like KSTAR is needed to implement various types of use cases for scheduling data and data analysis. The goal of a new data integration and process system is to design the KSTAR data scheduling on top of the Pulse Automation and Scheduling System (PASS) according to KSTAR events. The KSTAR Data Integration System (KDIS) is designed by using Big Data software infrastructures and frameworks. The KDIS handles events that are synchronized with the KSTAR EPICS events and other data sources such as the rest API and logs for integrating and processing data from different data sources and for visualizing data. In this paper, we explain the detailed design concept of KDIS and demonstrate a data scheduling use case with this system.}
}
@incollection{AGAPITO2019176,
title = {Computing Languages for Bioinformatics: Perl},
editor = {Shoba Ranganathan and Michael Gribskov and Kenta Nakai and Christian Schönbach},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology},
publisher = {Academic Press},
address = {Oxford},
pages = {176-186},
year = {2019},
isbn = {978-0-12-811432-2},
doi = {https://doi.org/10.1016/B978-0-12-809633-8.20364-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128096338203646},
author = {Giuseppe Agapito},
keywords = {Bioinformatics, Computing language, Object‐oriented programming, Perl, Perl language, Programming Language, Scripting},
abstract = {Perl is a general-purpose scripting language introduced by Larry Wall in 1987. The aim of Perl was to connect different languages and tools together by making compatible the various data format between them. Perl is still broadly used for its original purpose: working like mediator among the different tools, making data coming from one software in a format compatible with the format expected by the other tool. Going from processing and summarizing system logs, through manipulating databases, reformatting text files, and simple search-and-replace operations, as well as in handling data from the Human Genome Project, a task requiring massive amounts of data manipulation. The key points of Perl’s popularity are (i) regular expressions handling through the regular expression engine. (ii) The flexibility, Perl provides only three basic variable types: Scalars, Arrays, and Hashes. Finally, (iii) the portability. Perl works well on several operating systems, as well as on the web.}
}
@article{CHUAH201995,
title = {Towards comprehensive dependability-driven resource use and message log-analysis for HPC systems diagnosis},
journal = {Journal of Parallel and Distributed Computing},
volume = {132},
pages = {95-112},
year = {2019},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2019.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0743731519304137},
author = {Edward Chuah and Arshad Jhumka and Samantha Alt and Daniel Balouek-Thomert and James C. Browne and Manish Parashar},
keywords = {Large HPC systems, Correlation, Variance extraction, Error propagation and recovery, Cluster log-data},
abstract = {Failure analysis plays an important role in the reliability of data centers and high-performance computing (HPC) systems. Recent work have shown that both resource use data and failure logs can, separately and together, be used to detect system failure-inducing errors and diagnose system failures; the result of error propagation and (unsuccessful) execution of error recovery mechanisms. For more accurate and detailed failure diagnosis, knowledge of error propagation patterns and unsuccessful error recovery is important. To improve system reliability, knowledge of recovery protocols deployment is important. This paper describes and demonstrates application of a new diagnostics framework (CORRMEXT). CORRMEXT analyzes and reports error propagation patterns and degrees of success and failure of error recovery protocols. The steps in the framework are correlations of resource use metrics and error messages, and identification of the earliest times of change of system behaviour. The framework is illustrated with analyses of resource use data and message logs for three HPC systems operated by the Texas Advanced Computing Center (TACC). The illustrations are focused on groups of resource use counters and groups of errors; they reveal many interesting insights into patterns of: (i) network data and software errors, (ii) Lustre file-system and Linux operating system process errors, and (iii) memory and storage errors. We also confirm that: (i) correlations of resource use and errors can only be identified by applying different correlation algorithms, and (ii) the earliest times of change in system behaviour can only be identified by analyzing both the correlated resource use counters and correlated errors. We believe CORRMEXT is the first tool that have diagnosed error propagation paths and error recovery attempts on three different HPC systems. CORRMEXT will be put on the public domain to support systems administrators in diagnosing HPC system failures, on August 2018.}
}
@article{YARMOHAMMADI201717,
title = {Mining implicit 3D modeling patterns from unstructured temporal BIM log text data},
journal = {Automation in Construction},
volume = {81},
pages = {17-24},
year = {2017},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2017.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S0926580516302825},
author = {Saman Yarmohammadi and Reza Pourabolghasem and Daniel Castro-Lacouture},
keywords = {Building information modeling, Data mining, Sequential pattern analysis, Design log files, Performance management},
abstract = {Building information modeling is instrumental in documenting design, enhancing customer experience, and improving product functionality in capital projects. However, good building models do not happen by accident, but rather as a result of a managed process that involves several participants from different disciplines and backgrounds. Effective management of this process requires an ability to closely monitor the modeling process and correctly measure modelers' performance. Nevertheless, existing methods of performance monitoring in building design practices lack an objective measurement system to quantify modeling progress. The widespread utilization of Building Information Modeling (BIM) tools presents a unique opportunity to retrieve granular design process data and conduct accurate performance measurements. As a building's 3D model is gradually developed, model generation software packages, such as Autodesk Revit, automatically create log files that record design activities. This paper investigates what information these log files contain and how one can extract and further analyze the information to provide insight into the design modeling process. The specific objectives of this study were to: (1) investigate the presence of implicit patterns in 3-D design log files; and (2) to empirically characterize the performance of modelers based on the time it takes them to execute similar modeling tasks. To fulfill these objectives, design log files provided by an international architecture and design firm were analyzed. Using a tailored text file parser, user-model interaction data including modeler characteristics, command type, and command time were extracted from the journal files. To identify implicit command execution patterns, a sequence mining algorithm based on Generalized Suffix Trees (GST) was implemented. It was shown that there is a statistically significant difference between the average time it takes modelers to execute each command sequence. This study extends the existing knowledge by proposing a novel methodology to extract meaningful patterns from time-stamped unstructured design log data. This research contributes to the state of practice by providing a better understanding of information embedded in design log files.}
}
@article{CONSOLO20191077,
title = {Log files analysis and evaluation of circadian patterns for the early diagnosis of pump thrombosis with a centrifugal continuous-flow left ventricular assist device},
journal = {The Journal of Heart and Lung Transplantation},
volume = {38},
number = {10},
pages = {1077-1086},
year = {2019},
issn = {1053-2498},
doi = {https://doi.org/10.1016/j.healun.2019.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S1053249819314767},
author = {Filippo Consolo and Federico Esposti and Adrian Gustar and Michele {De Bonis} and Federico Pappalardo},
keywords = {left ventricular assist device, pump thrombosis, log files, circadian rhythm, time-frequency analysis},
abstract = {BACKGROUND
No clinical standardized methods exist to identify the early stage of the development of pump thrombosis in the setting of HVAD (Medtronic Inc., USA) implantation. We aimed at developing a clinically relevant tool to evaluate HVAD operation during long-term support and at identifying a new reliable marker for the early diagnosis of pump thrombosis reflecting altered patient-pump physiological interplay.
METHODS
We developed a novel algorithm based on time-frequency analysis of the HVAD log files allowing the detection of the intrinsic circadian rhythmicity of the pump power consumption. With this tool, we retrospectively evaluated (1) post-operative restoration of circadian rhythm (n = 14 patients), (2) long-term stability of circadian rhythmicity in patients with no reported adverse events (n = 12), and (3) alteration of circadian fluctuations in patients who suffered from pump thrombosis (n = 19).
RESULTS
We demonstrate (1) progressive development of circadian rhythm following post-operative recovery (93% of the patients, 23 ± 15 days after implantation), (2) long-term stability of circadian rhythmicity in patients with no thrombotic complications (92% of the patients; 962 (445–1447) days of support), and (3) severe instability and loss of circadian fluctuations before the thrombotic event (89% of the patients, 12 ± 6 days ahead of the clinical manifestation of overt pump thrombosis). Furthermore, we provide the first clinical evidence of recovery of circadian rhythmicity following non-surgical resolution of pump thrombosis.
CONCLUSIONS
Time-frequency analysis of the HVAD log files provides a new tool for the early diagnosis of pump thrombosis. Loss of circadian rhythmicity might trigger medical evaluation, improving the results of medical management of pump thrombosis, and decreasing the need for pump exchange.}
}
@article{LI2017260,
title = {Software cybernetics in BPM: Modeling software behavior as feedback for evolution by a novel discovery method based on augmented event logs},
journal = {Journal of Systems and Software},
volume = {124},
pages = {260-273},
year = {2017},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2016.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S0164121216000844},
author = {Chuanyi Li and Jidong Ge and Liguo Huang and Haiyang Hu and Budan Wu and Hao Hu and Bin Luo},
keywords = {Software cybernetics, Process discovery, Petri nets},
abstract = {Business Process Management (BPM) is a quickly developing management theory in recent years. The goal of BPM is to improve corporate performance by managing and optimizing the businesses process in and among enterprises. The goal is easier to achieve with the closed-loop feedback mechanism from business process execution to redesign in BPM life cycle, where the business process itself and the set of activities in BPM are viewed as a controlled object and a controller respectively. In this feedback control system, process mining plays an important role in generating feedback of process execution for redesign. However, the existing discovery methods cannot mine certain special structures from execution logs (e.g., implicit dependency, implicit place and short loops) correctly and their mining efficiencies cannot meet the requirements of online process mining. In this paper, we propose a novel discovery method to overcome these challenges based on a kind of augmented event log that will also bring new research directions for process discovery. A case study is presented for introducing how the mined model can be used in business process evolution. Results of experiments are described to show the improvements of the proposed algorithm compared with others.}
}
@article{PUTZ2019101602,
title = {A secure and auditable logging infrastructure based on a permissioned blockchain},
journal = {Computers & Security},
volume = {87},
pages = {101602},
year = {2019},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2019.101602},
url = {https://www.sciencedirect.com/science/article/pii/S0167404818313907},
author = {Benedikt Putz and Florian Menges and Günther Pernul},
keywords = {Log management, Secure logging, Log auditing, Permissioned blockchain, Digital forensics},
abstract = {Information systems in organizations are regularly subject to cyber attacks targeting confidential data or threatening the availability of the infrastructure. In case of a successful attack it is crucial to maintain integrity of the evidence for later use in court. Existing solutions to preserve integrity of log records remain cost-intensive or hard to implement in practice. In this work we present a new infrastructure for log integrity preservation which does not depend upon trusted third parties or specialized hardware. The system uses a blockchain to store non-repudiable proofs of existence for all generated log records. An open-source prototype of the resulting log auditing service is developed and deployed, followed by a security and performance evaluation. The infrastructure represents a novel software-based solution to the secure logging problem, which unlike existing approaches does not rely on specialized hardware, trusted third parties or modifications to the logging source.}
}
@article{RABIN2013264,
title = {A new cryomacroscope device (Type III) for visualization of physical events in cryopreservation with applications to vitrification and synthetic ice modulators},
journal = {Cryobiology},
volume = {67},
number = {3},
pages = {264-273},
year = {2013},
issn = {0011-2240},
doi = {https://doi.org/10.1016/j.cryobiol.2013.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0011224013002010},
author = {Yoed Rabin and Michael J. Taylor and Justin S.G. Feig and Simona Baicu and Zhen Chen},
keywords = {Cryomacroscopy, Synthetic ice modulators, Vitrification, Crystallization, Thermal stress, DP6},
abstract = {The objective of the current study is to develop a new cryomacroscope prototype for the study of vitrification in large-size specimens. The unique contribution in the current study is in developing a cryomacroscope setup as an add-on device to a commercial controlled-rate cooler and in demonstration of physical events in cryoprotective cocktails containing synthetic ice modulators (SIM)—compounds which hinder ice crystal growth. Cryopreservation by vitrification is a highly complex application, where the likelihood of crystallization, fracture formation, degradation of the biomaterial quality, and other physical events are dependent not only upon the instantaneous cryogenic conditions, but more significantly upon the evolution of conditions along the cryogenic protocol. Nevertheless, cryopreservation success is most frequently assessed by evaluating the cryopreserved product at its end states—either at the cryogenic storage temperature or room temperature. The cryomacroscope is the only available device for visualization of large-size specimens along the thermal protocol, in an effort to correlate the quality of the cryopreserved product with physical events. Compared with earlier cryomacroscope prototypes, the new Cryomacroscope-III evaluated here benefits from a higher resolution color camera, improved illumination, digital recording capabilities, and high repeatability in tested thermal conditions via a commercial controlled-rate cooler. A specialized software package was developed in the current study, having two modes of operation: (a) experimentation mode to control the operation of the camera, record camera frames sequentially, log thermal data from sensors, and save case-specific information; and (b) post-processing mode to generate a compact file integrating images, elapsed time, and thermal data for each experiment. The benefits of the Cryomacroscope-III are demonstrated using various tested mixtures of SIMs with the cryoprotective cocktail DP6, which were found effective in preventing ice growth, even at significantly subcritical cooling rates with reference to the pure DP6.}
}
@article{GUTERRESMARMITT202049,
title = {Platform for automatic patient quality assurance via Monte Carlo simulations in proton therapy},
journal = {Physica Medica},
volume = {70},
pages = {49-57},
year = {2020},
issn = {1120-1797},
doi = {https://doi.org/10.1016/j.ejmp.2019.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S1120179719305393},
author = {G. {Guterres Marmitt} and A. Pin and K. {Ng Wei Siang} and G. Janssens and K. Souris and M. Cohilis and J.A. Langendijk and S. Both and A. Knopf and A. Meijers},
keywords = {Patient QA, Scanned proton therapy, Independent dose calculation, Treatment log files},
abstract = {For radiation therapy, it is crucial to ensure that the delivered dose matches the planned dose. Errors in the dose calculations done in the treatment planning system (TPS), treatment delivery errors, other software bugs or data corruption during transfer might lead to significant differences between predicted and delivered doses. As such, patient specific quality assurance (QA) of dose distributions, through experimental validation of individual fields, is necessary. These measurement based approaches, however, are performed with 2D detectors, with limited resolution and in a water phantom. Moreover, they are work intensive and often impose a bottleneck to treatment efficiency. In this work, we investigated the potential to replace measurement-based approach with a simulation-based patient specific QA using a Monte Carlo (MC) code as independent dose calculation engine in combination with treatment log files. Our developed QA platform is composed of a web interface, servers and computation scripts, and is capable to autonomously launch simulations, identify and report dosimetric inconsistencies. To validate the beam model of independent MC engine, in-water simulations of mono-energetic layers and 30 SOBP-type dose distributions were performed. Average Gamma passing ratio 99 ± 0.5% for criteria 2%/2 mm was observed. To demonstrate feasibility of the proposed approach, 10 clinical cases such as head and neck, intracranial indications and craniospinal axis, were retrospectively evaluated via the QA platform. The results obtained via QA platform were compared to QA results obtained by measurement-based approach. This comparison demonstrated consistency between the methods, while the proposed approach significantly reduced in-room time required for QA procedures.}
}
@article{CASTRO201963,
title = {Hardware Transactional Memory meets memory persistency},
journal = {Journal of Parallel and Distributed Computing},
volume = {130},
pages = {63-79},
year = {2019},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2019.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0743731518303952},
author = {Daniel Castro and Paolo Romano and João Barreto},
keywords = {Transaction, Memory, Persistent, Hardware, System},
abstract = {Persistent Memory (PM) and Hardware Transactional Memory (HTM) are two recent architectural developments whose joint usage promises to drastically accelerate the performance of concurrent, data-intensive applications. Unfortunately, combining these two mechanisms using existing architectural supports is far from being trivial. This paper presents NV-HTM, a system that allows the execution of transactions over PM using unmodified commodity HTM implementations. NV-HTM exploits a hardware–software co-design technique, which is based on three key ideas: (i) relying on software to persist transactional modifications after they have been committed via HTM; (ii) postponingthe externalization of commit events to applications until it is ensured, via software, that any data version produced and observed by committed transactions is first logged in PM; (ii) pruning the commit logs via checkpointing schemes that not only bound the log space and recovery time, but also implement wear leveling techniques to enhance PM’s endurance. By means of an extensive experimental evaluation, we show that NV-HTM can achieve up to 10× speed-ups and up to 11.6× reduced flush operations with respect to state of the art solutions, which, unlike NV-HTM, require custom modifications to existing HTM systems.}
}
@article{KATSUTA2016701,
title = {Quantification of residual dose estimation error on log file-based patient dose calculation},
journal = {Physica Medica},
volume = {32},
number = {5},
pages = {701-705},
year = {2016},
issn = {1120-1797},
doi = {https://doi.org/10.1016/j.ejmp.2016.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S1120179716300382},
author = {Yoshiyuki Katsuta and Noriyuki Kadoya and Yukio Fujita and Eiji Shimizu and Kenichi Matsunaga and Haruo Matsushita and Kazuhiro Majima and Keiichi Jingu},
keywords = {Radiotherapy, VMAT, Log file, Patient QA, DVH},
abstract = {Purpose
The log file-based patient dose estimation includes a residual dose estimation error caused by leaf miscalibration, which cannot be reflected on the estimated dose. The purpose of this study is to determine this residual dose estimation error.
Methods and materials
Modified log files for seven head-and-neck and prostate volumetric modulated arc therapy (VMAT) plans simulating leaf miscalibration were generated by shifting both leaf banks (systematic leaf gap errors: ±2.0, ±1.0, and ±0.5mm in opposite directions and systematic leaf shifts: ±1.0mm in the same direction) using MATLAB-based (MathWorks, Natick, MA) in-house software. The generated modified and non-modified log files were imported back into the treatment planning system and recalculated. Subsequently, the generalized equivalent uniform dose (gEUD) was quantified for the definition of the planning target volume (PTV) and organs at risks.
Results
For MLC leaves calibrated within ±0.5mm, the quantified residual dose estimation errors that obtained from the slope of the linear regression of gEUD changes between non- and modified log file doses per leaf gap are in head-and-neck plans 1.32±0.27% and 0.82±0.17Gy for PTV and spinal cord, respectively, and in prostate plans 1.22±0.36%, 0.95±0.14Gy, and 0.45±0.08Gy for PTV, rectum, and bladder, respectively.
Conclusions
In this work, we determine the residual dose estimation errors for VMAT delivery using the log file-based patient dose calculation according to the MLC calibration accuracy.}
}
@article{CHEN20101897,
title = {Multi-level scaling properties of instant-message communications},
journal = {Physics Procedia},
volume = {3},
number = {5},
pages = {1897-1905},
year = {2010},
note = {The International Conference on Complexity and Interdisciplinary Sciences. The 3rd China-Europe Summer School on Complexity Sciences},
issn = {1875-3892},
doi = {https://doi.org/10.1016/j.phpro.2010.07.034},
url = {https://www.sciencedirect.com/science/article/pii/S1875389210002798},
author = {Guanxiong Chen and Xiaopu Han and Binghong Wang},
keywords = {Inter-event distribution, Instant message communication, Software operation, non-Poisson property, Human dynamics},
abstract = {To research the statistical properties of human’s communication behaviors is one of the highlight areas of Human Dynamics. In this paper, we analyze the instant message data of QICQ from volunteers, and discover that there are many forms of non-Poisson characters, such as inter-event distributions of sending and receiving messages, communications between two friends, log-in activities, the distribution of online time, quantities of messages, and so on. These distributions not only denote the pattern of human communication activities, but also relate to the statistical property of human behaviors in using software. We find out that most of these exponents distribute between −1 and −2, which indicates that the Instant Message (IM) communication behavior of human is different from Non-IM communication behaviors; there are many fat-tail characters related to IM communication behavior.}
}
@article{MARX2016141,
title = {Comparing various hardware/software solutions and conversion methods for Controller Area Network (CAN) bus data collection},
journal = {Computers and Electronics in Agriculture},
volume = {128},
pages = {141-148},
year = {2016},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2016.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0168169916307281},
author = {Samuel E. Marx and Joe D. Luck and Santosh K. Pitla and Roger M. Hoy},
keywords = {Communication, SAE J1939, Field machinery, Digital data logging},
abstract = {Various hardware and software solutions exist for collecting Controller Area Network (CAN) bus data. Digital data accuracy could vary based upon different data logging methods (e.g., hardware/software timing, processor timing, etc.). CAN bus data were collected from agricultural tractors using multiple data acquisition solutions to quantify differences among collection methods and demonstrate potential data accumulation rates. Two types of data were observed for this study. The first, CAN bus frame data, represents data collected for each line of hex data sent from an ECU. One issue with frame data is the resulting large file sizes, therefore a second logging format collected was an averaged frame signal, or waveform dataset. Because of its smaller file size, waveform data could be more desirable for long periods of collection. Percent difference was calculated from two sets of frame data logs using different hardware/software combinations, and a frame data log was also compared to a waveform data log. The resulting difference was less than 0.0025 RPM for engine speed comparisons, zero for fuel rate and fuel temperature comparisons, and the mean percent difference was less than 0.08% between the methods of data collection. The error production could have resulted from noise in hardware and processor times, but was not found to increase as time progressed. This showed that even though errors existed between logging methods, the magnitude of errors would not negatively impact any practical agricultural field research applications. Thus, data logged by the different devices was similar and files requiring less memory would be desired. Selecting a waveform CAN bus data logging option would likely maintain digital data accuracy while reducing file storage and processing needs.}
}
@article{BORREGO201968,
title = {Towards a reduction in architectural knowledge vaporization during agile global software development},
journal = {Information and Software Technology},
volume = {112},
pages = {68-82},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919300898},
author = {Gilberto Borrego and Alberto L. Morán and Ramón R. Palacio and Aurora Vizcaíno and Félix O. García},
keywords = {Agile global software development, Architectural knowledge, Knowledge vaporization, Documentation debt},
abstract = {Context
The adoption of agile methods is a trend in global software development (GSD), but may result in many challenges. One important challenge is architectural knowledge (AK) management, since agile developers prefer sharing knowledge through face-to-face interactions, while in GSD the preferred manner is documents. Agile knowledge-sharing practices tend to predominate in GSD companies that practice agile development (AGSD), leading to a lack of documents, such as architectural designs, data models, deployment specifications, etc., resulting in the loss of AK over time, i.e., it vaporizes.
Objective
In a previous study, we found that there is important AK in the log files of unstructured textual electronic media (UTEM), such as instant messengers, emails, forums, etc., which are the preferred means employed in AGSD to contact remote teammates. The objective of this paper is to present and evaluate a proposal with which to recover AK from UTEM logs. We developed and evaluated a prototype that implements our proposal in order to determine its feasibility.
Method
The evaluation was performed by conducting a study with agile/global developers and students, who used the prototype and different UTEM to execute tasks that emulate common situations concerning AGSD teams’ lack of documentation during development phases.
Results
Our prototype was considered a useful, usable and unobtrusive tool when retrieving AK from UTEM logs. The participants also preferred our prototype when searching for AK and found AK faster with the prototype than with UTEM when the origin of the AK required was unknown.
Conclusion
The participants’ performance and perceptions when using our prototype provided evidence that our proposal could reduce AK vaporization in AGSD environments. These results encourage us to evaluate our proposal in a long-term test as future work.}
}
@article{HAOUARI2017843,
title = {Quality Assessment of an Emergency Care Process Model based on Static and Dynamic Metrics},
journal = {Procedia Computer Science},
volume = {121},
pages = {843-851},
year = {2017},
note = {CENTERIS 2017 - International Conference on ENTERprise Information Systems / ProjMAN 2017 - International Conference on Project MANagement / HCist 2017 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2017},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.11.109},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917323116},
author = {Ghada Haouari and Sonia Ayachi Ghannouchi},
keywords = {Emergency Healthcare Process, BPM, BPMN Model, Quality metrics},
abstract = {Healthcare domain is a vital domain in which many processes are dealt with. More precisely, in an emergency department, it is fundamental to have an emergency care process with high quality. In this paper, we propose a solution based on an improved BPM lifecycle for assessing and improving the quality of such a process, using quality metrics. These metrics are inspired from those of software engineering and are classified into two main categories: static metrics and dynamic metrics. Static metrics are applied to BPMN models and allow obtaining values related to complexity and coupling. Dynamic metrics concern the execution of BPMN models and obtain their values from log files. Our proposed approach has been applied for the emergency care process. For this purpose, we evaluated its quality in two steps (static then dynamic) and finally some recommendations have been given in order to allow the designer to improve our emergency care process model.}
}
@article{WANG2019105,
title = {On a new method of estimating shear wave velocity from conventional well logs},
journal = {Journal of Petroleum Science and Engineering},
volume = {180},
pages = {105-123},
year = {2019},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2019.05.033},
url = {https://www.sciencedirect.com/science/article/pii/S0920410519304875},
author = {Pan Wang and Suping Peng},
keywords = {Shear wave velocity, Extreme learning machine, Mean impact value, Artificial intelligence, Conventional well logs, Ordos basin},
abstract = {Shear wave velocity is a critical parameter for the characterization of hydrocarbon reservoirs. Compared with compressional wave velocity which almost exist in every well, shear wave velocity hasn't been recorded in those days for the older wells due to the lack of logging equipment or limited funds. Furthermore, measuring shear wave velocity is fairly time- and money-consuming as it can only be gained by the analysis of core samples conducted in the lab or from the dipole sonic imager (DSI). To cope with the above puzzles, a new methodology, by integrating extreme learning machines (ELM) and technique of mean impact value (MIV), is proposed in this paper with the support of log data collected from two wells located in unconventional shale gas reservoir in Ordos Basin, China. Based on mean impact value, a well-trained ELM model is taken to identify optimal well logs and five well logs are identified to provide the significant and valid information for the estimation of shear wave velocity. 3201 data points collected from well L4 are used in constructing the model. Compared with artificial neural network used in the former studies Levenberg-Marquardt algorithm (ANN-LM), the ELM model's prediction accuracy has been evaluated, the results indicates that the ELM model outperforms ANN-LM model with faster calculating speed and better performance. Additionally, the efficiency of the model proposed here is well investigated by using another well with 3201 data points. Through the comparison among ELM, support vector regression (SVR), convolutional neural network (CNN) and four widely known empirical formulas, what can be concluded is that ELM model is more efficient in fast calculation and high precision. From the result, it can be demonstrated that the use of ELM model, combined with the analysis of mean impact value (MIV), is a more efficient and promising method for shear wave velocity estimation process from conventional well log data, which can be recognized as a promising tool with an extended application. And this research can be applied into a software system for rapid acquisition of shear wave velocity logs.}
}
@article{WOLNIEWICZ2014211,
title = {SedMob: A mobile application for creating sedimentary logs in the field},
journal = {Computers & Geosciences},
volume = {66},
pages = {211-218},
year = {2014},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2014.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0098300414000296},
author = {Pawel Wolniewicz},
keywords = {Lithology, Log, Digital field notebook, Smartphone, Tablet, Android},
abstract = {SedMob is an open-source, mobile software package for creating sedimentary logs, targeted for use in tablets and smartphones. The user can create an unlimited number of logs, save data from each bed in the log as well as export and synchronize the data with a remote server. SedMob is designed as a mobile interface to SedLog: a free multiplatform package for drawing graphic logs that runs on PC computers. Data entered into SedMob are saved in the CSV file format, fully compatible with SedLog.}
}
@article{SETIAWAN2018944,
title = {Improved behavior model based on sequential rule mining},
journal = {Applied Soft Computing},
volume = {68},
pages = {944-960},
year = {2018},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2018.01.035},
url = {https://www.sciencedirect.com/science/article/pii/S1568494618300413},
author = {Feri Setiawan and Bernardo Nugroho Yahya},
keywords = {Relevant behavior model, Log filtering, Process mining, Sequential rule mining},
abstract = {The fourth industrial revolution leads the manufacturing companies to develop future and smart factories by merging automation and digitalization to result a more efficient production method. An evolutionary and competitive experimental approach is necessary to foster the innovation and the rapid change of the automation and digitalization. Consequently, software becomes an important component of industrial automation. One of the major challenge in Industry 4.0 is to industrialize the production of software. Software factory, as one industry with a virtual production line to produce software for manufacturing companies, offers a form of flexible employment, called as telecommuting work. Although this form brings many benefits for both employee and employers, some risks associated with telecommuting work exist. Monitoring the employee behavior is one of the employer way to see the accountability of the employee. Hence, understanding the human behavior during the production process would be an important issue for fulfilling overall operational excellence in software factory. Among approaches proposed to discover the human behavior based on the sequence activities, process mining is one of which has received attentions lately. While most recent process mining approaches in the domain of human behavior address process discovery and post-analysis, few of them have paid attentions on pre-analysis. The pre-analysis is one of the ways to produce a reliable and high-quality of event log which purposely impacts on discovering a daily common behavior and disregarding irregular sequential behavior. This study aims to propose a new way of pre-analysis using sequential rule mining. The key contributions of this research first, is to determine the potential local behaviors using sequential rule mining considering time constraint. Second, the local behaviors are used to enhance event log for discovering relevant behavior model. Third, the mined model is verified by performing conformance checking approach to check the conformity between the behavior model and the real logs based on three measurements: f-measure, ABA, and DMF. The resulting local behaviors, called as rules, can be used for guiding stakeholders to pinpoint the relevant behavior for human capital and productivity enhancement.}
}
@article{BELOUAH2020105015,
title = {Transcriptomic and proteomic data in developing tomato fruit},
journal = {Data in Brief},
volume = {28},
pages = {105015},
year = {2020},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2019.105015},
url = {https://www.sciencedirect.com/science/article/pii/S235234091931371X},
author = {Isma Belouah and Camille Bénard and Alisandra Denton and Mélisande Blein-Nicolas and Thierry Balliau and Emeline Teyssier and Philippe Gallusci and Olivier Bouchez and Björn Usadel and Michel Zivy and Yves Gibon and Sophie Colombié},
keywords = {Proteomics, Transcriptomics, Tomato fruit development, Pericarp, Time-series, Absolute quantification, Protein turnover},
abstract = {Transcriptomic and proteomic analyses were performed on three replicates of tomato fruit pericarp samples collected at nine developmental stages, each replicate resulting from the pooling of at least 15 fruits. For transcriptome analysis, Illumina-sequenced libraries were mapped on the tomato genome with the aim to obtain absolute quantification of mRNA abundance. To achieve this, spikes were added at the beginning of the RNA extraction procedure. From 34,725 possible transcripts identified in the tomato, 22,877 were quantified in at least one of the nine developmental stages. For the proteome analysis, label-free liquid chromatography coupled to tandem mass spectrometry (LC-MS/MS) was used. Peptide ions, and subsequently the proteins from which they were derived, were quantified by integrating the signal intensities obtained from extracted ion currents (XIC) with the MassChroQ software. Absolute concentrations of individual proteins were estimated for 2375 proteins by using a mixed effects model from log10-transformed intensities and normalized to the total protein content. Transcriptomics data are available via GEO repository with accession number GSE128739. The raw MS output files and identification data were deposited on-line using the PROTICdb database (http://moulon.inra.fr/protic/tomato_fruit_development) and MS proteomics data have also been deposited to the ProteomeXchange with the dataset identifier PXD012877. The main added value of these quantitative datasets is their use in a mathematical model to estimate protein turnover in developing tomato fruit.}
}
@article{SPINELLIS2012666,
title = {Organizational adoption of open source software},
journal = {Journal of Systems and Software},
volume = {85},
number = {3},
pages = {666-682},
year = {2012},
note = {Novel approaches in the design and implementation of systems/software architecture},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2011.09.037},
url = {https://www.sciencedirect.com/science/article/pii/S0164121211002512},
author = {Diomidis Spinellis and Vaggelis Giannikas},
keywords = {Open source software, Technology adoption, Industrial practice},
abstract = {Organizations and individuals can use open source software (OSS) for free, they can study its internal workings, and they can even fix it or modify it to make it suit their particular needs. These attributes make OSS an enticing technological choice for a company. Unfortunately, because most enterprises view technology as a proprietary differentiating element of their operation, little is known about the extent of OSS adoption in industry and the key drivers behind adoption decisions. In this article we examine factors and behaviors associated with the adoption of OSS and provide empirical findings through data gathered from the US Fortune-1000 companies. The data come from each company's web browsing and serving activities, gathered by sifting through more than 278 million web server log records and analyzing the results of thousands of network probes. We show that the adoption of OSS in large US companies is significant and is increasing over time through a low-churn transition, advancing from applications to platforms. Its adoption is a pragmatic decision influenced by network effects. It is likelier in larger organizations and those with many less productive employees, and is associated with IT and knowledge-intensive work and operating efficiencies.}
}
@article{FITCHETT20151,
title = {An empirical characterisation of file retrieval},
journal = {International Journal of Human-Computer Studies},
volume = {74},
pages = {1-13},
year = {2015},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2014.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S107158191400127X},
author = {Stephen Fitchett and Andy Cockburn},
keywords = {Personal information management, Retrieval, Files, Navigation, Search},
abstract = {Retrieving files on personal computers is a fundamental component of interaction, yet there is surprisingly little empirical data characterising how it is carried out in realistic settings. We developed software, called FileMonitor, to dynamically record users׳ file retrieval activities, including data describing the files retrieved and the tools used to retrieve them. We then deployed the system in a four week log study of 26 participants׳ actual file retrievals on their personal computers. Follow-up interviews contextualised the findings. Results are presented in two sections focusing on the files (the number of files, patterns of revisitation, file types, etc.) and on the interface mechanisms used to retrieve them (file browsers, search tools, ‘recent files’ lists, etc.). We conclude by discussing implications for the design of next-generation file retrieval interfaces.}
}
@article{CINQUE2019521,
title = {A framework for on-line timing error detection in software systems},
journal = {Future Generation Computer Systems},
volume = {90},
pages = {521-538},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.08.025},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18309609},
author = {Marcello Cinque and Domenico Cotroneo and Raffaele Della Corte and Antonio Pecchia},
keywords = {On-line monitoring, Timing errors, Error detection, Event logging, Critical information systems},
abstract = {On-line timing error detection entails gathering and analyzing monitoring data to pinpoint deviations from the expected timing behavior of a given software system. Current solutions for system monitoring and runtime analysis present several practical drawbacks that limit their usability in real industrial systems, such as the need of kernel-level probes or the coarse per-node/per-process monitoring granularity. This paper proposes a novel framework for timing error detection that capitalizes on the systematic interleaving of logging instructions across the functional code in order to overcome above limitations. The paper faces the practical challenges related to the specification and implementation of a log weaving technique, detection algorithms, and a data centralization platform to collect and analyze fine-grained execution traces in distributed systems. We experiment the proposed framework in two real-world critical information systems from the Crisis Management and the Air Traffic Control domains. Results show that our framework achieves 95% timing error coverage and allows reconstructing error trends with high statistical confidence at negligible performance overhead.}
}
@article{SHIMARI2021102630,
title = {NOD4J: Near-omniscient debugging tool for Java using size-limited execution trace},
journal = {Science of Computer Programming},
volume = {206},
pages = {102630},
year = {2021},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2021.102630},
url = {https://www.sciencedirect.com/science/article/pii/S016764232100023X},
author = {Kazumasa Shimari and Takashi Ishio and Tetsuya Kanda and Naoto Ishida and Katsuro Inoue},
keywords = {Dynamic analysis, Logging, Software visualization},
abstract = {Logging is an important feature of a software system to record run-time information. Detailed logging allows developers to collect run-time information in situations where they cannot use an interactive debugger, such as continuous integration and web application server cases. However, extensive logging leads to larger execution traces because few instructions can be repeated many times. This paper presents our tool NOD4J, which monitors a Java program's execution within limited storage space constraints and annotates the source code with observed values in an HTML format. Developers can easily investigate the execution and share the report on a web server. We show two examples that our tool can debug defects using incomplete execution traces.}
}
@article{AYDEMIR2019244,
title = {A Hybrid Process Mining Approach for Business Processes in Financial Organizations},
journal = {Procedia Computer Science},
volume = {158},
pages = {244-253},
year = {2019},
note = {3rd WORLD CONFERENCE ON TECHNOLOGY, INNOVATION AND ENTREPRENEURSHIP"INDUSTRY 4.0 FOCUSED INNOVATION, TECHNOLOGY, ENTREPRENEURSHIP AND MANUFACTURE" June 21-23, 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.048},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919312074},
author = {Fikri Aydemir and Yasar Ugur Pabuccu and Fatih Basciftci},
keywords = {Data Minig, Process Mining, Business Process Management, Event Logs, SQL, Apache Spark},
abstract = {Business process management is an integral part of many organizations in doing their day-to-day business operations considering that they help organizations transfer their business critical information from one organizational entity to another in the form of automated state transitions. The data (or event logs) that have been generated during these transitions hold valuable insights for improving organizational business processes, highlighting problem areas and visualizing the actual vs the formal procedures. The goal of this paper is to summarize the development of a hybrid analytical approach that utilizes both SQL and NO-SQL based back-end platforms in harmony in order to carry out process mining for a participation bank in Turkey. For this purpose, first we have developed a hybrid analytical software infrastructure that is backed by MS SQL Server and Hadoop platform components in order to discover key business processes of the organization based on event data. We then established a process mining framework that visualizes the process performance indicators and proposes workflow design changes and carries out statistical tests for identifying performance fluctuations by particularly using an in-memory parallel processing framework, named Apache Spark.}
}
@article{WANG2017101,
title = {ReSeer: Efficient search-based replay for multiprocessor virtual machines},
journal = {Journal of Systems and Software},
volume = {126},
pages = {101-112},
year = {2017},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2016.07.032},
url = {https://www.sciencedirect.com/science/article/pii/S0164121216301248},
author = {Tao Wang and Jiwei Xu and Wenbo Zhang and Jianhua Zhang and Jun Wei and Hua Zhong},
keywords = {Deterministic replay, Virtual machine, Genetic algorithm, Memory checkpoint, Xen},
abstract = {Efficient replay of virtual machines is important for software debugging, fault tolerance, and performance analysis. The current approaches of replaying virtual machines record the details of system execution at runtime. However, these approaches incur much overhead, which affects the system performance. Especially, in a multiprocessor system, recording the shared memory operations of multiple processors leads to a large amount of computing overhead and log files. To address the above issue, this paper proposes ReSeer—a search-based replay approach for multiprocessor virtual machines. ReSeer consists of three phases including record, search, and replay. In the record phase, we record only necessary non-deterministic events at runtime, and incrementally take memory checkpoints at a defined interval. In the search phase, we encode all the possible execution paths as binary strings, and use a genetic algorithm to search expected execution paths achieving the expected checkpoint. In the replay phase, we replay the system execution according to the searched execution paths and the logged non-deterministic events. Compared with current approaches, ReSeer significantly reduces performance overhead at runtime by searching expected execution paths instead of recording all the operations of accessing shared memory. We have implemented ReSeer, and then evaluated it with a seriesof typical benchmarks deployed on an open source virtual machine—Xen. The experimental results show that ReSeer can reduce the record overhead at runtime efficiently.}
}
@incollection{LAZAR2017329,
title = {Chapter 12 - Automated data collection methods},
editor = {Jonathan Lazar and Jinjuan Heidi Feng and Harry Hochheiser},
booktitle = {Research Methods in Human Computer Interaction (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {329-368},
year = {2017},
isbn = {978-0-12-805390-4},
doi = {https://doi.org/10.1016/B978-0-12-805390-4.00012-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128053904000121},
author = {Jonathan Lazar and Jinjuan Heidi Feng and Harry Hochheiser},
keywords = {Log files, Stored application data, Activity logging, Web proxy, Interaction recording, Instrumented software},
abstract = {HCI researchers rely heavily on software tools for data collection. Widely available general-purpose software tools such as web servers and proxies can track page requests, providing detailed logs of user activity, complete with timestamps. Applications ranging from GUI desktops to email programs, web browsers, and web tools leave detailed records of user interactions. Activity-logging tools can track keystrokes and mouse movements, both within web pages and more generally. Instrumented software tools add activity tracking facilities to existing software, while custom research tools support presentation of tasks and collection of data tuned to the needs of specific studies. Selection of appropriate techniques requires careful selection of data collection techniques appropriate for study goals and granularity of required data.}
}
@article{DAWSON2021100198,
title = {Challenges and opportunities for wearable IoT forensics: TomTom Spark 3 as a case study},
journal = {Forensic Science International: Reports},
volume = {3},
pages = {100198},
year = {2021},
issn = {2665-9107},
doi = {https://doi.org/10.1016/j.fsir.2021.100198},
url = {https://www.sciencedirect.com/science/article/pii/S2665910721000293},
author = {Liam Dawson and Alex Akinbi},
keywords = {IoT forensics, Mobile forensics, Android forensics, TomTom, TomTom Spark 3, Fitness tracker},
abstract = {Wearable IoT devices like fitness trackers and smartwatches continue to create opportunities and challenges for forensic investigators in the acquisition and analysis of evidential artefacts in scenarios where such devices are a witness to a crime. However, current commercial and traditional forensic tools available to forensic investigators fall short of conducting device extraction and analysis of forensic artefacts from many IoT devices due to their heterogeneous nature. In this paper, we conduct a comprehensive forensic analysis and show artefacts of forensic value from the physical TomTom Spark 3 GPS fitness smartwatch, its companion app installed on an Android smartphone, and Bluetooth event logs located in the app’s metadata. Our forensic methodology and analysis involved the combination and use of a non-forensic tool, a commercial forensic tool, and a non-forensic manufacturer-independent analysis platform tool specifically designed for endurance athletes to identify, extract, analyse, and reconstruct user activity data in an investigative scenario. We show forensic metadata associated with the device information, past user activities, and audio files from the physical smartwatch. We recovered data associated with past user activities stored in proprietary activity files and databases maintained by the app on an Android smartphone. From the event logs, we show when user activity was synced with the app and uploaded to the device cloud storage. The results from our work provide vital references for forensic investigators to aid criminal investigations, highlight limitations of current forensic tools, and for developers of forensic tools an incentive into developing forensic software applications and tools that can decode all relevant data generated by wearable IoT devices.}
}
@article{KNUDSEN20161013,
title = {Input data for inferring species distributions in Kyphosidae world-wide},
journal = {Data in Brief},
volume = {8},
pages = {1013-1017},
year = {2016},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2016.06.043},
url = {https://www.sciencedirect.com/science/article/pii/S2352340916304115},
author = {Steen Wilhelm Knudsen and Kendall D. Clements},
keywords = {Sea chub, Drummer, Kyphosus, Scorpis, Girella},
abstract = {Input data files for inferring the relationship among the family Kyphosidae, as presented in (Knudsen and Clements, 2016) [1], is here provided together with resulting topologies, to allow the reader to explore the topologies in detail. The input data files comprise seven nexus-files with sequence alignments of mtDNA and nDNA markers for performing Bayesian analysis. A matrix of recoded character states inferred from the morphology examined in museum specimens representing Dichistiidae, Girellidae, Kyphosidae, Microcanthidae and Scorpididae, is also provided, and can be used for performing a parsimonious analysis to infer the relationship among these perciform families. The nucleotide input data files comprise both multiple and single representatives of the various species to allow for inference of the relationship among the species in Kyphosidae and between the families closely related to Kyphosidae. The ‘.xml’-files with various constrained relationships among the families potentially closely related to Kyphosidae are also provided to allow the reader to rerun and explore the results from the stepping-stone analysis. The resulting topologies are supplied in newick-file formats together with input data files for Bayesian analysis, together with ‘.xml’-files. Re-running the input data files in the appropriate software, will enable the reader to examine log-files and tree-files themselves.}
}
@article{BAGHAPOUR20185,
title = {A computer-based approach for data analyzing in hospital’s health-care waste management sector by developing an index using consensus-based fuzzy multi-criteria group decision-making models},
journal = {International Journal of Medical Informatics},
volume = {118},
pages = {5-15},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618305343},
author = {Mohammad Ali Baghapour and Mohammad Reza Shooshtarian and Mohammad Reza Javaheri and Sina Dehghanifard and Razieh Sefidkar and Amir Fadaei Nobandegani},
keywords = {Process mining, Data mining, Hospital, Health-care waste, Index, Decision-making},
abstract = {Background
Proper Health-Care Waste Management (HCWM) and integrated documentation in this sector of hospitals require analyzing massive data collected by hospital’s health experts. This study presented a quantitative software-based index to assess the HCWM process performance by integrating ontology-based Multi-Criteria Group Decision-Making techniques and fuzzy modeling that were coupled with data mining. This framework represented the Complex Event Processing (CEP) and Corporate Performance Management (CPM) types of Process Mining in which a user-friendly software namely Group Fuzzy Decision-Making (GFDM) was employed for index calculation.
Findings
Assessing the governmental hospitals of Shiraz, Iran in 2016 showed that the proposed index was able to determine the waste management condition and clarify the blind spots of HCWM in the hospitals. The index values under 50 were found in some of the hospitals showing poor process performance that should be at the priority of optimization and improvement.
Conclusion
The proposed framework has distinctive features such as modeling the uncertainties (risks) in hospitals’ process assessment and flexibility enabling users to define the intended criteria, stakeholders, and number of hospitals. Having computer-aided approach for decision process also accelerates the index calculation as well as its accuracy which would contribute to more willingness of hospitals’ experts and other end-users to use the index in practice. The methodology could efficiently be employed as a tool for managing hospitals’ event logs and digital documentation in big data environment not only for the health-care waste management, but also in other administrative wards of hospitals.}
}
@article{SUN2013e199,
title = {Initial experience with TrueBeam trajectory log files for radiation therapy delivery verification},
journal = {Practical Radiation Oncology},
volume = {3},
number = {4},
pages = {e199-e208},
year = {2013},
issn = {1879-8500},
doi = {https://doi.org/10.1016/j.prro.2012.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S187985001200210X},
author = {Baozhou Sun and Dharanipathy Rangaraj and Geethpriya Palaniswaamy and Sridhar Yaddanapudi and Omar Wooten and Deshan Yang and Sasa Mutic and Lakshmi Santanam},
abstract = {Purpose
Traditionally, initial and weekly chart checks involve checking various parameters in the treatment management system against the expected treatment parameters and machine settings. This process is time-consuming and labor intensive. We explore utilizing the Varian TrueBeam log files (Varian Medical System, Palo Alto, CA), which contain the complete delivery parameters for an end-to-end verification of daily patient treatments.
Methods and Materials
An in-house software tool for 3-dimensional (3D) conformal therapy, enhanced dynamic wedge delivery, intensity modulated radiation therapy (IMRT), volumetric modulated radiation therapy, flattening filter-free mode, and electron therapy treatment verification was developed. The software reads the Varian TrueBeam log files, extracts the delivered parameters, and compares them against the original treatment planning data. In addition to providing an end-to-end data transfer integrity check, the tool also verifies the accuracy of treatment deliveries. This is performed as part of the initial chart check for IMRT plans and after first fraction for the 3D plans. The software was validated for consistency and accuracy for IMRT and 3D fields.
Results
Based on the validation results the accuracy of MLC, jaw and gantry positions were well within the expected values. The patient quality assurance results for 127 IMRT patients and 51 conventional fields were within 0.25 mm for multileaf collimator positions, 0.3 degree for gantry angles, 0.13 monitor units for monitor unit delivery accuracy, and 1 mm for jaw positions. The delivered dose rates for the flattening filter-free modes were within 1% of the planned dose rates.
Conclusions
The end-to-end data transfer check using TrueBeam log files and the treatment delivery parameter accuracy check provides an efficient, reliable beam parameter check process for various radiation delivery techniques.}
}
@article{PETRAKI2020105657,
title = {Combined impact of road and traffic characteristic on driver behavior using smartphone sensor data},
journal = {Accident Analysis & Prevention},
volume = {144},
pages = {105657},
year = {2020},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2020.105657},
url = {https://www.sciencedirect.com/science/article/pii/S0001457519315933},
author = {Virginia Petraki and Apostolos Ziakopoulos and George Yannis},
keywords = {Driver behavior, Harsh events, Geometric characteristics, Traffic characteristics, Smartphone data},
abstract = {The objective of this research is to exploit high resolution driving behavior data collected via sensors of smartphones from 303 drivers in order to examine driver behavior at road segment and junction level. These sensor data are combined with traffic and road geometry characteristics and subsequently depicted spatially using Geographical Information System software. Events of harsh driver behavior (8592 harsh accelerations and 3946 harsh brakings) were mapped to delimited segments and junctions of two urban expressways in Athens, Greece. For the analysis, two multiple linear regression models and two log-linear regression models were developed. Results indicate that in road segments there is an increase in the number of harsh events if average traffic flow per lane increases in the respective areas. Furthermore, as the average occupancy increases in junctions, there is an increase in harsh accelerations, and as the average speed increases, more harsh deceleration events occur. It is evident that traffic characteristics (traffic flow & speed) have the most statistically significant impact on the frequency of harsh events compared to factors related to road geometry and driver behavior.}
}
@article{ANDREWS2020113265,
title = {Quality-informed semi-automated event log generation for process mining},
journal = {Decision Support Systems},
volume = {132},
pages = {113265},
year = {2020},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2020.113265},
url = {https://www.sciencedirect.com/science/article/pii/S0167923620300208},
author = {R. Andrews and C.G.J. {van Dun} and M.T. Wynn and W. Kratsch and M.K.E. Röglinger and A.H.M. {ter Hofstede}},
keywords = {Process mining, Data quality, Event log, Log extraction},
abstract = {Process mining, as with any form of data analysis, relies heavily on the quality of input data to generate accurate and reliable results. A fit-for-purpose event log nearly always requires time-consuming, manual pre-processing to extract events from source data, with data quality dependent on the analyst's domain knowledge and skills. Despite much being written about data quality in general, a generalisable framework for analysing event data quality issues when extracting logs for process mining remains unrealised. Following the DSR paradigm, we present RDB2Log, a quality-aware, semi-automated approach for extracting event logs from relational data. We validated RDB2Log's design against design objectives extracted from literature and competing artifacts, evaluated its design and performance with process mining experts, implemented a prototype with a defined set of quality metrics, and applied it in laboratory settings and in a real-world case study. The evaluation shows that RDB2Log is understandable, of relevance in current research, and supports process mining in practice.}
}
@article{PRASOPDEE201516,
title = {Data set from the proteomic analysis of Bithynia siamensis goniomphalos snails upon infection with the carcinogenic liver fluke Opisthorchis viverrini},
journal = {Data in Brief},
volume = {2},
pages = {16-20},
year = {2015},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2014.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S2352340914000171},
author = {Sattrachai Prasopdee and Smarn Tesana and Cinzia Cantacessi and Thewarach Laha and Jason Mulvenna and Rudi Grams and Alex Loukas and Javier Sotillo},
abstract = {The snail Bithynia siamensis goniomphalos acts as the first intermediate host for the human liver fluke Opisthorchis viverrini, the major cause of cholangiocarcinoma (CCA) in Northeast Thailand. This data article contains the results obtained from the analysis of the proteins differentially expressed in the snail B. siamensis goniomphalos upon infection with O. viverrini. It contains the data generated from iQuantitator software including a pdf of each sample with a protein׳s relative expression summary and a per-protein detailed analysis of all time points studied and an excel file for each sample containing the raw data from iQuantitator analysis, including ID, mean, standard deviation, credible interval, log2 and description for every protein identified in each of the samples.}
}
@article{KADOYA2018170,
title = {Quantifying the performance of two different types of commercial software programs for 3D patient dose reconstruction for prostate cancer patients: Machine log files vs. machine log files with EPID images},
journal = {Physica Medica},
volume = {45},
pages = {170-176},
year = {2018},
issn = {1120-1797},
doi = {https://doi.org/10.1016/j.ejmp.2017.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S1120179717306579},
author = {Noriyuki Kadoya and Yoshio Kon and Yoshiki Takayama and Takuya Matsumoto and Naoki Hayashi and Yoshiyuki Katsuta and Kengo Ito and Takahito Chiba and Suguru Dobashi and Ken Takeda and Keiichi Jingu},
keywords = {Radiotherapy, Quality assurance, EPID, Log file, Prostate cancer},
abstract = {We clarified the reconstructed 3D dose difference between two different commercial software programs (Mobius3D v2.0 and PerFRACTION v1.6.4). Five prostate cancer patients treated with IMRT (74 Gy/37 Fr) were studied. Log files and cine EPID images were acquired for each fraction. 3D patient dose was reconstructed using log files (Mobius3D) or log files with EPID imaging (PerFRACTION). The treatment planning dose was re-calculated on homogeneous and heterogeneous phantoms, and log files and cine EPID images were acquired. Measured doses were compared with the reconstructed point doses in the phantom. Next, we compared dosimetric metrics (mean dose for PTV, rectum, and bladder) calculated by Mobius3D and PerFRACTION for all fractions from five patients. Dose difference at isocenter between measurement and reconstructed dose for two software programs was within 3.0% in both homogeneous and heterogeneous phantoms. Moreover, the dose difference was larger using skip arc plan than that using full arc plan, especially for PerFRACTION (e.g., dose difference at isocenter for PerFRACTION: 0.34% for full arc plan vs. −4.50% for skip arc plan in patient 1). For patients, differences in dosimetric parameters were within 1% for almost all fractions. PerFRACTION had wider range of dose difference between first fraction and the other fractions than Mobius3D (e.g., maximum difference: 0.50% for Mobius3D vs. 1.85% for PerFRACTION), possibly because EPID may detect some types of MLC positioning errors such as miscalibration errors or mechanical backlash which cannot be detected by log files, or that EPID data might include image acquisition failure and image noise.}
}
@article{ETEMAD2020884,
title = {Estimation of lung and bronchial cancer registry completeness via capture-recapture method using population-based cancer registry in Khuzestan province in 2011},
journal = {Clinical Epidemiology and Global Health},
volume = {8},
number = {3},
pages = {884-889},
year = {2020},
issn = {2213-3984},
doi = {https://doi.org/10.1016/j.cegh.2020.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S2213398420300555},
author = {Kourosh Etemad and Mohammad Javad Mohammadi and Seyed Saeed {Hashemi Nazari} and Homayoun Amiri and Abdolhassan Talaiezadeh},
keywords = {Capture-recapture, Estimation, Log-linear, Lung cancer, Cancer registry},
abstract = {Introduction
Population-based cancer registry (PBCR) plays a significant role in burden of cancer estimation. Estimating the completeness of cancer registry is one of the criteria to assess the quality of data (results). Also, Capture-recapture is one of the methods used to assess completeness of cancer registry.
Objective
This study was aimed to estimate the completeness of lung and bronchial cancer registry using three-source capture-recapture method in Khuzestan province.
Materials and methods
This study was carried out using three-source capture-recapture method and data were obtained from three sources including medical records, death certificates and pathology labs (reports) in Khuzestan. In this study, total new cases of lung and bronchial cancer registered by the three sources in 2011, were enrolled. Among the sources, the common cases were identified and the completeness rate of lung and bronchial cancer registry was calculated by log-linear method using R software, finally.
Results
Totally, 426 new cases for lung cancer had reported from the three sources in 2011. The completeness index of lung cancer registry was totally estimated 72%, as well as it was independently 40%, 25% and 37.6% for the medical records, pathology reports and death certificates sources, respectively.
Conclusion
According to the results, after collection and remove repetition of the cases related to lung cancer, it should be added their number registered by the pathology (labs), medical records and death certificates sources by 28%–40%, to estimate actual incidence of lung cancer. Also, the actual incidence of lung cancer cases related to each of medical records, pathology and death certificates was calculated by adding the reported values by 60%, 75% and 63%, respectively.}
}
@article{ABBOTT20155088,
title = {Log Analysis of Cyber Security Training Exercises},
journal = {Procedia Manufacturing},
volume = {3},
pages = {5088-5094},
year = {2015},
note = {6th International Conference on Applied Human Factors and Ergonomics (AHFE 2015) and the Affiliated Conferences, AHFE 2015},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2015.07.523},
url = {https://www.sciencedirect.com/science/article/pii/S2351978915005247},
author = {Robert G. Abbott and Jonathan McClain and Benjamin Anderson and Kevin Nauer and Austin Silva and Chris Forsythe},
keywords = {Cyber security, Computer security, Human performance, log analysis, Activity recognition},
abstract = {Cyber security is a pervasive issue that impacts public and private organizations. While several published accounts describe the task demands of cyber security analysts, it is only recently that research has begun to investigate the cognitive and performance factors that distinguish novice from expert cyber security analysts. Research in this area is motivated by the need to understand how to better structure the education and training of cyber security professionals, a desire to identify selection factors that are predictive of professional success in cyber security and questions related to the development of software tools to augment human performance of cyber security tasks. However, a common hurdle faced by researchers involves gaining access to cyber security professionals for data collection activities, whether controlled experiments or semi-naturalistic observations. An often readily available and potentially valuable source of data may be found in the records generated through cyber security training exercises. These events frequently entail semi-realistic challenges that may be modeled on real-world occurrences, and occur outside normal operational settings, freeing participants from the sensitivities regarding information disclosure within operational environments. This paper describes an infrastructure tailored for the collection of human performance data within the context of cyber security training exercises. Techniques are described for mining the resulting data logs for relevant human performance variables. The results provide insights that go beyond current descriptive accounts of the cognitive processes and demands associated with cyber security job performance, providing quantitative characterizations of the activities undertaken in solving problems within this domain.}
}
@article{ROWE2014916,
title = {The Key Image and Case Log Application: New Radiology Software for Teaching File Creation and Case Logging that Incorporates Elements of a Social Network},
journal = {Academic Radiology},
volume = {21},
number = {7},
pages = {916-930},
year = {2014},
note = {Education Issue},
issn = {1076-6332},
doi = {https://doi.org/10.1016/j.acra.2014.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1076633214001445},
author = {Steven P. Rowe and Adeel Siddiqui and David Bonekamp},
keywords = {Digital teaching file, PACS},
abstract = {Rationale and Objectives
To create novel radiology key image software that is easy to use for novice users, incorporates elements adapted from social networking Web sites, facilitates resident and fellow education, and can serve as the engine for departmental sharing of interesting cases and follow-up studies.
Materials and Methods
Using open-source programming languages and software, radiology key image software (the key image and case log application, KICLA) was developed. This system uses a lightweight interface with the institutional picture archiving and communications systems and enables the storage of key images, image series, and cine clips. It was designed to operate with minimal disruption to the radiologists' daily workflow. Many features of the user interface have been inspired by social networking Web sites, including image organization into private or public folders, flexible sharing with other users, and integration of departmental teaching files into the system. We also review the performance, usage, and acceptance of this novel system.
Results
KICLA was implemented at our institution and achieved widespread popularity among radiologists. A large number of key images have been transmitted to the system since it became available. After this early experience period, the most commonly encountered radiologic modalities are represented. A survey distributed to users revealed that most of the respondents found the system easy to use (89%) and fast at allowing them to record interesting cases (100%). Hundred percent of respondents also stated that they would recommend a system such as KICLA to their colleagues.
Conclusions
The system described herein represents a significant upgrade to the Digital Imaging and Communications in Medicine teaching file paradigm with efforts made to maximize its ease of use and inclusion of characteristics inspired by social networking Web sites that allow the system additional functionality such as individual case logging.}
}
@article{QIN2020371,
title = {Ana-Hash Table Sorting Algorithm for Curve Objects in WEDM CAM},
journal = {Procedia CIRP},
volume = {95},
pages = {371-376},
year = {2020},
note = {20th CIRP CONFERENCE ON ELECTRO PHYSICAL AND CHEMICAL MACHINING},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.02.290},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120311409},
author = {Ling Qin and Min Zhang and Mo Chen and Zhi-Kai Zhou and Wan-Sheng Zhao},
keywords = {wire electrical discharge machining (WEDM), embedded CAM software, curve objects sorting, Ana-Hash Table},
abstract = {Converting tool path geometries of a computer-aided design (CAD) file to computerized numerical control (CNC) commands is the core functionality of a computer aided manufacturing (CAM) software for wire electrical discharge machining (WEDM). The number of curve objects (such as lines and arcs) for a complex tool path can reach more than 20000. However, curve objects in a CAD file are sorted by drawing sequence instead of processing sequence, and slow sorting will cause an increase in elapsed time (more than 120 seconds accordingly) and a hanging consequence to the running of the embedded CNC system. Finding adjacencies of curve objects is a key step in curve sequence sorting. Conventional algorithms targeted at adjacencies traverse all vertexes of CAD files, whose complexity is O(n2). According to the characteristics of a WEDM processing path, Ana-Hash Table Sorting algorithm is proposed to reduce the elapsed sorting time. This algorithm classifies vertexes into corresponding buckets in advance according to the coordinates, and adjacencies are judged among specific buckets, so that most useless traversals can be avoided. The complexity of the algorithm is O(nlogn). Noticing that the sorting acceleration ratio varies widely in table parameters, the optimization of table generating rules is given. The proposed algorithm works efficiently when the number of buckets per row is 1% to 1.5% of the number of curve objects. And the elapsed sorting time can be dramatically reduced by 99% as compared to the conventional traversal algorithm.}
}
@incollection{LEONARD20171,
title = {Chapter One - Web-Based Behavioral Modeling for Continuous User Authentication (CUA)},
editor = {Atif M. Memon},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {105},
pages = {1-44},
year = {2017},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2016.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0065245816300742},
author = {L.C. Leonard},
keywords = {n-grams, Continuous user authentication, Statistical language models, Behavioral models},
abstract = {Authentication plays an important role in how we interact with computers, mobile devices, the web, etc. For example, in recent years, more corporate information and applications have been accessible via the Internet and Intranet. Many employees are working from remote locations and need access to secure corporate files. During this time, it is possible for malicious or unauthorized users to gain access to the system. For this reason, it is logical to have some mechanism in place to detect whether the logged-in user is the same user in control of the user's session. Therefore, highly secure authentication methods must be used. We posit that each of us is unique in our use of computer systems. It is this uniqueness that is leveraged to “continuously authenticate users” while they use web software. To monitor user behavior, n-gram models are used to capture user interactions with web-based software. This statistical language model essentially captures sequences and subsequences of user actions, their orderings, and temporal relationships that make them unique by providing a model of how each user typically behaves. Users are then continuously monitored during software operations. Large deviations from “normal behavior” can possibly indicate malicious or unintended behavior. This approach is implemented in a system called Intruder Detector (ID) that models user actions as embodied in web logs generated in response to a user's actions. User identification through web logs is cost-effective and nonintrusive. For these experiments, we use two classification techniques; binary and multiclass classification.}
}
@article{AWAD2016110,
title = {Analyzing and repairing overlapping work items in process logs},
journal = {Information and Software Technology},
volume = {80},
pages = {110-123},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916301410},
author = {Ahmed Awad and Nesma M. Zaki and Chiara Di Francescomarino},
keywords = {Process logs, Humans work patterns, Performance measurement, Business processes, Log analysis},
abstract = {Context: In real life logs, it often happens that some human resources appear to have more than one task active concurrently, thus resulting in human multitasking. However, tasks that require some intellectual effort cannot be executed in parallel in real life. This misalignment between what actually happens and what is registered in the logs, however, is not reflected in the output of the different log-based performance measuring approaches, thus compromising the quality of the computed metrics. Objective: We introduce a novel approach to rewrite events in process execution logs for multitasking human resources. The approach is based on two typical human work patterns, the queuing and stacking patterns. The rewrite aims at serializing multi tasks for the same resource based on the work pattern detected. Thus, possibly better performance measures can be obtained. Method: We defined a quantitative approach to detect multitasking human performers and resolve them by serialization. The approach is prototyped and evaluated on a set of real-life software development process logs. Results: Our results show that the proposed approach contributes to find better results when log-based performance analysis techniques are applied to the repaired logs in comparison to the original logs. Conclusions: The work shows that based on the human work patterns, stacking or queuing, logs can be enhanced, so as to be possibly closer to what happened in the reality and to allow for more accurate performance measures.}
}
@article{CARBONE20191,
title = {Flux Puppy – An open-source software application and portable system design for low-cost manual measurements of CO2 and H2O fluxes},
journal = {Agricultural and Forest Meteorology},
volume = {274},
pages = {1-6},
year = {2019},
issn = {0168-1923},
doi = {https://doi.org/10.1016/j.agrformet.2019.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S0168192319301522},
author = {Mariah S. Carbone and Bijan Seyednasrollah and Tim T. Rademacher and David Basler and James M. {Le Moine} and Samuel Beals and James Beasley and Andrew Greene and Joseph Kelroy and Andrew D. Richardson},
keywords = {Manual chamber CO measurements, LI-820, LI-830, LI-840, LI-850, Open source, Portable system, Respiration},
abstract = {Manual chamber-based measurements of CO2 (and H2O) fluxes are important for understanding ecosystem carbon metabolism. Small opaque chambers can be used to measure leaf, stem and soil respiration. Larger transparent chambers can be used to measure net ecosystem exchange of CO2, and small jars often serve this purpose for laboratory incubations of soil and plant material. We developed an Android application (app), called Flux Puppy, to facilitate chamber-based flux measurements in the field and laboratory. The app is designed to run on an inexpensive handheld Android device, such as a tablet or phone, and it has a graphical user interface that communicates with a LI-COR LI-820 and LI-830 (CO2) or LI-840 and LI-850 (CO2/H2O) infrared gas analyzer. The app logs concentrations of CO2 and H2O, cell temperature and pressure at 1 Hz, displays the output graphically, and calculates the linear regression slope, R-squared, and standard error of the CO2 time series. A metadata screen allows users to enter operator, site, and plot information, as well as take a photograph using the Android device’s built-in camera, and log measurement location using the device GPS. Additionally, there is a notes field, which can be revised after the measurements are taken. Data files (the 1 s raw data, photograph, and metadata including statistics calculated from the raw data) are then transmitted off the device through file sharing options (Gmail, Outlook, Google Drive, Dropbox etc.). Because Flux Puppy code is open-source (available on GitHub) and the flux measurement system we describe is relatively inexpensive and straightforward to assemble, it should be of broad interest to the carbon cycling community.}
}
@article{BRIGHT20191,
title = {STRmix™ collaborative exercise on DNA mixture interpretation},
journal = {Forensic Science International: Genetics},
volume = {40},
pages = {1-8},
year = {2019},
issn = {1872-4973},
doi = {https://doi.org/10.1016/j.fsigen.2019.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S1872497318305192},
author = {Jo-Anne Bright and Kevin Cheng and Zane Kerr and Catherine McGovern and Hannah Kelly and Tamyra R. Moretti and Michael A. Smith and Frederick R. Bieber and Bruce Budowle and Michael D. Coble and Rashed Alghafri and Paul Stafford Allen and Amy Barber and Vickie Beamer and Christina Buettner and Melanie Russell and Christian Gehrig and Tacha Hicks and Jessica Charak and Kate Cheong-Wing and Anne Ciecko and Christie T. Davis and Michael Donley and Natalie Pedersen and Bill Gartside and Dominic Granger and MaryMargaret Greer-Ritzheimer and Erick Reisinger and Jarrah Kennedy and Erin Grammer and Marla Kaplan and David Hansen and Hans J. Larsen and Alanna Laureano and Christina Li and Eugene Lien and Emilia Lindberg and Ciara Kelly and Ben Mallinder and Simon Malsom and Alyse Yacovone-Margetts and Andrew McWhorter and Sapana M. Prajapati and Tamar Powell and Gary Shutler and Kate Stevenson and April R. Stonehouse and Lindsey Smith and Julie Murakami and Eric Halsing and Darren Wright and Leigh Clark and Duncan A. Taylor and John Buckleton},
keywords = {Forensic DNA interpretation, Probabilistic genotyping, STRmix, Inter-laboratory study, Intra-laboratory study},
abstract = {An intra and inter-laboratory study using the probabilistic genotyping (PG) software STRmix™ is reported. Two complex mixtures from the PROVEDIt set, analysed on an Applied Biosystems™ 3500 Series Genetic Analyzer, were selected. 174 participants responded. For Sample 1 (low template, in the order of 200 rfu for major contributors) five participants described the comparison as inconclusive with respect to the POI or excluded him. Where LRs were assigned, the point estimates ranging from 2 × 104 to 8 × 106. For Sample 2 (in the order of 2000 rfu for major contributors), LRs ranged from 2 × 1028 to 2 × 1029. Where LRs were calculated, the differences between participants can be attributed to (from largest to smallest impact): •varying number of contributors (NoC),•the exclusion of some loci within the interpretation,•differences in local CE data analysis methods leading to variation in the peaks present and their heights in the input files used,•and run-to-run variation due to the random sampling inherent to all MCMC-based methods. This study demonstrates a high level of repeatability and reproducibility among the participants. For those results that differed from the mode, the differences in LR were almost always minor or conservative.}
}
@article{UBACHS2020706,
title = {No influence of sarcopenia on survival of ovarian cancer patients in a prospective validation study},
journal = {Gynecologic Oncology},
volume = {159},
number = {3},
pages = {706-711},
year = {2020},
issn = {0090-8258},
doi = {https://doi.org/10.1016/j.ygyno.2020.09.042},
url = {https://www.sciencedirect.com/science/article/pii/S0090825820339524},
author = {Jorne Ubachs and Simone N. Koole and Max Lahaye and Cristina Fabris and Leigh Bruijs and Jules {Schagen van Leeuwen} and Henk W.R. Schreuder and R.H. Hermans and I.H. {de Hingh} and J. {van der Velden} and H.J. Arts and M. {van Ham} and P. {van Dam} and P. Vuylsteke and Jacco Bastings and Roy F.P.M. Kruitwagen and Sandrina Lambrechts and Steven W.M. {Olde Damink} and Sander S. Rensen and Toon {Van Gorp} and Gabe S. Sonke and Willemien J. {van Driel}},
keywords = {Sarcopenia, Ovarian cancer, Cachexia, Survival, OVHIPEC},
abstract = {Objective
Decrease in skeletal muscle index (SMI) during neoadjuvant chemotherapy (NACT) has been associated with worse outcome in patients with advanced ovarian cancer. To validate these findings, we tested if a decrease in SMI was a prognostic factor for a homogenous cohort of patients who received NACT in the randomized phase 3 OVHIPEC-trial.
Methods
CT-scans were performed at baseline and after two cycles of neoadjuvant chemotherapy in stage III ovarian cancer patients. The SMI (skeletal muscle area in cm2 divided by body surface area in m2) was calculated using SliceOMatic software. The difference in SMI between both CT-scans (ΔSMI) was calculated. Cox-regression analyses were performed to analyze the independent effect of a difference in SMI (ΔSMI) on outcome. Log-rank tests were performed to plot recurrence-free (RFS) and overall survival (OS). The mean number of adverse events per patient were compared between groups using t-tests.
Results
Paired CT-scans were available for 212 out of 245 patients (87%). Thirty-four of 74 patients (58%) in the group with a decrease in ΔSMI and 73 of 138 of the patients (53%) in the group with stable/increase in ΔSMI had died. Median RFS and OS did not differ significantly (p = 0.297 and p = 0.764) between groups. Patients with a decrease in SMI experienced more pre-operative adverse events, and more grade 3–4 adverse events.
Conclusion
Decreased SMI during neoadjuvant chemotherapy was not associated with worse outcome in patients with stage III ovarian cancer included in the OVHIPEC-trial. However, a strong association between decreasing SMI and adverse events was found.}
}
@article{KALANTARI2020101098,
title = {Expected seismic fragility of code-conforming RC moment resisting frames under twin seismic events},
journal = {Journal of Building Engineering},
volume = {28},
pages = {101098},
year = {2020},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2019.101098},
url = {https://www.sciencedirect.com/science/article/pii/S2352710219313634},
author = {A. Kalantari and H. Roohbakhsh},
keywords = {Seismic sequence, Nonlinear dynamic analysis, RC buildings, Seismic performance, Fragility curve},
abstract = {Seismic design codes require buildings to be designed only for one seismic event, neglecting the possibility of multiple earthquake events. However considerable damage and loss is observed due to aftershocks where buildings have been affected by the first event. The purpose of this research is to introduce fragility curves of code-conforming reinforced concrete moment resisting frames (RCMRFs) under aftershock. Three RC moment resisting frames of 4, 8 and 15 stories, are loaded and designed based on the latest three editions of the Iranian code of seismic design for buildings referred to as Standard No. 2800 (STD-2800). The nine designed frames are then numerically simulated in OpenSees software to perform nonlinear dynamic analysis. Twenty natural ground motion sequences, each including two seismic events, were selected from previous studies for the purpose of nonlinear incremental dynamic analysis. Maximum inter-story drift was employed as a damage index, to capture performance of the RC frames under the sequences. The accepted performance for the collapse level was taken from Iranian instructions for seismic rehabilitation of existing buildings, PBO-Publication No. 360, 2007, which is similar to ASCE 41-13. Seismic fragility values are calculated for the buildings in the second event after being damaged in different first event intensity scenarios. Assuming a log-normal distribution for the failure probability function, corresponding values of mean, μ, and standard deviation, β, for each case are calculated and discussed. Results indicate how the probability of failure in the seismic fragility curves may differ as a result of the effects of a first event scenario. It is also shown how updating the design criteria as well as the height of frames can affect fragility in a second event.}
}
@article{BERKOWITZ2021,
title = {Cost Analysis of Routine Vitrectomy Surgery},
journal = {Ophthalmology Retina},
year = {2021},
issn = {2468-6530},
doi = {https://doi.org/10.1016/j.oret.2021.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S2468653021000531},
author = {Sean T. Berkowitz and Paul Sternberg and Shriji Patel},
abstract = {Purpose
To use electronic health record (EHR) time logs to calculate the complete cost profiles of routine pars plana vitrectomy surgery.
Design
Economic analysis.
Participants
Patients undergoing elective vitrectomy procedures (Current Procedural Terminology codes 67040, 67041, and 67042) at Vanderbilt University Medical Center in fiscal year 2019.
Methods
Process flow mapping for routine vitrectomy surgery was used to define the operative episode. De-identified time logs were sourced from an internal perioperative data warehouse to calculate procedure-level durations. The costs of materials and overhead were calculated from internal financial management software. Costs per minute for space, equipment, and personnel were based on internal figures. These inputs were used for a time-driven activity-based costing (TDABC) analysis.
Main Outcome Measures
Complete cost profile of routine pars plana vitrectomy surgery.
Results
Cost analysis of routine vitrectomy surgery resulted in a total cost of $7169.79 per patient, which was $2053.85 more than the maximum Medicare reimbursement for the equivalent episode, $5115.93. Vitrectomy cases do not break even unless the case duration is fewer than 26.81 minutes, overhead is reduced by 53.78%, or reimbursement is increased by 40.15%. Reimbursement does not compensate for variable costs alone for cases lasting longer than 55.09 minutes. In the cohort used here, 68% of cases are completely unprofitable, with increasing losses directly proportional to the length of the case.
Conclusions
This analysis showed that true costs for routine vitrectomy procedures are significantly more than the maximum allowable Medicare reimbursement. Academic ophthalmology departments may benefit from more accurate costing approaches using existing EHR data. These approaches may be informative for policy discussion regarding appropriate reimbursement.}
}
@article{LEGRAND201724,
title = {49. Current uses of log files in the radiotherapy quality assurance workflow for IMRT and VMAT techniques},
journal = {Physica Medica},
volume = {44},
pages = {24},
year = {2017},
note = {Abstracts of the 56èmes Journées Scientifiques de la Société Française de Physique Médicale},
issn = {1120-1797},
doi = {https://doi.org/10.1016/j.ejmp.2017.10.074},
url = {https://www.sciencedirect.com/science/article/pii/S112017971730546X},
author = {C. Legrand},
abstract = {Introduction
During IMRT and VMAT treatment, radiotherapy systems record actual axis information such as gantry/collimator angles, leafs and banks positions and MU delivered. After the treatment is completed, this information is stored to a log file. It allows users to compare planned and delivered data for each parameter. For the purpose of IMRT/VMAT quality assurance (QA), these files are of great interest and are increasingly used.
Methods
This presentation gives an up-to-date overview of the uses of log files. After a short introduction on the content of log files and their potential limitations, current applications and software available are presented. Next, feedback of the author in the day-to-day running of his medical physics department is presented. Finally, perspectives in log files usability are considered.
Results
Log files allow quantifying MU and axis deviations relative to the DICOM RT Plan. In troubleshooting issues, logs can help finding where the problem might come from. As a pre-treatment quality control tool, quantified deviations allow comparison with control limits, and then ensure that the machine has correctly interpreted the DICOM RT plan. Another application would be the comparison between delivered 3D dose calculated from log files in the patient CT (or CBCTs) with the prescribed treatment plan. Extensive application would be to correlate modulation indexes calculated from the DICOM RT Plan with log files deviation in order to optimize QA process.
Conclusions
Log files are complementary to measurement-based QA method. Knowing their limitations, they provide useful mechanical and dosimetric information that can be used for fixing issues. They can also be employed in the (pre)treatment QA process.}
}
@article{TERAUCHI2021,
title = {Factors affecting the removal time of separated instruments},
journal = {Journal of Endodontics},
year = {2021},
issn = {0099-2399},
doi = {https://doi.org/10.1016/j.joen.2021.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0099239921003290},
author = {Yoshi Terauchi and Christopher Sexton and Leif K. Bakland and George Bogen},
keywords = {Canal curvature, dental operating microscopes, instrument retrieval, preparation time, separated instrument, ultrasonic instruments},
abstract = {Introduction
Separated endodontic instruments may adversely affect the outcome of endodontic treatment. The combination of ultrasonic techniques and dental operating microscopes appear to be effective in removal of separated instruments when compared to more randomized techniques. This study evaluated the roles of root canal curvature and separated instrument length on the time needed to loosen and retrieve the instrument fragments.
Methods
The retrieval procedures of 128 separated instruments referred to a private endodontic practice for retreatment by general practitioners were evaluated in patients that were monitored for a minimum of six months. Preoperative CBCT images were used to measure separated instrument lengths in relation to the degrees of canal curvatures. Ultrasonic instruments were used in the initial phase to remove tooth structure and to loosen the fractured instrument. In the second phase, either ultrasonic instruments, wire loops, or XP Shapers were used for fragment removal. The time periods for all procedures were recorded. Statistical analysis was completed applying log-normal regression, structural equation modeling and linear regression using Stata Version 14.2 software.
Results
All separated instruments were successfully retrieved. Using the protocol in this study, 89.8% of the instruments were removed using ultrasonic instruments alone with a mean time of 221 seconds. The instrument removal time was dependent on both the instrument length and the root canal curvature. Additionally, preparation times were proportionately longer with increasing separated instrument lengths when the loop device was required.
Conclusion
The preparation phase appears to have an important role in the retrieval of separated instruments. Preparation times for both non-loop and loop groups demonstrate that length and curvature are independent predictors of the log-transformed time. Generally, procedure times were extended with increasing file lengths and higher degrees of canal curvature.}
}
@article{BRYANT2020101817,
title = {Improving SIEM alert metadata aggregation with a novel kill-chain based classification model},
journal = {Computers & Security},
volume = {94},
pages = {101817},
year = {2020},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2020.101817},
url = {https://www.sciencedirect.com/science/article/pii/S016740482030095X},
author = {Blake D. Bryant and Hossein Saiedian},
keywords = {Network monitoring, Intrusion detection, Kill-chain, Advanced persistent threat, APT, Security information and event management, SIEM, Security log ontology, Computer network defense, Attack ontology, Threat framework},
abstract = {Today’s information networks face increasingly sophisticated and persistent threats, where new threat tools and vulnerability exploits often outpace advancements in intrusion detection systems. Current detection systems often create too many alerts, which contain insufficient data for analysts. As a result, the vast majority of alerts are ignored, contributing to security breaches that might otherwise have been prevented. Security Information and Event Management (SIEM) software is a recent development designed to improve alert volume and content by correlating data from multiple sensors. However, insufficient SIEM configuration has thus far limited the promise of SIEM software for improving intrusion detection. The focus of our research is the implementation of a hybrid kill-chain framework as a novel configuration of SIEM software. Our research resulted in a new log ontology capable of normalizing security sensor data in accordance with modern threat research. New SIEM correlation rules were developed using the new log ontology, and the effectiveness of the new configuration was tested against a baseline configuration. The novel configuration was shown to improve detection rates, give more descriptive alerts, and lower the number of false positive alerts.}
}
@article{201316,
title = {Identifying the real causes of plant upsets},
journal = {World Pumps},
volume = {2013},
number = {10},
pages = {16-17},
year = {2013},
issn = {0262-1762},
doi = {https://doi.org/10.1016/S0262-1762(13)70265-7},
url = {https://www.sciencedirect.com/science/article/pii/S0262176213702657},
abstract = {With safety an absolute top concern on an oil rig, any software monitoring system should be totally reliable. Premier Oil's Balmoral facility has experienced zero software outages since replacing an aging printer system with MAC Solutions’ ProcessVue software two years ago. The reliability of the software means that data from the safety-critical plant is always logged, enabling engineers to identify the cause of a plant upset, including the sequence of events (SOEs).}
}
@article{MAO2018167,
title = {From big data to knowledge: A spatio-temporal approach to malware detection},
journal = {Computers & Security},
volume = {74},
pages = {167-183},
year = {2018},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2017.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167404817302705},
author = {Weixuan Mao and Zhongmin Cai and Yuan Yang and Xiaohong Shi and Xiaohong Guan},
keywords = {Malware detection, Data-driven security analysis, File co-occurrence, Graph based semi-supervised learning, Content-agnostic},
abstract = {The deployment of endpoint protection has been gradually migrated from individual clients to remote cloud servers, which is termed as cloud based security service. The new paradigm of security defense produces a large amount of data and log files, and motivates data-driven techniques for detecting malicious software. This paper conducts an empirical study on the log of a real cloud based security service to characterize the occurrence of executable files in end hosts, which concerns 124,782 benign and 113,305 malicious executable files occurred in 165,549,417 end hosts. The end hosts and the timestamps that an executable file occurs in provide insights into the distribution of software in wild from spatial and temporal perspectives, respectively. Meanwhile, we investigate the strategies behind the characterizations, and observe the preferential attachment process and the periodicity of file occurrence in end hosts. The observed different occurrence patterns of benign and malicious files in end hosts inspire us a new scalable approach to malware detection. We learn from the characterizations that, the associated files shared more spatial and temporal information in common are more likely to be same in their labels, either benign or malicious. Thus, we devise a graph based semi-supervised learning algorithm for real-time malware detection by taking into account the spatio-temporal information of the distribution of executable files. Experimental results demonstrate that our approach increases the performance on malware detection by 14.7% over previous techniques on average.}
}
@article{RUPERT2020106351,
title = {Metagenomic data of bacterial community from different land uses at the river basin, Kelantan},
journal = {Data in Brief},
volume = {33},
pages = {106351},
year = {2020},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2020.106351},
url = {https://www.sciencedirect.com/science/article/pii/S2352340920312440},
author = {Rennielyn Rupert and Grace Joy Chin Wei Lie and Daisy Vanitha John and Kogila Vani Annammala and Jaeyres Jani and Kenneth Francis Rodrigues},
keywords = {Metagenomics, Clustering analysis, Taxonomy tree, Land-uses, Kelantan river basin},
abstract = {The data provided in the article includes the sequence of bacterial 16S rRNA gene from a high conservation value forest, logged forest, rubber plantation and oil palm plantation collected at Kelantan river basin. The logged forest area was previously notified as a flooding region. The total gDNA of bacterial community was amplified via polymerase chain reaction at V3-V4 regions using a pair of specific universal primer. Amplicons were sequenced on Illumina HiSeq paired-end platform to generate 250 bp paired-end raw reads. Several bioinformatics tools such as FLASH, QIIME and UPARSE were used to process the reads generated for OTU analysis. Meanwhile, R&D software was used to construct the taxonomy tree for all samples. Raw data files are available at the Sequence Read Archive (SRA), NCBI and data information can be found at the BioProject and BioSample, NCBI. The data shows the comparison of bacterial community between the natural forest and different land uses.}
}
@article{KUBACKI20191,
title = {Exploring operational profiles and anomalies in computer performance logs},
journal = {Microprocessors and Microsystems},
volume = {69},
pages = {1-15},
year = {2019},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2019.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0141933118303405},
author = {Marcin Kubacki and Janusz Sosnowski},
keywords = {Performance monitoring, Anomaly detection, Computer logs, Correlation analysis, Time series},
abstract = {Operational/functional problems in computer systems can be identified by monitoring and exploring performance metrics. These metrics can also be used to evaluate system activity profiles and manage relevant infrastructure (hardware and software). The critical point is finding features that make it possible to distinguish normal from abnormal system behaviour and to reveal emerging trends. This paper proposes a systematic methodology for deriving such features based on diverse observation perspectives in time (direct and aggregated) and defined specific data objects. We introduce a novel data model which combines collected samples into higher level observation objects (pulses and their compositions). This model is supported with original analysis algorithms for evaluating system behaviour. These provide useful sample/object statistics significantly enhanced with derived correlation formulas and periodicity properties. Compared with classical approaches, our model assures deeper and more accurate insight into system operation under real workload conditions as it uses new evaluation metrics and a wider scope of observation features (e.g. those related to pulse objects). In the paper, we perform exploratory studies covering real performance logs from several university servers. The results are interpreted in light of various statistical data views, including multidimensional correlation findings covering performance, event and process logs.}
}
@article{AMATO2020172,
title = {A semantic-based methodology for digital forensics analysis},
journal = {Journal of Parallel and Distributed Computing},
volume = {138},
pages = {172-177},
year = {2020},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2019.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S0743731519300644},
author = {Flora Amato and Aniello Castiglione and Giovanni Cozzolino and Fabio Narducci},
keywords = {Digital forensics, Text analysis, Log analysis, Correlation, Cybersecurity},
abstract = {Nowadays, more than ever, digital forensics activities are involved in any criminal, civil or military investigation and represent a fundamental tool to support cyber-security. Investigators use a variety of techniques and proprietary software forensics applications to examine the copy of digital devices, searching hidden, deleted, encrypted, or damaged files or folders. Any evidence found is carefully analysed and documented in a “finding report” in preparation for legal proceedings that involve discovery, depositions, or actual litigation. The aim is to discover and analyse patterns of fraudulent activities. In this work, a new methodology is proposed to support investigators during the analysis process, correlating evidence found through different forensics tools. The methodology was implemented through a system able to add semantic assertion to data generated by forensics tools during extraction processes. These assertions enable more effective access to relevant information and enhanced retrieval and reasoning capabilities.}
}
@article{SENTHIVEL2017S57,
title = {SCADA network forensics of the PCCC protocol},
journal = {Digital Investigation},
volume = {22},
pages = {S57-S65},
year = {2017},
issn = {1742-2876},
doi = {https://doi.org/10.1016/j.diin.2017.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S1742287617301998},
author = {Saranyan Senthivel and Irfan Ahmed and Vassil Roussev},
keywords = {SCADA forensics, SCADA protocol, PCCC, Network traffic analysis, Programmable logic controller},
abstract = {Most SCADA devices have few built-in self-defence mechanisms, and tend to implicitly trust communications received over the network. Therefore, monitoring and forensic analysis of network traffic is a critical prerequisite for building an effective defense around SCADA units. In this work, we provide a comprehensive forensic analysis of network traffic generated by the PCCC(Programmable Controller Communication Commands) protocol and present a prototype tool capable of extracting both updates to programmable logic and crucial configuration information. The results of our analysis show that more than 30 files are transferred to/from the PLC when downloading/uploading a ladder logic program using RSLogix programming software including configuration and data files. Interestingly, when RSLogix compiles a ladder-logic program, it does not create any low-level representation of a ladder-logic file. However, the low-level ladder logic is present and can be extracted from the network traffic log using our prototype tool. The tool extracts SMTP configuration from the network log and parses it to obtain email addresses, username and password. The network log contains password in plain text.}
}
@article{CALVOORTEGA2020S989,
title = {PO-1777: Validation of the Pylinac software platform for MLC log file analysis},
journal = {Radiotherapy and Oncology},
volume = {152},
pages = {S989-S990},
year = {2020},
note = {ESTRO 2020 - Online Congress, 28 November to 1 December 2020},
issn = {0167-8140},
doi = {https://doi.org/10.1016/S0167-8140(21)01795-3},
url = {https://www.sciencedirect.com/science/article/pii/S0167814021017953},
author = {J.F. {Calvo Ortega} and M. Hermida-López and S. Moragues-Femenía and C. Laosa-Bello and J. Casals-Farran}
}
@article{CASARRUBEA2010124,
title = {Temporal patterns analysis of rat behavior in hole-board},
journal = {Behavioural Brain Research},
volume = {208},
number = {1},
pages = {124-131},
year = {2010},
issn = {0166-4328},
doi = {https://doi.org/10.1016/j.bbr.2009.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S0166432809006780},
author = {Maurizio Casarrubea and Filippina Sorbera and Magnus Magnusson and Giuseppe Crescimanno},
keywords = {Hole-board, Anxiety, t-pattern analysis, Exploratory behavior, Head-dip, Edge-sniff, Rat},
abstract = {The aim of present research was to analyze the temporal structure of rodent's anxiety-related behavior in hole-board apparatus (HB). Fifteen male Wistar rats were tested for 10min. Video files, collected for each subject, were coded by means of a software coder and event log files generated for each subject. To assess temporal relationships among behavioral events, log files were processed by means of a t-pattern analysis. 14 two-element t-patterns, four t-patterns encompassing 3 events and 2 t-patterns encompassing 4 and 5 events respectively were revealed. It was demonstrated that rat behavior in HB was mainly structured on the basis of the temporal patterning among exploratory events; these ones were the most structured t-patterns detected and appeared mainly during the first 5min of exploration, while grooming t-patterns were present prevalently after the fifth minute. Specific t-pattern parameters, such as overall occurrences and mean duration of each given t-pattern in each subject, were also studied. Present research: (a) reports for the first time that some behavioral events occur sequentially and with significant constraints on the interval lengths separating them; (b) presents the temporal flows of some behavioral elements through multimodal behavioral vectors; (c) could also be used to improve HB test reliability and its ability to detect even small induced behavioral changes.}
}
@article{GAWAND2017484,
title = {Securing a Cyber Physical System in Nuclear Power Plants Using Least Square Approximation and Computational Geometric Approach},
journal = {Nuclear Engineering and Technology},
volume = {49},
number = {3},
pages = {484-494},
year = {2017},
issn = {1738-5733},
doi = {https://doi.org/10.1016/j.net.2016.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S1738573316302881},
author = {Hemangi Laxman Gawand and A.K. Bhattacharjee and Kallol Roy},
keywords = {Cyber Physical Systems (CPS), Convexity, Computational Geometry, Least Square Approximation (LSA), DataStream Analysis, Nuclear Power Plant (NPP), Real-time Control System, Sequential Probability Ratio Test (SPRT), Cumulative Sum (CUSUM), Security},
abstract = {In industrial plants such as nuclear power plants, system operations are performed by embedded controllers orchestrated by Supervisory Control and Data Acquisition (SCADA) software. A targeted attack (also termed a control aware attack) on the controller/SCADA software can lead a control system to operate in an unsafe mode or sometimes to complete shutdown of the plant. Such malware attacks can result in tremendous cost to the organization for recovery, cleanup, and maintenance activity. SCADA systems in operational mode generate huge log files. These files are useful in analysis of the plant behavior and diagnostics during an ongoing attack. However, they are bulky and difficult for manual inspection. Data mining techniques such as least squares approximation and computational methods can be used in the analysis of logs and to take proactive actions when required. This paper explores methodologies and algorithms so as to develop an effective monitoring scheme against control aware cyber attacks. It also explains soft computation techniques such as the computational geometric method and least squares approximation that can be effective in monitor design. This paper provides insights into diagnostic monitoring of its effectiveness by attack simulations on a four-tank model and using computation techniques to diagnose it. Cyber security of instrumentation and control systems used in nuclear power plants is of paramount importance and hence could be a possible target of such applications.}
}
@article{MVALLE201719,
title = {Applying process mining techniques in software process appraisals},
journal = {Information and Software Technology},
volume = {87},
pages = {19-31},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917300393},
author = {Arthur {M. Valle} and Eduardo {A.P. Santos} and Eduardo {R. Loures}},
keywords = {Process mining, Software process assessment, SCAMPI, Data collection and analysis, Process Mining Extension to SCAMPI},
abstract = {Context
Process assessments are performed to identify the current maturity of organizations in relation to best practices. Existing process assessment methods, although widely used, have limitations such as: dependence on the competencies of appraisers; high amount of effort and resources required; subjectivity to analyze data and to judge on the implementation of practices; low confidence in sampling and its representativeness. Currently, due to the increasing use of information systems to support process execution, detailed information on the implementation of processes are recorded as event logs, transaction logs, etc. This fact enables the usage of process mining techniques as a powerful tool for process analysis. It allows using a significant amount of data with agility and reliability for process assessments.
Objective
The objective of this paper is to present the development and application of a feasible, usable and useful method, which reduces the limitations of current SCAMPI method and defines how to apply process mining techniques in SCAMPI-based process appraisals.
Method
Research method comprises nine steps that were performed in a manner that raised questions in the first four steps were answered by the last four steps of the research design.
Results
The method “Process Mining Extension to SCAMPI” was designed, developed, applied in two cases and submitted for review by experts who judged it viable, usable, and useful.
Conclusions
As per this research, process mining techniques are suitable to be applied in software process assessments since they are aligned with the purposes of data collection and analysis tasks. In addition to that, the resulting method was judged by experts as something that reduces identified limitations of one of the most used process assessment method.}
}
@article{PRAESTEGAARD2018153,
title = {[P183] Replacement of EPID-based patient-specific QA by treatment analysis software},
journal = {Physica Medica},
volume = {52},
pages = {153},
year = {2018},
note = {Abstracts from the 2nd European Congress of Medical Physics},
issn = {1120-1797},
doi = {https://doi.org/10.1016/j.ejmp.2018.06.483},
url = {https://www.sciencedirect.com/science/article/pii/S1120179718309657},
author = {Lars Hjorth Praestegaard and Ulrik Vindelev Elstroem and Mai-Britt Linaa},
abstract = {Purpose
It is standard practice to perform QA of IMRT and VMAT plans by delivering the plans to the EPID (Electronic Portal Imaging Device) and comparing the measured dose to the calculated dose by an independent dose calculation engine prior to treatment. However, this procedure is both time consuming and reduces the patient throughput because the whole treatment must be delivered and measured using a clinical accelerator. In addition, noise and drift of the EPID require considerable extra resources to investigate false dose deviations and to perform repetitive EPID measurements. Accordingly, there is a need for a more streamlined QA approach with the same level of patient safety.
Methods
The Treatment Delivery Tool is an in house-developed C# application that can analyze both Varian Clinac and Varian TrueBeam treatment delivery log files. For each treatment the deviation between the delivered and planned fluence is calculated using the planned/actual MLC positions and the planned/actual MU for each time step in the log file. In addition, the application calculates all key technical parameters of the treatment delivery (e.g. max. MLC deviation, max. MU deviation, and max. MLC speed).
Results
The Treatment Delivery Tool automatically analyzes all daily treatments of an accelerator within one minute using a standard PC. Over a test period of two months 14 treatments with a significant deviation of the delivered fluence (more than 2% for more than 2% of the fluence area) were found. The deviations are related to a high MLC modulation (small MLC gap) and a large MU deviation. The test period also demonstrates that the Treatment Delivery Tool detects treatment errors with a much higher accuracy than possible with EPID-based QA.
Conclusions
The Treatment Delivery Tool is an effective and accurate tool for detecting dose delivery errors. In our department the application is complemented by an independent dose calculation by MobiusCalc (Mobius, USA) and a comprehensive accelerator QA program detecting any significant deviation between treatment delivery and data in the log file. This combined approach is very efficient and provides the same level of patient safety as for EPID-based patient-specific QA.}
}
@article{SUN201733,
title = {FRLink: Improving the recovery of missing issue-commit links by revisiting file relevance},
journal = {Information and Software Technology},
volume = {84},
pages = {33-47},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.11.010},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916303792},
author = {Yan Sun and Qing Wang and Ye Yang},
keywords = {Missing links, Issue reports, Commit analysis, Information retrieval (IR), Mining software repositories},
abstract = {Context: Though linking issues and commits plays an important role in software verification and maintenance, such link information is not always explicitly provided during software development or maintenance activities. Current practices in recovering such links highly depend on tedious manual examination. To automatically recover missing links, several approaches have been proposed to compare issue reports with log messages and source code files in commits. However, none of such approaches looked at the role of non-source code complementary documents in commits; nor did they consider the distinct roles each piece of the source code played in the same commit. Objective: We propose to revisit the definition of relevant files contributing to missing link recovery. More specifically, our work extends existing approaches from two perspectives: (1) Inclusion extension: incorporating complementary documents (i.e., non-source documents) to learn from more relevant data; (2) Exclusion extension: analyzing and filtering out irrelevant source code files to reduce data noise. Method: We propose a File Relevance-based approach (FRLink), to implement the above two considerations. FRLink utilizes non-source documents in commits, since they typically clarify code changes details, with similar textual information from corresponding issues. Moreover, FRLink differentiates the roles of different source code files in a single commit and discards files containing no similar code terms as those in issues based on similarity analysis. Results: FRLink is evaluated on 6 projects and compared with RCLinker, which is the latest state-of-the-art approach in missing link recovery. The result shows that FRLink outperforms RCLinker in F-Measure by 40.75% when achieving the highest recalls. Conclusion: FRLink can significantly improve the performance of missing link recovery compared with existing approaches. This indicates that in missing link recovery studies, sophisticated data selection and processing techniques necessitate more discussions due to the increasing variety and volume of information associated with issues and commits.}
}
@article{MISHRA2020104240,
title = {Composite rock types as part of a workflow for the integration of mm-to cm-scale lithological heterogeneity in static reservoir models},
journal = {Marine and Petroleum Geology},
volume = {114},
pages = {104240},
year = {2020},
issn = {0264-8172},
doi = {https://doi.org/10.1016/j.marpetgeo.2020.104240},
url = {https://www.sciencedirect.com/science/article/pii/S0264817220300234},
author = {Achyut Mishra and Kuncho D. Kurtev and Ralf R. Haese},
keywords = {Sedimentary heterogeneity, High resolution reservoir modelling, Lithotype log, Colorlith},
abstract = {Lithological heterogeneity in the form of sedimentary structures such as cross and planar and as interfaces between massive beddings is known to impact fluid flow and fluid-rock reactions in the subsurface. Such heterogeneity exists at mm-to cm-scale and is typically not accounted for in conventional geological models due to technical restrictions posed by the wireline log resolution and the inefficiency of reservoir modelling softwares to handle such fine scale information. Hence, multiphase flow and reactive transport simulations based on conventional models fail to appropriately account for the impact of sedimentary heterogeneity. We present a new workflow which allows building reservoir models where such fine-scale lithological heterogeneity can be integrated, thereby overcoming the limitations associated with log resolution and modelling softwares. First, the rock types are coupled as composites in a systematic manner at a discretization level that can be handled by the modelling softwares. The mm-scale rock properties quantified from core samples are accounted for in the composite rock type characterization. Lithotype logs of the composites are derived by modifying the color-coded depth record based on a combination of different wireline logs. The lithotype logs form the basis for property population in the reservoir models. The coupling of core and wireline data allows to account for the mm-to cm-scale heterogeneity in coarsely discretised reservoir models. The application of the new workflow is demonstrated for the development of a 2-D reservoir model at the CO2CRC's Otway Research site with a vertical resolution of 5 cm while incorporating sedimentary heterogeneity at 5 mm scale. The composite rock type method would allow for the impact of fine scale heterogeneity to be directly accounted for in static reservoir models.}
}
@article{OLASOLOALONSO201787,
title = {Evaluation of MLC performance in VMAT and dynamic IMRT by log file analysis},
journal = {Physica Medica},
volume = {33},
pages = {87-94},
year = {2017},
issn = {1120-1797},
doi = {https://doi.org/10.1016/j.ejmp.2016.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S1120179716311231},
author = {José Olasolo-Alonso and Alejandro Vázquez-Galiñanes and Santiago Pellejero-Pellejero and José Fernando Pérez-Azorín},
keywords = {Trajectory log file, Dynalog, IMRT, VMAT},
abstract = {Purpose
This multi-institution study assessed the positioning accuracy of multileaf collimators (MLC) by analyzing log files. It determined the main machine parameters that affect MLC positioning errors for pre-TrueBeam (Clinac) and TrueBeam linacs.
Methods
Around 30,000 dIMRT and VMAT log files belonging to 6 linacs from 4 different centers were analyzed. An in-house software was used to calculate 95th percentile and RMS error values and their correlation with certain parameters such as maximum leaf speed, mean leaf speed and gantry angle. The effect of MLC communication delay on error statistics was assessed in Clinac linacs. To that end MLC positioning error statistics were calculated with and without the delay effect.
Results
For dIMRT treatments in Clinac linacs the mean leaf RMS error was 0.306mm with and 0.030mm without the delay effect. Leaf RMS error was closely linked to maximum and mean leaf speeds, but without the delay effect that link was weaker. No trend was observed between bank RMS error and gantry angle. Without the delay effect larger bank RMS errors were obtained for gantry angles with leaf movements against gravity. For VMAT treatments in TrueBeam linacs the mean leaf RMS error was 0.038mm. A link was also observed between leaf RMS error and maximum and mean leaf speeds.
Conclusion
TrueBeam MLC positioning errors are substantially lower than those of Clinac linacs. In Clinac machines the analysis of dynalogs without the delay effect allows us to study the influence of factors that are masked by the delay effect.}
}
@article{SREENIVAS201114,
title = {Detecting keyloggers based on traffic analysis with periodic behaviour},
journal = {Network Security},
volume = {2011},
number = {7},
pages = {14-19},
year = {2011},
issn = {1353-4858},
doi = {https://doi.org/10.1016/S1353-4858(11)70076-9},
url = {https://www.sciencedirect.com/science/article/pii/S1353485811700769},
author = {R Sreeram Sreenivas and R Anitha},
abstract = {Security and privacy are key challenges for every computer user. While various security mechanisms, such as anti-virus, anti-spyware, updates and patches, are extensively applied, they cannot provide total security. Software keyloggers represent a fast-growing class of malware. Keylogging software records the user's typed keystrokes and saves them to a log file, and is therefore capable of logging sensitive information such as passwords, usernames, PINs and so on.1 It can also record user activities such as online chat, capture screenshots, note URLs visited and more. Moreover, some keyloggers can upload the log file directly to attackers through remote connections.2}
}

@article{KARANTH20171,
title = {The cost of developing a computerized tailored interactive multimedia intervention vs. a print based Photonovella intervention for HPV vaccine education},
journal = {Evaluation and Program Planning},
volume = {63},
pages = {1-6},
year = {2017},
issn = {0149-7189},
doi = {https://doi.org/10.1016/j.evalprogplan.2017.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0149718916302397},
author = {Siddharth S. Karanth and David R. Lairson and Lara S. Savas and Sally W. Vernon and María E. Fernández},
keywords = {HPV vaccine, Vaccine education, Tailoring, Tablet computer, IPad, Photonovella, Fotonovela, Intervention development, Cost},
abstract = {Mobile technology is opening new avenues for healthcare providers to create and implement tailored and personalized health education programs. We estimate and compare the cost of developing an i-Pad based tailored interactive multimedia intervention (TIMI) and a print based (Photonovella) intervention to increase human papillomavirus (HPV) immunization. The development costs of the interventions were calculated using a societal perspective. Direct cost included the cost of planning the study, conducting focus groups, and developing the intervention materials by the research staff. Costs also included the amount paid to the vendors who produced the TIMI and Photonovella. Micro cost data on the staff time and materials were recorded in logs for tracking personnel time, meeting time, supplies and software purchases. The costs were adjusted for inflation and reported in 2015 USD. The total cost of developing the Photonovella was $66,468 and the cost of developing the TIMI was $135,978. The amortized annual cost for the interventions calculated at a 3% discount rate and over a 7-year period was $10,669 per year for the Photonovella and $21,825 per year for the TIMI intervention. The results would inform decision makers when planning and investing in the development of interactive multimedia health interventions.}
}
@article{URMENETAULLOA2021159,
title = {Deformación miocárdica en miocardiopatía dilatada no isquémica mediante feature tracking. Factibilidad e implicaciones pronósticas},
journal = {Revista Española de Cardiología},
volume = {74},
number = {2},
pages = {159-166},
year = {2021},
issn = {0300-8932},
doi = {https://doi.org/10.1016/j.recesp.2019.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S0300893219306104},
author = {Javier {Urmeneta Ulloa} and Eduardo {Pozo Osinalde} and Juan Lizandro Rodríguez-Hernández and Hugo {Martínez Fernández} and Fabián Islas and Alberto {de Agustín} and Pedro Marcos-Alberca and Patricia Mahía and Miguel Ángel Cobos and Paula {Hernández Mateo} and José Ángel Cabrera and María {Luaces Méndez} and José Juan {Gómez de Diego} and Ana Bustos and Carlos Macaya and Leopoldo {Pérez de Isla}},
keywords = {Miocardiopatía dilatada, Deformación miocárdica, , Cardiorresonancia magnética, Dilated cardiomyopathy, Myocardial strain, Feature tracking, Cardiac magnetic resonance},
abstract = {Resumen
Introducción y objetivos
El análisis de la deformación miocárdica puede aportar información adicional a la fracción de eyección del ventrículo izquierdo (FEVI) en la miocardiopatía dilatada no isquémica (MDNI). El objetivo es analizar la factibilidad del estudio del strain del ventrículo izquierdo mediante feature tracking (FT) de cardiorresonancia magnética en la MDNI y determinar su relevancia clínica y pronóstica.
Métodos
Se incluyó retrospectivamente a los pacientes consecutivos con MDNI sometidos a cardiorresonancia magnética. Se obtuvieron el strain global longitudinal, circunferencial y radial del ventrículo izquierdo de secuencias convencionales de cine mediante un software de análisis de FT. Se evaluó su asociación con el evento combinado (insuficiencia cardiaca, implante de desfibrilador en prevención secundaria y muerte).
Resultados
Se pudo realizar el FT en los 98 pacientes evaluados (edad, 68± 13 años; el 72% varones). La concordancia intraobservador e interobservadores fue buena para el strain global longitudinal y circunferencial, y más limitada para el radial. El strain global circunferencial se asoció de manera independiente (OR=1,16; p=0,045) con la normalización de la FEVI en el seguimiento y fue el único parámetro morfológico con asociación independiente (OR=1,15; p=0,038) con el evento combinado. Un valor <–8,2% fue capaz de predecir la aparición de este evento en el seguimiento (Log-rank test, 4,6; p=0,032)
Conclusiones
El análisis del strain del ventrículo izquierdo mediante FT es factible y reproducible en MDNI. El strain global circunferencial fue capaz de predecir la recuperación de la FEVI y la aparición de eventos cardiovasculares mayores en el seguimiento.
Introduction and objectives
Myocardial strain analysis could provide additional information to left ventricular ejection fraction (LVEF) in nonischemic dilated cardiomyopathy (NIDC). Our aim was to analyze the feasibility of left ventricular strain evaluation using cardiac magnetic resonance feature tracking (FT) in NIDC, and to determine its clinical and prognostic impact.
Methods
We retrospectively included consecutive patients with NIDC who underwent cardiac magnetic resonance. Left ventricular global longitudinal, circumferential and radial strain were obtained from standard cine sequences using FT analysis software. We evaluated their association with a composite endpoint (heart failure, implantable cardioverter-defibrillator in secondary prevention, or death).
Results
FT analysis could be performed in all of the 98 patients (mean age 68±13 years, 72% men). Intra- and interobserver concordance was good for global longitudinal and circumferential strain but was worse for radial strain. Global circumferential strain was independently associated (OR, 1.16; P=.045) with LVEF normalization during follow-up and was the only morphological parameter independently associated with the composite endpoint (OR, 1.15; P=.038). A cutoff value <−8.2% was able to predict the incidence of this event during follow-up (log-rank 4.6; P=.032).
Conclusions
Left ventricular strain analysis with FT is feasible and reproducible in NIDC. Global circumferential strain was able to predict LVEF recovery and the appearance of major cardiovascular events during follow-up. Full English text available from:www.revespcardiol.org/en}
}
@article{CHOW2021255,
title = {Peer Learning Through Multi-Institutional Case Conferences: Abdominal and Cardiothoracic Radiology Experience},
journal = {Academic Radiology},
volume = {28},
number = {2},
pages = {255-260},
year = {2021},
issn = {1076-6332},
doi = {https://doi.org/10.1016/j.acra.2020.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S1076633220300428},
author = {Ryan A. Chow and Nelly Tan and Travis S Henry and Jeffrey P. Kanne and Aarti Sekhar},
keywords = {Peer learning},
abstract = {Rationale and Objectives
We describe a model of multi-institutional, multisociety, online case conferences that is a case-based group discussion of selected (nonrandom) cases which are subsequently hosted on social media and online platforms (e.g., YouTube, websites) to be available for a wider audience.
Materials and Methods
Using online conferencing software (Zoom, GoToMeeting), participants from both abdominal and cardiothoracic radiologists engage in separate, subspecialty one-hour meetings discussing a variety of meaningful cases. Participants take turns presenting their cases to the group and discuss significant findings, interpretations, differential diagnoses, and any other teaching points. All of the case conferences for both societies are recorded and edited to be uploaded on YouTube and their respective websites.
Results
Participants from these conferences log in from 14 institutions in 7 states across the United States. The YouTube videos reach thousands of people around the world. The abdominal case conference on YouTube has received almost 1,300 views with 90 videos uploaded. The thoracic (the Society of Thoracic Radiology) case conference has been running for over 7 years, with 226 videos uploaded to YouTube and 38,200 views, 1426 subscribers, and a total watch time of over 525,800 minutes. Twitter has been utilized by both groups to promote online viewership.
Conclusion
Our model is feasible and effective compared to traditional peer review. The cases selected are deliberate and focused on quality improvement and/or education. We harness online engagement, specifically social media presence, which has opened new opportunities to educate our peers and reach a global audience, including the nonradiologic community, to learn about radiology and unique practices.}
}
@article{YEKSAYEV2016107,
title = {Pipeline Networks E-modelling Based on CityComTM Technology: Experience of Industrial Implementation for Large Water-supply Systems},
journal = {Procedia Engineering},
volume = {154},
pages = {107-114},
year = {2016},
note = {12th International Conference on Hydroinformatics (HIC 2016) - Smart Water for the Future},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.07.427},
url = {https://www.sciencedirect.com/science/article/pii/S1877705816318161},
author = {Arsen Yeksayev},
keywords = {hydro-modeling, implementation, software platform, CityCom, grid operation},
abstract = {In the article below the author describes best practices of his company's 25-years experience in the field of development and industrial implementation of complex corporate-level software systems for solutions to everyday production and planning tasks in water supply. All software systems described have been developed using the specialized CityCom™ platform that includes instrumentation tools not only for GIS-based inventory and hydro-modeling but also for dispatch management, accident localization control, providing events logs & analysis (flow parameters, damages, works, machinery usage, diagnostics), planning of capital repairs and etc. The basic principle underlying the software systems provided is the main tenet of informatics science: any data has to be identified and stored only once and in one place. So all of the CityCom™ corporate-level projects use a single corporate database that consist of hundreds of tables where all the data is stored in accordance with the above-mentioned principle. The feature of CityCom™ that appears to be most in demand is the ability to perform hydraulic modeling with no limits imposed on the following parameters: network dimension, number of loops, number of sources and pump stations working simultaneously. However, a key feature of CityCom™ hydro-modeling instrumentation is that it can be used not as «standalone» application but as an integral part of a complex system combining all aspects of water-supply enterprise data modeling solution. The economic advantages of its implementation are ensured by: - optimizing of pipeline networks topology and parameters (reconstructions and switching); - optimizing of hydraulic modes according to diurnal fluctuations of water consumption (dispatching of current switches and pump stations control); - reducing the number of accidents as well as defects and damages (10-12% per year during the first 5-6 years, then stabilizing) and as a result, reducing the cost of repairs; - reducing of potable water loss down to 5..6% of water supply volume; - improving ergonomics and staff productivity. All implementations for water supply and sewerage systems are located in Russia. Largest of them in terms of scale are in the cities (population / pipelines network length): Nizhny Novgorod (1.300.000/5.000km), Novokuznetsk (550.000/1.800km), Naberezhnye Chelny (530.000/ 1.400km), Yoshkar-Ola (270.000/1.300km). Payback of expenses for implementation of the above has made from 2 to 5 years.}
}
@article{JIMENEZ201824,
title = {44 Dynamic Tumor Tracking with SBRT dedicated system VERO (Brainlab): Quality control and PTV volume reduction},
journal = {Physica Medica},
volume = {56},
pages = {24-25},
year = {2018},
note = {Abstracts of the 57èmes Journées Scientifiques de la Société Française de Physique Médicale},
issn = {1120-1797},
doi = {https://doi.org/10.1016/j.ejmp.2018.09.057},
url = {https://www.sciencedirect.com/science/article/pii/S1120179718312195},
author = {G. Jimenez and J. Mazurier and D. Marre and N. Mathy and D. Zarate and J. Camilleri and O. Gallocher and B. Pinel and C. Chevelle and D. Franck and I. Latorzeff},
abstract = {Introduction
SBRT for mobile tumors requires a breathing control method. Since 2014, the dedicated SBRT system VERO (Brainlab-Mitsubishi) provides a real-time dynamic tracking method: “dynamic tracking” (DT). We present this method and the results obtained for quality control, PTV volume and treatment time reduction.
Methods
The robotic irradiation head of the VERO system (6 MV) is equipped with a 5 mm MLC and follows in real time the target position, thanks to gyroscopic movements guided by infrared and radiological control. DT method was used for liver and pancreas (41 and 3 patients) and thoracic lesions (60 patients) for which the movement amplitude, evaluated by 4DCT (RPM Varian), was >7 mm and after marker implantation (Visicoil, IBA). The treatment plan (TPS IPLAN, Brainlab) was performed with 6 to 8 non-coplanar beams and the Monte Carlo algorithm for lungs on the exhale phase. PTV Margins were determined with the Van Herk et al. 2000 method. PTVDT volume (using DT) was compared to PTVITV (calculated with MIP images for Internal Target Volume, ITV method) for 60 patients. Treatment times were evaluated for each session for 104 patients. DT mechanical and dosimetric accuracy were evaluated with QUASAR Respiratory Motion et CIRS 008A dynamic thorax phantoms, EBT3 films associated to Doselab Pro software, and with log files analysis. Quality control for 104 patients was performed with Brainlab mobile platform and calibrated films.
Results
PTV margins are 5 mm for lung and 8 mm for liver. The mean PTVDT was 28.8 cc (6.5 to 14.3 cc) and 46.4 cc for PTVITV (10.4 to 139 cc), so a 40% volume reduction. The mean treatment time for DT was 25 min, lower than the one for ITV method for which control CBCT are necessary. The patient breathing rates (frequency and amplitude) are often irregular during treatment and different from the reference 4DCT. This doesn’t affect the treatment delivery and target tracking. With 4 year a follow-up, the clinical tolerance during and after SBRT with DT is very good with 1 case of symptomatic grade 3 radiation pneumonitis (PR) and 8 asymptomatic PR for 60 patients. The DT mecanical accuracy was measured <0.7 mm and the dosimetric accuracy <1 mm. The patient QC with films has showed an agreement between calculation and measurement for 95% patients with 95% des points with γ<1 (3%-1 mm).
Conclusion
DT allows treatments with breathing control for all patients, even those with irregular breathing rates. Treatment times are lower than classical treatments based on ITV with an accuracy <1 mm and a PTV volume reduction of 40%.}
}
@article{KRISHNAKUMAR20111039,
title = {Development and applications of energy-specific fluence monitor for field monitoring},
journal = {Applied Radiation and Isotopes},
volume = {69},
number = {7},
pages = {1039-1045},
year = {2011},
issn = {0969-8043},
doi = {https://doi.org/10.1016/j.apradiso.2011.01.042},
url = {https://www.sciencedirect.com/science/article/pii/S0969804311000753},
author = {D.N. Krishnakumar and K.M. Somayaji and R. Venkatesan and V. Meenakshisundaram},
keywords = {CsI detector, Radiation instrument, Argon plume dispersion, Environmental survey},
abstract = {A portable energy-specific fluence monitor is developed for field monitoring as well as to serve as stand-alone data acquisition system to measure dose rate due to routine releases at various locations in and around Nuclear Power Reactors. The data from an array of such monitors deployed over a region of interest would help in evolving a methodology to arrive at the source term evaluation in the event of a postulated nuclear incident. The other method that exists for this purpose is by conducting tracer experiments using known release of a gas like SF6 into the atmosphere and monitoring their concentrations downwind. The above instrument enables one to use the routine release of 41Ar as a tracer gas. The Argon fluence monitor houses a CsI(Tl) detector and associated miniature electronics modules for conditioning the signal from the detector. Data logging and in-situ archival of the data are controlled by a powerful web enabled communication controller preloaded with Microsoft Windows Compact Edition (WIN CE). The application software is developed in Visual Basic.NET under Compact Framework and deployed in the module. The paper gives an outline of the design aspects of the instrument, associated electronics and calibration of the instrument, including the preliminary results obtained using the instrument. The utility of the system is established by carrying out field survey around Madras Atomic Power Station (MAPS), consisting of two Pressurized Heavy Water Reactors (PHWR), by mapping the 41Ar plume. Additional features such as enhancing the monitor capability with embedded GPS along with real-time linking using wireless networking techniques are also being incorporated.}
}
@article{BOTEZ201864,
title = {8. Analysis of dose variations and sources of error in VMAT treatments with an EPID-based in vivo dosimetry system},
journal = {Physica Medica},
volume = {56},
pages = {64},
year = {2018},
note = {Abstracts of the 10th National Congress of the Associazione Italiana di Fisica Medica},
issn = {1120-1797},
doi = {https://doi.org/10.1016/j.ejmp.2018.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S1120179718300772},
author = {L. Botez and S. Bresciani and A. Miranti and A. Maggio and A. {Di Dia} and C. Bracco and M. Stasi},
abstract = {Purpose
To evaluate the daily dose delivery discrepancies catching sources of errors in VMAT treatments with a 3D EPID-based in vivo dosimetry (IVD) system.
Methods and materials
The IVD software PerFRACTION (Sun Nuclear Corporation) was used. PerFRACTION is based on the information concerning MLC and collimator positions obtained by EPID images and on the information of log files, such as MUs and the gantry angles. From these informations, a collapsed-cone/superposition algorithm (SDC) is used to calculate the dose distribution on daily CBCT images [1]. It also uses the SDC to produce an entire dose volume to check against that from the TPS. First of all, we calculated the dose discrepancies (ΔD%) at the isocenter (ID) and the 3D gamma passing rate (%GP) between our reference (AAA) and SDC algorithm, for 81 plans. About IVD, 3D dose distributions were reconstructed for 405 fractions of 53 patients, including prostates (21%), lungs (15%), Head & Neck (17%), PBI (10%), and palliatives (38%). We evaluated Δ D% between reference and daily dose distribution in terms of dose to 95% of the volume (D95%), dose to 2% of the volume (D2%) and average dose of the target.
Results
We obtained for ID an average ΔD% equal to (1.8 ± 0.8)% and an average %GP equal to (99 ± 2)%. For IVD, the mean ΔD% was equal to (−0.3 ± 2.2)% for D95%, (0.9 ± 1.4)% for D2%, and (0.5 ± 1.2)% for average dose. 75% of fractions showed ΔD% inferior to 2%. For 74 (18%) of the analyzed dose points, ΔD% was greater than the 3%. For the fractions that exceed 3%, the errors are due to setup (28.4%), patient anatomy (40.5%), setup + anatomy (27%) and bolus positioning (4%). No machine-based errors were observed for these patients.
Conclusion
3D EPID-based IVD is a powerful method to cacht and quantify delivery discrepancies during radiotherapy process.}
}
@article{LEITE2014349,
title = {Crossover clinical trial of the influence of the use of adhesive on biofilm formation},
journal = {The Journal of Prosthetic Dentistry},
volume = {112},
number = {2},
pages = {349-356},
year = {2014},
issn = {0022-3913},
doi = {https://doi.org/10.1016/j.prosdent.2013.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0022391313003855},
author = {Andressa R.P. Leite and Danny O. Mendoza-Marin and André G. Paleari and Larissa S. Rodriguez and Andréia A. Roccia and Vivian B. Policastro and Marco A. Compagnoni and Raphael F. {de Souza} and Ana C. Pero},
abstract = {Statement of problem
Contrasting results have been reported regarding the influence of the use of adhesive on biofilm formation.
Purpose
The purpose of this study was to evaluate the influence of the use of adhesive on the formation of biofilm on the internal surface of complete dentures and the palatal mucosa of denture wearers.
Material and methods
Thirty participants with well-fitting complete dentures were randomly divided according to the experimental design: protocol 1, adhesive use during the first 15 days, followed by no use of adhesive over the next 15 days; protocol 2, no use of adhesives during the first 15 days, followed by adhesive use over the next 15 days. After each period, material from the mucosa and intaglio of the maxillary dentures was collected. Replicate aliquots were plated onto Petri dishes containing selective media for Candida spp, Streptococcus mutans, and a nonselective culture medium. Colony-forming units were expressed as log (CFU+1)/mL. In addition, the internal surfaces of the maxillary and mandibular complete dentures were stained and photographed. From the photographs, the total internal surface and the surface stained with biofilm were quantified (software ImageTool 3.00), and the percentage of the biofilm-covered area (%) on the maxillary and mandibular dentures was calculated and compared with 2-way ANOVA. For the nonselective culture medium, data were compared with the paired-sample t test, and the Wilcoxon signed rank test was performed to compare the colony counts of Candida spp and Streptococcus mutans (α=.05).
Results
Similar colony counts were found with or without the use of adhesive for the mucosa and internal surfaces of maxillary dentures, irrespective of the culture medium (P>.05). The area of dentures covered with biofilm was influenced by the use of adhesive (P=.025), regardless of the type of denture (P=.121).
Conclusions
The use of adhesive did not alter the colony counts of microorganisms from the palatal mucosa and maxillary dentures of complete denture wearers during the 15-day period, but it did influence the area covered with biofilm on the internal surfaces of the complete dentures.}
}
@article{CHABOT201583,
title = {An ontology-based approach for the reconstruction and analysis of digital incidents timelines},
journal = {Digital Investigation},
volume = {15},
pages = {83-100},
year = {2015},
note = {Special Issue: Big Data and Intelligent Data Analysis},
issn = {1742-2876},
doi = {https://doi.org/10.1016/j.diin.2015.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S1742287615000869},
author = {Yoan Chabot and Aurélie Bertaux and Christophe Nicolle and Tahar Kechadi},
keywords = {Digital forensics, Event reconstruction, Forensic ontology, Knowledge extraction, Ontology population, Timeline analysis},
abstract = {Due to the democratisation of new technologies, computer forensics investigators have to deal with volumes of data which are becoming increasingly large and heterogeneous. Indeed, in a single machine, hundred of events occur per minute, produced and logged by the operating system and various software. Therefore, the identification of evidence, and more generally, the reconstruction of past events is a tedious and time-consuming task for the investigators. Our work aims at reconstructing and analysing automatically the events related to a digital incident, while respecting legal requirements. To tackle those three main problems (volume, heterogeneity and legal requirements), we identify seven necessary criteria that an efficient reconstruction tool must meet to address these challenges. This paper introduces an approach based on a three-layered ontology, called ORD2I, to represent any digital events. ORD2I is associated with a set of operators to analyse the resulting timeline and to ensure the reproducibility of the investigation.}
}
@article{SARGOS20201341,
title = {Adjuvant radiotherapy versus early salvage radiotherapy plus short-term androgen deprivation therapy in men with localised prostate cancer after radical prostatectomy (GETUG-AFU 17): a randomised, phase 3 trial},
journal = {The Lancet Oncology},
volume = {21},
number = {10},
pages = {1341-1352},
year = {2020},
issn = {1470-2045},
doi = {https://doi.org/10.1016/S1470-2045(20)30454-X},
url = {https://www.sciencedirect.com/science/article/pii/S147020452030454X},
author = {Paul Sargos and Sylvie Chabaud and Igor Latorzeff and Nicolas Magné and Ahmed Benyoucef and Stéphane Supiot and David Pasquier and Menouar Samir Abdiche and Olivier Gilliot and Pierre Graff-Cailleaud and Marlon Silva and Philippe Bergerot and Pierre Baumann and Yazid Belkacemi and David Azria and Meryem Brihoum and Michel Soulié and Pierre Richaud},
abstract = {Summary
Background
Adjuvant radiotherapy reduces the risk of biochemical progression in prostate cancer patients after radical prostatectomy. We aimed to compare adjuvant versus early salvage radiotherapy after radical prostatectomy, combined with short-term hormonal therapy, in terms of oncological outcomes and tolerance.
Methods
GETUG-AFU 17 was a randomised, open-label, multicentre, phase 3 trial done at 46 French hospitals. Men aged at least 18 years who had an Eastern Cooperative Oncology Group performance status of 1 or less, localised adenocarcinoma of the prostate treated with radical prostatectomy, who had pathologically-staged pT3a, pT3b, or pT4a (with bladder neck invasion), pNx (without pelvic lymph nodes dissection), or pN0 (with negative lymph nodes dissection) disease, and who had positive surgical margins were eligible for inclusion in the study. Eligible patients were randomly assigned (1:1) to either immediate adjuvant radiotherapy or delayed salvage radiotherapy at the time of biochemical relapse. Random assignment, by minimisation, was done using web-based software and stratified by Gleason score, pT stage, and centre. All patients received 6 months of triptorelin (intramuscular injection every 3 months). The primary endpoint was event-free survival. Efficacy and safety analyses were done on the intention-to-treat population. The trial is registered with ClinicalTrials.gov, NCT00667069.
Findings
Between March 7, 2008, and June 23, 2016, 424 patients were enrolled. We planned to enrol 718 patients, with 359 in each study group. However, on May 20, 2016, the independent data monitoring committee recommended early termination of enrolment because of unexpectedly low event rates. At database lock on Dec 19, 2019, the overall median follow-up time from random assignment was 75 months (IQR 50–100), 74 months (47–100) in the adjuvant radiotherapy group and 78 months (52–101) in the salvage radiotherapy group. In the salvage radiotherapy group, 115 (54%) of 212 patients initiated study treatment after biochemical relapse. 205 (97%) of 212 patients started treatment in the adjuvant group. 5-year event-free survival was 92% (95% CI 86–95) in the adjuvant radiotherapy group and 90% (85–94) in the salvage radiotherapy group (HR 0·81, 95% CI 0·48–1·36; log-rank p=0·42). Acute grade 3 or worse toxic effects occurred in six (3%) of 212 patients in the adjuvant radiotherapy group and in four (2%) of 212 patients in the salvage radiotherapy group. Late grade 2 or worse genitourinary toxicities were reported in 125 (59%) of 212 patients in the adjuvant radiotherapy group and 46 (22%) of 212 patients in the salvage radiotherapy group. Late genitourinary adverse events of grade 2 or worse were reported in 58 (27%) of 212 patients in the adjuvant radiotherapy group versus 14 (7%) of 212 patients in the salvage radiotherapy group (p<0·0001). Late erectile dysfunction was grade 2 or worse in 60 (28%) of 212 in the adjuvant radiotherapy group and 17 (8%) of 212 in the salvage radiotherapy group (p<0·0001).
Interpretation
Although our analysis lacked statistical power, we found no benefit for event-free survival in patients assigned to adjuvant radiotherapy compared with patients assigned to salvage radiotherapy. Adjuvant radiotherapy increased the risk of genitourinary toxicity and erectile dysfunction. A policy of early salvage radiotherapy could spare men from overtreatment with radiotherapy and the associated adverse events.
Funding
French Health Ministry and Ipsen.}
}
@article{GOMESPEREIRA2016106,
title = {Current and future trends in marine image annotation software},
journal = {Progress in Oceanography},
volume = {149},
pages = {106-120},
year = {2016},
issn = {0079-6611},
doi = {https://doi.org/10.1016/j.pocean.2016.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0079661116301240},
author = {Jose Nuno Gomes-Pereira and Vincent Auger and Kolja Beisiegel and Robert Benjamin and Melanie Bergmann and David Bowden and Pal Buhl-Mortensen and Fabio C. {De Leo} and Gisela Dionísio and Jennifer M. Durden and Luke Edwards and Ariell Friedman and Jens Greinert and Nancy Jacobsen-Stout and Steve Lerner and Murray Leslie and Tim W. Nattkemper and Jessica A. Sameoto and Timm Schoening and Ronald Schouten and James Seager and Hanumant Singh and Olivier Soubigou and Inês Tojeira and Inge {van den Beld} and Frederico Dias and Fernando Tempera and Ricardo S. Santos},
keywords = {Underwater visual surveys, Image analysis, Image annotation, Data collection, Data storage, Monitoring, Marine imaging},
abstract = {Given the need to describe, analyze and index large quantities of marine imagery data for exploration and monitoring activities, a range of specialized image annotation tools have been developed worldwide. Image annotation – the process of transposing objects or events represented in a video or still image to the semantic level, may involve human interactions and computer-assisted solutions. Marine image annotation software (MIAS) have enabled over 500 publications to date. We review the functioning, application trends and developments, by comparing general and advanced features of 23 different tools utilized in underwater image analysis. MIAS requiring human input are basically a graphical user interface, with a video player or image browser that recognizes a specific time code or image code, allowing to log events in a time-stamped (and/or geo-referenced) manner. MIAS differ from similar software by the capability of integrating data associated to video collection, the most simple being the position coordinates of the video recording platform. MIAS have three main characteristics: annotating events in real time, posteriorly to annotation and interact with a database. These range from simple annotation interfaces, to full onboard data management systems, with a variety of toolboxes. Advanced packages allow to input and display data from multiple sensors or multiple annotators via intranet or internet. Posterior human-mediated annotation often include tools for data display and image analysis, e.g. length, area, image segmentation, point count; and in a few cases the possibility of browsing and editing previous dive logs or to analyze the annotations. The interaction with a database allows the automatic integration of annotations from different surveys, repeated annotation and collaborative annotation of shared datasets, browsing and querying of data. Progress in the field of automated annotation is mostly in post processing, for stable platforms or still images. Integration into available MIAS is currently limited to semi-automated processes of pixel recognition through computer-vision modules that compile expert-based knowledge. Important topics aiding the choice of a specific software are outlined, the ideal software is discussed and future trends are presented.}
}
@article{KATOUAH2019740,
title = {Synthesis of new Cu(II)-benzohydrazide nanometer complexes, spectral, modeling, CT-DNA binding with potential anti-inflammatory and anti-allergic theoretical features},
journal = {Materials Science and Engineering: C},
volume = {96},
pages = {740-756},
year = {2019},
issn = {0928-4931},
doi = {https://doi.org/10.1016/j.msec.2018.11.034},
url = {https://www.sciencedirect.com/science/article/pii/S0928493117344405},
author = {Hanadi A. Katouah and Jabir H. Al-Fahemi and Marwa G. Elghalban and Fawaz A. Saad and Ismail A. Althagafi and Nashwa M. El-Metwaly and Abdalla M. Khedr},
keywords = {Nanometer Cu(II) complexes, DFT/B3LYP, Anti-inflammatory, Anti-allergic docking and DNA binding},
abstract = {New nanometer Cu(II)-benzohydrazide complexes were synthesized and characterized. Mono negative tetra-dentate mode is the general feature proposed for all coordinating ligands. Variable structural forms were established, square-planer, tetrahedral and octahedral arrangements around copper centers. XRD and TEM studies displayed a nanometer size for crystalline compounds. TGA analysis of new complexes showed low thermal stability due to the presence of crystal water molecules. Kinetic parameters were calculated using two comparative methods for assertion. ESR study was performed on three chosen complexes to estimate essential spectral parameters and assert on proposed geometries. Gaussian09 software program and applying DFT/B3LYP method was used for optimizing all structures to give the best arrangement for atoms. Essential indexes were extracted from log files as well as other indexes were computed based on frontier energy gaps. Potential theoretical anti-inflammatory, antitumor and anti-allergic studies were executed using Autodock 4.2 tools. Essential energies were calculated over docking complexes corresponding to 5HN1, 5AV1 and 4H1L protein receptors for three pathogens (inflammation, liver cancer and allergy, respectively). H2L5 ligand displays significant activity towards inflammation and allergy diseases. Such potential feature will give a well insight about their biological attitude in future experimentation.}
}
@article{COSSOLI2014763,
title = {Test Bench for the Characterization of Batteries for Use in SAPS},
journal = {Energy Procedia},
volume = {57},
pages = {763-772},
year = {2014},
note = {2013 ISES Solar World Congress},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2014.10.284},
url = {https://www.sciencedirect.com/science/article/pii/S1876610214016518},
author = {P. Cossoli and L. Vera and A. Busso},
keywords = {Solar energy, photovoltaic, translation to standard test condition, grid connected photovoltaic systems.},
abstract = {This article presents the development of a test bench for the characterization of batteries for solar uses as well as results of tests conducted using solar batteries. The test bench comprises three main blocks: the thermal module, the power supply and the measuring module. The thermal module consists of a thermally regulated bath with a heating and cooling system capable of maintaining a constant working temperature with an accuracy of ± 0.3°C. The power supply is a bipolar Kepco, BOP 36-12D that allows charging and discharging of batteries with a control unit that allows its automatic operation. Finally, the measuring system comprises a set of sensors and circuits with the capability of logging battery voltage, charge / discharge current, density and temperature. All these variables are measured with an error lower than that specified by the standard IEC 61427 - 2005. Gathered data are transmitted to a PC where they are recorded and displayed in real time. The software supports various charge / discharge profiles with different values of the current, the bath temperature and test duration hence allowing the possibility of cyclic testing. The test bench developed is currently the first of its kind in the country with the capability to comply with standards as to the characterization of solar batteries is concerned.}
}
@article{GARG20171,
title = {RITS-MHT: Relative indexed and time stamped Merkle hash tree based data auditing protocol for cloud computing},
journal = {Journal of Network and Computer Applications},
volume = {84},
pages = {1-13},
year = {2017},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2017.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S1084804517300668},
author = {Neenu Garg and Seema Bawa},
keywords = {Proof of possession, Data integrity audit, Third party audit, Cloud computing},
abstract = {Cloud computing is a recent phrase in marketing for a concept which has been known for years: Outsourcing. Cloud computing provides cornucopian of gratuities for both individual users and business organizations. ‘Cloud’ is more of a notion where files and data are hosted online and can be accessed when required via a number of methods, anywhere, and at any time. That is the gist of it. Providing storage space to cloud users is an appealing service of cloud computing. Although cloud storage provides benefits of location independence access to data, reduced burden for hardware and software maintenance and many more, yet this service has several challenges in the range of security and preserving data. Cloud servers may exist in white puffy shapes in the azure, but these are not immune to temporal errors. To ensure that outsourced data is secure and is not tampered, cloud provider must allow data proprietor to periodically audit data integrity. Numerous Remote Data Auditing (RDA) protocols have been proposed by researchers so far. In presented work, to cope with this problem we analyze the efficiency issues of a current protocol for data integrity auditing in cloud storage and propose an approach based on Relative Index and Time Stamped Merkle Hash Tree (RITS−−MHT) which integrates MHT with relative index of a node resulting in reduction of computation cost of searching a data block from O(n) in Wang's protocol to O(log n) and time of last modification to data, thereby guarantying freshness of data respectively. RITS−−MHT ensures that the outsourced data has not been polluted as well as it assures that the recent copy of data is reclaimed. This protocol supports public auditing of data, and efficiently supports data dynamic operations like insertion, modification, deletion of outsourced data at minimal computational cost. The security of proposed protocol has been proved in Random Oracle Model (ROM). As compared to Wang's protocol, RITS−−MHT is more efficient in terms of computation cost.}
}
@article{CRAIEM201622,
title = {Association of thoracic aorta calcium and non cardiac vascular events in cardiac disease-free individuals},
journal = {Atherosclerosis},
volume = {245},
pages = {22-27},
year = {2016},
issn = {0021-9150},
doi = {https://doi.org/10.1016/j.atherosclerosis.2015.11.030},
url = {https://www.sciencedirect.com/science/article/pii/S0021915015302264},
author = {Damian Craiem and Dr Gilles Chironi and Mariano E. Casciaro and Marie-Emmanuelle Sirieix and Elie Mousseaux and Alain Simon},
keywords = {Cardiovascular risk, Cerebrovascular disease, Coronary artery calcium, Vascular calcification, Peripheral vascular disease},
abstract = {Objective
Thoracic aorta calcium (TAC) is measurable on the same computed tomography (CT) scan as coronary artery calcium (CAC) but has still unclear clinical value. We assessed TAC and CAC relations with non-cardiac vascular events history in a cohort of subjects at risk for cardiovascular disease.
Methods
We analyzed retrospectively 1000 consecutive subjects having undergone CAC detection by non-contrast multi-slice CT with measurement field longer than usual in order to measure total TAC including aortic arch calcium. We also determined partial TAC restricted to ascending and descending thoracic aorta sites by removing arch calcium from total TAC. Calcium deposits were measured with a custom made software using Agatston score.
Results
Compared with the rest of the cohort, the 30 subjects with non-cardiac vascular event history had higher median values [95% CI] of total TAC (282 [28–1809] vs 39 [0–333], p < 0.01) and partial TAC (4 [0–284] vs 0 [0–5], p < 0.01) but no different value of CAC (73 [0–284] vs 16 [0–148]). Odds ratio [95% CI] of having non-cardiac vascular event per 1-SD increase in log-transformed calcium value was significant for total TAC but not for CAC, if total TAC and CAC were entered separately (1.56 [1.12–2.24], p < 0.01 and 1.13 [0.86–1.50], respectively) or together (1.57 [1.10–2.32], p < 0.01 and 0.98 [0.73–1.32], respectively) in the logistic adjusted model.
Conclusion
TAC assessment simultaneous with CAC detection provides complementary information on the extra coronary component of cardiovascular risk beyond CAC's coronary risk prediction. Further studies are required to prospectively confirm this result.}
}
@article{DABBS2019S199,
title = {Lung Transplant Clinicians’ Perceptions of Pocket PATH Synergy, an Interactive Health Technology (IHT) to Monitor Patients Remotely},
journal = {The Journal of Heart and Lung Transplantation},
volume = {38},
number = {4, Supplement },
pages = {S199},
year = {2019},
issn = {1053-2498},
doi = {https://doi.org/10.1016/j.healun.2019.01.482},
url = {https://www.sciencedirect.com/science/article/pii/S105324981930484X},
author = {A.J. Devito Dabbs and T. Irizarry and M. Alrawashdeh and J.M. Pilewski and M. Morrell and J. D'Cunha and M. Dew},
abstract = {Purpose
PocketPATH Synergy (PPS) is an IHT that combines customized smartphone software (PocketPATH: Personal Assistant for Tracking Health) with PocketPATH Link, a clinician website, to share data collected through the smartphone app between lung transplant recipients (LTR) and the transplant team. The success of PPS depends on clinicians' adoption of the website. Before possible full-scale deployment, our aims were to assess clinicians' intention to use the website, their actual use, and perceptions regarding its usability and acceptability.
Methods
All 22 members of the lung transplant team (including surgeons, pulmonologists, nurses) were invited to participate. Responders were oriented to the website and asked about their intention to use the website. After the website was deployed, clinicians were followed prospectively as they monitored data from 27 LTRs uploaded in real-time to the website over a 2-month period. Clinicians were instructed to view the website at will. At the end of the 2-months, clinicians were sent an on-line survey including the Perceived Ease of Use and Usefulness scales and open-ended questions related to acceptability of the website (benefits and barriers).
Results
18/22 (88%) agreed to use the website (physicians 11, nurses 7). Over the 2 months, 2/11 (18%) of physicians logged in at least once and 4/18 (22%) of professionals (all nurses) met the threshold of weekly log-ins. 15/18 (83%) completed the post-survey regardless of whether they logged-in to the website. Over 85% agreed that the website was useful and easy to use. The most commonly cited benefits were to: monitor patients between visits (92%) and identify potential problems early (83%). The most common barriers were: too busy to use the website (83%), interruption of workflow (75%), forgot log-in (50%), and inconvenient to log-on to another website besides the electronic health record (EHR) (33%).
Conclusion
Clinicians' intention to use, perceived usability and acceptability of the website were favorable, but actual use was low. Future work should integrate data into the EHR and use of the website into clinicians' workflow. Data suggest that as barriers are resolved, assessment of how PPS impacts clinical management will be a crucial next step. Funding: NR010711}
}
@article{SU2012424,
title = {Cardiac Output Derived From Arterial Pressure Waveform Analysis in Patients Undergoing Liver Transplantation: Validity of a Third-Generation Device},
journal = {Transplantation Proceedings},
volume = {44},
number = {2},
pages = {424-428},
year = {2012},
issn = {0041-1345},
doi = {https://doi.org/10.1016/j.transproceed.2011.12.036},
url = {https://www.sciencedirect.com/science/article/pii/S0041134511016988},
author = {B.C. Su and Y.F. Tsai and C.Y. Chen and H.P. Yu and M.W. Yang and W.C. Lee and C.C. Lin},
abstract = {Background
Hemodynamic monitoring is essential to a successful liver transplantation procedure. FloTrac, a hemodynamic monitor that uses arterial-waveform-based pulse contour analysis for cardiac output (CO) measurement, has proven useful in many clinical settings. One of the primary foci of FloTrac's recent third-generation software upgrade was improving its accuracy in low systemic vascular resistance status. We evaluated the accuracy of the upgraded FloTrac monitor during liver transplantation.
Materials and methods
Twenty-eight patients undergoing liver transplantation were enrolled in the study. Two sets of CO were measured with a radial arterial line connected to a FloTrac monitor (COFT) and a pulmonary artery catheter connected to a continuous cardiac output Vigilence monitor (COPAC). Simultaneous CO measurement was performed and recorded every 5 minutes throughout the surgery. Bland-Altman analysis was used to estimate the accuracy. The comparative method and reference method were considered interchangeable if the limits of agreement did not exceed a threshold set a priori at the greater of ±1 L/min, or a percentage error of lesser than 30%.
Results
In all, 3234 paired data were collected. The bias was −0.8 L/min and the limits of agreements were −5.6 to 4.0 L/min. Percentage error was 75%. Regression analysis of the systemic vascular resistance index (SVRI) and the bias between COPAC and COFT showed that the bias was inversely related to the SVRI [r2 = 0.49; P < .001, y = −32.1983 + 9.9978 Log(x)].
Conclusions
Despite a software upgrade, the effectiveness of the FloTrac artery-derived cardiac output monitor for CO measurement during liver transplantation remains limited.}
}
@article{SANTOS201413,
title = {A programmable information system for management and analysis of aquatic species range data in California},
journal = {Environmental Modelling & Software},
volume = {53},
pages = {13-26},
year = {2014},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2013.10.024},
url = {https://www.sciencedirect.com/science/article/pii/S1364815213002673},
author = {Nicholas R. Santos and Jacob V.E. Katz and Peter B. Moyle and Joshua H. Viers},
keywords = {Expert opinion, Biological observation, Species distribution, Geographic information systems (GIS), Database, Arcpy},
abstract = {The decline of species worldwide is both alarming and difficult to document due to a lack of reliable information on the geospatial extent and corresponding status of a given taxon. Freshwater habitats are disproportionately degraded globally with resultant declines in populations in freshwater fishes and subsequent retractions in biogeographic ranges. Conservation challenges in freshwater are compounded because aquatic taxa are inherently difficult to map. We addressed this problem for California freshwater fishes by developing the software and underlying database. The software consists of a Python program, database, and suite of tools using ESRI ArcGIS scripting interfaces to translate species range data into an electronic record set of occurrences housed in Microsoft Access. The system was designed to capture, store, map, and report on the spatial and temporal dynamics of targeted species by using standard spatial units as primary indexing objects to meet current natural resource management objectives. However, the software not only tracks the provenance of underlying empirical records through space and time, but also is robust to inferential modeling results and expert knowledge, which allows for future empirical discovery and validation. After importing and standardizing 274,555 records from 154 data layers, we found that most existing records are highly concentrated spatially, representing only 39% of the mapping domain. We also determined that most empirical records are skewed toward recreational fisheries, with few records documenting the range of native species found in California. Future biogeographic mapping efforts will be aided by the baseline data and updated range maps contained in the database. Although the system is currently used for the inventory and mapping of native freshwater fish species in California, the underlying informatics framework is agnostic to biological taxonomy or spatial realm allowing other to adapt the computer code and database for their own needs.}
}
@article{GREGORY2021110189,
title = {The impact of cyclical, multi-decadal to centennial climate variability on arsenic sequestration in lacustrine sediments},
journal = {Palaeogeography, Palaeoclimatology, Palaeoecology},
volume = {565},
pages = {110189},
year = {2021},
issn = {0031-0182},
doi = {https://doi.org/10.1016/j.palaeo.2020.110189},
url = {https://www.sciencedirect.com/science/article/pii/S0031018220306374},
author = {B.R.B. Gregory and R.T. Patterson and J.M. Galloway and E.G. Reinhardt},
keywords = {Itrax-XRF, Wavelet analysis, Spectral analysis, Solar cycles, Pacific Decadal Oscillation, Arctic Oscillation},
abstract = {Examining paleoclimate-driven changes of elemental contaminants, such as Arsenic (As), increases the understanding of the mobility and fate of elements under a warming climate scenario. To characterize the variability in As sequestration in the sediments of a freshwater system in response to decadal- to centennial-scale climate oscillations, a freeze-core (CON01) was recovered from Control Lake, Northwest Territories. Radiocarbon dating of 13 bulk-organic samples provided temporal reference to core depth. Sediment geochemistry was determined using Itrax X-ray fluorescence core-scanning (Itrax-XRF). Elemental concentrations were measured on a sub-set of samples using ICP-MS after multi-acid (MA) digestion to assess the accuracy of Itrax-XRF results through a multivariate log-ratio (MLC) calibration. Comparison of Itrax-XRF to ICP-MS using the MLC in ItraXelerate software show Pearson's R2 values >0.75, with the exception of As (R2 = 0.44). MLC-calibrated Itrax-XRF elemental data were centered log-ratio (CLR) transformed to eliminate issues related to data closure. During the ca. 3300-yr sedimentary record, moderate-strength negative correlations between AsCLR and KCLR (Spearman's ρ = −0.38, p-value < 0.001, n = 785), and AsCLR and TiCLR, (Spearman's ρ = −0.52, p-value < 0.001, n = 785) suggest that As is primarily sequestered in sediments during intervals of warmer temperatures and higher productivity. Proxies for sediment particle size (TiCLR, KCLR) and As concentration (AsCLR) were examined for response to quasi-periodic climate oscillations using spectral analysis. Significant periodicities were observed with approximately 4–13, 30–60, 90–120, and 160–280 yr periods in TiCLR, KCLR, and AsCLR records. These frequencies are interpreted as corresponding to the North Atlantic Oscillation and/or 8–14-yr Schwabe sunspot cycles, 30–60-yr Pacific Decadal Oscillation, and centennial-scale solar cycles (e.g., 90-yr Gleissberg cycle; 205-yr Suess cycle). Coeval occurrence of these periodicities revealed through wavelet analysis of Control Lake geochemistry data suggests that these climate cycles only impact Control Lake when they occur concurrently.}
}
@article{BAUMGRASS2013148,
title = {Bridging the gap between role mining and role engineering via migration guides},
journal = {Information Security Technical Report},
volume = {17},
number = {4},
pages = {148-172},
year = {2013},
note = {Special Issue: ARES 2012 7th International Conference on Availability, Reliability and Security},
issn = {1363-4127},
doi = {https://doi.org/10.1016/j.istr.2013.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1363412713000198},
author = {Anne Baumgrass and Mark Strembeck},
keywords = {RBAC, Migration, Model comparison, Role engineering, Role mining},
abstract = {In the context of role-based access control (RBAC), mining approaches, such as role mining or organizational mining, can be applied to derive permissions and roles from a system's configuration or from log files. In this way, mining techniques document the current state of a system and produce current-state RBAC models. However, such current-state RBAC models most often follow from structures that have evolved over time and are not the result of a systematic rights management procedure. In contrast, role engineering is applied to define a tailored RBAC model for a particular organization or information system. Thus, role engineering techniques produce a target-state RBAC model that is customized for the business processes supported via the respective information system. The migration from a current-state RBAC model to a tailored target-state RBAC model is, however, a complex task. In this paper, we present a systematic approach to migrate current-state RBAC models to target-state RBAC models. In particular, we use model comparison techniques to identify differences between two RBAC models. Based on these differences, we derive migration rules that define which elements and element relations must be changed, added, or removed. A migration guide then includes all migration rules that need to be applied to a particular current-state RBAC model to produce the corresponding target-state RBAC model. We conducted two comparative studies to identify which visualization technique is most suitable to make migration guides available to human users. Based on the results of these comparative studies, we implemented tool support for the derivation and visualization of migration guides. Our software tool is based on the Eclipse Modeling Framework (EMF). Moreover, this paper describes the experimental evaluation of our tool.}
}
@article{ZAHRAN202094,
title = {3D-modeling and lithostratigraphic correlation of the subsurface upper cretaceous Duwi phosphates at Wadi Ash-Shaghab, East Sibaiya area, southern Egypt},
journal = {Solid Earth Sciences},
volume = {5},
number = {2},
pages = {94-102},
year = {2020},
issn = {2451-912X},
doi = {https://doi.org/10.1016/j.sesci.2020.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2451912X2030009X},
author = {Esam Zahran},
keywords = {3D-modeling, Phosphate ore, Campanian, Duwi Formation, Sibaiya area, Aswan, Egypt},
abstract = {The present work deals with the three-dimensional (3D) modeling of the subsurface phosphate ore body at Wadi Ash-Shaghab, East Sibaiya area, Southern Egypt. The construction of lithologic logs of 23 boreholes and 3D modeling in the region, is based on RockWorks2006, Geographic Information System (GIS) and Surfer 9 softwares. The outputs include stratigraphic cross-sections, solid stratigraphic models, stratigraphic Fence diagrams, structural elevation surface diagrams and isopach maps of both phosphate ore body and overburden rocks. The shape, thickness and quality of ore body, together with the overburden rocks have been estimated. The reserve of phosphate ore body at Wadi Ash-Shaghab area is suggested to be about 8673.280 tons, in the study area. However, the subsurface lithostratigraphic correlation in East Sibaiya area showed that the maximum thickness (3 m), of the ore body was found at Wadi El-Batur, Umm Salamah and El Mahamid El-Gedida, whereas the minimum thickness (60 cm–120 cm) was recorded at Umm Tundubah and Umm Hegarah. Moreover, this lithostratigraphic correlation showed that the overburden's thickness increases from east (3.0–7.0 m) towards the west (10–40 m) of the studied area.}
}
@article{VERSEN20151395,
title = {Test setup for reliability studies of DDR2 SDRAM},
journal = {Microelectronics Reliability},
volume = {55},
number = {9},
pages = {1395-1399},
year = {2015},
note = {Proceedings of the 26th European Symposium on Reliability of Electron Devices, Failure Physics and Analysis},
issn = {0026-2714},
doi = {https://doi.org/10.1016/j.microrel.2015.06.028},
url = {https://www.sciencedirect.com/science/article/pii/S002627141530024X},
author = {M. Versen and W. Ernst and G. Singh and P. Gulati},
keywords = {DRAM retention solder simulation},
abstract = {A DDR2 DRAM test setup is developed and implemented on the Griffin III ATE test system from HILEVEL Technologies. The test system provides a raw platform for performing various mixed signal and digital tests. In order to configure patterns easily in a vector format, a software platform is developed to manage test patterns according to the user's analysis needs. As examples, retention test patterns with disabled self-refresh are applied to 2Gbit DDR2 SDRAM of two different DRAM vendors. The devices are characterized in respect to their intrinsic leakage and data retention behavior under the influence of stress conditions such as temperature or access algorithm. The tests are automated and test data is logged for an off-line data analysis. Data is recorded before and after solder simulation steps in order to observe a retention time degradation.}
}
@article{ARIFIN20181864,
title = {Data for in-situ industrial site characterization with the applications of combined subsurface and surface mapping},
journal = {Data in Brief},
volume = {18},
pages = {1864-1868},
year = {2018},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2018.04.119},
url = {https://www.sciencedirect.com/science/article/pii/S2352340918304724},
author = {Mohd Hariri Arifin and John Stephen Kayode and Muhammad Azrief Azahar and Habibah Jamil and Saznira Fadila Ahmad Sabri},
abstract = {The paper presents the data from the surface and subsurface mapping of this area for the purpose of siting industrial city in the area. The field data collected combine with the borehole data was to successfully apply these to solving geological, environmental and engineering complications posed by the complexity of the subsurface geological structures underlain this area. The Electrical Resistivity, (ER) and Induced Polarization, (IP) data were initially processed using RES2DINV software model to generate the depth to the lithological units together with topographic correction. The 2-D ER and IP data were collected from 23rd April 2017 up until 7th May 2017 covering a total of about 17.6 km along 44 survey lines using ABEM Terrameter SAS4000 for the field measurement. A total of 20 Borehole logs data were recorded to better characterized in-situ, the subsurface geological formations emplaced in the study area. The study area is located at Bagan Datuk, Perak Darul Ridzuan situated on Latitude 2° 44.653'N and Longitudes 104° 28.79' E along the west coast Peninsula Malaysia. The topography of the area is generally flat low–laying and elevation range from about 0 m to 32 m above mean sea level (MSL).}
}
@article{VILLAGOMEZ201448,
title = {Sulfide response analysis for sulfide control using a pS electrode in sulfate reducing bioreactors},
journal = {Water Research},
volume = {50},
pages = {48-58},
year = {2014},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2013.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0043135413007719},
author = {D.K. Villa-Gomez and J. Cassidy and K.J. Keesman and R. Sampaio and P.N.L. Lens},
keywords = {Control, pS, Sulfide, Bioreactor, Organic loading rate},
abstract = {Step changes in the organic loading rate (OLR) through variations in the influent chemical oxygen demand (CODin) concentration or in the hydraulic retention time (HRT) at constant COD/SO42− ratio (0.67) were applied to create sulfide responses for the design of a sulfide control in sulfate reducing bioreactors. The sulfide was measured using a sulfide ion selective electrode (pS) and the values obtained were used to calculate proportional–integral–derivative (PID) controller parameters. The experiments were performed in an inverse fluidized bed bioreactor with automated operation using the LabVIEW software version 2009®. A rapid response and high sulfide increment was obtained through a stepwise increase in the CODin concentration, while a stepwise decrease to the HRT exhibited a slower response with smaller sulfide increment. Irrespective of the way the OLR was decreased, the pS response showed a time-varying behavior due to sulfide accumulation (HRT change) or utilization of substrate sources that were not accounted for (CODin change). The pS electrode response, however, showed to be informative for applications in sulfate reducing bioreactors. Nevertheless, the recorded pS values need to be corrected for pH variations and high sulfide concentrations (>200mg/L).}
}
@article{SINGH2019267,
title = {Blood Pressure Monitoring System using Wireless technologies},
journal = {Procedia Computer Science},
volume = {152},
pages = {267-273},
year = {2019},
note = {International Conference on Pervasive Computing Advances and Applications- PerCAA 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919306684},
author = {Bharat Singh and Shabana Urooj and Sakshi Mishra and Surojeet Haldar},
keywords = {Drug Delivery System, Fuzzy Inference System, Mean Arterial Blood Pressure, Sodium Nitroprusside, Maximum a posteriori estimators},
abstract = {This paper presents a simple solution for monitoring blood pressure in an economic and user-friendly method. Combining the concepts of Internet of Things with an Arduino microcontroller and a pressure sensor a Blood Pressure Monitoring System using Wireless Technologies are developed. The project aims to setup a network so that concerned people can remotely access patient’s blood pressure readings. Bluetooth and Wi-Fi technology are used to access results on hand held devices like mobiles, tabs, laptops etc. The project also incorporates a prediction algorithm via MATLAB software program. Readings can be recorded overtime manually and when into the program such a data log is passed, it predicts possible blood pressure values for the patient and as well as suggest medical assistance like dosage of medicines}
}
@article{BERNARDI2018111,
title = {The relation between developers’ communication and fix-Inducing changes: An empirical study},
journal = {Journal of Systems and Software},
volume = {140},
pages = {111-125},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.02.065},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218300426},
author = {Mario Luca Bernardi and Gerardo Canfora and Giuseppe A. {Di Lucca} and Massimiliano {Di Penta} and Damiano Distante},
keywords = {Bug management, Developers’ communication, Social network analysis, Empirical study},
abstract = {Background Many open source and industrial projects involve several developers spread around the world and working in different timezones. Such developers usually communicate through mailing lists, issue tracking systems or chats. Lack of adequate communication can create misunderstanding and could possibly cause the introduction of bugs. Aim This paper aims at investigating the relation between the bug inducing and fixing phenomenon and the lack of written communication between committers in open source projects. Method We performed an empirical study that involved four open source projects, namely Apache httpd, GNU GCC, Mozilla Firefox, and Xorg Xserver. For each project change history data, issue tracker comments, mailing list messages, and chat logs were analyzed in order to answer four research questions about the relation between the social importance and communication level of committers and their proneness to induce bug fixes. Results and implications Results indicate that the majority of bugs are fixed by committers who did not induce them, a smaller but substantial percentage of bugs is fixed by committers that induced them, and very few bugs are fixed by committers that were not directly involved in previous changes on the same files of the fix. More importantly, committers inducing fixes tend to have a lower level of communication between each other than that of other committers. This last finding suggests that increasing the level of communication between fix-inducing committers could reduce the number of fixes induced in a software project.}
}
@article{PHADNIS2019100360,
title = {Sample size calculation for small sample single-arm trials for time-to-event data: Logrank test with normal approximation or test statistic based on exact chi-square distribution?},
journal = {Contemporary Clinical Trials Communications},
volume = {15},
pages = {100360},
year = {2019},
issn = {2451-8654},
doi = {https://doi.org/10.1016/j.conctc.2019.100360},
url = {https://www.sciencedirect.com/science/article/pii/S2451865419300274},
author = {Milind A. Phadnis},
keywords = {Clinical trial, Exact test, Single-arm, Survival, Weibull},
abstract = {Background
Sample size calculations are critical to the planning of a clinical trial. For single-arm trials with time-to-event endpoint, standard software provides only limited options. The most popular option is the log-rank test. A second option assuming exponential distribution is available on some online websites. Both these approaches rely on asymptotic normality for the test statistic and perform well for moderate-to-large sample sizes.
Methods
As many new treatments in the field of oncology are cost-prohibitive and have slow accrual rates, researchers are often faced with the restriction of conducting single arm trials with potentially small-to-moderate sample sizes. As a practical solution, therefore, we consider the option of performing the sample size calculations using an exact parametric test with the test statistic following a chi-square distribution. Analytic results of sample size calculations from the two methods with Weibull distributed survival times are briefly compared using an example of a clinical trial on cholangiocarcinoma and are verified through simulations.
Results
Our simulations suggest that in the case of small sample phase II studies, there can be some practical benefits in using the exact test that could affect the feasibility, timeliness, financial support, and ‘clinical novelty’ factor in conducting a study. The exact test is a good option for designing small-to-moderate sample trials when accrual and follow-up time are adequate.
Conclusions
Based on our simulations for small sample studies, we conclude that a statistician should assess sensitivity of his calculations obtained through different methods before recommending a sample size to their collaborators.}
}
@article{ATAYERO2019103705,
title = {Trends and patterns of broadband Internet access speed in a Nigerian university campus: A robust data exploration},
journal = {Data in Brief},
volume = {23},
pages = {103705},
year = {2019},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2019.103705},
url = {https://www.sciencedirect.com/science/article/pii/S235234091930054X},
author = {Aderemi A. Atayero and Segun I. Popoola and Oluwaseun J. Adeyemi and David G. Afolayan and Matthew B. Akanle and Victor Adetola and Emmanuel Adetiba},
keywords = {Smart campus, Broadband internet access, Data bit rate, Mobile communication, Knowledge management},
abstract = {Efficient broadband Internet access is required for optimal productivity in smart campuses. Besides access to broadband Internet, delivery of high speed and good Quality of Service (QoS) are pivotal to achieving a sustainable development in the area of education. In this data article, trends and patterns of the speed of broadband Internet provided in a Nigerian private university campus are largely explored. Data transmission speed and data reception speed were monitored and recorded on daily basis at Covenant University, Nigeria for a period of twelve months (January–December, 2017). The continuous data collection and logging were performed at the Network Operating Center (NOC) of the university using SolarWinds Orion software. Descriptive statistics, correlation and regression analyses, Probability Density Functions (PDFs), Cumulative Distribution Functions (CDFs), Analysis of Variance (ANOVA) test, and multiple comparison post-hoc test are performed using MATLAB 2016a. Extensive statistical visualizations of the results obtained are presented in tables, graphs, and plots. Availability of these data will help network administrators to determine optimal network latency towards efficient deployment of high-speed broadband communication networks in smart campuses.}
}
@article{LIU20132753,
title = {A design and implementation of and triggering system for LXI instruments},
journal = {Measurement},
volume = {46},
number = {8},
pages = {2753-2764},
year = {2013},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2013.04.034},
url = {https://www.sciencedirect.com/science/article/pii/S0263224113001450},
author = {Zhao-qing Liu and Shao-wu Pan and Yi-gang Zhang},
keywords = {Synchronization and triggering, LXI, M/MA module, LXI mother board},
abstract = {In this paper, a software–hardware mixed design of synchronization and triggering system for multi-functional LXI instruments is proposed. A solution based on open-source software PTPd and DP83640 is introduced and the synchronous deviation can be limited to ±20ns. And the trigger state machine is implemented in a FPGA while trigger service requests and trigger resources are processed or managed by Linux application programs. Equipped with IVI-COM programming interface and Lua scripting platform, the design can be used to build an efficient and interchangeable test system. In addition, an event log mechanism is also provided to assist fault analysis. Test results indicate that the design strictly conforms to the IviLxiSync Specification, and the precision of synchronization and triggering system satisfies the recommendations of LXI Device Specification.}
}
@article{ABOUSENNA2021,
title = {A peer-to-peer logic environment to validate flashing yellow arrow decision support system},
journal = {Journal of Traffic and Transportation Engineering (English Edition)},
year = {2021},
issn = {2095-7564},
doi = {https://doi.org/10.1016/j.jtte.2020.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S209575642100009X},
author = {Hatem Abou-Senna and Essam Radwan and Hesham Eldeeb},
keywords = {Traffic engineering, Peer-to-peer logic, Flashing yellow arrow, Decision support system, Permissive left-turn, Four section head},
abstract = {The flashing yellow arrow (FYA) signal display creates an opportunity to enhance the left-turn phase with a variable mode that can be changed on demand. This research develops an integrated general purpose data collection module that time stamps detector and phase state changes within a National Electrical Manufacturers Association (NEMA) actuated traffic signal controller to provide recommendations for the flashing yellow arrow left-turn mode on a cycle-by-cycle basis. 115 left-turn approaches at 38 intersections with locations across the State of Florida were analyzed totaling 1370 h of video data processed including off-peak and peak conditions. Video data extraction was an essential step in developing the gap thresholds for the permissive left-turn. Actual intersection field data were obtained through loop detector mapping to the controller in the lab in real-time mode using a peer-to-peer logic environment. A custom communications software was developed to retrieve instantaneous channel input data, synchronize opposing through green phase, analyze traffic information, provide the algorithm decision, and generate a real-time log recording the events to determine whether it would be optimal to switch the red arrow to a flashing yellow arrow. The algorithm determines the time interval between the successive arrivals of vehicles and computes the corresponding headway for each lane by cycle on a second-by-second basis. Peer-to-peer logic is a necessary step to verify and validate new traffic concepts prior to field-testing and offers the advantage of acquiring and analyzing real-time traffic data coupled with video feed with the benefit of a safe environment.}
}
@article{DALLACOSTA20198,
title = {AVERAGE LIFE OF BALLOON GASTROSTOMY TUBES (BGT): ANALYSIS OF 239 DEVICES},
journal = {Nutrition},
volume = {65},
pages = {8},
year = {2019},
note = {SINPE conference abstracts},
issn = {0899-9007},
doi = {https://doi.org/10.1016/j.nut.2019.08.024},
url = {https://www.sciencedirect.com/science/article/pii/S0899900719301649},
author = {C. {Dalla Costa} and A. Bracco and E. Conterno and E. Lenta and V. Prandi},
abstract = {Considering the limited technical data provided by the manufacturing companies, the aim of this study was to evaluate the average life of different silicon BGT. All patients in home enteral nutrition treatment through BGT from 01/05/2015 to 06/09/2018 were evaluated. All patients were trained and assessed over time by the same staff using the same instruction set and operating methods. Patients’ data (age, sex) and BGT's data (date of placement/removal, reason for removal, company, size, number of BGTs placed for each patient) were recorded. Statistical processing was performed with IBM SPSS software. 239 BGT placed in 44 patients (14 males, 30 females; average age 72: min 23 – max 95) were assessed. Of these, 132 BGT were produced by Bard, 51 by Kimberly-Clark, 55 by Nutricia. BGT sizes are shown in graph 1, whereas the number of BGT placed per patient is shown in graph 2. The average life of BGT was 169 days (134 Bard, 166 Kimberly-Clark, 208 Nutricia). The difference among companies in terms of duration is statistically significant (Log Rank Mantel-Cox p=0.000, see figure 3). A multivariate analysis by diameter, age, sex, and number of BGT per person was also performed. The diameter was the only variable proved to be statistically significant of BGT life. BGT lifespan were found different and variable with the manufacturer, in particular the Nutricia company's tubes lasted longer. Besides the diameter of the device was found to have a significant impact on BGT life.}
}
@article{HADI2013544,
title = {A Support System for Generating SCORM Compliant Open Source Software Usage Manuals},
journal = {Procedia Computer Science},
volume = {22},
pages = {544-550},
year = {2013},
note = {17th International Conference in Knowledge Based and Intelligent Information and Engineering Systems - KES2013},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.09.134},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913009277},
author = {Akhmad Syaikhul Hadi and Takashi Yukawa and Yukikazu Murakami},
keywords = {open source software, installation manual, SCORM, learning management system},
abstract = {Open Source Software (OSS) is software whose source code that is open to the public through the Internet. Currently, OSS is widely used in many aspects of IT society. Because OSS development is community based, unlike commercial software, the lack of good documentation or the maintenance of manuals is one of the main problems of using OSS. Due to its rapid development, OSS manuals become easily obsolete. Moreover, the installation or the usage varies depending on the operating system. To solve the documentation problems, Murakami et al. proposed a method of automatically generating a web manual for installing an OSS by editing the log information recorded during the installation process. Unfortunately, the web manual generated by this system was not suitable for wide use in learning management systems. Therefore, this paper extends the sys- tem by Murakami et al. to one with the ability to deliver an automatically generated Web manual on an e-learning management system, modify the content of the manual, and skip unnecessary information in the learning process.}
}
@article{ASMUS20184,
title = {[OA010] A 4D Monte Carlo (MC) dose calculation framework with statistical breathing phase sampling to quantify interplay effects},
journal = {Physica Medica},
volume = {52},
pages = {4},
year = {2018},
note = {Abstracts from the 2nd European Congress of Medical Physics},
issn = {1120-1797},
doi = {https://doi.org/10.1016/j.ejmp.2018.06.082},
url = {https://www.sciencedirect.com/science/article/pii/S1120179718305647},
author = {von Muenchow Asmus and Katrin Straub and Christoph Losert and Roel Shpani and Jan Hofmaier and Philipp Freislederer and Christian Heinz and Christian Thieke and Matthias Soehn and Markus Alber and Ralf Floca and Claus Belka and Katia Parodi and Michael Reiner and Florian Kamp},
abstract = {Purpose
The interplay between dose application by complex techniques and respiratory motion of a tumor can potentially lead to undesirable and non-intuitive deviations from the planned dose distribution. We developed a 4D dose calculation framework to precisely simulate dose distributions for moving target volumes. In this study we randomly sample the simulated application of delivered dose fragments on 4D-CT phases.
Methods
The workflow combines MC dose calculation with linac-log files and dose accumulation based on 4D-CT images. Treatment plan fragments of 0.2s duration are retrieved from linac-log data and calculated on ten 4D-CT phases using MCverify/Hyperion V2.4 (research version of Monaco 3.2). The resulting dose fragments allow the simulation of arbitrary respiratory curves (e.g. changes in breathing frequency and pattern) with a resolution of 0.2 s by assigning every fragment to a distinct 4D-CT phase. Using deformable image registration (plastimatch) the dose fragments are accumulated by AVID, a software system for automated processing and analysis of radiotherapy data. In addition to the patient’s recorded normalized breathing curve, three statistical approaches are implemented: (1) random phase shift of the breathing curve, (2) random phase assignment of every 0.2s dose fragment and (3) random phase assignment of 1MU dose fragments.
Results
An exemplary 3 Gy, VMAT, SBRT treatment of a 9 cm3 lung tumor with 1.6 cm crano-caudal movement was analyzed. 128 random treatments were simulated for the three statistical approaches. In all three cases the mean 4D-CT calculated dose (original plan calculated on ten 4D-CT phases) and the mean of the randomly simulated doses agree. The dose deviations for the 128 runs are very similar for the three methods (average dose deviations: σD2%≈0.41%,σD50%≈0.25% and σD98%≈0.68% for the three approaches).
Conclusions
The described random assignments (2) and (3) have similar statistics as the random start phase approach (1) and are hence promising methods to comprehensively cover treatment plan and technique specific interplay effects without the need for daily breathing curves. The MU-based random sampling approach (3) is in addition independent of the linac-log data. The introduced framework has the potential to comprehensively include breathing motion induced interplay effects in the treatment planning and evaluation process.}
}
@article{RICARD2013197,
title = {GeoTemp™ 1.0: A MATLAB-based program for the processing, interpretation and modelling of geological formation temperature measurements},
journal = {Computers & Geosciences},
volume = {57},
pages = {197-207},
year = {2013},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2013.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0098300413001039},
author = {Ludovic P. Ricard and Jean-Baptiste Chanu},
keywords = {Guided User Interface, Temperature modelling, Heat flow, Geothermal, Thermal conductivity},
abstract = {The evaluation of potential and resources during geothermal exploration requires accurate and consistent temperature characterization and modelling of the sub-surface. Existing interpretation and modelling approaches of 1D temperature measurements are mainly focusing on vertical heat conduction with only few approaches that deals with advective heat transport. Thermal regimes are strongly correlated to rock and fluid properties. Currently, no consensus exists for the identification of the thermal regime and the analysis of such dataset. We developed a new framework allowing the identification of thermal regimes by rock formations, the analysis and modelling of wireline logging and discrete temperature measurements by taking into account the geological, geophysical and petrophysics data. This framework has been implemented in the GeoTemp software package that allows the complete thermal characterization and modelling at the formation scale and that provides a set of standard tools for the processing wireline and discrete temperature data. GeoTempTM operates via a user friendly graphical interface written in Matlab that allows semi-automatic calculation, display and export of the results. Output results can be exported as Microsoft Excel spreadsheets or vector graphics of publication quality. GeoTemp™ is illustrated here with an example geothermal application from Western Australia and can be used for academic, teaching and professional purposes.}
}
@article{RENDEL20181206,
title = {A novel experimental system for the exploration of CO2-water-rock interactions under conditions relevant to CO2 geological storage},
journal = {Chemical Engineering Journal},
volume = {334},
pages = {1206-1213},
year = {2018},
issn = {1385-8947},
doi = {https://doi.org/10.1016/j.cej.2017.11.067},
url = {https://www.sciencedirect.com/science/article/pii/S1385894717319794},
author = {Pedro M. Rendel and Domenik Wolff-Boenisch and Ittai Gavrieli and Jiwchar Ganor},
keywords = {Reactor engineering, Mineral dissolution and precipitation, Thermodynamics, Kinetics, Flow-through reactor, CO geological storage},
abstract = {This paper describes the design and experimental validation of a novel flow-through reactor system conceived for experimental studies to determine the kinetics and thermodynamics of mineral precipitation and dissolution in environmental conditions relevant to CO2 geological storage. The experimental system was designed to work under a confining pressure of up to 150 bar, temperature up to 150 °C and corrosive conditions. The unique design allows the injection of precise amounts of liquid CO2 into the reactor while avoiding the formation of multiple CO2 phases. The modular design enables the in-situ measurement of pH using a pressure resistant in-line probe and electronic gauges which record pressure and temperature at multiple points. The system enables the user to withdraw liquid samples without disturbing the experimental conditions in the reactor. Customized computer software was developed and connected to the system to provide automatic data-logging capabilities, remote process control and the ability to partially shut-down the system in case of safety issues.}
}
@article{ALNEKLAWY20179,
title = {Online Embryology teaching using learning management systems appears to be a successful additional learning tool among Egyptian medical students},
journal = {Annals of Anatomy - Anatomischer Anzeiger},
volume = {214},
pages = {9-14},
year = {2017},
issn = {0940-9602},
doi = {https://doi.org/10.1016/j.aanat.2017.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0940960217300882},
author = {Ahmed Farid Al-Neklawy},
keywords = {Online Anatomy teaching, Online teaching for Egyptian medical students, Online Embryology teaching, Anatomy e-learning},
abstract = {Although the traditional didactic lecture is considered to be efficient for presenting information and providing explanations, it usually does not provide adequate time for deeper learning activities. So, traditional lecture is one of the most widely criticized educational methods. Virtual learning environment (VLE) is a specially designed environment that facilitates teachers’ management of educational courses for their students, using computer hardware and software, which involves distance learning. In this study, we evaluated the experiment of online teaching of General Embryology for Egyptian undergraduate medical students using WizIQ learning management system. A total of 100 students were invited to submit an online survey at the end of the course to evaluate delivery of instruction, creation of an environment that supported learning, and administrative issues. Most of the students reported that they were strongly satisfied with the efficacy of the instructional methods and were strongly satisfied with the degree of clarity of the course material. They strongly accepted the page format and design of the virtual classroom and strongly agreed that the learning environment supported the learning procedure. The item of easy logging into the virtual classroom had aberrant variable responses; it recorded the lowest mean response; this variation in responses was due to technical factors as the students used different devices with different speeds of internet connections. Ninety percent of students have strongly recommended the course attendance for their fellow students. These results demonstrate that online Anatomy teaching using learning management systems appears to be a successful additional learning tool among Egyptian medical students.}
}
@article{CAPORUSCIO2020681,
title = {Smart-troubleshooting connected devices: Concept, challenges and opportunities},
journal = {Future Generation Computer Systems},
volume = {111},
pages = {681-697},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19306491},
author = {Mauro Caporuscio and Francesco Flammini and Narges Khakpour and Prasannjeet Singh and Johan Thornadtsson},
keywords = {Resilience, Dependability, Fault-tolerance, Self-healing, Self-repair, Diagnostics, Prognostics, Event correlation, Log analytics, Embedded systems, Cyber–physical systems, Internet of Things},
abstract = {Today’s digital world and evolving technology has improved the quality of our lives but it has also come with a number of new threats. In the society of smart-cities and Industry 4.0, where many cyber–physical devices connect and exchange data through the Internet of Things, the need for addressing information security and solve system failures becomes inevitable. System failures can occur because of hardware failures, software bugs or interoperability issues. In this paper we introduce the industry-originated concept of “smart-troubleshooting” that is the set of activities and tools needed to gather failure information generated by heterogeneous connected devices, analyze them, and match them with troubleshooting instructions and software fixes. As a consequence of implementing smart-troubleshooting, the system would be able to self-heal and thus become more resilient. This paper aims to survey frameworks, methodologies and tools related to this new concept, and especially the ones needed to model, analyze and recover from failures in a (semi)automatic way. Smart-troubleshooting has a relation with event analysis to perform diagnostics and prognostics on devices manufactured by different suppliers in a distributed system. It also addresses management of appropriate product information specified in possibly unstructured formats to guide the troubleshooting workflow in identifying fault—causes and solutions. Relevant research is briefly surveyed in the paper in order to highlight current state-of-the-art, open issues, challenges to be tackled and future opportunities in this emerging industry paradigm.}
}
@article{LICORISH20141578,
title = {Understanding the attitudes, knowledge sharing behaviors and task performance of core developers: A longitudinal study},
journal = {Information and Software Technology},
volume = {56},
number = {12},
pages = {1578-1596},
year = {2014},
note = {Special issue: Human Factors in Software Development},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S095058491400038X},
author = {Sherlock A. Licorish and Stephen G. MacDonell},
keywords = {Core developers, Psycholinguistics, Content analysis, Attitudes, Knowledge sharing, Task performance},
abstract = {Context
Prior research has established that a few individuals generally dominate project communication and source code changes during software development. Moreover, this pattern has been found to exist irrespective of task assignments at project initiation.
Objective
While this phenomenon has been noted, prior research has not sought to understand these dominant individuals. Previous work considering the effect of team structures on team performance has found that core communicators are the gatekeepers of their teams’ knowledge, and the performance of these members was correlated with their teams’ success. Building on this work, we have employed a longitudinal approach to study the way core developers’ attitudes, knowledge sharing behaviors and task performance change over the course of their project, based on the analysis of repository data.
Method
We first used social network analysis (SNA) and standard statistical analysis techniques to identify and select artifacts from ten different software development teams. These procedures were also used to select central practitioners among these teams. We then applied psycholinguistic analysis and directed content analysis (CA) techniques to interpret the content of these practitioners’ messages. Finally, we inspected these core developers’ activities as recorded in system change logs at various points in time during systems’ development.
Results
Among our findings, we observe that core developers’ attitudes and knowledge sharing behaviors were linked to their involvement in actual software development and the demands of their wider project teams. However, core developers appeared to naturally possess high levels of insightful characteristics, which became evident very early during teamwork.
Conclusions
Project performance would likely benefit from strategies aimed at surrounding core developers with other competent communicators. Core developers should also be supported by a wider team who are willing to ask questions and challenge their ideas. Finally, the availability of adequate communication channels would help with maintaining positive team climate, and this is likely to mitigate the negative effects of distance during distributed developments.}
}
@article{MARQUEZCHAMORRO20171,
title = {Run-time prediction of business process indicators using evolutionary decision rules},
journal = {Expert Systems with Applications},
volume = {87},
pages = {1-14},
year = {2017},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.05.069},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417303950},
author = {Alfonso E. Márquez-Chamorro and Manuel Resinas and Antonio Ruiz-Cortés and Miguel Toro},
keywords = {Business process management, Process mining, Predictive monitoring, Business process indicator, Evolutionary algorithm},
abstract = {Predictive monitoring of business processes is a challenging topic of process mining which is concerned with the prediction of process indicators of running process instances. The main value of predictive monitoring is to provide information in order to take proactive and corrective actions to improve process performance and mitigate risks in real time. In this paper, we present an approach for predictive monitoring based on the use of evolutionary algorithms. Our method provides a novel event window-based encoding and generates a set of decision rules for the run-time prediction of process indicators according to event log properties. These rules can be interpreted by users to extract further insight of the business processes while keeping a high level of accuracy. Furthermore, a full software stack consisting of a tool to support the training phase and a framework that enables the integration of run-time predictions with business process management systems, has been developed. Obtained results show the validity of our proposal for two large real-life datasets: BPI Challenge 2013 and IT Department of Andalusian Health Service (SAS).}
}
@article{KIRKWOOD2014742,
title = {Radiation-induced skin injury after complex endovascular procedures},
journal = {Journal of Vascular Surgery},
volume = {60},
number = {3},
pages = {742-748},
year = {2014},
issn = {0741-5214},
doi = {https://doi.org/10.1016/j.jvs.2014.03.236},
url = {https://www.sciencedirect.com/science/article/pii/S0741521414006144},
author = {Melissa L. Kirkwood and Gary M. Arbique and Jeffrey B. Guild and Carlos Timaran and R. James Valentine and Jon A. Anderson},
abstract = {Background
Radiation-induced skin injury is a serious potential complication of fluoroscopically guided interventions. Transient erythema occurs at doses of 2 to 5 Gy, whereas permanent epilation, ulceration, and desquamation are expected at doses above this level. Complex endovascular procedures (CEPs), such as fenestrated endovascular aortic aneurysm repair (FEVAR), are associated with high radiation doses, yet the prevalence of radiation-induced skin injury is unknown. We hypothesized that skin injury after these exposures is likely to be underrecognized and underreported. This study examined the frequency and severity of deterministic effects and evaluated patient characteristics that might predispose to radiation injury in CEP.
Methods
CEP was defined as a procedure with a radiation dose ≥5 Gy (National Council on Radiation Protection and Measurements threshold for substantial radiation dose level [SRDL]). Radiation dose and operating factors were recorded for all CEPs performed in a hybrid room during a 30-month period. Patient medical records were retrospectively reviewed for evidence of skin injury. Patients were seen in follow-up daily until discharge and then at weeks 2 and 6, months 3 and 6, and 1 year. Phone interviews were conducted to determine the presence of any skin-related complaints. Peak skin dose (PSD) distributions were calculated for FEVARs with custom software employing input data from fluoroscopic machine logs. These calculations were validated against Gafchromic film (Ashland Inc, Covington, Ky) measurements. Dose was summed for the subset of patients with multiple procedures within 6 months of the SRDL event, consistent with Joint Commission recommendations.
Results
Sixty-one CEPs reached a reference air kerma (RAK) of 5 Gy (50 FEVARs, six embolizations, one thoracic endovascular aortic repair, one endovascular aneurysm repair, one carotid intervention, and two visceral interventions). The patient cohort was 79% male and had a mean body mass index of 31. The average RAK was 8 ± 2 Gy (5.0-15.9 Gy). Sixteen patients had multiple CEPs within 6 months of the SRDL event, with a mean cumulative RAK of 12 ± 3 Gy (7.0-18.4 Gy). The mean FEVAR PSD was 6.6 ± 3.6 Gy (3.7-17.8 Gy), with a mean PSD/RAK ratio of 0.78. Gafchromic film dose measurements were not statistically different from PSD estimations, with a constant of proportionality of 0.99. Three patients were lost to follow-up before their first postoperative visit. No radiation skin injuries were found.
Conclusions
This study represents the largest analysis of deterministic skin injury after CEPs, and our results suggest that it is less frequent than expected and not increased in CEPs.}
}
@incollection{POZENEL2020305,
title = {Chapter Six - Using clickstream data to enhance reverse engineering of Web applications},
editor = {Ali R. Hurson and Veljko Milutinović},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {116},
number = {1},
pages = {305-349},
year = {2020},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2019.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0065245819300324},
author = {Marko Poženel and Boštjan Slivnik},
keywords = {Clickstreams, User sessions, Session reconstruction, Reverse engineering, Web applications},
abstract = {Due to advances in Web technologies, existing Web applications are rewritten or replaced by new ones. As a result of either ad hoc or agile development, many of them lack proper technical documentation. Nevertheless, the domain knowledge built into these applications is valuable, which is why the reverse engineering, an activity aimed at detecting software components and their interrelationships to provide multiple views of software systems at a higher level of abstraction, of existing Web applications is becoming an important issue. Apart from the static reverse engineering based on examining the system's source code, analyzing the dynamic aspect of Web applications often proves worthwhile. One important source of data the dynamic analysis of a Web application can be based on are HTTP server log files. User sessions, results of clickstream analysis, and session reconstruction in particular, can be used as the basis for the first automatic step of reverse engineering, employed in order to gain a quick insight into Web application's source code. It is shown how clickstream data can be used to reveal not only the intensity of connections between individual Web application's source code artifacts but also the overall structure of a Web application. The extracted structure, based either on code artifacts names or their usage, is presented visually as an ATG, with code artifacts belonging to the same application module grouped together. Because session reconstruction is an inherently probabilistic process and thus in general produces noisy data, clustering the code artifacts becomes a challenging task. It is shown that multidimensional scaling and even a simple graph drawing approaches yield better representation of the application transition graph than hierarchical clustering. The method was tested against the results obtained by an expert (the author of the Web application used as a test case). Additionally, the method can also be used for verifying the structure obtained by manual reverse engineering of the application's source code.}
}
@article{TSAI20121363,
title = {Sharetouch: A system to enrich social network experiences for the elderly},
journal = {Journal of Systems and Software},
volume = {85},
number = {6},
pages = {1363-1369},
year = {2012},
note = {Special Issue: Agile Development},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2012.01.023},
url = {https://www.sciencedirect.com/science/article/pii/S016412121200009X},
author = {Tsai-Hsuan Tsai and Hsien-Tsung Chang and Yi-Ming Chang and Gung-Shiung Huang},
keywords = {Sharetouch, Touch panel, Community pond, Waterball, Multimedia sharing, Technology Acceptance Model},
abstract = {The Sharetouch system is designed for raising users’ participation in community events. We put three subsystems into Sharetouch: (1) community pond, (2) Waterball interactive game, and (3) multimedia sharing. Sharetouch is based on an optical touch device designed by the Joyplux Company with an infrared LED and camera. This device can support multi-touch functions within a large display area. The software of Sharetouch was developed within XNA and .NET frameworks. We project the users as fish in our community pond. Sharetouch displays all the friends as fish when the users log into the system. Therefore, the number of fish equals the number of friends of the users. This design encourages users to make more friends to increase the number of fish. Waterball is a game that combines virtual images and real objects. The concept is based on the Nintendo Wii games, as players hold controllers (real objects) to play the games (virtual images). We also apply the concept of the cloud flash drive to multimedia sharing to avoid the trouble of carrying a real flash disk. This study employed the TAM measure to measure the validity of Sharetouch in this social platform. Our findings indicated that all proposed hypotheses had a positive and significant impact on the intention of older people to interact with Sharetouch. Unlike the computer-based system, Sharetouch is created as a user-friendly interface system. Sharetouch can enrich the users’ social network experiences through its hardware and software architectures.}
}
@article{KORTENBRUCK2017227,
title = {Machine operation profiles generated from ISO 11783 communication data},
journal = {Computers and Electronics in Agriculture},
volume = {140},
pages = {227-236},
year = {2017},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2017.05.039},
url = {https://www.sciencedirect.com/science/article/pii/S0168169916311589},
author = {Dietrich Kortenbruck and Hans W. Griepentrog and Dimitris S. Paraforos},
keywords = {Machinery use, ISO 11783, Operation profile, CAN, State model, Process optimization},
abstract = {An operation profile is a detailed description of machinery use and provides information about the production process (e.g. time and energy requirements). The high complexity of agricultural machine use over time with its various implements and production processes makes it difficult to automate the operation profile generation for agricultural machinery. Today, modern communication interfaces, like the ISO 11783 (ISOBUS), allow a comfortable data acquisition with comprehensive parameter information of the machine and production process status. This paper describes how the generation of operation profiles can be automated and what kind of machine data and status information are required. Experiments were conducted with a tractor-machine combination during field cultivation. The machine was equipped with a CAN data logger to record ISOBUS messages and GNSS position data during normal operation. A software tool was setup and programmed to drive the data logger. Fields were automatically detected by algorithms to avoid the time consuming manual definition of field boundaries. A graphical user interface simplified the setup and use of the software application. Furthermore, the algorithms analyzed the machine communication regarding the actual machine state based on position, machine activity and work state. The KTBL time classification 2013 was used as a basis for working time analysis as it is designed to divide the overall operational time of agricultural machines into clearly defined, qualitatively different partial time fractions. The machine state could be visualized in the software by a map to show the user where problems occurred e.g. in time losses while waiting or machine breakdown. A diagram presented the timespans spent in different time fractions for comparison to other, similar operations. Combined with other infrastructure data, specific operation profiles could be used to gain more detailed knowledge about machine use or to improve future operations.}
}
@article{STREET2014315,
title = {Provider interaction with the electronic health record: The effects on patient-centered communication in medical encounters},
journal = {Patient Education and Counseling},
volume = {96},
number = {3},
pages = {315-319},
year = {2014},
note = {Communication in Healthcare: Lessons from Diversity},
issn = {0738-3991},
doi = {https://doi.org/10.1016/j.pec.2014.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0738399114001840},
author = {Richard L. Street and Lin Liu and Neil J. Farber and Yunan Chen and Alan Calvitti and Danielle Zuest and Mark T. Gabuzda and Kristin Bell and Barbara Gray and Steven Rick and Shazia Ashfaq and Zia Agha},
keywords = {Patient centered communication, Electronic medical records, Physician workflow},
abstract = {Objective
The computer with the electronic health record (EHR) is an additional ‘interactant’ in the medical consultation, as clinicians must simultaneously or in alternation engage patient and computer to provide medical care. Few studies have examined how clinicians’ EHR workflow (e.g., gaze, keyboard activity, and silence) influences the quality of their communication, the patient's involvement in the encounter, and conversational control of the visit.
Methods
Twenty-three primary care providers (PCPs) from USA Veterans Administration (VA) primary care clinics participated in the study. Up to 6 patients per PCP were recruited. The proportion of time PCPs spent gazing at the computer was captured in real time via video-recording. Mouse click/scrolling activity was captured through Morae, a usability software that logs mouse clicks and scrolling activity. Conversational silence was coded as the proportion of time in the visit when PCP and patient were not talking. After the visit, patients completed patient satisfaction measures. Trained coders independently viewed videos of the interactions and rated the degree to which PCPs were patient-centered (informative, supportive, partnering) and patients were involved in the consultation. Conversational control was measured as the proportion of time the PCP held the floor compared to the patient.
Results
The final sample included 125 consultations. PCPs who spent more time in the consultation gazing at the computer and whose visits had more conversational silence were rated lower in patient-centeredness. PCPs controlled more of the talk time in the visits that also had longer periods of mutual silence.
Conclusions
PCPs were rated as having less effective communication when they spent more time looking at the computer and when there was more periods of silence in the consultation. Because PCPs increasingly are using the EHR in their consultations, more research is needed to determine effective ways that they can verbally engage patients while simultaneously managing data in the EHR.
Practice implications
EHR activity consumes an increasing proportion of clinicians’ time during consultations. To ensure effective communication with their patients, clinicians may benefit from using communication strategies that maintain the flow of conversation when working with the computer, as well as from learning EHR management skills that prevent extended periods of gaze at computer and long periods of silence. Next-generation EHR design must address better usability and clinical workflow integration, including facilitating patient-clinician communication.}
}
@article{SAWADA2013613,
title = {Possible association between non-invasive parameter of flow-mediated dilatation in brachial artery and whole coronary plaque vulnerability in patients with coronary artery disease},
journal = {International Journal of Cardiology},
volume = {166},
number = {3},
pages = {613-620},
year = {2013},
issn = {0167-5273},
doi = {https://doi.org/10.1016/j.ijcard.2011.11.101},
url = {https://www.sciencedirect.com/science/article/pii/S0167527311021589},
author = {Takahiro Sawada and Takuo Emoto and Yoshiki Motoji and Megumi Hashimoto and Hiroko Kageyama and Daisuke Terashita and Taiji Mizoguchi and Takao Mizuguchi and Masamichi Iwasaki and Kazuki Taira and Hiroshi Okamoto and Yosuke Matsuo and Sushi-ku Kim and Akira Takarada and Mitsuhiro Yokoyama},
keywords = {Flow-mediated dilatation, Endothelial dysfunction, Virtual histology intravascular ultrasound, Thin-cap fibroatheroma},
abstract = {Background
Despite being a relatively widely-used non-invasive parameter of endothelial dysfunction, little is known regarding the relationship between flow-mediated dilatation (FMD) and coronary plaque vulnerability in patients with coronary artery disease (CAD).
Methods
111 CAD patients (age; 68.9±9.3) who underwent both coronary intervention and FMD were enrolled. Spectral analyses of intravascular ultrasound radiofrequency data for both culprit and non-culprit lesions were performed using Virtual Histology™ software. Plaque burden was described based on fibrotic, fibro-fatty, dense calcium, and necrotic core (NC) components, and thin-cap fibroatheroma (TCFA) was defined as focal NC rich (>10%) plaques touching the lumen with a percent-plaque volume exceeding 40%.
Results
Averaged %FMD was 2.86±2.03% (median 2.27%, 25th 1.40%, 75th 4.20%). NC volumes were negatively correlated with log%FMD for both culprit and non-culprit lesions (P=0.001, r=0.31 and P=0.03, r=0.21, respectively). We divided the patients into three tertiles according to %FMD; 38 were lower (≤1.75%), 41 were middle (>1.75%, but ≤3.5%), and 32 were upper tertile (>3.5%). The prevalence rate of TCFA increased with decreasing %FMD tertile and the incidence of major adverse cardiac events was significantly higher in lower %FMD tertile. Multivariate logistic regression analyses showed that the most powerful predictive factor for TCFA was log%FMD (P<0.0001), and ROC curve analysis identified %FMD of <2.81% (AUC=0.82, sensitivity: 91.2%, specificity: 66.7%) as the optimal cut-off point for predicting the presence of TCFA.
Conclusions
Impaired endothelial function in brachial arteries may be associated with whole coronary plaque vulnerability and poor clinical outcome in patients with CAD.}
}
@article{URMENETAULLOA2021159,
title = {Myocardial strain in nonischemic dilated cardiomyopathy with feature tracking. Feasibility and prognostic implications},
journal = {Revista Española de Cardiología (English Edition)},
volume = {74},
number = {2},
pages = {159-166},
year = {2021},
issn = {1885-5857},
doi = {https://doi.org/10.1016/j.rec.2019.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S1885585720300670},
author = {Javier {Urmeneta Ulloa} and Eduardo {Pozo Osinalde} and Juan Lizandro Rodríguez-Hernández and Hugo {Martínez Fernández} and Fabián Islas and Alberto {de Agustín} and Pedro Marcos-Alberca and Patricia Mahía and Miguel Ángel Cobos and Paula {Hernández Mateo} and José Ángel Cabrera and María {Luaces Méndez} and José Juan {Gómez de Diego} and Ana Bustos and Carlos Macaya and Leopoldo {Pérez de Isla}},
keywords = {Dilated cardiomyopathy, Myocardial strain, Feature tracking, Cardiac magnetic resonance, Miocardiopatía dilatada, Deformación miocárdica, , Cardiorresonancia magnética},
abstract = {Introduction and objectives
Myocardial strain analysis could provide additional information to left ventricular ejection fraction (LVEF) in nonischemic dilated cardiomyopathy (NIDC). Our aim was to analyze the feasibility of left ventricular strain evaluation using cardiac magnetic resonance feature tracking (FT) in NIDC, and to determine its clinical and prognostic impact.
Methods
We retrospectively included consecutive patients with NIDC who underwent cardiac magnetic resonance. Left ventricular global longitudinal, circumferential and radial strain were obtained from standard cine sequences using FT analysis software. We evaluated their association with a composite endpoint (heart failure, implantable cardioverter-defibrillator in secondary prevention, or death).
Results
FT analysis could be performed in all of the 98 patients (mean age 68±13 years, 72% men). Intra- and interobserver concordance was good for global longitudinal and circumferential strain but was worse for radial strain. Global circumferential strain was independently associated (OR, 1.16; P=.045) with LVEF normalization during follow-up and was the only morphological parameter independently associated with the composite endpoint (OR, 1.15; P=.038). A cutoff value <−8.2% was able to predict the incidence of this event during follow-up (log-rank 4.6; P=.032).
Conclusions
Left ventricular strain analysis with FT is feasible and reproducible in NIDC. Global circumferential strain was able to predict LVEF recovery and the appearance of major cardiovascular events during follow-up.
Resumen
Introducción y objetivos
El análisis de la deformación miocárdica puede aportar información adicional a la fracción de eyección del ventrículo izquierdo (FEVI) en la miocardiopatía dilatada no isquémica (MDNI). El objetivo es analizar la factibilidad del estudio del strain del ventrículo izquierdo mediante feature tracking (FT) de cardiorresonancia magnética en la MDNI y determinar su relevancia clínica y pronóstica.
Métodos
Se incluyó retrospectivamente a los pacientes consecutivos con MDNI sometidos a cardiorresonancia magnética. Se obtuvieron el strain global longitudinal, circunferencial y radial del ventrículo izquierdo de secuencias convencionales de cine mediante un software de análisis de FT. Se evaluó su asociación con el evento combinado (insuficiencia cardiaca, implante de desfibrilador en prevención secundaria y muerte).
Resultados
Se pudo realizar el FT en los 98 pacientes evaluados (edad, 68± 13 años; el 72% varones). La concordancia intraobservador e interobservadores fue buena para el strain global longitudinal y circunferencial, y más limitada para el radial. El strain global circunferencial se asoció de manera independiente (OR=1,16; p=0,045) con la normalización de la FEVI en el seguimiento y fue el único parámetro morfológico con asociación independiente (OR=1,15; p=0,038) con el evento combinado. Un valor <–8,2% fue capaz de predecir la aparición de este evento en el seguimiento (Log-rank test, 4,6; p=0,032)
Conclusiones
El análisis del strain del ventrículo izquierdo mediante FT es factible y reproducible en MDNI. El strain global circunferencial fue capaz de predecir la recuperación de la FEVI y la aparición de eventos cardiovasculares mayores en el seguimiento.}
}
@article{POLIG2018260,
title = {A hardware compilation framework for text analytics queries},
journal = {Journal of Parallel and Distributed Computing},
volume = {111},
pages = {260-272},
year = {2018},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2017.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S0743731517301727},
author = {Raphael Polig and Kubilay Atasu and Heiner Giefers and Christoph Hagleitner and Laura Chiticariu and Frederick Reiss and Huaiyu Zhu and Peter Hofstee},
keywords = {Text analytics, FPGA, Query compilation, Accelerator},
abstract = {Unstructured text data is being generated at an unprecedented rate in the form of Twitter feeds, machine logs or medical records. The analysis of this data is an important step to gaining significant insight regarding innovation, security and decision-making. The performance of traditional compute systems struggles to keep up with the rapid data growth and the expected high quality of information extraction. To cope with this situation, a compilation framework is presented that can transform text analytics queries into a hardware description. Deployed on an FPGA, the queries can be executed 60 times faster on average compared to a multi-threaded software implementation. The performance has been evaluated on two generations of high-end server systems including two generations of FPGAs, demonstrating the performance gains from advanced technology.}
}
@article{HOSSEINI2019103976,
title = {Enhancing the security of patients’ portals and websites by detecting malicious web crawlers using machine learning techniques},
journal = {International Journal of Medical Informatics},
volume = {132},
pages = {103976},
year = {2019},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2019.103976},
url = {https://www.sciencedirect.com/science/article/pii/S1386505619303454},
author = {Nafiseh Hosseini and Fatemeh Fakhar and Behzad Kiani and Saeid Eslami},
keywords = {Security of patient portal, Malicious crawlers, Support vector machines, Feature extraction},
abstract = {Introduction
There is increasing demand for access to medical information via patients’ portals. However, one of the challenges towards widespread utilisation of such service is maintaining the security of those portals. Recent reports show an alarming increase in cyber-attacks using crawlers. These software programs crawl web pages and are capable of executing various commands such as attacking web servers, cracking passwords, harvesting users’ personal information, and testing the vulnerability of servers. The aim of this research is to develop a new effective model for detecting malicious crawlers based on their navigational behavior using machine-learning techniques.
Method
In this research, different methods of crawler detection were investigated. Log files of a sample of compromised web sites were analysed and the best features for the detection of crawlers were extracted. Then after testing and comparing several machine learning algorithms including Support Vector Machine (SVM), Bayesian Network and Decision Tree, the best model was developed using the most appropriate features and its accuracy was evaluated.
Results
Our analysis showed the SVM-based models can yield higher accuracy (f-measure = 0.97) comparing to Bayesian Network (f-measure = 0.88) and Decision Tree (f-measure = 0.95) and artificial neural network (ANN) (f-measure = 0.87)for detecting malicious crawlers. However, extracting proper features can increase the performance of the SVM (f-measure = 0.98), the Bayesian network (f-measure = 0.94) and the Decision Tree (f-measure = 0.96) and ANN (f-measure = 0.92).
Conclusion
Security concerns are among the potential barriers to widespread utilisation of patient portals. Machine learning algorithms can be accurately used to detect malicious crawlers and enhance the security of sensitive patients’ information. Selecting appropriate features for the development of these algorithms can remarkably increase their accuracy.}
}
@incollection{DI2016313,
title = {Chapter 25 - Solubility Methods},
editor = {Li Di and Edward H. Kerns},
booktitle = {Drug-Like Properties (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Boston},
pages = {313-324},
year = {2016},
isbn = {978-0-12-801076-1},
doi = {https://doi.org/10.1016/B978-0-12-801076-1.00025-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128010761000253},
author = {Li Di and Edward H. Kerns},
keywords = {Kinetic solubility, Thermodynamic solubility, Nephelometry, Turbidimetry, Shake flask},
abstract = {Solubility can be estimated from equations using the structural properties pKa, pH, logP, and melting point. Commercial software is available to calculate equilibrium solubility from structure, which is useful for comparing analogs in a chemical series, but not as useful for calculating an exact solubility value. Kinetic solubility methods are relevant for discovery because the mimic discovery kinetic experimental conditions. They mix dimethyl sulfoxide solutions of compounds with an aqueous solution for several hours in microplates and measure compound concentration using a plate reader with UV, nephelometry, or turbidimetry detection. Thermodynamic (equilibrium) solubility methods are relevant for preformulation, formulation, physiologically based pharmacokinetics modeling, and regulatory filing. They mix solid compound with an aqueous solution for a long time and quantitate with sensitive and specific high-performance liquid chromatography or liquid chromatography-mass spectrometry methods. Custom solubility methods mimic a specific aqueous matrix of interest (e.g., bioassay media, intestinal fluid, formulation vehicles) to obtain data that are relevant to answering a specific project question.}
}
@incollection{LOEW2010273,
title = {Chapter 11 - The Virtual Cell Project},
editor = {Edison T. Liu and Douglas A. Lauffenburger},
booktitle = {Systems Biomedicine},
publisher = {Academic Press},
address = {San Diego},
pages = {273-288},
year = {2010},
isbn = {978-0-12-372550-9},
doi = {https://doi.org/10.1016/B978-0-12-372550-9.00011-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780123725509000110},
author = {Leslie M. Loew and James C. Schaff and Boris M. Slepchenko and Ion I. Moraru},
abstract = {Publisher Summary
This chapter appraises the current design and structure of the Virtual Cell problem-solving environment. The Virtual Cell (Vcell) is a computational modeling software environment that has been designed to address how cellular architecture shapes and controls the response of cells to their environment. It facilitates the organization of experimental data into quantitative hypotheses and the generation of predictions from them. Incorporation of experimental microscope images within full 3-dimensional spatial models of signal transduction networks is a key feature of the Virtual Cell. Reaction, diffusion, advection, membrane transport, and electrophysiology are the biophysical mechanisms that are supported by the Virtual Cell. A variety of solvers are provided for ordinary differential equations and non-spatial stochastic simulations. To organize and make predictions from quantitative data, mathematical models that can produce simulations of the biology are required. VCell is a modular computational framework that permits construction of models, application of numerical solvers to perform simulations, and analysis of simulation results. Public models and simulations are available to anyone who logs on to VCell, as specified by their creators; the number of simulations is larger than the number of models, because models typically have several simulations associated with them. Models and geometries are maintained in an access-controlled central database. There are three database browsers, collected as three tabs within a “Database Manager” window, that provide a Windows Explorer file type of interface, one for each of the focused workspaces in VCell (BioModel Workspace, MathModel Workspace, Geometry Workspace).}
}
@article{VANDENBROEK201589,
title = {FDES, a GPU-based multislice algorithm with increased efficiency of the computation of the projected potential},
journal = {Ultramicroscopy},
volume = {158},
pages = {89-97},
year = {2015},
issn = {0304-3991},
doi = {https://doi.org/10.1016/j.ultramic.2015.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0304399115300127},
author = {W. {Van den Broek} and X. Jiang and C.T. Koch},
keywords = {Forward dynamical electron scattering (FDES), Multislice simulation, HRTEM, Diffraction, CBED},
abstract = {While the computational complexity of calculation of the projected potential in the multislice algorithm through reciprocal space scales quadratically with the number of atoms A per slice, a pure real-space calculation scales linearly with A. A hybrid strategy is introduced that has a theoretical complexity of O(AlogA), but that, when measured, outperforms both the reciprocal-space and the real-space approach by approximately an order in A and a large factor, respectively. This strategy is implemented in a new program, dubbed forward dynamical electron scattering (FDES), which simulates high resolution transmission electron microscopy images, diffraction patterns and convergent beam electron diffraction patterns. FDES attains a further increase in speed by running on a graphics processing unit and is made available to the community as open software.}
}
@article{KHOSHBAKHT2016357,
title = {Investigating Induction Log response in the presence of natural fractures},
journal = {Journal of Petroleum Science and Engineering},
volume = {145},
pages = {357-369},
year = {2016},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2016.05.023},
url = {https://www.sciencedirect.com/science/article/pii/S0920410516301930},
author = {Farhad Khoshbakht and Mohammad Reza Rasaie and Ali Shekarifard},
keywords = {Induction Log, Fracture, Carbonate reservoir, Hydrocarbon saturation, Bulk conductivity},
abstract = {Resistivity logging is one of the main types of logging techniques which is used to determine saturation of hydrocarbon. Wrong interpretation of these logs leads to costly risks in determination of pay zones and calculation of hydrocarbon in place. Lithology, fluid types, texture, rock-fluid interaction, shale, anisotropy, bed geometry, borehole shape, mud invasion, and natural fractures (if existed) disturb the reading of resistivity logs. In this regard, the main objectives of this paper is to better understand the effects of fractures on Induction Logs and considering the results in a real dataset. From a highly fractured reservoir. We used a commercial software to model the response of a generic Induction Log in the presence of natural fractures. The effects of different characteristics of fractures including length, aperture, inclination, and distance from borehole wall on the Induction Log were investigated. The present study showed that all characteristics of fractures influence the Induction Log measurements in different degrees. Among them, fracture's conductivity is the most controversial one. For less conductive fractures than matrix, the fracture conductivity has minor impact on the bulk conductivity; however, when fracture to matrix conductivity ratio reaches 10–1000, the bulk conductivity increases to above the actual conductivity of the matrix. In very high conductive fractures, when fracture to matrix conductivity ratio is 10+3 to 10+6, fractures have adverse impact on the bulk conductivity, and the recorded conductivity rapidly decreases below the actual conductivity of matrix. The effect of fracture length and aperture on the Induction Logs is somehow similar. The model showed that fracture inclination has minor impact on the Induction Log's response.}
}
@article{GRAY20171,
title = {Wireless data management system for environmental monitoring in livestock buildings},
journal = {Information Processing in Agriculture},
volume = {4},
number = {1},
pages = {1-17},
year = {2017},
issn = {2214-3173},
doi = {https://doi.org/10.1016/j.inpa.2016.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2214317316300348},
author = {James Gray and Thomas M. Banhazi and Alexander A. Kist},
keywords = {Sensors, Environmental monitoring, Networking, Software engineering},
abstract = {The impact of air quality on the health, welfare and productivity of livestock needs to be considered, especially when livestock are kept in enclosed buildings. The monitoring of such environmental factors allows for the development of appropriate strategies to reduce detrimental effects of sub-optimal air quality on the respiratory health of both livestock and farmers. In 2009, an environmental monitoring system was designed, developed and tested that allowed for the monitoring of a number of airborne pollutants. One limitation of the system was the manual collection of logged data from each unit. This paper identifies limitations of the current environmental monitoring system and suggests a range of networking technologies that can be used to increase usability. Consideration is taken for the networking of environmental monitoring units, as well as the collection of recorded data. Furthermore, the design and development of a software system that is used to collate and store recorded environmental data from multiple farms is explored. In order to design such a system, simplified software engineering processes and methodologies have been utilised. The main steps taken in order to complete the project were requirements elicitation with clients, requirements analysis, system design, implementation and finally testing. The outcome of the project provided a potential prototype for improving the environmental monitoring system and analysis informing the benefit of the implementation.}
}
@article{WOMBLE201361,
title = {Diving into the analysis of time–depth recorder and behavioural data records: A workshop summary},
journal = {Deep Sea Research Part II: Topical Studies in Oceanography},
volume = {88-89},
pages = {61-64},
year = {2013},
note = {Fourth International Symposium on Bio-logging Science},
issn = {0967-0645},
doi = {https://doi.org/10.1016/j.dsr2.2012.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S0967064512001038},
author = {Jamie N. Womble and Markus Horning and Mary-Anne Lea and Michael J. Rehberg},
keywords = {Diving, Behavior, Foraging behavior, Analytical techniques},
abstract = {Directly observing the foraging behavior of animals in the marine environment can be extremely challenging, if not impossible, as such behavior often takes place beneath the surface of the ocean and in extremely remote areas. In lieu of directly observing foraging behavior, data from time–depth recorders and other types of behavioral data recording devices are commonly used to describe and quantify the behavior of fish, squid, seabirds, sea turtles, pinnipeds, and cetaceans. Often the definitions of actual behavioral units and analytical approaches may vary substantially which may influence results and limit our ability to compare behaviors of interest across taxonomic groups and geographic regions. A workshop was convened in association with the Fourth International Symposium on Bio-logging in Hobart, Tasmania on 8 March 2011, with the goal of providing a forum for the presentation, review, and discussion of various methods and approaches that are used to describe and analyze time–depth recorder and associated behavioral data records. The international meeting brought together 36 participants from 14 countries from a diversity of backgrounds including scientists from academia and government, graduate students, post-doctoral fellows, and developers of electronic tagging technology and analysis software. The specific objectives of the workshop were to host a series of invited presentations followed by discussion sessions focused on (1) identifying behavioral units and metrics that are suitable for empirical studies, (2) reviewing analytical approaches and techniques that can be used to objectively classify behavior, and (3) identifying cases when temporal autocorrelation structure is useful for identifying behaviors of interest. Outcomes of the workshop included highlighting the need to better define behavioral units and to devise more standardized processing and analytical techniques in order to ensure that results are comparable across studies and taxonomic groups.}
}
@article{BLOOMFIELD20171,
title = {Opening the Duke electronic health record to apps: Implementing SMART on FHIR},
journal = {International Journal of Medical Informatics},
volume = {99},
pages = {1-10},
year = {2017},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2016.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S1386505616302738},
author = {Richard A. Bloomfield and Felipe Polo-Wood and Joshua C. Mandel and Kenneth D. Mandl},
keywords = {Electronic health records, HL7 FHIR, Interoperability, Software},
abstract = {Objective
Recognizing a need for our EHR to be highly interoperable, our team at Duke Health enabled our Epic-based electronic health record to be compatible with the Boston Children’s project called Substitutable Medical Apps and Reusable Technologies (SMART), which employed Health Level Seven International’s (HL7) Fast Healthcare Interoperability Resources (FHIR), commonly known as SMART on FHIR.
Methods
We created a custom SMART on FHIR-compatible server infrastructure written in Node.js that served two primary functions. First, it handled API management activities such rate-limiting, authorization, auditing, logging, and analytics. Second, it retrieved the EHR data and made it available in a FHIR-compatible format. Finally, we made required changes to the EHR user interface to allow us to integrate several compatible apps into the provider- and patient-facing EHR workflows.
Results
After integrating SMART on FHIR into our Epic-based EHR, we demonstrated several types of apps running on the infrastructure. This included both provider- and patient-facing apps as well as apps that are closed source, open source and internally-developed. We integrated the apps into the testing environment of our desktop EHR as well as our patient portal. We also demonstrated the integration of a native iOS app.
Conclusion
In this paper, we demonstrate the successful implementation of the SMART and FHIR technologies on our Epic-based EHR and subsequent integration of several compatible provider- and patient-facing apps.}
}
@article{ATALLAH2020241,
title = {A prospective multicentre REFCOR study of 470 cases of head and neck Adenoid cystic carcinoma: epidemiology and prognostic factors},
journal = {European Journal of Cancer},
volume = {130},
pages = {241-249},
year = {2020},
issn = {0959-8049},
doi = {https://doi.org/10.1016/j.ejca.2020.01.023},
url = {https://www.sciencedirect.com/science/article/pii/S0959804920300551},
author = {Sarah Atallah and Odile Casiraghi and Nicolas Fakhry and Michel Wassef and Emmanuelle Uro-Coste and Florent Espitalier and Anne Sudaka and Marie Christine Kaminsky and Stéphanie Dakpe and Laurence Digue and Olivier Bouchain and Sylvain Morinière and Muriel Hourseau and Chloé Bertolus and Franck Jegoux and Juliette Thariat and Valentin Calugaru and Philippe Schultz and Pierre Philouze and Olivier Mauvais and Christian A. Righini and Cécile Badoual and Nicolas Saroul and Jean Michel Goujon and Jean Paul Marie and Rabah Taouachi and Esteban Brenet and Anne Aupérin and Bertrand Baujat},
keywords = {Adenoid cystic carcinoma, Prognostic factors, Event-free survival, REFCOR},
abstract = {Background
Adenoid cystic carcinoma (ACC) accounts for 1% of malignant head and neck tumours [1] and 10% of salivary glands malignant tumours. The main objective of our study is to investigate the prognostic factors influencing the event-free survival (EFS) of patients with ACC.
Patients and methods
A multicentre prospective study was conducted from 2009 to 2018. All 470 patients with ACC whose survival data appear in the REFCOR database were included in the study. The main judgement criterion was EFS. Both a bivariate survival analysis using log-rank test and a multivariate using Cox model were performed using the R software.
Results
Average age was 55 years. Females accounted for 59.4% of the cohort. The body mass index (BMI) was normal in 86% of cases. Tumours were located in minor salivary glands in 60% of cases. T3/T4 stages represented 58%; 89% of patients were cN0. histological grade III was observed on 21% of patients. The EFS and overall 5-year survival rates were 50% and 85%, respectively. After adjustment, the most significant pejorative prognostic factors were age ≥65 years (hazard ratio [HR] = 1.67), BMI<16.5 (HR = 2.62), and lymph node invasion cN (HR = 2.08).
Conclusion
Age, BMI and N stage are the three main clinical prognostic factors determining EFS identified in this prospective series of patients with ACC. Such findings open new research perspectives on the influence of these components on initial patient care.}
}
@article{BASCOULMOLLEVI2018153,
title = {Longitudinal health-related quality of life analysis in oncology with time to event approaches, the STATA command qlqc30_TTD},
journal = {Computer Methods and Programs in Biomedicine},
volume = {158},
pages = {153-159},
year = {2018},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0169260717301049},
author = {C. Bascoul-Mollevi and Marion Savina and Amélie Anota and Antoine Barbieri and David Azria and Franck Bonnetain and Sophie Gourgou},
keywords = {Health-related quality of life, EORTC QLQ-C30, Longitudinal analysis, Time to event approach, Time to deterioration, Time until definitive deterioration},
abstract = {Background and objective
Health-related quality of life (HRQoL) has become one relevant and available alternative endpoint of clinical trials in cancer research to evaluate efficiency of care both for the patient and health system. HRQoL in oncology is mainly assessed using the 30-item European Organisation for Research and Treatment of Cancer Quality of Life—Questionnaire Core 30 (EORTC QLQ-C30). The EORTC QLQ-C30 questionnaire is usually assessed at different times along the clinical trials in order to analyze the kinetics of HRQoL evolution and to fully assess the impact of the treatment on the patient's HRQoL level. In this perspective, the realization of a longitudinal HRQoL analysis is essential and the time to HRQoL score deterioration approach is a method which is more and more used in clinical trials.
Method
Using the Stata software, we developed a QLQ-C30 specific command, qlqc30_TTD, which implements longitudinal strategies based on the time to event methods by considering the time to HRQoL score deterioration. This user-written command providing automatic execution of the Time To Deterioration (TTD) and Time Until Definitive Deterioration (TUDD) methods.
Result
The program implements all published definitions and provides the Kaplan–Meier curves for each dimension (by group) and a table with the Hazard Ratio and Log-Rank test.
Conclusion
The longitudinal analysis of HRQoL data in cancer clinical trials remains complex with only few programs like ours computed. This program will be of great help and will allow a more systematic and quicker analysis of the HRQoL data in clinical trials in oncology.}
}
@article{PERUMALLA201693,
title = {Model-based Dynamic Control of Speculative Forays in Parallel Computation},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {327},
pages = {93-107},
year = {2016},
note = {The 8th International Workshop on Practical Application of Stochastic Modeling, PASM 2016},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2016.09.025},
url = {https://www.sciencedirect.com/science/article/pii/S1571066116300706},
author = {Kalyan S. Perumalla and Mohammed M. Olama and Srikanth B. Yoginath},
keywords = {Reversible execution, Parallel Computing, Speculative Execution, Model-based Execution},
abstract = {In simulations running in parallel, the processors would have to synchronize with other processors to maintain correct global order of computations. This can be done either by blocking computation until correct order is guaranteed, or by speculatively proceeding with the best guess (based on local information) and later correcting errors if/as necessary. Since the gainful lengths of speculative forays depend on the dynamics of the application software and hardware at runtime, an online control system is necessary to dynamically choose and/or switch between the blocking and speculative strategies. In this paper, we formulate the reversible speculative computing in large-scale parallel computing as a dynamic linear feedback control (optimization) system model and evaluate its performance in terms of time and cost savings as compared to the traditional (forward) computing. We illustrate with an exact analogy in the form of vehicular travel under dynamic, delayed route information. The objective is to assist in making the optimal decision on what computational approach is to be chosen, by predicting the amount of time and cost savings (or losing) under different environments represented by different parameters and probability distribution functions. We consider the cases of Gaussian, exponential and log-normal distribution functions. The control system is intended for incorporating into speculative parallel applications such as optimistic parallel discrete event simulations to decide at runtime when and to what extent speculative execution can be performed gainfully.}
}
@article{PISEGNA2014S61,
title = {An Interdisciplinary Approach to Improving Exclusive Breast Milk Feeding Rate at Discharge},
journal = {Journal of Obstetric, Gynecologic & Neonatal Nursing},
volume = {43},
pages = {S61-S62},
year = {2014},
note = {Proceedings of the AWHONN 2014 Convention},
issn = {0884-2175},
doi = {https://doi.org/10.1111/1552-6909.12346},
url = {https://www.sciencedirect.com/science/article/pii/S0884217515316610},
author = {Lily Pisegna and Jeanine Pyka},
keywords = {Baby Friendly, exclusive breast milk, interdisciplinary, exclusive breastfeeding, NICU, breast milk supply, lactation consultants, electronic medical record, bed huddle},
abstract = {Newborn Care Poster Presentation
Objective
To improve the exclusive breastfeeding and breast milk feeding rates at discharge to 71% by the third quarter of 2013.
Design
An interdisciplinary taskforce was created with representatives from the Women and Infants Service Line. Breastfeeding education was revised to support the 10 steps to becoming a Baby Friendly Hospital. Baby Friendly goals and other quality initiatives were adopted by each unit to improve exclusive breastfeeding at discharge. The electronic medical record (EMR) and interdisciplinary resources were leveraged to identify and support mothers throughout their hospitalizations.
Sample
All well newborns and all premature infants 28 to 34 weeks gestation (100%).
Methods
Retrospective chart review using software to abstract charts based on the Joint Commission definitions for perinatal core measures.
Implementation Strategies
An interdisciplinary taskforce was formed to standardize and reliably implement evidence-based, supportive hospital practices for breastfeeding, including skin to skin in labor and delivery and post anesthesia care units; EMR identification and interdisciplinary plan of care for newborns having difficulty latching or breastfeeding; EMR documentation of 24-hour breast milk totals; and bedside pumping logs and tracking of adequate expressed breast milk supply by 2 weeks in the neonatal intensive care unit (NICU). All mothers are encouraged to log and track their breast milk production on the breast pump log throughout hospitalization. Lactation consultants develop an interdisciplinary plan of care on all infants having difficulty breastfeeding or requiring specialized care. The plan of care is attached to a specific location within the neonatal EMR. Lactation consultants and registered nursed in the NICU have daily bed huddles to identify mothers who are having difficulty establishing breast milk supply.
Results
Based on all the strategies the taskforce implemented, we increased our exclusive breast milk rate at discharge from a baseline of 49% (July-September 2011) to 71% (April-June 2013).
Conclusion/Implications for Nursing Practice
Adopting an interdisciplinary approach involving all key stakeholders throughout hospitalization is critical to the success of improving exclusive breast milk rate at discharge. Implementing evidence-based practice and engaging the staff to take accountability was key to our success.}
}
@article{PRAKONGKEP201020,
title = {SEM image analysis for characterization of sand grains in Thai paddy soils},
journal = {Geoderma},
volume = {156},
number = {1},
pages = {20-31},
year = {2010},
issn = {0016-7061},
doi = {https://doi.org/10.1016/j.geoderma.2010.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0016706110000169},
author = {Nattaporn Prakongkep and Anchalee Suddhiprakarn and Irb Kheoruenromne and Robert J. Gilkes},
keywords = {Image analysis, Scanning electron microscopy, Grain size, Grain shape, Paddy soils, Thailand},
abstract = {This study presents a detailed characterization of sand grains in 34 samples of Thai paddy soils (mostly Alfisols and Ultisols) that have formed from diverse parent materials. Computer analysis of SEM images of thin sections provided a novel and reliable method to analyse the size and shape of quartz grains using the software ImageJ 1.34S. Sand grain size expressed as the feret value showed a log normal distribution for 16 samples whereas for the other samples the distribution was positively skewed. A logit transformation of grain circularity values generated a normal distribution for 26 samples and eight samples had bimodal distributions. These granulometric data indicate the diversity of the soil parent materials and the observed soil layers represent discrete sedimentary events with materials of different origins or different depositional energies. Pedogenesis including pedoturbation has not disrupted these sedimentary layers. Some sand grains consist of gold, brass, copper, ilmenite, zircon, magnetite, barite or monazite. The diversity of these grains reflects the complex geology of the catchments providing the sediments and some anthropogenic materials may have been incorporated into the soils.}
}
@article{LIN20131208,
title = {Booting, browsing and streaming time profiling, and bottleneck analysis on android-based systems},
journal = {Journal of Network and Computer Applications},
volume = {36},
number = {4},
pages = {1208-1218},
year = {2013},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2013.02.024},
url = {https://www.sciencedirect.com/science/article/pii/S1084804513000623},
author = {Ying-Dar Lin and Cheng-Yuan Ho and Yuan-Cheng Lai and Tzu-Hsiung Du and Shun-Lee Chang},
keywords = {Android, Booting, Browsing, Streaming, Time profiling},
abstract = {Android-based systems perform slowly in three scenarios: booting, browsing, and streaming. Time profiling on Android devices involves three unique constraints: (1) the execution flow of a scenario invokes multiple software layers, (2) these software layers are implemented in different programming languages, and (3) log space is limited. To compensate for the first and second constraints, we assumed a staged approach using different profiling tools applied to different layers and programming languages. As for the last constraint and to avoid generating enormous quantities of irrelevant log data, we began profiling scenarios from an individual module, and then iteratively profiled an increased number of modules and layers, and finally consolidated the logs from different layers to identify bottlenecks. Because of this iteration, we called this approach a staged iterative instrumentation approach. To analyze the time required to boot the devices, we conducted experiments using off-the-shelf Android products. We determined that 72% of the booting time was spent initializing the user-space environment, with 44.4% and 39.2% required to start Android services and managers, and preload Java classes and resources, respectively. Results from analyzing browsing performance indicate that networking is the most significant factor, accounting for at least 90% of the delay in browsing. With regard to online streaming, networking and decoding technologies are two most important factors occupying 77% of the time required to prepare a 22MB video file over a Wi-Fi connection. Furthermore, the overhead of this approach is low. For example, the overhead of CPU loading is about 5% in the browsing scenario. We believe that this proposed approach to time profiling represents a major step in the optimization and future development of Android-based devices.}
}
@article{HIRSCH2016167,
title = {Log-normal frailty models fitted as Poisson generalized linear mixed models},
journal = {Computer Methods and Programs in Biomedicine},
volume = {137},
pages = {167-175},
year = {2016},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2016.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0169260716305272},
author = {Katharina Hirsch and Andreas Wienke and Oliver Kuss},
keywords = {Poisson, GLMM, Survival, Frailty, Piecewise constant baseline hazard, },
abstract = {Background and objectives
The equivalence of a survival model with a piecewise constant baseline hazard function and a Poisson regression model has been known since decades. As shown in recent studies, this equivalence carries over to clustered survival data: A frailty model with a log-normal frailty term can be interpreted and estimated as a generalized linear mixed model with a binary response, a Poisson likelihood, and a specific offset. Proceeding this way, statistical theory and software for generalized linear mixed models are readily available for fitting frailty models. This gain in flexibility comes at the small price of (1) having to fix the number of pieces for the baseline hazard in advance and (2) having to “explode” the data set by the number of pieces.
Methods
In this paper we extend the simulations of former studies by using a more realistic baseline hazard (Gompertz) and by comparing the model under consideration with competing models. Furthermore, the SAS macro %PCFrailty is introduced to apply the Poisson generalized linear mixed approach to frailty models.
Results
The simulations show good results for the shared frailty model. Our new %PCFrailty macro provides proper estimates, especially in case of 4 events per piece.
Conclusions
The suggested Poisson generalized linear mixed approach for log-normal frailty models based on the %PCFrailty macro provides several advantages in the analysis of clustered survival data with respect to more flexible modelling of fixed and random effects, exact (in the sense of non-approximate) maximum likelihood estimation, and standard errors and different types of confidence intervals for all variance parameters.}
}
@incollection{JAMES201029,
title = {Chapter 3 - Configuring the Desktop Delivery Controller},
editor = {Gareth R. James},
booktitle = {Citrix XenDesktop Implementation},
publisher = {Syngress},
address = {Boston},
pages = {29-63},
year = {2010},
isbn = {978-1-59749-582-0},
doi = {https://doi.org/10.1016/B978-1-59749-582-0.00003-8},
url = {https://www.sciencedirect.com/science/article/pii/B9781597495820000038},
author = {Gareth R. James},
abstract = {Publisher Summary
The aim of this chapter is to draw together the most basic functions of the XenDesktop broker. It provides initial configuration of the Desktop Delivery Controller. The configuration section is separated into two parts; the basic settings are enough to get one to the stage of being able to remotely attach to a virtual desktop, and the advanced settings cover configuration that may or may not be necessary depending on the environment. Some of the advanced configuration settings have dependencies on the interaction with the hypervisor. After the XenDesktop software has been installed, the user may notice two errors in the event logs, the first regarding the configuration of the Active Directory OU (Organizational Unit) and the second error regarding the license server.}
}
@article{VONDAVIER2017631,
title = {Interdisciplinary research agenda in support of assessment of collaborative problem solving: lessons learned from developing a Collaborative Science Assessment Prototype},
journal = {Computers in Human Behavior},
volume = {76},
pages = {631-640},
year = {2017},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2017.04.059},
url = {https://www.sciencedirect.com/science/article/pii/S0747563217303023},
author = {Alina A. {von Davier} and Jiangang Hao and Lei Liu and Patrick Kyllonen},
keywords = {Collaborative problem solving, Computer-based assessment, Simulation-based task},
abstract = {Evidence from labor-market economics and predictive validity studies in psychology suggests that collaborative problem solving (CPS) is an increasingly important skill for both academic and career success in the 21st century. While there is a general agreement that collaborative problem solving is an important skill, there is less agreement on how to build an assessment to measure it, especially at scale and as a standardized test. Developing the type of CPS assessment envisioned in this work will require interdisciplinary synergy, involving learning science, data science, psychometrics, and software engineering. In this conceptual paper, we present our identification and novel instantiation of five interdisciplinary research strands supporting the development of a CPS assessment. We discuss how these research strands can comprehensively address some of the shortcomings of existing CPS assessments, such as collecting and managing the data from the process of collaboration in structured log files, or considering a statistical definition of collaboration in the design of the collaborative tasks. We describe the Collaborative Science Assessment Prototype developed at Educational Testing Service (ETS) under the proposed interdisciplinary research agenda to illustrate how these research strands can be operationalized.}
}
@article{LATHA2019285,
title = {Telemedicine Setup using Wireless Body Area Network over Cloud},
journal = {Procedia Computer Science},
volume = {165},
pages = {285-291},
year = {2019},
note = {2nd International Conference on Recent Trends in Advanced Computing ICRTAC -DISRUP - TIV INNOVATION , 2019 November 11-12, 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.01.074},
url = {https://www.sciencedirect.com/science/article/pii/S187705092030082X},
author = {R Latha and P Vetrivelan and S Geetha},
keywords = {Wireless body area networks, Data Analysis, Teleconsultation, Health records},
abstract = {Healthcare has been grown into a new paradigm because of Telemedicine. Hence patients get easy access of teleconsultations for diagnosis and treatments. Patients get numerous benefits in practicing telemedicine when there is a need of an immediate consultation, being anywhere. In this paper, through wireless body area network, telemedicine has been investigated and practiced for remote patients constantly being monitored. The medical records of patients are stored very carefully in the cloud. The doctors create a network who can respond to emergency conditions of patients. Here, three different parameters viz., blood viscosity, blood pressure and blood sugar level are measured, constantly monitored, and sent to cloud for storage and analysis. Then, for emergency condition, on the spot treatment is given after teleconsultation, by the doctor near the patient’s locality; otherwise teleconsultation alone is given. The healthcare professionals at the care centers have remote monitoring capability as well as registers patients for storing and monitoring health information anytime, anywhere. Security during teleconsultation is also possible as the data are stored in cloud. Cloud gives dynamic, scalable and portable infrastructure. This is achieved through the use of trusted software, compliance understanding, lifecycle management, portability, continuous monitoring, and choice of right people. It is possible through secure log-in access, risk tolerance, cost-benefit analysis giving protection to data, applications and infrastructure. There is high-level security like avoiding attack, no unauthorized leaks and exposure of data and no weak access of control. This is achieved through data-centric approach where the data is encrypted and strengthened through the authorization process of requiring strong passwords and two factor authentication.}
}
@article{VILLALOBOSC2016263,
title = {Supervivencia de cáncer cervicouterino escamoso y adenocarcinoma en pacientes atendidas en el Instituto Nacional del Cáncer, 2009-2013},
journal = {Gaceta Mexicana de Oncología},
volume = {15},
number = {5},
pages = {263-267},
year = {2016},
issn = {1665-9201},
doi = {https://doi.org/10.1016/j.gamo.2016.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S166592011630075X},
author = {Michael {Villalobos C.} and Carolina {Wendling C.} and Claudia {Sierra H.} and Oscar {Valencia C.} and Marcela {Cárcamo I.} and Patricio {Gayán P.}},
keywords = {Cáncer cervicouterino, Supervivencia, Análisis de supervivencia, Uterine cervical neoplasm, Survival, Survival analysis},
abstract = {Resumen
Introducción
Mundialmente, el cáncer cervicouterino es el cuarto tipo de cáncer más frecuente en mujeres, siendo la incidencia estimada de 528,000 nuevos casos anuales (2012). En Chile, se registraron alrededor de 1,000 casos nuevos durante el 2008. La tasa de mortalidad para el 2008 fue de 7.5 por 100,000 mujeres.
Objetivo
Describir la supervivencia global en pacientes con diagnóstico de cáncer cervicouterino entre el 2009 y el 2013 atendidas en el Instituto Nacional del Cáncer.
Material y método
Serie de casos. Se utilizaron las planillas de registro y software del Registro Hospitalario de Cáncer (2011). Se identificaron todas las pacientes pertenecientes al Servicio de Salud Metropolitano Norte, O‘Higgins y Maule, diagnosticadas con cáncer cervicouterino en el 2009-2013. Para el análisis de supervivencia, se consideró como tiempo cero la fecha de diagnóstico mediante biopsia, y como evento, la fecha de defunción. Se realizó análisis de supervivencia según histología y estadios, utilizando método de Kaplan Meier. Las curvas de supervivencia se compararon mediante test de log-rank. Se consideró un nivel de confianza del 95%.
Resultados
La supervivencia global fue del 67.05% a 5 años. Según histología, la supervivencia global a 5 años fue del 67.33% para el escamoso y del 67.13% para el adenocarcinoma. La supervivencia global según estadio fue: del 90.40% etapa i, del 77.8% etapa ii, del 47.4% etapa iii y del 26.45% etapa iv.
Conclusión
Las curvas de supervivencia calculadas en este trabajo son acordes a resultados logrados internacionalmente en centros oncológicos desarrollados. No existe diferencia estadísticamente significativa al considerar el tipo histológico.
Introduction
Cervico-uterine cancer is the fourth most common type of cancer in women worldwide, with an estimated incidence of 528,000 new cases annually (2012). Around 1,000 new cases were registered in Chile during 2008. The mortality rate for 2008 was 7.5 per 100,000 women.
Objective
To report on the overall survival of patients with a diagnosis of cervico-uterine cancer treated in the National Cancer Institute between 2009 and 2013.
Materials and methods
A case series study using the records and Hospital Cancer Registry Software (2011). All patients belonging to the Salud Metropolitano Norte, O‘Higgins and Maule Health Service diagnosed with cervico-uterine between the years 2009 and 2013 were identified. The variables taken into account for the analysis of survival were time zero, date of diagnosis by means of biopsy, and as event, the date of death. An analysis of survival according to histology and stages was performed using the Kaplan Meier method. The survival curves were compared using the Log-rank test. A confidence level of 95% was used.
Results
The overall survival at 5 years was 67.05%. According to histology, the overall survival at 5 years was 67.33% for squamous cell, and 67.13 for adenocarcinoma. The overall survival according to stage was: 90.40% stage I, 77.8% stage II, 47.4% stage III and 26.45% stage IV.
Conclusion
The calculated survival curves in this study are similar to the results achieved internationally in developed oncology centres. There is no statistically significant difference with the histology type.}
}
@article{ELMETWALY201986,
title = {Synthesis and characterization for novel Cu(II)-thiazole complexes-dyes and their usage in dyeing cotton to be special bandage for cancerous wounds},
journal = {Journal of Molecular Structure},
volume = {1194},
pages = {86-103},
year = {2019},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2019.05.080},
url = {https://www.sciencedirect.com/science/article/pii/S0022286019306507},
author = {Nashwa El-Metwaly and Ismail Althagafi and Abdalla M. Khedr and Jabir H. Al-Fahemi and Hanadi A. Katouah and Aisha S. Hossan and Aisha Y. Al-Dawood and Gamil A. Al-Hazmi},
keywords = {Cu(II)-thiazole complexes, Bandage for cancerous wounds, CT-DNA, Molecular docking, DFT/B3LYP},
abstract = {New series of Cu(II)-thiazole complexes were synthesized using variable p-substituted, N-aryl-2-oxo-2-(thiazol-2-ylamino)-acetohydrazonoyl cyanide. All new syntheses were elucidated by, analytical, spectral and conformational study. The binuclear feature (2 M:1L), was proposed for all complexes, through mono-negative tetra-dentate chelation mode in octahedral or square-planer geometry. The formulae of chosen compounds, were confirmed by, 1HNMR and mass spectral analysis. The ideal distribution for atomic-skeletons, was performed utilizing Gaussian09 software, to confirm the bonding mode. Also, substantial parameters were extracted from output-files (log &chk), beside others calculated based on frontier energy gaps. The superiority of Cu(II) complexes, was predicated from such conformational study. Also and by applying MOE module, the docking process was performed for most syntheses against selected pathogen proteins (1miu, 4k9g and 5jm5), which have been tested practically in application. The extracted docking-data, showed clear superiority and promising efficiency for Cu(II) complexes as anticancer drugs compared to free derivatives. Traditional screening was conducted over new complexes against three carcinoma cell lines (HCT-116, MCF-7 & HepG-2) as well as healthy cell line (HSF). IC50 values showed considerable toxicity of Cu(II)-4e complex versus HCT-116 cell line. The antitumor screening was conducted over cotton fabric after dyeing by complexes, to test the degree of success to be used as a special bandage for cancerous wounds. The most lighted observation was, the effect of released-pigmenting complex on colon cancer cell line, while the absence of any effect on healthy cell. Also, the released-pigment, controlled the pH of cancerous wound, which is significantly preferable.}
}
@article{GARDAVAUD201832,
title = {56 Peak Skin Dose evaluation for vascular clinical procedures in interventional radiology: a comparison between three computation numerical solutions},
journal = {Physica Medica},
volume = {56},
pages = {32},
year = {2018},
note = {Abstracts of the 57èmes Journées Scientifiques de la Société Française de Physique Médicale},
issn = {1120-1797},
doi = {https://doi.org/10.1016/j.ejmp.2018.09.069},
url = {https://www.sciencedirect.com/science/article/pii/S1120179718312316},
author = {F. Gardavaud and S. Tavolaro and N. Gussenmeyer-Mary and F. Cornelis and F. Boudghène},
abstract = {Introduction
This study compared the Peak Skin Dose (PSD) evaluation performed by a dose monitoring software with two other numerical solutions for interventional clinical procedures realized in an angiographic room.
Methods
26 Patients, with standard Body Mass Index, have benefited interventional vascular procedures, performed in an angiographic room (GE Innova IGS 540, GE HealthCare, Buc, France). For each clinical case, evaluated on various embolization conditions, PSD calculations were performed across three numerical solutions: with Em.Dose v 3.0.1[1] (Esprimed, Villejuif, France), noted PSDEmDose; with Innova DoseMap[2] (GE HealthCare, Buc, France), noted PSDInnova and with DoseWatch v2.3 (GE HealthCare, Buc, France), noted PSDDoseWatch. 3D-angiographic acquisitions were taken into account for all the PSD calculation solutions. Log files were retrieved to get PSDInnova. The use of the super elliptical phantom computation model, tailored for each patient, was always forced to get the most accurate PSDEmDose values estimated from DICOM files including all the angiographic sequences (2D and/or 3D) and all the fluoroscopy sequences registered by the operator. For the three numerical solutions, a calibration factor, to take into account DAP-meter flat chamber measuring error, was integrated to calculate all the PSD values. PSDDoseWatch values were statistically compared with PSDInnova values and PSDEmDose values.
Results
PSD mean values were 560±540 mGy, 559±494 mGy and 571±551 mGy for PSDDoseWatch, PSDInnova and PSDEmDose respectively. PSDDoseWatch values was strongly correlated to the PSDInnova values and also to the PSDEmDose values (P>0.9999 for each combination with Dunn multiple comparisons non-parametric test).
Conclusions
Based on this study, PSDDoseWatch values are consistent to be used in clinical routine by experienced and trained medical physicists when patients benefit interventional vascular procedures, involving thoracic, abdominal and/or pelvic exposure area, in an angiographic room with a single X-rays tube.}
}
@article{PLASTARAS20152157,
title = {Adverse events associated with fluoroscopically guided lumbosacral transforaminal epidural steroid injections},
journal = {The Spine Journal},
volume = {15},
number = {10},
pages = {2157-2165},
year = {2015},
issn = {1529-9430},
doi = {https://doi.org/10.1016/j.spinee.2015.05.034},
url = {https://www.sciencedirect.com/science/article/pii/S1529943015005562},
author = {Christopher Plastaras and Zachary L. McCormick and Cynthia Garvan and Donald Macron and Anand Joshi and Gary Chimes and Wesley Smeal and Joshua Rittenberg and David J. Kennedy},
keywords = {Fluoroscopic injection, Transforaminal, Epidural, Complications, Adverse effects, Steroid injection},
abstract = {Background context
Although the types and incidence of adverse events (AEs) associated with transforaminal epidural steroid injection (TFESI) have been described, no study has used a systematic standardized questionnaire to solicit AEs from patients to capture an accurate range and incidence of complications.
Purpose
The aim was to systematically identify the types and incidence of AEs associated with TFESI. Additionally, this study evaluated demographic and clinical factors that may predict a higher risk of an AE.
Study design/Setting
This was a retrospective cohort study from a multiphysician academic PM&R clinic.
Patient sample
Patients, aged 19 to 89, who underwent a fluoroscopically guided TFESI for lumbosacral radicular pain between 2004 and 2007 were included.
Outcome measures
The relationship of AEs with gender, age, trainee presence, steroid type, preprocedure visual analog scale (VAS) pain score, systolic blood pressure, fluoroscopy time, and corticosteroid injectate volume was analyzed.
Methods
Adverse event data were collected using a survey both immediately and at 24 to 72 hours after TFESI. Statistical analysis was performed using the chi-square, Fisher exact, or Wilcoxon rank sum two-sided tests. Logistic regression analysis was also performed. C.P. is the owner of Rehabilitation Institute of Chicago Physiatric Log & Analysis System computer software.
Results
In 1,295 consecutive patients undergoing 2,025 TFESI procedures, immediate AEs and delayed AEs occurred after 182 (9.2%) and 305 (20.0%) injections, respectively. The most common immediate AEs were: vasovagal reaction (4.2%) and interrupted procedure from intravascular flow (1.7%). Common delayed AEs included: pain exacerbation (5.0%), injection site soreness (3.9%), headache (3.9%), facial flushing/sweating (1.8%), and insomnia (1.6%). Significant associations were identified between AEs and gender, age, preprocedure VAS, steroid type, and fluoroscopy time. Trainee involvement in the procedure did not impact the complication rate.
Conclusions
Fluoroscopically guided lumbosacral TFESI is associated with a similar rate of minor AEs both immediately and 24 to 72 hours after procedure that are typical of other axial corticosteroid injections. Permanent AEs were not found in this sample. The most common AEs associated with TFESI include vasovagal episodes, procedure interruption from intravascular flow, pain exacerbation, injection site soreness, headache, and insomnia.}
}
@article{TOLONE2013134,
title = {Estimation of genetic and phenotypic parameters for bacteriological status of the udder, somatic cell score, and milk yield in dairy sheep using a threshold animal model},
journal = {Livestock Science},
volume = {151},
number = {2},
pages = {134-139},
year = {2013},
issn = {1871-1413},
doi = {https://doi.org/10.1016/j.livsci.2012.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S1871141312004350},
author = {M. Tolone and V. Riggio and B. Portolano},
keywords = {Infection status, Somatic cell score, Milk yield, Sheep, Threshold model},
abstract = {The objective of this study was to estimate the genetic parameters for infection status (INF), as indicator of mastitis, SCS (i.e., log-transformed SCC), and milk yield (MY), by using a Gibbs sampling algorithm. The data comprised 17,843 test-day records of 2040 ewes. The pedigree file included 2948 animals. A bivariate variance component analysis was performed using the TM software. Fixed effects considered in the analysis were litter size, parity, flock by test-day interaction, year by season of lambing interaction, and stage of lactation; whereas the animal, and the permanent environmental effect within and across lactations were considered as random as well as the error. Flat priors were used for both fixed effects and variance components. Parameters were drawn from the posterior conditional distributions. The posterior means of heritability for MY, SCS and INF were equal to 0.14, 0.09, and 0.09, respectively; whereas the repeatability within lactation was around 0.30 for the three traits, and ranged between 0.29 and 0.41 across lactations. The genetic correlation between INF and SCS was equal to 0.93, suggesting that selection for low SCS would also lead to a reduced incidence of mastitis. On the other hand, the positive and moderate genetic correlation between mastitis and milk yield (0.59) confirms the antagonistic association between udder health and milk yield. Therefore, in breeding programs that emphasize milk yield, the unfavorable genetic correlation between milk yield and mastitis, may result in an increased incidence of the latter.}
}
@article{TURK2017638,
title = {Potentials of Blockchain Technology for Construction Management},
journal = {Procedia Engineering},
volume = {196},
pages = {638-645},
year = {2017},
note = {Creative Construction Conference 2017, CCC 2017, 19-22 June 2017, Primosten, Croatia},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.08.052},
url = {https://www.sciencedirect.com/science/article/pii/S187770581733179X},
author = {Žiga Turk and Robert Klinc},
keywords = {blockchain, building information modelling, building information management, information systems, intellectual property rights, construction contracts, trust},
abstract = {Blockchain technology enables distributed, encrypted and secure logging of digital transactions. It is the underlying technology of Bitcoin and other cryptocurrencies. Blockchain is expected to revolutionize computing in several areas, particularly where centralization was unnatural and privacy was important. In the paper, we present research on where and how this technology could be useful in the construction industry. The work is based on the study of literature on open issues that exist in construction process management. These are than matched to the capabilities of blockchain. We are motivated by the fact that construction projects involve a dynamic grouping of several companies. We study the degree to which the relationships among them are hierarchical or peer-to-peer and note that particularly in information intensive phases, centralization of information management was necessary because of technology. When using un-constraining technology, communication patterns among participants show a peer-to-peer nature of the relationships. In such environment, blockchain can provide a trustworthy infrastructure for information management during all building life-cycle stages. Even if building information modelling (BIM) is used, which assumes a centralized building information model, there is a role for blockchain to manage information on who did what and when and thus provide a basis for any legal arguments that might occur. On the construction site blockchain can improve the reliability and trustworthiness of construction logbooks, works performed and material quantities recorded. In the facility maintenance phase, blockchain's main potential is the secure storage of sensor data which are sensitive to privacy. We conclude that blockchain provides solutions to many current problems in construction information management. However, it is more likely that it will be built into generic IT infrastructure on top of which construction applications are built, rather than used directly by authors of construction related software. It has a potential to make construction processes less centralized which opens needs for research in that direction.}
}
@incollection{CAPUTO2010219,
title = {8 - DVS Archiving and Storage},
editor = {Anthony C. Caputo},
booktitle = {Digital Video Surveillance and Security},
publisher = {Butterworth-Heinemann},
address = {Boston},
pages = {219-249},
year = {2010},
isbn = {978-1-85617-747-4},
doi = {https://doi.org/10.1016/B978-1-85617-747-4.00008-1},
url = {https://www.sciencedirect.com/science/article/pii/B9781856177474000081},
author = {Anthony C. Caputo},
abstract = {Publisher Summary
Digital video security (DVS) archiving is the process of recording live video streams for later review. A DVS system includes a workstation where the video management system (VMS) is monitored and a VMS server where the software is installed controls the cameras, video streaming, storage, and archiving. This video workstation displays multiple video streams for an extended period of time whether during specific events, work hours, or 24/7 and it demands horsepower designed to play video. The VMS server is built to handle recording video, which requires more computing horsepower and hard drive space. The server and workstation can only be one in the same with small implementations of two or four cameras, and even with that taxing the system it cannot and should not be used for anything other than a VMS server/workstation. The control of the DVS network begins with authorization and authentication by limiting the number of people who have access into the DVS system through ID and passwords. In addition to controlling log in, granting access to select physical media access control (MAC) addresses only allows unique computers to join the network.}
}
@article{HUANG2021,
title = {Prognostic impact of the ratio of preoperative CA19-9 to liver enzyme levels in pancreatic cancer patients with jaundice (predictability of combined CA19-9/AST and CA19-9/γ-GGT for jaundiced PDAC patients)},
journal = {Pancreatology},
year = {2021},
issn = {1424-3903},
doi = {https://doi.org/10.1016/j.pan.2021.05.300},
url = {https://www.sciencedirect.com/science/article/pii/S1424390321004701},
author = {Xumin Huang and Zipeng Lu and Kai Zhang and Guangfu Wang and Baobao Cai and Pengfei Wu and Jie Yin and Yi Miao and Kuirong Jiang},
keywords = {CA19-9, Liver enzyme, Pancreatic cancer, Survival predictor, Total bilirubin},
abstract = {Background
Carbohydrate antigen 19–9 (CA19-9) has been reported as the most significant survival predictor of patients with pancreatic ductal adenocarcinoma (PDAC). However, the elevation of CA19-9 could interfere with obstructive jaundice and the predictive value of CA19-9 in PDAC patients with jaundice remains to be analyzed and elucidated to find possible adjustments.
Objective
To evaluate the predictability of preoperative CA19-9 and its adjustments for the overall survival (OS) of PDAC patients by analyzing the relationship between preoperative serum CA19-9 and total bilirubin (TBIL).
Methods
A total of 563 consecutive patients who underwent surgery for primary pancreatic adenocarcinoma in our center between January 2015 and September 2018 were retrospectively reviewed. Clinicopathologic information was collected and preoperative parameters such as CA19-9, CEA, TBIL, γ-GGT, AST, ALT, and ALP were recorded as well as overall survival rates, which began from the date of operation to that of death or the last follow-up. Kaplan-Meier survival curves with log-rank test and Cox regression models were applied using SPSS and the survival and survminer packages in R software.
Results
Using 39/390/1000 as the cut-off values for preoperative serum CA19-9, significant capability of OS stratification was found in the total cohort (p < 0.001, MST = 29.7/19.1/15.2/12.1 months) and patients with TBIL <102.6 μmol/L (p < 0.001, MST = 32.2/19.6/15.0/11.2 months). However, in the subgroup of TBIL≥102.6 μmol/L, this classification method was replaced by the combined scoring of CA19-9/AST and CA19-9/γ-GGT.
Conclusions
As an independent predictor of overall survival of PDAC patients, preoperative serum CA19-9 is defective in survival stratification when TBIL≥102.6 μmol/L but a positive survival prognosis could be achieved with the application of combined preoperative CA19-9/AST and CA19-9/γ-GGT.}
}
@article{SENECLAUZE20186,
title = {9 MLC error detection with EPIBeam for VMAT patient quality assurance},
journal = {Physica Medica},
volume = {56},
pages = {6},
year = {2018},
note = {Abstracts of the 57èmes Journées Scientifiques de la Société Française de Physique Médicale},
issn = {1120-1797},
doi = {https://doi.org/10.1016/j.ejmp.2018.09.022},
url = {https://www.sciencedirect.com/science/article/pii/S1120179718311840},
author = {A. Seneclauze and P. Boissard and T. Baudier and P. Dupuis and C. Malet},
abstract = {Introduction
In recent years, the part of Volumetric Modulated Arc Therapy (VMAT) in external radiotherapy treatments has become more and more important. In fact, this technique allows to deliver the right dose to the tumor region while sparing surrounding organs. To get this complexity, the correct functioning of the Multi-Leaf Collimator (MLC) is particularly critical. Controls are recommended to ensure that the accelerator is able to deliver correctly each treatment plan. Electronic Portal Imaging Devices (EPIDs) may be useful to perform some of patient specific Quality Assurance (QA) [1]. This study proposes to evaluate if, associated with the software EPIBeam (Dosisoft®), they are able to detect MLC errors which could have a significant clinical impact.
Methods
Ten VMAT plans QA for patients treated for prostate cancer using a Synergy (Elekta®) accelerator were performed using two methods. The first method uses ArcCheck (SunNuclear®) and the second uses the EPID IView GT (Elekta®) associated with the EPIBeam software. Gamma analyses were used using global 3%/ 3 mm criterion. In addition, for each plan, MLC errors were introduced. All the leaves of the same bank were shift by 0.5, 1, 2 and 3 mm in the opening direction. The clinical impact of these errors has been evaluated in the MONACO planning software (Elekta®). These modified plans were checked in their turn. A control is considered satisfactory if more than 95% of the points have a gamma index greater than 1.
Results
For MLC error free treatment plans, EPIBeam achieves better gamma analysis results: mean  = 99.8% [Min: 98.7%/ Max  = 100%] versus 99% [Min: 98.1%/ Max: 99.7%] with ArcCheck. For errors less than or equal to 1 mm, all controls were within the tolerances for both modalities. For 2 mm errors, the number of processing plan within tolerances is 1/10 for ArcCheck versus 4/10 with EPIBeam. For 3 mm errors, no plan is within tolerances for the ArcCHECK while a plan passes for EPIbeam. It should be noted that the 2 and 3 mm errors induces significate changes on the dose distribution inside the patient. For example, an increase of the maximum rectal wall dose of 2.9% for 2 mm and 4.9% for 3 mm are observed.
Conclusions
With the 3%/3 mm criterion for gamma index analysis, less MLC errors were detected with EPIBEAM as with ArcCheck. However, by working on the analysis criteria, the error detection capability may be improved. It should be also interesting to combine EPIBeam with other controls such as log file analysis to have better detection efficiency while keeping time savings [2].}
}
@article{REINHARDT2019216,
title = {Lab::Measurement—A portable and extensible framework for controlling lab equipment and conducting measurements},
journal = {Computer Physics Communications},
volume = {234},
pages = {216-222},
year = {2019},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2018.07.024},
url = {https://www.sciencedirect.com/science/article/pii/S0010465518302819},
author = {S. Reinhardt and C. Butschkow and S. Geissler and A. Dirnaichner and F. Olbrich and C.E. Lane and D. Schröer and A.K. Hüttel},
keywords = {Measurement control, GPIB, USB T&M, Ethernet, VXI-11, VISA, SCPI, Perl},
abstract = {Lab::Measurement is a framework for test and measurement automatization using Perl 5. While primarily developed with applications in mesoscopic physics in mind, it is widely adaptable. Internally, a layer model is implemented. Communication protocols such as IEEE 488 [1], USB Test & Measurement [2], or, e.g., VXI-11 [3] are addressed by the connection layer. The wide range of supported connection backends enables unique cross-platform portability. At the instrument layer, objects correspond to equipment connected to the measurement PC (e.g., voltage sources, magnet power supplies, multimeters, etc.). The high-level sweep layer automates the creation of measurement loops, with simultaneous plotting and data logging. An extensive unit testing framework is used to verify functionality even without connected equipment. Lab::Measurement is distributed as free and open source software.
Program summary
Program Title: Lab::Measurement 3.660 Program Files doi: http://dx.doi.org/10.17632/d8rgrdc7tz.1 Program Homepage: https://www.labmeasurement.de Licensing provisions: GNU GPL v23 3The precise license terms are more permissive. Lab::Measurement is distributed under the same licensing conditions as Perl 5 itself, a frequent choice in the Perl ecosystem. This means that it can be used and distributed according to the terms of either the GNU General Public License (version 1 or any later version) or the Artistic License; the choice of license is up to the user. Programming language: Perl 5 Nature of problem: Flexible, lightweight, and operating system independent control of laboratory equipment connected by diverse means such as IEEE 488 [1], USB [2], or VXI-11 [3]. This includes running measurements with nested measurement loops where a data plot is continuously updated, as well as background processes for logging and control. Solution method: Object-oriented layer model based on Moose [4], abstracting the hardware access as well as the command sets of the addressed instruments. A high-level interface allows simple creation of measurement loops, live plotting via GnuPlot [5], and data logging into customizable folder structures. [1] F. M. Hess, D. Penkler, et al., LinuxGPIB. Support package for GPIB (IEEE 488) hardware, containing kernel driver modules and a C user-space library with language bindings. http://linux-gpib.sourceforge.net/ [2] USB Implementers Forum, Inc., Universal Serial Bus Test and Measurement Class Specification (USBTMC), revision 1.0 (2003). http://www.usb.org/developers/docs/devclass_docs/ [3] VXIbus Consortium, VMEbus Extensions for Instrumentation VXIbus TCP/IP Instrument Protocol Specification VXI-11 (1995). http://www.vxibus.org/files/VXI_Specs/VXI-11.zip [4] Moose—Apostmodern object system for Perl 5. http://moose.iinteractive.com [5] E. A. Merritt, et al., Gnuplot. An Interactive Plotting Program. http://www.gnuplot.info/}
}
@article{XU201540,
title = {Knowle: A semantic link network based system for organizing large scale online news events},
journal = {Future Generation Computer Systems},
volume = {43-44},
pages = {40-50},
year = {2015},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2014.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X14000636},
author = {Zheng Xu and Xiao Wei and Xiangfeng Luo and Yunhuai Liu and Lin Mei and Chuanping Hu and Lan Chen},
keywords = {News events, Health domain, Big data, Semantic link network},
abstract = {An explosive growth in the volume, velocity, and variety of the data available on the Internet has been witnessed recently. The data originated from multiple types of sources including mobile devices, sensors, individual archives, social networks, Internet of Things, enterprises, cameras, software logs, health data has led to one of the most challenging research issues of the big data era. In this paper, Knowle—an online news management system upon semantic link network model is introduced. Knowle is a news event centrality data management system. The core elements of Knowle are news events on the Web, which are linked by their semantic relations. Knowle is a hierarchical data system, which has three different layers including the bottom layer (concepts), the middle layer (resources), and the top layer (events). The basic blocks of the Knowle system—news collection, resources representation, semantic relations mining, semantic linking news events are given. Knowle does not require data providers to follow semantic standards such as RDF or OWL, which is a semantics-rich self-organized network. It reflects various semantic relations of concepts, news, and events. Moreover, in the case study, Knowle is used for organizing and mining health news, which shows the potential on forming the basis of designing and developing big data analytics based innovation framework in the health domain.}
}
@article{BARAKAT2019168,
title = {Structural modeling of the Alam EL-Bueib Formation in the jade oil field, Western Desert, Egypt},
journal = {Journal of African Earth Sciences},
volume = {156},
pages = {168-177},
year = {2019},
issn = {1464-343X},
doi = {https://doi.org/10.1016/j.jafrearsci.2019.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S1464343X19301347},
author = {Moataz Kh Barakat and Nader H. El-Gendy and Mohamed A. El-Bastawesy},
keywords = {Seismic interpretation, Structural style, Alam EL-Bueib Formation, Jade oil field, Western Desert of Egypt},
abstract = {The Jade oil field is located in the Matruh Basin in the northern portion of the Western Desert, Egypt. The region is one of the most productive areas for hydrocarbons in Egypt. The main producing reservoir in the area is the Cretaceous Alam-El Bueib Formation. The main goal of the present study was done through construction structural model using Petrel™ Schlumberger software by integrating thirty 2D seismic sections with eight digital well logs. Two-way time and depth structure maps were achieved, besides the geological cross section. Seismic and well log data are used to identify and correlate the four sandstone units of the Alam EL-Bueib Formation, AEB-3A, AEB-3D, AEB-3E and AEB-3G, where only the AEB-3D to 3G sandstones are hydrocarbon-bearing in the Jade oil field. The Alam EL-Bueib Formation in the Jade oil field structure dips slightly to the northwest as part of an anticlinal fold bounded by a northeast-southwest striking reversed fault, the main tectonic element of the study area. This fault was caused by compressional forces during the Syrian arc movement in Late Cretaceous. The area east of the main fault shows antithetic Riedel-shear faults (75° to the main fault) and a corresponding narrow graben to the south of the study area. The hydrocarbon-bearing Alam EL-Bueib Formation of the Jade oil field revealed the main trapping style following the tectonic evolution during Late Cretaceous. It so provides an excellent strategy in the well explored basins in the western Desert of Egypt.}
}
@article{RAZAK20121,
title = {Online Instructional Consultation (OICon) Model for Higher Education Institution (HEIs)},
journal = {Procedia - Social and Behavioral Sciences},
volume = {67},
pages = {1-15},
year = {2012},
note = {3rd INTERNATIONAL CONFERENCE ON E-LEARNING, ICEL 2011},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2012.11.302},
url = {https://www.sciencedirect.com/science/article/pii/S1877042812052883},
author = {Abd Hadi Bin Abdul Razak and Ang Ling Weay and Nur Fadziana Faisal Mohamed},
keywords = {Online instructional consultation, Nonverbal cues, Computer-mediated communication, Higher education institution},
abstract = {Virtual discussion between students and lecturers nowadays become easier and effective by using computer-mediated communication (CMC) tools. The issue on geographical distance is no longer a problem but there are some aspects that need attention, such as the consultation documentation, record log and the ability to see the records of these virtual relationships. Most current virtual communications software is focused on the communication and less on the process of before and after the virtual communication. In order to enhance the virtual communication process, we have designed and developed an Online Instructional Consultation (OICon) model to facilitate student-lecturer consultation for higher education mentor-mentee system in Malaysia. The model consists of 5 interrelated domains that are personalization, consultation processes and task involved, features and multimedia components, consultation content and consultation document. It provides alternative means on delivery of the contents and services as well as provides participants a range of option. By this way, students and lecturer can involve actively through online consultation at remote places, not just between lecturer and students but also on post-session discussion among peers. Point of correspondence includes implementation of CMC tools to facilitate online consultation processes for academic advisory purpose. OICon model was then transformed into a prototype system to verify the model. Based on the evaluation conducted, we found that users are relatively positive towards implementation of multimedia communication tools for consultation in higher education and they agreed that the OICon model is crucial for enhance and promote interactivity for consultation among students and lecturers.}
}
@article{SEDAGHAT2021354,
title = {Climate change and thermo-solar patterns of office buildings with/without window films in extreme hot-arid climate of Kuwait},
journal = {Solar Energy},
volume = {217},
pages = {354-374},
year = {2021},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2021.02.051},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X21001651},
author = {Ahmad Sedaghat and Mohammad Sabati and Fadi Alkhatib and Seyed {Amir Abbas Oloomi} and Farhad Sabri and Hayder Salem and Waqar {Jan Zafar} and Mahdi {Ashtian Malayer}},
keywords = {Cooling load, Energy saving, Green building, Solar window film, Radiation control},
abstract = {This research attempts to develop a systematic method to address climate changes by studying temperature-humidity patterns as the two major meteorological parameters and to moderate these effects using a commercially available window film on interior windows of an office building with double-glazing. A three-floor educational building at Australian College of Kuwait (ACK) was equipped with temperature, humidity and illuminance sensors for two similar size offices, one with 3 M Neutral 20 window films. Total readings of 50,000 entries from each sensor was recorded every 3 min by a home-designed microprocessor-based logging system for three months of June, July, and August 2019. In parallel, the ACK building was simulated in EnergyPlus and DesignBuilder software and calibrated with the experimental measurements. Histograms and a probability density function (PDF) of temperature and humidity are built by a representative rational function as a model of main climate parameters variations. By examining min-mean-max values of data, it is observed that the office with window films has increased indoor humidity and, in most instances, reduced temperature by 2–5 °C compared to double-glazed bare windows which can significantly reduce cooling loads in extreme hot-arid climates. Simulation results are presented on the energy saved and the reduced CO2 footprints by applying the solar window films in the ACK building.}
}
@article{ALTARESCU2019e57,
title = {50. ASSESSMENT OF MICROBIAL CONTAMINATION TO ELIMINATE ARTIFACTS IN SEQUENCING-BASED PGT-A/PGT-SR},
journal = {Reproductive BioMedicine Online},
volume = {39},
pages = {e57},
year = {2019},
issn = {1472-6483},
doi = {https://doi.org/10.1016/j.rbmo.2019.04.103},
url = {https://www.sciencedirect.com/science/article/pii/S1472648319304808},
author = {G. Altarescu and R. Granit and T. Dror and S. Kirshberg and T. Rosen and S. Shaviv and S. Zeligson and R. Beeri and P. Renbaum and D.A. Zeevi},
keywords = {PGT-S, PGT-A, metagenomics, quality contro},
abstract = {Introduction
Low pass high throughput sequencing has become a choice method for obtaining chromosome copy number information from biopsies of pre-implantation embryos. In addition to large copy number variants (CNVs) in human embryo samples, data sets from these routine procedures also provide information regarding the microbial content of each specimen and it's derived sequencing library. The aim of this study was to probe raw sequencing data from clinical PGT-A/PGT-SR cycles for microbial contamination which might explain artifactual or difficult to interpret CNV results in contaminated samples.
Materials & Methods
Between January 2018 and July 2018, PGT-A/PGT-SR was performed on a total of 448 blastomere/blastocyst biopsies at Shaare Zedek Medical Center, Jerusalem, Israel. Library prep and sequencing was performed using the VeriSeq kit (Illumina) according to the manufacturer's protocol. Derivative log ratio (DLR) noise measurements as well as whole chromosomal and segmental CNVs were identified and reported using BlueFuse software. To assess microbial contamination, raw fastq files from each sequencing library were input to PathoScope, a metagenomics suite, for non-human read parsing and microbe identification. Both bacterial and DNA viral taxonomes were fed to PathoScope for non-human read mapping.
Results
An average of 0.18%+/-0.76% of total reads per sequencing data set, mapped to bacterial/viral genomes, and a positive correlation was observed between qualitative copy number noise measurement (DLR) and microbial sequence contamination. Notably, 6 of the most heavily contaminated samples (>0.9% non-human read fraction) presented with over-representation of sequencing reads mapping to the Streptococcus suis genome. In depth post-hoc analysis indicated that sequencing libraries from all 6 samples were prepared from the same index PCR primer which had likely been contaminated by the manufacturer during or shortly after oligo synthesis.
Conclusions
Microbial contamination is a genuine threat to the quality of molecular analyses such as PGT-A and PGT-SR. This study draws attention to the prospect that clinical labs, conducting sequencing-based PGT-A/PGT-SR, consider quality control measures to control for artifactual results that may arise from data sets that are contaminated by genomes of foreign organisms.}
}
@article{TAN20112196,
title = {Assessment of Risk to School Buildings Resulting from Distant Earthquakes},
journal = {Procedia Engineering},
volume = {14},
pages = {2196-2204},
year = {2011},
note = {The Proceedings of the Twelfth East Asia-Pacific Conference on Structural Engineering and Construction},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2011.07.276},
url = {https://www.sciencedirect.com/science/article/pii/S1877705811013531},
author = {K.T. Tan and H. Abdul Razak},
keywords = {Seismic fragility curves, Risk assessmenta},
abstract = {The effects of far distant earthquakes felt in regions with low and moderate seismicity has increased markedly over the last decade. Henceforth fragility curves in this study are to indicate whether buildings are safe to enter or not after an earthquake event occurring more than 300km away. The probability of light, moderate and severe damage states occurring on two and four storey reinforced concrete buildings up to a peak ground acceleration of 0.2g were predicted. The building models were constructed using finite element software based on eight node brick elements with three degrees of freedom at each node. The analysis was carried out using a dynamic response spectrum to calculate the peak inter-storey drift ratios. For the development of fragility curves, the results were collated following a log-normal mean distribution. The results showed that light damage is likely to occur with a probability of over 10% within a 50 year period, while the existing record is unlikely to cause structural damage. The fragility curves were comparable to those for RC moment resisting frames obtained by (Rossetto and Elnashai 2003) at a lower drift limit, with approximately 2% of inter-storey drift ratio.}
}
@article{MYKONIATIS2020450,
title = {A Real-Time Condition Monitoring and Maintenance Management System for Low Voltage Industrial Motors Using Internet-of-Things},
journal = {Procedia Manufacturing},
volume = {42},
pages = {450-456},
year = {2020},
note = {International Conference on Industry 4.0 and Smart Manufacturing (ISM 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.050},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920306090},
author = {Konstantinos Mykoniatis},
keywords = {Industrial Motors, Condition Monitoring, Internet of Things, Temperature, Vibration, RFID, Notifications},
abstract = {Maintenance and sound operating industrial equipment are critical for any manufacturing company. Standardization of the manufacturing infrastructure and establishment of a systematic maintenance program is essential for this process. However, condition monitoring must also be an integral part of a smart manufacturing program that seeks to improve and optimize the operational efficiency of production systems. Absence of this type of data-driven observations and mindset in manufacturing decision making, may result in safety risks, missing critical signs, or occurrence of unexpected repairs that could bring the equipment to a halt. Moreover, routine maintenance may short the actual useful life of some equipment and when scheduled maintenance shutdowns occur too often, they increase downtime and cost. This paper presents the design and development of a real-time condition monitoring system for managing industrial low voltage motors using internet-of-things. The system can record and monitor vibration and temperature conditions of an industrial motor and transmit the data through a wireless network to a data logging center. The current prototype was developed using open source software and hardware and can successfully identify abnormal motor conditions from sensor input values that exceed predefined setpoints. When a motor is approaching an abnormal condition, the system changes state and informs the user through a mobile alert. The motor management system requires the user to perform an RFID enforced inspection. The alarm system remains active until authorized personnel visits the equipment and scans the dedicated RFID batch. Furthermore, the motor management system can be accessed remotely to allow the user to visualize the current condition of the motor. Finally, we present a pilot experiment that was conducted to test the condition monitoring capabilities of the prototype and we discuss future steps in order to further develop the current system to a smart predictive maintenance system capable of detecting and predicting specific motor faults. Click here to enter text.}
}
@article{WALICKI2011821,
title = {Sequence partitioning for process mining with unlabeled event logs},
journal = {Data & Knowledge Engineering},
volume = {70},
number = {10},
pages = {821-841},
year = {2011},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2011.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X11000607},
author = {Michał Walicki and Diogo R. Ferreira},
keywords = {Process mining, Sequential pattern mining, Sequence partitioning, Combinatorics on words},
abstract = {Finding the case id in unlabeled event logs is arguably one of the hardest challenges in process mining research. While this problem has been addressed with greedy approaches, these usually converge to sub-optimal solutions. In this work, we describe an approach to perform complete search over the search space. We formulate the problem as a matter of finding the minimal set of patterns contained in a sequence, where patterns can be interleaved but do not have repeating symbols. This represents a new problem that has not been previously addressed in the literature, with NP-hard variants and conjectured NP-completeness. We solve it in a stepwise manner, by generating and verifying a list of candidate solutions. The techniques, introduced to address various subtasks, can be applied independently for solving more specific problems. The approach has been implemented and applied in a case study with real data from a business process supported in a software application.}
}
@article{CHATTERJEE2014372,
title = {Web software fault prediction under fuzzy environment using MODULO-M multivariate overlapping fuzzy clustering algorithm and newly proposed revised prediction algorithm},
journal = {Applied Soft Computing},
volume = {22},
pages = {372-396},
year = {2014},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2014.03.030},
url = {https://www.sciencedirect.com/science/article/pii/S1568494614001379},
author = {Subhashish Chatterjee and Arunava Roy},
keywords = {Web software reliability, Fuzzy clustering, Fuzzy time series, Server logs, Web errors, Algorithm},
abstract = {In recent years some research works have been carried out on web software error analysis and reliability predictions. In all these works the web environment has been considered as crisp one, which is not a very realistic assumption. Moreover, web error forecasting remains unworthy for the researchers for quite a long time. Furthermore, among various well known forecasting techniques, fuzzy time series based methods are extensively used, though they are suffering from some serious drawbacks, viz., fixed sized intervals, using some fixed membership values (0, 0.5, and 1) and moreover, the defuzzification process only deals with the factor that is to be predicted. Prompted by these facts, the present authors have proposed a novel multivariate fuzzy forecasting algorithm that is able to remove all the aforementioned drawbacks as also can predict the future occurrences of different web failures (considering the web environment as fuzzy) with better predictive accuracy. Also, the complexity analysis of the proposed algorithm is done to unveil its better run time complexity. Moreover, the comparisons with the other existing frequently used forecasting algorithms were performed to demonstrate its better efficiency and predictive accuracy. Additionally, at the very end, the developed algorithm was applied on the real web failure data of http://www.ismdhanbad.ac.in/, the official website of ISM Dhanbad, India, collected from the corresponding HTTP log files.}
}
@article{CANDIDO2019377,
title = {Stratigraphic modeling of a transgressive barrier-lagoon in the Permian of Paraná Basin, southern Brazil},
journal = {Journal of South American Earth Sciences},
volume = {90},
pages = {377-391},
year = {2019},
issn = {0895-9811},
doi = {https://doi.org/10.1016/j.jsames.2018.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S0895981118303158},
author = {Mariane Candido and Joice Cagliari and Francisco Manoel Wohnrath Tognoli and Ernesto Luiz Correa Lavina},
keywords = {Barrier-lagoon system, Coal-bearing succession, Rio Bonito formation, Stratigraphic modeling parameters, Barrier transgression},
abstract = {Barrier-lagoon systems occur in most coastal regions worldwide. The sedimentary record of these environments is of scientific and economic relevance because important coal deposits are associated with them. We show the evolution of an ancient coal-bearing barrier-lagoon system in the subsurface on the southern Paraná Basin, Brazil, from a stratigraphic modeling approach. The main objective is to simulate and understand the origin and evolution of this system controlled by relative sea level changes and sediment supply. Database comprises 75 logged boreholes that provided data about the main coal bed deposited in the back-barrier, as well as 19 cored and logged boreholes distributed along two stratigraphic sections, one parallel (NE-SW) and another perpendicular (NW-SE) to the paleocoastline. The dataset was uploaded in the Diffusive Oriented Normal and Inverse Simulation of Sedimentation (DIONISOS) software to establish basic parameters and simulate different scenarios. Described facies are interpreted as swamp, lagoon, sandy barrier and tide-influenced deposits, developed during a relative sea level rise of 16 m. The coal bearing succession was deposited during the transgressive system tract of fourth-order depositional sequence and included peat deposition in a swamp in the back-barrier, barrier breakup and landward migration. Results are of high relevance for understanding this paleoenvironment evolution and similar systems in intracratonic basins. This environment occurs both in ancient and recent coasts, and the present model supports the understanding of several barrier-lagoon systems with coal formation in the world.}
}
@article{KRAEMER20153152,
title = {Real Time Validation of Online Situation Awareness Questionnaires in Simulated Approach Air Traffic Control},
journal = {Procedia Manufacturing},
volume = {3},
pages = {3152-3159},
year = {2015},
note = {6th International Conference on Applied Human Factors and Ergonomics (AHFE 2015) and the Affiliated Conferences, AHFE 2015},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2015.07.864},
url = {https://www.sciencedirect.com/science/article/pii/S2351978915008653},
author = {Jan Kraemer and Heinz-Martin Süß},
keywords = {Situation Awareness, Online validation, Air traffic control, Online probe techniques},
abstract = {Measuring Situation Awareness (SAw) to evaluate an operator's ability to handle complex dynamic situations and the use of assistance systems have become a standard approach in Human Factors research. Ideally, the operators should be supported by enabling and disabling assistance systems depending on their SAw. On the one hand, if the situation's complexity increases and therefore SAw is likely to be reduced, additional systems may help to prevent overextension by taking control over the specific task or task components. On the other hand, there has been evidence that high levels of automation may reduce the ability to intervene in a timely manner if needed due to mental underload. Adjusting the usage of assistance systems based on the operator's SAw may help to overcome both limitations. However, existing measurement tools for SAw require post-simulation analysis. This way it is not possible to make decisions based on the operator's SAw in parallel with the situation at hand. Software capable of analyzing the current state of a given situation has been developed to allow real time assessment of SAw. This software was designed to measure SAw of approach air traffic controllers in the real time NLR ATM Research Simulator. SAw was measured by presenting questionnaires during three different scenarios. Before an item was presented, the simulation's log files were analyzed to provide the software with the correct answer. This way, validating responses and evaluating SAw immediately was possible. In a first study, 57 non-expert subjects were presented with online probe questionnaires in real time simulated approach air traffic control scenarios. It was found that the software was able to measure SAw in real time. In the future, such systems could be used to make decisions about the need for further assistance while the situation is still happening. This way, operators would only get the necessary amount of assistance without reducing their work to passive monitoring.}
}
@article{AMMENWERTH2014655,
title = {A nationwide computerized patient medication history: Evaluation of the Austrian pilot project “e-Medikation”},
journal = {International Journal of Medical Informatics},
volume = {83},
number = {9},
pages = {655-669},
year = {2014},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2014.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1386505614001051},
author = {Elske Ammenwerth and Georg Duftschmid and Walter Gall and Werner O. Hackl and Alexander Hoerbst and Stefan Janzek-Hawlat and Martina Jeske and Martin Jung and Klemens Woertz and Wolfgang Dorda},
keywords = {Patient safety, Medication reconciliation, Prescriptions, Medical order entry systems, Evaluation studies, Electronic health records, Pharmacy services},
abstract = {Purpose
To manage medication treatment and to assure medication safety, health care professionals need a complete overview of all drugs that have been prescribed or are taken by a patient. In 2009, Austria launched the pilot project “e-Medikation” in three pilot regions. E-Medikation gives access to a patient's nationwide medication list and includes medication safety checks. The objective of this paper is to report on the evaluation results and lessons learnt.
Methods
A formative evaluation study performed between July and December 2011 comprised a standardized survey of participating physicians, pharmacists, and patients, as well as an analysis of the e-Medikation log files.
Results
During the evaluation period, 18,310 prescriptions and 13,797 dispensings were documented, and 22,359 medication safety checks were performed. Overall, 61 physicians, 68 pharmacists, and 553 patients responded to a written survey. The results showed high acceptance of the idea of e-Medikation among pharmacists and patients and mixed acceptance among physicians. The satisfaction with the quality of the software used in the pilot project was low.
Conclusions
The overall aim to increase medication safety seems achievable through e-Medikation, but several limitations of the pilot project need to be solved before a national rollout. Based on the evaluation results and after redesign of e-Medikation, Austria is now planning a nationwide introduction of e-Medikation starting in 2015.}
}
@article{NAGHIPUR2016336,
title = {Twelve-year survival of 2-surface composite resin and amalgam premolar restorations placed by dental students},
journal = {The Journal of Prosthetic Dentistry},
volume = {116},
number = {3},
pages = {336-339},
year = {2016},
issn = {0022-3913},
doi = {https://doi.org/10.1016/j.prosdent.2016.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0022391316001463},
author = {Safa Naghipur and Igor Pesun and Anthony Nowakowski and Aaron Kim},
abstract = {Statement of problem
Composite resin and amalgam restorations are indicated for the restoration of posterior teeth. With increased esthetic demands, long-term clinical studies are required to evaluate the restorative success and reasons for failure of these materials.
Purpose
The purpose of this retrospective study was to determine the survival and reasons for failure of directly placed 2-surface composite resin restorations and directly placed 2-surface amalgam restorations on premolars placed by Canadian dental students.
Material and methods
Using The University of Manitoba’s dental management software and paper charts, all 2-surface composite resin and 2-surface amalgam restorations placed on premolars between January 1, 2002, and May 30, 2014, were included. Short-term failure (within 2 years), long-term failure, and reasons for failure were collected. A Kaplan-Meier survival estimate with an associated P value comparing composite resin to amalgam restoration curves was performed using SPSS statistical software.
Results
Over 12 years, 1695 composite resin and 1125 amalgam 2-surface premolar restorations were placed. Of these restorations, 134 composite resins (7.9%) and 66 amalgams (5.9%) failed. Short-term failures (2 years or less) consisted of 57 composite resin (4%) and 23 amalgam (2.3%) restorations. Long-term failures (greater than 2 years) consisted of 77 composite resin (4.5%) and 43 amalgam (3.8%) restorations. After 12 years of service, the survival probability of composite resin restorations was 86% and that of amalgam restorations 91.5%. The differences in composite resin and amalgam survival curves were also found to be statistically significant (P=.009 for Log-rank test). The main reasons for failure were recurrent caries and fracture of the tooth being restored.
Conclusions
Within the limitations of this study, both composite resin and amalgam restorations had acceptable success rates and similar failure modes. Recurrent caries was still the most common reason for failure.}
}
@article{JANARTHANAN2020209,
title = {Design and development of a sensored glove for home-based rehabilitation},
journal = {Journal of Hand Therapy},
volume = {33},
number = {2},
pages = {209-219},
year = {2020},
issn = {0894-1130},
doi = {https://doi.org/10.1016/j.jht.2020.03.023},
url = {https://www.sciencedirect.com/science/article/pii/S0894113020300715},
author = {Vinesh Janarthanan and Md Assad-Uz-Zaman and Mohammad Habibur Rahman and Erin McGonigle and Inga Wang},
keywords = {Flex sensor, Game, Rehab Glove, Hand therapy},
abstract = {Study Design
Descriptive.
Introduction
Rehabilitation programs that focus on motor recovery of the upper limb require long-term commitment from the clinicians/therapists, require one-to-one caring, and are usually labor-intensive.
Purpose of the Study
To contribute to this area, we have developed a sensored hand glove integrated with a computer game (Flappy Bird) to engage patients playing a game where the subject's single/multiple fingers are involved, representing fine motor skill occupational therapeutic exercises.
Methods
We described the sensored rehab glove, its hardware design, electrical and electronic design and instrumentation, software design, and pilot testing results.
Results
Experimental results supported that the developed rehab glove system can be effectively used to engage a patient playing a computer game (or a mobile phone game) that can record the data (ie, game score, finger flexion/extension angle, time spent in a therapeutic session, etc.) and put it in a format that could be easily read by a therapist or displayed to the therapists/patients in different graph formats.
Conclusions
We introduced a sensored rehab glove for home-based therapy. The exercise training using the glove is repetitious, functional, and easy to follow and comply with.}
}
@article{EPOV20131103,
title = {A geosteering tool for horizontal well logging},
journal = {Russian Geology and Geophysics},
volume = {54},
number = {9},
pages = {1103-1107},
year = {2013},
issn = {1068-7971},
doi = {https://doi.org/10.1016/j.rgg.2013.07.022},
url = {https://www.sciencedirect.com/science/article/pii/S1068797113001570},
author = {M.I. Epov and V.L. Mironov and K.V. Muzalevskiy and I.N. Eltsov and U.P. Salomatov},
keywords = {geosteering, downhole radar, saturated formation, oil-water contact, ultrabroadband nanosecond electromagnetic pulse},
abstract = {A theoretical study has been performed to check the possibility of using ultrabroadband nanosecond electromagnetic pulses as a geosteering tool for horizontal drilling to estimate the distance to the oil-water contact (OWC) in a floating oil accumulation. The voltage of a microwave-bandwidth pulse at the dipole receiver of a downhole radar was modeled for the case of a horizontal borehole near OWC in a formation saturated with oil and water. Numerical solutions to the boundary problem formulated on the basis of the Maxwell equations were obtained with the Microwave Studio software (www.cst.com). The frequency-dependent dielectric constants of the layered saturated formation and the drilling fluid were assumed according to experimentally tested models. The modeling has demonstrated that nanosecond electromagnetic pulses arriving from a layered oil-water contact can in principle be acquired and the distance from the wellbore to the OWC median can be inferred from the respective time delays recorded by a downhole radar. Additionally, the possible dynamic range and accuracy of sensing have been estimated.}
}
@article{LUBKE20171006,
title = {Extracting and Conserving Production Data as Test Cases in Executable Business Process Architectures},
journal = {Procedia Computer Science},
volume = {121},
pages = {1006-1013},
year = {2017},
note = {CENTERIS 2017 - International Conference on ENTERprise Information Systems / ProjMAN 2017 - International Conference on Project MANagement / HCist 2017 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2017},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.11.130},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917323323},
author = {Daniel Lübke},
keywords = {Process Mining, Regression Test, Unit Test, Test Case Extraction},
abstract = {Because executable business processes are an important and critical software asset of organizations because they control and integrate critical information systems. Thus, testing them thoroughly is a very important task within the software development process. However, failures due to implementation defects still occur in production, which in turn means that the development team needs to analyze, fix and repair the failing processes. In order to support the activities of reproducing the problem outside of the production system and to create better test cases for verifying the fixed implementation, we propose to use process mining techniques on the production process event logs to aid the support & development teams. With our approach it is possible to automatically extract a working unit test case with all partner services being mocked that can run in a development environment. Within in this paper we present the extraction algorithm, our implementation, and possible ways to integrate the tool into the support & development process.}
}
@article{OCAYA20111408,
title = {A framework for collaborative remote experimentation for a physical laboratory using a low cost embedded web server},
journal = {Journal of Network and Computer Applications},
volume = {34},
number = {4},
pages = {1408-1415},
year = {2011},
note = {Advanced Topics in Cloud Computing},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2011.03.024},
url = {https://www.sciencedirect.com/science/article/pii/S1084804511000786},
author = {R.O. Ocaya},
keywords = {Embedded, Controller, Collaboration, Experiment, PIC processor},
abstract = {A TCP/IP-based scientific instrument control and data distribution system have been designed and realized as part of a low cost e-learning strategy to allow an institution of learning to offer remote access to a distant, physical laboratory for collaborative experimentation using a highly embedded web server. In principle a single experimental setup in the remote laboratory can be monitored and logged simultaneously by any number of permitted clients through a simple, custom web browser interface. The system features an IEEE 802.3 compliant full-duplex Medium Access Controller (MAC) and Physical Layer Device connected to actuators and measurement transducers for control, data acquisition, distribution and logging over a local high-speed TCP/IP network. The designed system differs in its approach from most contemporary approaches in that it specifies circuit level components required to set up a low cost collaborative remote experimentation server. The system hardware comprises a server made up of a PIC18f2620 and ENC28j60 and client PC terminals interconnected through a network hub. The software comprises the firmware, written in C and Javascript, and a simple client web browser written in the Visual Basic.NET framework. This custom client browser approach circumvents the restrictions that standard client browsers place on direct file system access while optimizing data acquisition and transport while better handling several exception scenarios and implementing an authentication mechanism for secure client access. The system suggests a simplified external ROM-based client authentication solution to the problem of embedded system security that is of growing global concern. The figures of merit of the system, such as the round-trip times and the inter-sample times, are determined. Finally, typical data outputs of two networked PCs in a collaborative monitoring of temperatures in Newton’s law of cooling experiment are presented.}
}
@article{HASSELBERG2021,
title = {The Prognostic Value of Right Atrial Strain Imaging in Patients with Precapillary Pulmonary Hypertension},
journal = {Journal of the American Society of Echocardiography},
year = {2021},
issn = {0894-7317},
doi = {https://doi.org/10.1016/j.echo.2021.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0894731721001413},
author = {Nina E. Hasselberg and Nobuyuki Kagiyama and Yuko Soyama and Masataka Sugahara and Akiko Goda and Keiko Ryo-Koriyama and Omar Batel and Murali Chakinala and Marc A. Simon and John Gorcsan},
keywords = {Right atrial strain, Right atrial reservoir function, Speckle-tracking echocardiography, Pulmonary hypertension, Prognosis, Mortality},
abstract = {Background
Right ventricular (RV) failure in patients with pulmonary hypertension (PH) is associated with unfavorable clinical events and a poor prognosis. Elevation of right atrial (RA) pressure is established as a marker for RV failure. However, the additive prognostic value of RA mechanical function is unclear.
Methods
The authors tested the hypothesis that RA function by strain echocardiography has prognostic usefulness by studying 165 consecutive patients with precapillary PH defined invasively: mean pulmonary artery pressure ≥ 25 mm Hg and pulmonary capillary wedge pressure < 15 mm Hg. Speckle-tracking strain analyses of the right atrium and right ventricle were performed, along with routine measures. Peak RA strain values from six segments using generic speckle-tracking software were averaged to RA peak longitudinal strain, representing RA global reservoir function. The primary end point was all-cause mortality during 5 years of follow-up. RA strain was similarly analyzed in a control group of 16 normal subjects for comparison.
Results
There were 151 patients with PH (mean age, 55 ± 16 years; 73% women; mean World Health Organization functional class, 2.6 ± 0.6), after 14 exclusions (three with atrial septal defects and 11 with left ventricular ejection fractions < 50%). RA strain measurement was feasible in 93% of patients and RV strain measurement in 88%. RA peak longitudinal strain was significantly reduced in patients with PH compared with control subjects, as expected (P < .001). During 5-year follow-up, 73 patients (48%) died. Patients with RA peak strain in the lowest quartile (<25%) had a significant risk for death (P = .006), even after correcting for confounding variables. RA strain was independently associated with survival in multivariate analysis (P = .039) and had additive prognostic value to RV strain (log-rank P = .01) in subgroup analysis.
Conclusions
RA peak longitudinal strain had additive prognostic usefulness to other clinical measures, including RV strain, RA area, and RA pressure, in patients with PH. RA mechanical function by strain imaging has potential for clinical applications in patients with PH.}
}
@article{RAHMAN2018233,
title = {Online molecular image repository and analysis system: A multicenter collaborative open-source infrastructure for molecular imaging research and application},
journal = {Computers in Biology and Medicine},
volume = {96},
pages = {233-240},
year = {2018},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2018.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0010482518300763},
author = {Mahabubur Rahman and Hiroshi Watabe},
keywords = {Molecular imaging, Open source, Image analysis, Electronic laboratory notebook (ELN), Knowledge management},
abstract = {Molecular imaging serves as an important tool for researchers and clinicians to visualize and investigate complex biochemical phenomena using specialized instruments; these instruments are either used individually or in combination with targeted imaging agents to obtain images related to specific diseases with high sensitivity, specificity, and signal-to-noise ratios. However, molecular imaging, which is a multidisciplinary research field, faces several challenges, including the integration of imaging informatics with bioinformatics and medical informatics, requirement of reliable and robust image analysis algorithms, effective quality control of imaging facilities, and those related to individualized disease mapping, data sharing, software architecture, and knowledge management. As a cost-effective and open-source approach to address these challenges related to molecular imaging, we develop a flexible, transparent, and secure infrastructure, named MIRA, which stands for Molecular Imaging Repository and Analysis, primarily using the Python programming language, and a MySQL relational database system deployed on a Linux server. MIRA is designed with a centralized image archiving infrastructure and information database so that a multicenter collaborative informatics platform can be built. The capability of dealing with metadata, image file format normalization, and storing and viewing different types of documents and multimedia files make MIRA considerably flexible. With features like logging, auditing, commenting, sharing, and searching, MIRA is useful as an Electronic Laboratory Notebook for effective knowledge management. In addition, the centralized approach for MIRA facilitates on-the-fly access to all its features remotely through any web browser. Furthermore, the open-source approach provides the opportunity for sustainable continued development. MIRA offers an infrastructure that can be used as cross-boundary collaborative MI research platform for the rapid achievement in cancer diagnosis and therapeutics.}
}
@article{SOUZA20174574,
title = {Sex effects on net protein and energy requirements for growth of Saanen goats},
journal = {Journal of Dairy Science},
volume = {100},
number = {6},
pages = {4574-4586},
year = {2017},
issn = {0022-0302},
doi = {https://doi.org/10.3168/jds.2016-11895},
url = {https://www.sciencedirect.com/science/article/pii/S0022030217302527},
author = {A.P. Souza and N.R. St-Pierre and M.H.R.M. Fernandes and A.K. Almeida and J.A.C. Vargas and K.T. Resende and I.A.M.A. Teixeira},
keywords = {allometry, comparative slaughter, degree of maturity, mature weight, nutrient requirement},
abstract = {ABSTRACT
Requirements for growth in the different sexes remain poorly quantified in goats. The objective of this study was to develop equations for estimating net protein (NPG) and net energy (NEG) for growth in Saanen goats of different sexes from 5 to 45 kg of body weight (BW). A data set from 7 comparative slaughter studies (238 individual records) of Saanen goats was used. Allometric equations were developed to determine body protein and energy contents in the empty BW (EBW) as dependent variables and EBW as the allometric predictor. Parameter estimates were obtained using a linearized (log-transformation) expression of the allometric equations using the MIXED procedure in SAS software (SAS Institute Inc., Cary, NC). The model included the random effect of the study and the fixed effects of sex (intact male, castrated male, and female; n = 94, 73, and 71, respectively), EBW, and their interactions. Net requirements for growth were estimated as the first partial derivative of the allometric equations with respect to EBW. Additionally, net requirements for growth were evaluated based on the degree of maturity. Monte Carlo techniques were used to estimate the uncertainty of the calculated net requirement values. Sex affected allometric relationships for protein and energy in Saanen goats. The allometric equation for protein content in the EBW of intact and castrated males was log10 protein (g) = 2.221 (±0.0224) + 1.015 (±0.0165) × log10 EBW (kg). For females, the relationship was log10 protein (g) = 2.277 (±0.0288) + 0.958 (±0.0218) × log10 EBW (kg). Therefore, NPG for males was greater than for females. The allometric equation for the energy content in the EBW of intact males was log10 energy (kcal) = 2.988 (±0.0323) + 1.240 (±0.0238) × log10 EBW (kg); of castrated males, log10 energy (kcal) = 2.873 (±0.0377) + 1.359 (±0.0283) × log10 EBW (kg); and of females, log10 energy (kcal) = 2.820 (±0.0377) + 1.442 (±0.0281) × log10 EBW (kg). The NEG of castrated males was greater than that of intact males and lower than that of females. Using degree of maturity for estimating NPG and NEG, we could remove the differences between sexes. These results indicate that NPG and NEG differ among sexes in growing Saanen goats, and this difference should be accounted for by feeding systems. Including the degree of maturity as predictor cancels out those differences across sexes in protein and energy requirements.}
}
@article{HORSKY20121202,
title = {Interface design principles for usable decision support: A targeted review of best practices for clinical prescribing interventions},
journal = {Journal of Biomedical Informatics},
volume = {45},
number = {6},
pages = {1202-1216},
year = {2012},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2012.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S1532046412001499},
author = {Jan Horsky and Gordon D. Schiff and Douglas Johnston and Lauren Mercincavage and Douglas Bell and Blackford Middleton},
keywords = {Clinical decision support systems (CDSSs), Electronic health records systems (EHRs), System design and development, Software usability, Human–computer interaction (HCI), Patient safety},
abstract = {Developing effective clinical decision support (CDS) systems for the highly complex and dynamic domain of clinical medicine is a serious challenge for designers. Poor usability is one of the core barriers to adoption and a deterrent to its routine use. We reviewed reports describing system implementation efforts and collected best available design conventions, procedures, practices and lessons learned in order to provide developers a short compendium of design goals and recommended principles. This targeted review is focused on CDS related to medication prescribing. Published reports suggest that important principles include consistency of design concepts across networked systems, use of appropriate visual representation of clinical data, use of controlled terminology, presenting advice at the time and place of decision making and matching the most appropriate CDS interventions to clinical goals. Specificity and contextual relevance can be increased by periodic review of trigger rules, analysis of performance logs and maintenance of accurate allergy, problem and medication lists in health records in order to help avoid excessive alerting. Developers need to adopt design practices that include user-centered, iterative design and common standards based on human–computer interaction (HCI) research methods rooted in ethnography and cognitive science. Suggestions outlined in this report may help clarify the goals of optimal CDS design but larger national initiatives are needed for systematic application of human factors in health information technology (HIT) development. Appropriate design strategies are essential for developing meaningful decision support systems that meet the grand challenges of high-quality healthcare.}
}
@article{SCHLOLAUT201817,
title = {The Varve Interpolation Program 3.0.0 - A unique and easy to use tool for incompletely varved sediments},
journal = {Quaternary Geochronology},
volume = {48},
pages = {17-24},
year = {2018},
issn = {1871-1014},
doi = {https://doi.org/10.1016/j.quageo.2018.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S1871101417301450},
author = {Gordon Schlolaut},
keywords = {Varve, Annual laminations, Interpolation, Lake, Limnology, Chronology},
abstract = {Annually laminated (varved) sediments are particularly interesting for palaeo-environmental reconstruction, since they allow independent, high resolution chronological control. But often varved archives contain intervals in which seasonal layers did not form every year. To construct independent varve chronologies from such archives varve interpolation is required. Here, a significantly advanced Varve Interpolation Program (VIP) is presented, which uses a novel approach to interpolate varves. Through probability distribution fitting the varve thickness frequency distribution of an interval is approximated and used for interpolation. The new approach is superior in many respects to the approach used in the original VIP 1.0.0, but retains all the advantages, such as objectivity, reproducibility and the fact that it does not rely on well varved intervals. The program is designed for common log-normally distributed varve thickness frequency distributions. It is realised in Matlab, but comes with a graphical user interface, which allows easy handling and requires no prior knowledge about Matlab. The program can be run fully automated, but also allows manual control of settings. While the accuracy of the results depends on the characteristics of the data and the degree of varve formation/preservation, the application examples presented here show that under suitable conditions accuracies in the range of 2% can be achieved.}
}
@article{BOUSSADI2013964,
title = {Validity of a clinical decision rule-based alert system for drug dose adjustment in patients with renal failure intended to improve pharmacists’ analysis of medication orders in hospitals},
journal = {International Journal of Medical Informatics},
volume = {82},
number = {10},
pages = {964-972},
year = {2013},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2013.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S1386505613001342},
author = {A. Boussadi and T. Caruba and A. Karras and S. Berdot and P. Degoulet and P. Durieux and B. Sabatier},
keywords = {Computer-assisted, Decision support techniques, Software validation, Medication errors/prevention & control, Pharmaceutical preparations/administration & dosage, Drug dosage calculations, Drug prescriptions},
abstract = {Objective
The main objective of this study was to assess the diagnostic performances of an alert system integrated into the CPOE/EMR system for renally cleared drug dosing control. The generated alerts were compared with the daily routine practice of pharmacists as part of the analysis of medication orders.
Materials and methods
The pharmacists performed their analysis of medication orders as usual and were not aware of the alert system interventions that were not displayed for the purpose of the study neither to the physician nor to the pharmacist but kept with associate recommendations in a log file. A senior pharmacist analyzed the results of medication order analysis with and without the alert system. The unit of analysis was the drug prescription line. The primary study endpoints were the detection of drug dose prescription errors and inter-rater reliability (Kappa coefficient) between the alert system and the pharmacists in the detection of drug dose error.
Results
The alert system fired alerts in 8.41% (421/5006) of cases: 5.65% (283/5006) “exceeds max daily dose” alerts and 2.76% (138/5006) “under-dose” alerts. The alert system and the pharmacists showed a relatively poor concordance: 0.106 (CI 95% [0.068–0.144]). According to the senior pharmacist review, the alert system fired more appropriate alerts than pharmacists, and made fewer errors than pharmacists in analyzing drug dose prescriptions: 143 for the alert system and 261 for the pharmacists. Unlike the alert system, most diagnostic errors made by the pharmacists were ‘false negatives’. The pharmacists were not able to analyze a significant number (2097; 25.42%) of drug prescription lines because understaffing.
Conclusion
This study strongly suggests that an alert system would be complementary to the pharmacists’ activity and contribute to drug prescription safety.}
}
@article{OZCEVIK2020101992,
title = {Energy aware endurance framework for mission critical aerial networks},
journal = {Ad Hoc Networks},
volume = {96},
pages = {101992},
year = {2020},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2019.101992},
url = {https://www.sciencedirect.com/science/article/pii/S1570870518306772},
author = {Yusuf Özçevik and Berk Canberk},
keywords = {Aerial networks, Endurance, Pickup and delivery problem},
abstract = {Recently, aerial networks have been offered as assistant communication infrastructures against some temporary operations especially in mission critical events. However, the lifetime of an aerial component is much less than the estimated operational time because of power supply capacity restrictions. Thus, there should be some replenishment processes between Aerial Base Stations (ABSs) to maintain the physical presence of the topology, i.e. the endurance. During a replenishment process, the relation between energy consumption and flight characteristic of a drone should be carefully taken in to account, because motional energy consumption is much more than the communicational and computational ones. For this purpose, we provide an energy aware endurance framework by modeling an energy consumption digraph model, representing an Aerial Pickup and Delivery Problem (APDP) for flight planning and implementing a novel algorithmic solution. Moreover, replenishment factor (γ) and flight planning cost (δ) parameters are presented to compare the evaluation of the proposed method with a simple replenishment approach. A social event in a stadium is selected as the evaluation environment, and is simulated for three scenarios using Software in the Loop (SITL) simulator with MAVLink Proxy drone controller. According to the evaluation results obtained from simulation logs, our design provides 13% more endurance per ABS (η), an average of 15% less energy consumption w.r.t. δ parameter and 11% less γ value compared to the simple replenishment strategy.}
}
@article{DUNN2018A435,
title = {PO-522 Global proteome and phosphoprotein profiling of meningiomas reveals novel potential therapeutic targets and biomarkers},
journal = {ESMO Open},
volume = {3},
pages = {A435},
year = {2018},
note = {Abstracts of the 25th Biennial Congress of the European Association for Cancer Research, Amsterdam, The Netherlands, 30 June – 3 July 2018},
issn = {2059-7029},
doi = {https://doi.org/10.1136/esmoopen-2018-EACR25.1023},
url = {https://www.sciencedirect.com/science/article/pii/S2059702920321086},
author = {J. Dunn and O. Hanemann and S. Ferluga and E. Lasonder and V. Sharma and D. Hilton and C. Adams and M. Futschik},
abstract = {ABSTRACT
Introduction
Meningiomas are the most common primary intracranial brain tumour arising from meningeal tissue. Despite the majority of them displaying benign features, they can cause mild to severe morbidity. The current main therapeutic approach is complete tumour resection commonly with adjunct radiation therapy. However, tumour location can hamper complete resection and chemotherapies are ineffective. In this study we aim to elucidate the pathogenic signature of these tumours and identify novel molecular targets by deciphering the global proteome and phosphoprotein profile of different grades of meningiomas.
Material and methods
Tumour lysates were collected from grade I, II and III frozen meningioma specimens and three normal healthy human meninges. Phosphoprotein purification was performed using Qiagen® PhosphoProtein Purification Kit. Proteins were separated by SDS-PAGE followed by in-gel tryptic digestion. Extracted peptides were purified and analysed by electrospray ionisation LC-MS/MS. Raw mass spectrometry files were analysed using MaxQuantTM. Expression data were validated by Western blot and immunohistochemistry. In silico functional annotation of expression data was completed using Perseus 1.5.0.31 software suite, Ingenuity Pathway Analysis (IPA®) and DAVID 6.8.
Results and discussions
We have quantified 3888 proteins and 3074 phosphoproteins across all grades of meningioma and normal meninges. Comparative analysis identified 181 proteins and 338 phosphoproteins to be commonly significantly upregulated (log2 fold-change ≥1.5; p<0.05) among all grades vs. normal meninges, and further identified differential expression profiles between meningioma grades. Expression data was validated on samples analysed by MS and on an additional cohort of meningiomas for upregulated proteins including SET, EGFR, SRSF1, CKAP4 and HK2; and phosphoproteins including AKT1, AKT2 and RB1. Gene Ontology revealed commonly upregulated proteins to be enriched in terms including RNA helicase activity, whilst phosphoproteins were enriched in the phosphoprotein associated terms of signal complex assembly, Rho guanyl-nucleotide exchange factor activity and EGFR signalling.
Conclusion
In summary, we performed a comprehensive quantitative proteomic analysis from meningioma tissue of all WHO grades compared to healthy meninges and have identified several potential candidates that may hold therapeutic potential for targeted treatment of these tumours.}
}

@article{NEJADI201721,
title = {History matching and uncertainty quantification of discrete fracture network models in fractured reservoirs},
journal = {Journal of Petroleum Science and Engineering},
volume = {152},
pages = {21-32},
year = {2017},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2017.01.048},
url = {https://www.sciencedirect.com/science/article/pii/S0920410517302309},
author = {Siavash Nejadi and Japan J. Trivedi and Juliana Leung},
keywords = {Discrete fracture network models, Fractured reservoirs, Assisted history matching, Uncertainty quantification, Eigenvector decomposition},
abstract = {Fractured reservoirs are highly heterogeneous and can be characterized by the probability distributions of fracture properties in a discrete fracture network model. The relationship between production performance and the fracture parameters is vastly nonlinear, rendering the process of adjusting model parameters to match both the static geological and dynamic production data challenging. This creates a need for a comprehensive history matching workflow for fractured reservoirs, which considers different local as well as global fracture parameters and leads to multiple equally-probable realizations of the discrete fracture network model parameters for uncertainty quantification. This paper presents an integrated approach for the history matching of fractured reservoirs. This new methodology includes generating multiple discrete fracture models, upscaling them for numerical multiphase flow simulation, and updating the fracture properties using dynamic flow responses such as continuous rate and pressure measurements. Available geological and tectonic information such as well-logs, seismic, and structural maps are incorporated into commercially available DFN modeling and simulation software to infer the probability distributions of relevant fracture parameters (including aperture, length, connectivity, and intensity) and to generate multiple discrete fracture network model realizations. The fracture models are further upscaled into an equivalent continuum dual-porosity model in the software using either analytical approaches or dynamic methods. The upscaled models are subjected to the flow simulation, and their production performances are compared to the true recorded responses. An automated history matching algorithm is implemented to reduce the uncertainties of the fracture properties. Components of vectors representing the principal flow directions and average fracture orientations are obtained by means of eigenvector decomposition of the permeability tensor and are optimized in the algorithm. In addition, both global fracture intensity and the local grid based intensity, which highly affect the fluid flow pattern and rate in different regions of the reservoir, are adjusted. A case study with various fracture sets is presented. The initial realizations were generated by means of Monte Carlo simulations, using the observed fractures at the well locations. Fracture intensity, orientation, and conductivity of different fracture sets were the uncertain parameters in our studies. Using the proposed methodology, parameters of different fracture sets were satisfactorily updated. Implementation of this automated history matching approach resulted in multiple equally probable discrete fracture network models and their upscaled flow simulation models that honor the geological information, and at the same time they match the dynamic production history.}
}
@article{REA2010293,
title = {Performance of chest compressions by laypersons during the Public Access Defibrillation Trial},
journal = {Resuscitation},
volume = {81},
number = {3},
pages = {293-296},
year = {2010},
issn = {0300-9572},
doi = {https://doi.org/10.1016/j.resuscitation.2009.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S030095720900625X},
author = {Thomas D. Rea and Ronald E. Stickney and Alidene Doherty and Paula Lank},
keywords = {Cardiopulmonary resuscitation, Defibrillation, Heart arrest},
abstract = {Background
Increasing evidence indicates that health professionals often may not achieve guideline standards for cardiopulmonary resuscitation (CPR). Little is known about layperson CPR performance.
Methods
The investigation was a retrospective cohort study of cardiac arrest patients treated by layperson CPR and one model of automated external defibrillator (AED) as part of the Public Access Defibrillation Trial (n=26). CPR was measured using software that integrates the event log, ECG signal, and thoracic impedance signal. We assessed chest compression fraction (proportion of attempted resuscitation spent performing chest compressions), prompted compression fraction (proportion of attempted resuscitation spent performing compressions during AED-prompted periods), compression rate, and compressions per minute.
Results
Of the 26 cases, 13 presented with ventricular fibrillation and 13 with nonshockable rhythms. Overall, during the period when patients did not have spontaneous circulation, the median chest compression fraction was 34% (IQR 17–48%), median prompted chest compression fraction was 49% (IQR 30–66%), and the median chest compression rate was 96/min (IQR 90–110/min). Taken together, the median chest compression delivered per minute among all arrests was 29 (IQR 20–42). CPR characteristics differed according to initial rhythm: median chest compression per minute was 20 (IQR 13–29) among ventricular fibrillation and 42 (IQR 28–47) among nonshockable rhythms (p=0.003).
Conclusions
In this study of trained laypersons, CPR varied substantially and often did not achieve guideline parameters. The findings suggest a need to improve CPR training, consider changes to CPR protocols, and/or improve the AED–rescuer interface.}
}
@article{LOPEZDELARA2010305,
title = {Valoración del peso, talla e IMC en niños, adolescentes y adultos jóvenes de la Comunidad Autónoma de Madrid},
journal = {Anales de Pediatría},
volume = {73},
number = {6},
pages = {305-319},
year = {2010},
issn = {1695-4033},
doi = {https://doi.org/10.1016/j.anpedi.2010.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S1695403310002079},
author = {D. {López de Lara} and P. {Santiago Paniagua} and M. {Tapia Ruiz} and M.D. {Rodríguez Mesa} and R. {Gracia Bouthelier} and A. {Carrascosa Lezcano}},
keywords = {Crecimiento, Talla, Peso, IMC, Madrid, Growth, Height, Weight, BMI, Madrid},
abstract = {Resumen
Introducción
Recientemente se han fusionado los datos de 4 estudios de crecimiento realizados en poblaciones de Andalucía, Barcelona, Bilbao y Zaragoza, configurándose el estudio transversal español de crecimiento 2008. Con el objetivo de comprobar si existían o no diferencias entre la población de Madrid y las incluidas en el estudio español y de esta forma evaluar la aplicabilidad de este estándar de referencia también en nuestra comunidad autónoma, hemos realizado un estudio transversal en la Comunidad de Madrid valorando peso, talla e IMC en una muestra de sujetos.
Pacientes y métodos
Hemos analizado una muestra de 6.463 sujetos (3.055 mujeres y 3.408 varones) con edades comprendidas entre 3 y 24 años. Todos estaban sanos, eran de raza caucásica y tenían origen español. Las diferencias entre los datos de Madrid y las poblaciones incluidas en el estudio transversal español 2008 se evaluaron mediante regresión lineal múltiple del logaritmo de la talla, el peso y el IMC ajustado por grupo de edad y por área geográfica de procedencia. Se ha utilizado el procedimiento de comparaciones múltiples de Tukey para los contrastes de los diferentes rangos de edad. El análisis estadístico se realizó mediante el paquete estadístico SAS versión 8.2.
Resultados
Se exponen los valores de la media aritmética y desviación estándar de peso, talla e IMC por grupos de edades para varones y mujeres, así como su distribución percentilada. No encontramos diferencias de relevancia clínica para los valores de peso, talla e IMC de nuestra población y los correspondientes del estudio transversal español 2008. Respecto a otros estudios realizados hace más de 20 años observamos un incremento en los valores de todos los percentiles de peso y talla.
Conclusiones
En resumen, los datos de referencia que ofrece el estudio español de crecimiento 2008 son aplicables en la Comunidad Autónoma de Madrid. Además teniendo en cuenta que la comparación de los estudios transversales recientes realizados en 5 comunidades autónomas (Andalucía, Aragón, Cataluña, Madrid y País Vasco) no mostraron diferencias significativas en las medias de los parámetros antropométricos de peso, talla, IMC, ni en la talla final, podría considerarse a la población española actual como una población homogénea desde el punto de vista antropométrico y extender por tanto la aplicabilidad del estudio español de crecimiento 2008 al resto del país.
Introduction
The data of four growth studies involving populations from Andalusia, Barcelona, Bilbao and Zaragoza have recently been reported as part of the Spanish Cross-sectional Growth Study 2008 (SCGS). With the aim of detecting possible differences between the population of the Madrid region and those of the SCGS, and by so-doing assess the applicability of the conclusions of this reference work to the Madrid region, a cross-sectional study of the latter was undertaken, recording the weight, height and body mass index (BMI).
Subjects and methods
We have analyzed 6463 subjects (3055 females and 3408 males) aged 3–24 years. All subjects were healthy, Caucasian, and of Spanish origin. Differences between the results of the Madrid and SCGS studies were sought by multiple linear regression analysis of the log of the height, weight and BMI data adjusted for age and geographical area. The Tukey multiple comparisons test was used to analyse differences in age ranges. All calculations were performed using SAS v. 8.2 software.
Results
Means and standard deviations are provided for the weight, height and BMI of women and men; distributions by percentiles are also provided. No differences of clinical importance were seen in the weight, height or BMI between the subjects of the Madrid region and those of the SCGS. However, comparisons with the results of other studies performed more than 20 years ago revealed an increase in the weight and height values in all percentiles.
Conclusions
In summary, the official Spanish SCGS reference data for 2008 are similar to those recorded for the Madrid region. Bearing in mind that recent cross-sectional studies undertaken in Andalusia, Aragon, Catalonia, the Basque Country and the present work show no significant differences in mean weights, heights or BMIs in any age group, nor in the final height attained by adults, the Spanish population would appear to be anthropometrically homogeneous. The conclusions of the SCGS may therefore be applicable to the entire country.}
}
@article{WANG2015372,
title = {A GIS-based spatial correlation analysis for ambient air pollution and AECOPD hospitalizations in Jinan, China},
journal = {Respiratory Medicine},
volume = {109},
number = {3},
pages = {372-378},
year = {2015},
issn = {0954-6111},
doi = {https://doi.org/10.1016/j.rmed.2015.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0954611115000098},
author = {Wenqiao Wang and Yangyang Ying and Quanyuan Wu and Haiping Zhang and Dedong Ma and Wei Xiao},
keywords = {AECOPD hospitalization, Ambient air pollution, GIS, Spatial autocorrelation},
abstract = {Summary
Background
Acute exacerbations of COPD (AECOPD) are important events during disease procedure. AECOPD have negative effect on patients' quality of life, symptoms and lung function, and result in high socioeconomic costs. Though previous studies have demonstrated the significant association between outdoor air pollution and AECOPD hospitalizations, little is known about the spatial relationship utilized a spatial analyzing technique- Geographical Information System (GIS).
Objective
Using GIS to investigate the spatial association between ambient air pollution and AECOPD hospitalizations in Jinan City, 2009.
Methods
414 AECOPD hospitalization cases in Jinan, 2009 were enrolled in our analysis. Monthly concentrations of five monitored air pollutants (NO2, SO2, PM10, O3, CO) during January 2009–December 2009 were provided by Environmental Protection Agency of Shandong Province. Each individual was geocoded in ArcGIS10.0 software. The spatial distribution of five pollutants and the temporal-spatial specific air pollutants exposure level for each individual was estimated by ordinary Kriging model. Spatial autocorrelation (Global Moran's I) was employed to explore the spatial association between ambient air pollutants and AECOPD hospitalizations. A generalized linear model (GLM) using a Poisson distribution with log-link function was used to construct a core model.
Results
At residence, concentrations of SO2, PM10, NO2, CO, O3 and AECOPD hospitalization cases showed statistical significant spatially clustered. The Z-score of SO2, PM10, CO, O3, NO2 at residence is 15.88, 13.93, 12.60, 4.02, 2.44 respectively, while at workplace, concentrations of PM10, SO2, O3, CO and AECOPD hospitalization cases showed statistical significant spatially clustered. The Z-score of PM10, SO2, O3, CO at workplace is 11.39, 8.07, 6.10, and 5.08 respectively. After adjusting for potential confounders in the model, only the PM10 concentrations at workplace showed statistical significance, with a 10 μg/m3 increase of PM10 at workplace associated with a 7% (95%CI: [3.3%, 10%]) increase of hospitalizations due to AECOPD.
Conclusions
Ambient air pollution is correlated with AECOPD hospitalizations spatially. A 10 μg/m3 increase of PM10 at workplace was associated with a 7% (95%CI: [3.3%, 10%]) increase of hospitalizations due to AECOPD in Jinan, 2009. As a spatial data processing tool, GIS has novel and great potential on air pollutants exposure assessment and spatial analysis in AECOPD research.}
}
@article{CHO2012446,
title = {Revision of wFMM – A Wideband Fast Multipole Method for the two-dimensional complex Helmholtz equation},
journal = {Computer Physics Communications},
volume = {183},
number = {2},
pages = {446-447},
year = {2012},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2011.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0010465511003420},
author = {Min Hyung Cho and Wei Cai},
keywords = {Wideband Fast Multipole Method, Helmholtz equation, Fast solver},
abstract = {The Wideband Fast Multipole Method for the two-dimensional complex Helmholtz equation program is updated. The new version uses significantly less memory than the original version and uses almost constant memory for all the wavenumbers k when the number of particles is given. The CPU time is also improved slightly. Additionally, the memory leak problems and errors from external variables when it is used in an iterative solver are fixed. The new version wFMM and other useful codes are available from the website http://fastmultipole.org/.
New version program summary
Manuscript Title: Revision of wFMM – A Wideband Fast Multipole Method for the two-dimensional complex Helmholtz equation Authors: Min Hyung Cho and Wei Cai Program Title: 2D-WFMM Journal Reference: Catalogue identifier: AEHI_v2_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEHI_v2_0.html Program obtainable from: CPC Program Library, Queenʼs University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 4669 No. of bytes in distributed program, including test data, etc.: 46 849 Programming language: C Computer: Any Operating system: Any operating system with gcc compiler. For the multi-thread computing, the gcc version 4.4 or newer is recommended RAM: Depending on the number of particles N and wavenumber k Number of processors used: Multi-core processors with shared memory Keywords: Wideband Fast Multipole Method, Helmholtz equation, Fast solver Classification: 4.8, 4.12 External routines/libraries: OpenMP (http://openmp.org/wp/) Subprograms used: None Catalogue identifier of previous version: AEHI_v1_0 Journal reference of previous version: Computer Physics Communications 181 (12) (2010) 2086 Does the new version supersede the previous version?: Yes Nature of problem: Evaluate the interaction between N particles governed by the fundamental solution of 2D complex Helmholtz equation with wide range of wavenumber k. Solution method: Multilevel Fast Multipole Algorithm in a hierarchical quad-tree data structure with a cutoff level, which combines low frequency method and high frequency method. Reasons for the new version: Improve the efficiency of the program including memory usage, repeated use in an iterative solver or other programs, and a minor speed up. Summary of revisions: First, the tree searching method in downward pass is modified to accommodate the higher level of tree structure and save memory. The original version used a queue data structure to visitTable 1Memory and CPU time comparisons between the old and new version wFMM for real k.N=7002 in a [−10,10]2 box, tree depth = 6kCutoffErrorMemory (MB)CPU time (s)NewOldNewOldDirect10−1011.71E−073649673442784010−513.81E−07364961354173500.116.08E−083641024486468600.222.70E−073641072486578400.327.76E−073631084516988200.431.15E−063641013486693100.534.19E−073641046517193100.631.23E−063641105517193100.741.34E−063631018496693100.841.10E−063631047486698000.941.78E−06363103949669800146.00E−07363110651719310254.80E−07363108654719310566.64E−073631748656898001062.63E−0736419899910398005062.59E−084485756114611659800 Table 2Memory and CPU time comparisons between the old and new version wFMM for complex k.N=7002 in a [−10,10]2 box, tree depth = 6kCutoffErrorMemory (MB)CPU time (s)NewOldNewOldDirect10−10+0.1i11.39E−07363104156742156010−5+0.1i11.39E−0736310635675220500.1+0.1i18.11E−0836310706281235200.2+0.1i25.82E−0736310676281289100.3+0.1i23.66E−0636410345668333200.4+0.1i31.63E−0636310426688357700.5+0.1i37.83E−0736310647195357700.6+0.1i34.39E−0636310216173357700.7+0.1i41.98E−0636410657294352800.8+0.1i42.05E−0636310657396352800.9+0.1i42.26E−0636310347597352801+0.1i47.29E−07364106378104357702+0.1i58.66E−07363109995119357705+0.1i66.30E−0736317661881943577010+0.1i63.29E−0736419484794943675050+0.1i62.48E−0844756606653681536260 the tree from the top to bottom level. However, in this new version, the queue method is replaced with a simple recursion algorithm. As a result, the code uses less memory, and the high level tree refinement becomes more efficient. Secondly, memory leaks and external variables that caused problems in the repeated usage in an iterative solver, are fixed. The original version had no problem as stand alone software. However, when it was used repeatedly in other codes, the code did not release the memory after it was called. This causes significant problems for large matrix systems. In the new version, all the memory allocations are tracked and freed as soon as they become unnecessary. Consequently, the new version uses almost constant memory for all wavenumbers k when the number of particles N is fixed. The new version does not stack up the memory in an iterative solver. Also, in the original version, several external variables were not initialized when the code was called multiple times. This resulted in incorrect numerical solutions and a memory allocation error in worst cases. In the new version, external variables are converted to internal ones. The code is also tested by calling it multiple times in other solvers (results will be published soon). As a minor improvement, some of the complex number operations are simplified, and the running time is slightly reduced. Finally, a simple makefile is added in the package for the easy compile. “>>make” will compile one of the test cases described in the readme.rtf file. The new version of wFMM is compared with the original version in Tables 1 and 2 with the same parameters presented in the original paper. Numerical tests are conducted with a machine consisting of two quad-core Intel Xeon 3.00 GHz processors, 32 GB memory, and a gcc version 4.5.1 running on Fedora release 11. All the results in the tables can be obtained by modifying the testrun.c file. Both tables show the significant memory savings and minor CPU time reduction for both real and complex k for the number of particles N=490000. Running time: The CPU time depends on the number of particles N and its distribution, wavenumber k, and number of cores in a machine. The computation time increases as NlogN.}
}
@article{GUSTAFSSON2013286,
title = {A naturalistic study of commuter cyclists in the greater Stockholm area},
journal = {Accident Analysis & Prevention},
volume = {58},
pages = {286-298},
year = {2013},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2012.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0001457512002266},
author = {Louise Gustafsson and Jeffery Archer},
keywords = {Commuter cyclists, Naturalistic study},
abstract = {Few naturalistic studies have been carried out with commuter cyclists to discover the types of problems they encounter on a daily basis. The study presented here has been commissioned by the City of Stockholm municipality and focuses specifically on commuter cyclists in the Greater Stockholm area. The aim of the study was to describe and pinpoint accessibility and safety problems, but also to generate an accessible geographical interface that could serve as a traffic planning tool for cycle network improvement. Statistical surveys in the Stockholm area have shown a rapid growth in the number of cyclists as well as an increase in problems associated with an overburdened cycle infrastructure. Given the heightened emphasis on transport system sustainability, the City of Stockholm is faced with the challenging task of trying to maintain and encourage the upward trend in commuter cycling through a process that involves problem identification, classification, prioritisation and resolution. An innovative methodology involving the use of GPS logging devices and small video cameras was developed and supported with analysis software designed specifically for the purposes of this study. Experienced commuter cyclists were recruited to cycle 17 different major cycle routes to and from the suburbs and inner city area during morning and afternoon peak traffic hours during the main cycle season. Over 500 safety and accessibility/mobility problems were identified and recorded from the data collected from 16 commuter cyclists. The method and representation of data proved successful for strategic traffic planning work at City of Stockholm and has since provided invaluable input for and the development of a new cycle plan for Greater Stockholm. Indirectly, the results of this work have also contributed to longer term safety and environmental targets.}
}
@article{YANG201652,
title = {Performance assessment and tuning for exchange of clinical documents cross healthcare enterprises},
journal = {Computer Standards & Interfaces},
volume = {47},
pages = {52-61},
year = {2016},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2016.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0920548916300162},
author = {Cheng-Yi Yang and Chien-Tsai Liu},
keywords = {Electronic health records (EHRs), System interoperability, Integrating the healthcare enterprise (IHE), Cross-enterprise document sharing (XDS.b), Performance testing, OpenXDS},
abstract = {Background
To integrate electronic health records (EHRs) from diverse document sources across healthcare providers, facilities, or medical institutions, the IHE XDS.b profile can be considered as one of the solutions. In this research, we have developed an EHR/OpenXDS system which adopted the OpenXDS, an open source software that complied with the IHE XDS.b profile, and which achieved the EHR interoperability.
Objective
We conducted performance testing to investigate the performance and limitations of this EHR/OpenXDS system.
Methodology
The performance testing was conducted for three use cases, EHR submission, query, and retrieval, based on the IHE XDS.b profile for EHR sharing. In addition, we also monitored the depletion of hardware resources (including the CPU usage, memory usage, and network usage) during the test cases execution to detect more details of the EHR/OpenXDS system's limitations.
Results
In this EHR/OpenXDS system, the maximum affordable workload of the EHR submissions were 400 EHR submissions per hour, the DSA CPU usage was 20%, memory usage was 1380MB, the network usages were 0.286KB input and 7.58KB output per minute; the DPA CPU usage was 1%, memory usage was 1770MB, the network usages were 7.75KB input and 1.54KB output per minute; the DGA CPU usage was 24%, memory usage was 2130MB, the network usages were 1.3KB input and 0.174KB output per minute. The maximum affordable workload of the EHR queries were 600 EHR queries per hour, the DCA CPU usage was 66%, the memory usage was 1660MB, the network usages were 0.230KB input and 0.251KB output per minute; the DGA CPU usage was 1%, the memory usage was 1890MB, the network usages were 0.273KB input and 0.22KB output per minute. The maximum affordable workload of the EHR retrievals were 2000 EHR retrievals, the DCA CPU usage was 79%, the memory usage was 1730MB, the network usages were 19.55KB input and 1.12KB output per minute; the DPA CPU usage was 3.75%, the memory usage was 2310MB, and the network usages were 0.956KB input and 19.57KB output per minute.
Discussion and conclusion
From the research results, we suggest that future implementers who deployed the EHR/OpenXDS system should consider the following aspects. First, to ensure how many service volumes would be provided in the environment and then to adjust the hardware resources. Second, the IHE XDS.b profile is adopted by the SOAP (Simple Object Access Protocol) web service, it might then move onto the Restful (representational state transfer) web service which is more efficient than the SOAP web service. Third, the concurrency process ability should be added in the OpenXDS source code to improve the hardware usage more efficiently while processing the ITI-42, ITI-18, and ITI-43 transactions. Four, this research suggests that the work should continue on adjusting the memory usage for the modules of the OpenXDS thereby using the memory resource more efficiently, e.g., the memory configuration of the JVM (Java Virtual Machine), Apache Tomcat, and Apache Axis2. Fifth, to consider if the hardware monitoring would be required in the implementing environment. These research results provided some test figures to refer to, and it also gave some tuning suggestions and future works to continue improving the performance of the OpenXDS.}
}
@article{KIRKWOOD2013715,
title = {Surgeon education decreases radiation dose in complex endovascular procedures and improves patient safety},
journal = {Journal of Vascular Surgery},
volume = {58},
number = {3},
pages = {715-721},
year = {2013},
issn = {0741-5214},
doi = {https://doi.org/10.1016/j.jvs.2013.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0741521413007805},
author = {Melissa L. Kirkwood and Gary M. Arbique and Jeffrey B. Guild and Carlos Timaran and Jayer Chung and Jon A. Anderson and R. James Valentine},
abstract = {Objective
Complex endovascular procedures such as fenestrated endovascular aneurysm repair (FEVAR) are associated with higher radiation doses compared with other fluoroscopically guided interventions (FGIs). The purpose of this study was to determine whether surgeon education on radiation dose control can lead to lower reference air kerma (RAK) and peak skin dose (PSD) levels in high-dose procedures.
Methods
Radiation dose and operating factors were recorded for FGI performed in a hybrid room over a 16-month period. Cases exceeding 6 Gy RAK were investigated according to institutional policy. Information obtained from these investigations led to surgeon education focused on reducing patient dose. Points addressed included increasing table height, utilizing collimation and angulation, decreasing magnification modes, and maintaining minimal patient-to-detector distance. Procedural RAK doses and operating factors were compared 8 months pre- (group A) and 8 months post- (group B) educational intervention using analysis of variance with Tukey pairwise comparisons and t-tests. PSD distributions were calculated using custom software employing input data from fluoroscopic machine logs.
Results
Of 447 procedures performed, 300 FGIs had sufficient data to be included in the analysis (54% lower extremity, 11% thoracic endovascular aneurysm repair, 10% cerebral, 8% FEVAR, 7% endovascular aneurysm repair, 5% visceral, and 5% embolization). Twenty-one cases were investigated for exceeding 6 Gy RAK. FEVAR comprised 70% of the investigated cases and had a significantly higher median RAK dose compared with all other FGIs (P < .0001). There was no difference in body mass index between groups A and B; however, increasing body mass index was an indicator for increased RAK. PSD calculations were performed for the 122 procedures that focused on the thorax and abdomen (group A, 80 patients; group B, 42 patients). Surgeon education most strongly affected table height, with an average table height elevation of 10 cm per case after education (P < .0001). The dose index (PSD/RAK ratio) was used to track changes in operating practices, and it decreased from 1.14 to 0.79 after education (P < .0001). These changes resulted in an estimated 16% reduction in PSD. There was a trend toward a decrease in patient to detector distance, and the use of collimation increased from 25% to 40% (P < .001) for all cases; however, these did not result in a decrease in PSD. The number of cases that exceeded 6 Gy RAK did not change after education; however, the proportion of non-FEVAR cases that exceeded 6 Gy decreased from 40% to 20%.
Conclusions
Surgeon education on the appropriate use of technical factors during FGIs improved operating practice, reduced patient radiation dose, and decreased the number of non-FEVAR cases that exceeded 6 Gy. It is essential that vascular surgeons be educated in best operating practices to lower PSD; nonetheless, FEVAR remains a high-dose procedure.}
}
@article{PESLIER20104543,
title = {Crystallization, melt inclusion, and redox history of a Martian meteorite: Olivine-phyric shergottite Larkman Nunatak 06319},
journal = {Geochimica et Cosmochimica Acta},
volume = {74},
number = {15},
pages = {4543-4576},
year = {2010},
issn = {0016-7037},
doi = {https://doi.org/10.1016/j.gca.2010.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S001670371000267X},
author = {A.H. Peslier and D. Hnatyshin and C.D.K. Herd and E.L. Walton and A.D. Brandon and T.J. Lapen and J.T. Shafer},
abstract = {The Larkman Nunatak (LAR) 06319 olivine-phyric shergottite is composed of zoned megacrysts of olivine (Fo76–55 from core to rim), pyroxene (from core to rim En70Fs25Wo5, En50Fs25Wo25, and En45Fs45Wo10), and Cr-rich spinel in a matrix of maskelynite (An52Ab45), pyroxene (En30–40Fs40–55Wo10–25,), olivine (Fo50), Fe–Ti oxides, sulfides, phosphates, Si-rich glass, and baddeleyite. LAR 06319 experienced equilibration shock pressures of 30–35GPa based on the presence of localized shock melts, mechanical deformation of olivine and pyroxene, and complete transformation of plagioclase to maskelynite with no relict birefringence. The various phases and textures of this picritic basalt can be explained by closed system differentiation of a shergottitic melt. Recalculated parent melt compositions obtained from melt inclusions located in the core of the olivine megacrysts (Fo>72) resemble those of other shergottite parent melts and whole-rock compositions, albeit with a lower Ca content. These compositions were used in the MELTS software to reproduce the crystallization sequence. Four types of spinel and two types of ilmenite reflect changes in oxygen fugacity during igneous differentiation. Detailed oxybarometry using olivine-pyroxene-spinel and ilmenite-titanomagnetite assemblages indicates initial crystallization of the megacrysts at 2 log units below the Fayalite-Magnetite-Quartz buffer (FMQ–2), followed by crystallization of the groundmass over a range of FMQ–1 to FMQ+0.3. Variation is nearly continuous throughout the differentiation sequence. LAR 06319 is the first member of the enriched shergottite subgroup whose bulk composition, and that of melt inclusions in its most primitive olivines, approximates that of the parental melt. The study of this picritic basalt indicates that oxidation of more than two log units of FMQ can occur during magmatic fractional crystallization and ascent. Some part of the wide range of oxygen fugacities recorded in shergottites may consequently be due to this process. The relatively reduced conditions at the beginning of the crystallization sequence of LAR 06319 may imply that the enriched shergottite mantle reservoir is slightly more reduced than previously thought. As a result, the total range of Martian mantle oxygen fugacities is probably limited to FMQ−4 to −2. This narrow range could have been generated during the slow crystallization of a magma ocean, a process favored to explain the origin of shergottite mantle reservoirs.}
}
@article{IACCARINO20151,
title = {Prognostic value of MGMT promoter status in non-resectable glioblastoma after adjuvant therapy},
journal = {Clinical Neurology and Neurosurgery},
volume = {132},
pages = {1-8},
year = {2015},
issn = {0303-8467},
doi = {https://doi.org/10.1016/j.clineuro.2015.01.029},
url = {https://www.sciencedirect.com/science/article/pii/S0303846715000591},
author = {Corrado Iaccarino and Elena Orlandi and Francesco Ruggeri and Davide Nicoli and Federica Torricelli and Massimo Maggi and Davide Cerasti and Anna Pisanello and Giuseppe Pedrazzi and Elisabetta Froio and Pellegrino Crafa and Nunzia D'Abbiero and Maria Michiara and Reza Ghadirpour and Franco Servadei},
keywords = {Glioblastoma, Frameless neuronavigation, MGMT, Radiotherapy, Temozolamide},
abstract = {Background
Methylation of MGMT promoter has been identified as a favourable predictive factor of benefit from XRT/TMZ→TMZ. Patients with non-resectable glioblastoma (GBM) generally exhibit a poor prognosis, even after XRT/TMZ. Few data are available concerning the predictive value of MGMT promoter methylation in this population.
Methods
This is an observational retrospective study in patients with malignant brain glioma, treated between June 2008 and October 2011 and followed up until April 2012 at the Neurosurgery-Neurotraumatology Unit of the University Hospital of Parma and at the Neurosurgery Unit of IRCCS “ASMN” of Reggio Emilia, Italy. The medical records of an overall number of 174 patients with a newly diagnosed GBM were reviewed. Volumetry analysis of the lesions was performed on pre- and post-operative neuroimaging by Voxar 3D Ebit AET software. The genetic characterization was performed on paraffin embedded tissue from all resected tumours. Isolation of nucleic acids, bisulfite modification of DNA, methylation-specific PCR and sequencing analyses were done mainly on fresh tissue from biopsy withdrawals. Within 3–4 weeks after either biopsy or surgery, patients were assigned to receive XRT/TMZ→TMZ: treatment included XRT (60Gy in 30 fractions)/TMZ (daily dose of 75mg/m2)/TMZ (150–200mg/m2 per day for 5 days of every 28-day cycle).
Results and discussion
A total of 55 consecutive patients (23 men, 22 women) fulfilled inclusion criteria consisting of age over 18 years, supratentorial histologically proven primary malignant glioma, complete determination of the MGMT methylation status, no prior history of surgery, XRT and/or chemotherapy, adequate clinical and radiological follow-up no lesser than 6 months. Twenty-three patients underwent neuronavigation needle biopsy (B Group) and thirty-two patients were operated with craniotomy for tumour resection (R Group). The pre-operative mean age was similar between groups (61.7±10.7 vs 60.3±11.8 years in the B and R groups respectively; p>0.05). The B groups showed a slightly lower KPS than the R Group (82.1±17.3 vs 90.3±14.1 respectively; p>0.05). The mean pre-operative volume of the tumour did not differ between groups (46.2±40.2cm3 vs 44.1±33.2cm3 in the R Group and B Group respectively; p>0.05). The MGMT promoter was methylated in 12 patients (51.2%) of B Group and in 17 patients (53.1%) of R Group. XRT/TMZ→TMZ was accomplished in 11 patients (47.8%) of B Group and in 24 patients (75%) of R Group; in 24/29 methylated patients (82.8%) and in 11/26 unmethylated patients (42.3%). Survival analysis of methylated vs unmethylated tumours was statistically significant (Log Rank Mantel Cox: 0.019 in B Group and 0.023 in R Group). In B Group the mean overall survival (OS) of methylated patients was 11.4 months (IC 95% 6.5–16.4) vs 4.8 months (95% IC, 2.6–7.0) of unmethylated patients. In R Group the mean OS was 21.7 months (95% IC, 16.9–26.6) for methylated patients and 14.0 months (95% IC, 8.5–19.4) for unmethylated patients. At the multivariate Cox regression analysis conducted on the total population (55 patients), XRT and TMZ were found to be predictive of OS. In the R Group, KPS, XRT and TMZ correlated with a better outcome. In the B Group, XRT and MGMT promoter methylation were favourably related with OS.
Conclusion
MGMT promoter unmethylation has a predominant unfavourable impact on clinical outcomes even in the subpopulation of patients with non-resectable GBM. The unmethylated MGMT promoter status could be considered the main predictor of poor prognosis in biopsied GBM, due to the greater probability of patients not having benefits from adjuvant therapies and not being able to accomplish XRT/TMZ→TMZ. The frameless neuronavigation biopsy technique is safe and effective for predictive evaluation and could help in treatment decision making.}
}
@article{BUSA20122494,
title = {ARVO-CL: The OpenCL version of the ARVO package — An efficient tool for computing the accessible surface area and the excluded volume of proteins via analytical equations},
journal = {Computer Physics Communications},
volume = {183},
number = {11},
pages = {2494-2497},
year = {2012},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2012.04.019},
url = {https://www.sciencedirect.com/science/article/pii/S0010465512001580},
author = {Ján Buša and Shura Hayryan and Ming-Chya Wu and Ján Buša and Chin-Kun Hu},
keywords = {ARVO, Proteins, Solvent accessible area, Excluded volume, Stereographic projection, OpenCL package},
abstract = {Introduction of Graphical Processing Units (GPUs) and computing using GPUs in recent years opened possibilities for simple parallelization of programs. In this update, we present the modernized version of program ARVO [J. Buša, J. Dzurina, E. Hayryan, S. Hayryan, C.-K. Hu, J. Plavka, I. Pokorný, J. Skivánek, M.-C. Wu, Comput. Phys. Comm. 165 (2005) 59]. The whole package has been rewritten in the C language and parallelized using OpenCL. Some new tricks have been added to the algorithm in order to save memory much needed for efficient usage of graphical cards. A new tool called ‘input_structure’ was added for conversion of pdb files into files suitable for work with the C and OpenCL version of ARVO.
New version program summary
Program title: ARVO-CL Catalog identifier: ADUL_v2_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/ADUL_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 11834 No. of bytes in distributed program, including test data, etc.: 182528 Distribution format: tar.gz Programming language: C, OpenCL. Computer: PC Pentium; SPP’2000. Operating system: All OpenCL capable systems. Has the code been vectorized or parallelized?: Parallelized using GPUs. A serial version (non GPU) is also included in the package. Classification: 3. External routines: cl.hpp (http://www.khronos.org/registry/cl/api/1.1/cl.hpp) Catalog identifier of previous version: ADUL_v1_0 Journal reference of previous version: Comput. Phys. Comm. 165(2005)59 Does the new version supercede the previous version?: Yes Nature of problem: Molecular mechanics computations, continuum percolation Solution method: Numerical algorithm based on the analytical formulas, after using the stereographic transformation. Reasons for new version: During the past decade we have published a number of protein structure related algorithms and software packages [1,2,3,4,5,6] which have received considerable attention from researchers and interesting applications of such packages have been found. For example, ARVO [4] has been used to find that ratios of volume V to surface area A, for proteins in Protein Data Bank (PDB) distribute in a narrow range [7]. Such a result is useful for finding native structures of proteins. Therefore, we consider that there is a demand to revise and modernize these tools and to make them more efficient. Here we present the new version of the ARVO package. The original ARVO package was written in the FORTRAN language. One of the reasons for the new version is to rewrite it in C in order to make it more friendly to the young researchers who are not familiar with FORTRAN. Another, more important reason is to use the possibilities for speeding-up provided by modern graphical cards. We also want to eliminate the necessity of re-compiling the program for every molecule. For this purpose, we have added the possibility of using general pdb [8] files as an input. Once compiled, the program can receive any number of input files successively. Also, we found it necessary to go through the algorithm and to make some tricks for avoiding unnecessary memory usage so that the package becomes more efficient. Summary of revisions: 1. New tool. ARVO is designed to calculate the volume and accessible surface area of an arbitrary system of overlapping spheres (representing atoms), the biomolecules being just one albeit important, application. The user provides the coordinates and radii of the spheres as well as the radius of the probe sphere (water molecule for biomolecules). In the old version of ARVO the input of data was organized immediately in the code, which made it necessary to re-compile the program after every change in the input data. In the current version a module called ‘input_structure’ has been created to input the data from an independent external file. The coordinates and radii are stored in the file with extension *.ats (see the directory ‘input’ in the package). Each line in the file corresponds to one sphere (atom) and has the format 24.733−4.992−13.2562.800. The first three numbers are the (x,y,z) coordinates of the atom and the last one is the radius. It is important to remember that the radius of the probe sphere must be already added to this number. In the above example, the value 2.800 is obtained by the formula “sphere radius+probe sphere radius”. In the case of the arbitrary system of spheres the file *.ats is created by the user. In the case of proteins the ‘input_structure’ takes as an input a file in the format compatible with Protein Data Bank (pdb) format [8] and creates a corresponding *.ats file. It also assigns automatically, radii to individual spheres and (optionally) adds to all radii the probe sphere (water molecule) radius. As output, it produces a file containing coordinates of spheres together with radii. This file works automatically as an input for ARVO. Using an external tool allows users to create their own mappings of atoms and radii without the need to re-compile the tool ‘input_structure’ or program ARVO. It is again the user’s responsibility to assign proper radii to each type of atom. One can use any of the published standard sets of radii (see for example, [9,10,11,12,13]). Alternatively, the user can assign his own values for radii immediately in the module input_structure. The radii are assigned in a special file with extension *pds (see the documentation) which consists of lines like this: ATOM CA ALA 2.0 which is read as “the Calpha atom of Alanine has radius 2.0 Angstroms”. Here we provide for testing of the file rashin.pds where the radii are assigned according to [12]. The output file contains only recognized atoms. Atoms that were not recognized (are not part of mapping) are written to a separate log file allowing the user to review and correct the mapping files later. 2. The Language. Implementing the program in C is a natural first step when translating a program into OpenCL. This implementation is rewritten line-by-line from the original FORTRAN version of ARVO. 3. OpenCL implementation. OpenCL [14] is an open standard for parallel programming of heterogeneous systems. Unlike other parallelization technologies like CUDA [15] or ATI Stream [16] which are interconnected with specific hardware (produced by NVIDIA or ATI, respectively), OpenCL is vendor-independent, and programs written in OpenCL can be run on any hardware of companies supporting this standard, including AMD, INTEL, and NVIDIA. Programs written in OpenCL can be run without much change both on CPUs and GPUs. Improvements as compared with the original version: Support for files in the format as created by ‘input_structure’; input of parameters (name of input file) via command line; dynamic size of arrays—removal of the necessity to re-compile the program after any change in size of structures; memory allocation according to the real demands of the application; replacing north pole test by slight reduction of the radius (see below). To compile an OpenCL program, one needs to download and install the appropriate driver and software development kit (SDK). The program itself consists of two parts: a part running on the CPU and a part running on the GPU. The CPU initializes communication between the computer and the GPU, load data, processes and exports results. The GPU does the parallel part of calculation, consisting of the search for neighboring atoms and calculating the contribution of the area and volume of the individual atom to the total area and volume of the molecule. For details of the algorithm, please read Refs. [3,4]. In programming using OpenCL, more attention must be given to memory used than in a classical approach. Memory of the device is usually limited and therefore, some changes to the original algorithm are necessary. First, unlike in the FORTRAN version of the program, no structures containing the list of neighbor atoms are created. The search for the neighbors is done on-line, when the calculation of the contribution from individual atoms is being performed. Table 1Comparison of volumes and surface areas of different proteins obtained by original ARVO and by the new version. Different strategies for dealing with the “north pole” are applied. The first column contains the PDB ID of the protein and the number of atoms. Second column contains the volume of the protein obtained with original ARVO (upper number) and the difference with the new approach (lower number). Third column contains the same as in the second column for the surface area. Fourth column contains the number of rotations of the molecule in original ARVO (upper number) and the number of atoms whose radii have been reduced in the new version (lower number). Fifth column contains the relative errors for the volume (upper number) and the area (lower number).Protein atoms #Volume diffArea diffRotat. reduct.δvolume (%) δarea (%)3rn323,951.1804696858.3226363−1.04⋅10−7957−0.000025−0.0000071−1.02⋅10−73cyt40,875.86739511,455.4748323−3.85⋅10−61600−0.0015750.00141541.24⋅10−42act38,608.2430389054.00735041.28⋅10−416570.0494800.00173321.91⋅10−52brd43,882.73547910,918.20352921−7.84⋅10−71738−0.000344−0.0000971−8.88⋅10−78tln56,698.98888312,496.97806415−1.70⋅10−62455−0.0009660.00045943.67⋅10−61rr8105,841.50219227,983.15977218−6.60⋅10−74108−0.000699−0.0002144−7.65⋅10−71xi51743,445.092001863,139.88270314.42⋅10−715,6960.0077090.00007018.11⋅10−9 The strategy behind the North Pole check and molecule rotation [4, Sec. 4.7] has been changed. If during the north pole test, the north pole of the active sphere lies close to the surface of a neighboring sphere, the radius of such a neighboring sphere is multiplied by 0.9999 instead of rotating the whole molecule. This allows the algorithm to continue normally. Changing the radius of one atom changes the area and the volume of this atom by 0.02% and 0.03%, respectively. As the atom’s contribution to the total area (volume) of the protein is usually only a part of the atom’s total area (volume) and since there are many atoms in the protein itself, the change of total area (volume) is much smaller than 0.02% (0.03%). Testings showed relative errors ranging from 10−4 down to 10−8. An additional benefit of this approach is, that the whole molecule is not rotated and therefore no errors are introduced there which would occur during such rotation. We were even able to find a protein (1S1I having 31,938 atoms), where, after several hundreds of rotations, ARVO was not able to find such a position that the original north pole test could pass. For such proteins the new approach is the only one possible. Some data obtained using the north pole test (with rotation) and those without the north pole test (with radii reduction) are summarized in Table 1. The radius of water molecule was set to 1.4 Å, and Rashin’s set of the van der Waals radii of atoms [12] was used. The first column contains the protein name and the number of atoms. Each cell of the second and the third columns contains two numbers. The upper number is the volume (surface area) obtained using the original ARVO algorithm [4] with conventional north pole test and rotation. The lower number shows the difference coming from using the new approach. The upper number in the fourth column shows the number of rotations when using the original version and the second number is the number of atoms for which the radius has been reduced. The relative error of volume (upper number) and area (lower number) obtained by using radius reduction are shown in the last column. It can be seen clearly that the error is negligible. The disadvantage is that calculations using OpenCL are done with single precision only. This comes from the fact that the OpenCL standard does not support double precision float number operations as a basic part but as an extension only. This means that availability of double precision calculations depends on the device (CPU, GPU) vendor. Switching to double precision calculations downgrades speed performance (calculations in double precision are 8–2 times slower than the same calculations in single precision). Another problem is that after using the double precision switch, all calculations are done with double precision which leads to problems with insufficient memory. This problem can be bypassed by explicitly switching to single precision where possible but this requires careful modification of the whole program source. Since on our GPU (NVIDIA GTX 480) double precision was available, we have decided to use the double precision only for the critical parts of algorithm (s.a. integral calculation), leaving non-critical parts in single precision. This allowed us to speed up the calculation and to obtain acceptable results. Results of the test calculations are given in Table 2. All calculations except for 2brd0 have been performed using water radius 1.4 Å. The first column contains the protein name and the number of atoms. The second column contains computation time in seconds (in FORTRAN/CPU—upper part and OpenCL/GPU—lower part). The third column is a speed-up (time on the CPU divided by time on the GPU). The fourth and fifth columns contain the volume and area calculated in FORTRAN (upper number) and the difference when compared to results obtained by OpenCL (lower number). As one can see, the area and the volume obtained using FORTRAN (in double precision) and the OpenCL implementation (combination of single and double precisions) are practically the same. This is even more clear from the relative error of the OpenCL implementation as shown in the last column (upper number for volume, and lower number for area). As to computational time, FORTRAN (C) implementation is appropriate in the case when the calculation takes approximately less than 2 s. This is because in the case of OpenCL some time–about 0.3–1.5 s on testing configuration–is needed for the initialization of the device and for starting the communication. Speed-up is clearly visible for large proteins when the parallel approach can be exploited, but complexity of protein needs to be taken into account as well. Compare the times for 2brd (water radius 1.4 Å) and 2brd0 (water radius 0 Å). The difference is in the number of neighbors (overlapping spheres). While, for water radius 1.4 Å the number of neighbors is high and using the GPU is efficient, for water radius 0 Å it is better to use CPU. All results were obtained on a test configuration with CPU Intel Core i7 930 processor running at 2.8 GHz and a GPU NVIDIA GeForce GTX 480. Table 2The table contains comparative data on precision and computational times obtained by FORTRAN vs. OpenCL implementations of ARVO. The structure of the columns is similar to Table 1. Note that last protein (1s1i) was not calculated using FORTRAN implementation and comparison presented is between C and OpenCL version. This is because we were not able to find such rotation that north pole test would pass.Protein atoms #Time F95 (s) OpenCLSpeed upVolume diffArea diffδvolume (%) δarea (%)1eca8.236.0126,072.0030697004.1681381.65⋅10−510311.370.0043100.0004987.11⋅10−62ptn13.729.0139,273.2209339227.570716−2.01⋅10−516291.52−0.007906−0.005795−6.28⋅10−52brd15.779.9143,882.73513610,918.203432−1.44⋅10−517381.59−0.0063260.0014711.35⋅10−52brd00.290.9122,412.82580722,546.123881−9.13⋅10−517380.32−0.020471−0.008437−9.17⋅10−48tln23.3213.7456,698.98855012,496.977990−5.34⋅10−624551.70−0.003028−0.008708−4.64⋅10−41rr830.8917.67105,841.50149227,983.1595581.93⋅10−541081.750.020445−0.000802−2.87⋅10−61s1i286.8133.95816,980.348702253,160.674893−1.40⋅10−431,9388.45−1.1407630.0494781.95⋅10−5 At the time of writing, OpenCL allowed the allocation of only 1/4 of the total memory of the devices (CPU, GPU) by one call to malloc. This can be bypassed by four individual calls of memory allocation requesting 1/4 of the total devices’ memory. It is advisable to use a dedicated GPU for the calculations since sharing a GPU for calculations and displaying graphics can lead to unexpected results due to common access to the memory of devices. Restrictions: The program does not account for possible cavities inside the molecule. The current version works in a combination of single and double precisions (see Summary of revisions for details). Running time: Depends on the size of the molecule under consideration. For molecules whose running time was less than 2 s in the old version the performance is likely to decrease. This changes considerably when larger molecules are calculated (in test configuration speed-ups up to 34 were obtained). References:[1]F. Eisenmenger, U.H.E. Hansmann, S. Hayryan, C.-K. Hu, Comput. Phys. Commun. 138 (2001) 192.[2]F. Eisenmenger, U.H.E. Hansmann, S. Hayryan, C.-K. Hu, Comput. Phys. Commun. 174 (2006) 422.[3]S. Hayryan, C.-K. Hu, J. Skrivánek, E. Hayryan, I. Pokorný, J. Comput. Chem. 26 (2005) 334.[4]J. Busa, J. Dzurina, E. Hayryan, S. Hayryan, C.-K. Hu, J. Plavka, I. Pokorný, J. Skrivánek, M.-C. Wu, Comput. Phys. Commun. 165 (2005) 59.[5]J. Busa, S. Hayryan, C.-K. Hu, J. Skrivánek, M.-C. Wu, J. Comput. Chem. 30 (2009) 346.[6]J. Busa, S. Hayryan, C.-K. Hu, J. Skrivánek, M.-C. Wu, Comput. Phys. Commun. 181 (2010) 2116.[7]M.-C. Wu, M.S. Li, W.-J. Ma, M. Kouza, C.-K. Hu, EPL 96 (2011) 68005.[8]http://www.rcsb.org.[9]B. Lee, F.M. Richards, J. Mol. Biol. 55 (1971) 379.[10]F.M. Richards, Annu. Rev. Bipohys. Bioeng. 6 (1977) 151.[11]A. Shrake, J.A. Rupley, J. Mol. Biol. 79 (1973) 351.[12]A.A. Rashin, M. Iofin, B. Honig, Biochemistry 25 (1986) 3619.[13]C. Chotia, Nature 248 (1974) 338.[14]http://www.khronos.org/opencl/.[15]http://www.nvidia.com/object/cuda_home_new.html.[16]http://www.amd.com/stream.}
}
@article{FABER20111414,
title = {Overexpression of Dicer predicts poor survival in colorectal cancer},
journal = {European Journal of Cancer},
volume = {47},
number = {9},
pages = {1414-1419},
year = {2011},
issn = {0959-8049},
doi = {https://doi.org/10.1016/j.ejca.2011.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0959804911000244},
author = {C. Faber and D. Horst and F. Hlubek and T. Kirchner},
keywords = {Colorectal cancer, MicroRNA, Carcinogenesis, Immunohistochemistry},
abstract = {Aims
The RNASE III endonuclease Dicer is one of the key enzymes of microRNA biogenesis. The influence of Dicer-expression in tumour cells on the prognosis of patients with several cancers has been studied with controversial results among different cancer types. To date no one has examined the effect of this biomarker on survival in colorectal carcinoma. Thus, we aimed to study the influence of Dicer expression on survival in colorectal cancer.
Methods
We performed immunohistochemical analyses on formalin-fixed paraffin embedded (FFPE) cancer tissue with an antibody against the Dicer protein. Tumour material from 237 cases was available from patients with colorectal adeonocarcinomas with moderate differentiation (G2) and without evidence of lymph-node (N0) or distant metastasis (M0). Sixty-four cases were in T2 and 173 in T3 stages. A tissue microarray (TMA) was constructed with each tumour in triplicate. Each tumour was assigned to a scoring scale of 0–3, depending on the cytoplasmatic expression of Dicer. A Kaplan–Maier analysis was performed and the log-rank test was used for significance levels by using SPSS v.17 software.
Results
The expression of Dicer in colorectal carcinoma shows a strong association with poor survival (cancer specific survival=CSS, p<0,001) as well as with reduced progression free survival (PFS, p<0,001). In the group with no Dicer staining there was no recorded relapse (0/15) compared with 10/18 relapses in the group with the strongest staining of Dicer.
Conclusions
Strong expression of the central microRNA biosynthesis enzyme Dicer predicts poor prognosis in patients with colorectal cancer. This is in line with investigations on prostate cancer. Contradictory, in breast, lung and ovary cancer Dicer has been shown to be a marker of good prognosis. Further studies on the cellular functions of Dicer need to address these issues.}
}
@article{COUSIN2012ix168,
title = {491P - Body Composition is Linked to Toxicity and Outcome in Patients (PTS) Included in Phase I Trials},
journal = {Annals of Oncology},
volume = {23},
pages = {ix168-ix169},
year = {2012},
note = {Abstract Book of the 37th ESMO Congress Vienna, Austria, 28 September - 2 October 2012},
issn = {0923-7534},
doi = {https://doi.org/10.1016/S0923-7534(20)33053-2},
url = {https://www.sciencedirect.com/science/article/pii/S0923753420330532},
author = {S. Cousin and A. Hollebecque and S. Koscielny and A. Varga and V. Baracos and J. Soria and S. Antoun},
abstract = {ABSTRACT
Background
Phase I clinical trials are dose- and toxicity–finding studies designed to identify the recommended phase II dose of new drugs. Thus, it appears crucial to distinguish toxicity directly related to the tested drug or to pts characteristics. Because previous studies have shown that sarcopenia could be linked to drug toxicity, we have evaluated the effects of body composition parameters on the toxicities incidence among phase I included pts.
Methods
We have carried out a prospective single institution study which included all pts consecutively treated from January 2011 to July 2011 in our phase I unit, irrespective of the tumor type and/or drug nature. Clinical and biological nutritional parameters were recorded. Analysis of the computerized tomography (CT) images realized within 30 days before inclusion was used to evaluate cross-sectional areas (cm2) of visceral adipose tissue (VAT), subcutaneous adipose tissue (SAT) and muscle tissue (MT). The 3rd lumbard vertebra (L3) was chosen as a landmark since L3 and whole-body measurements are linearly related. Images were analyzed using Slice-O-Matic software V4.3 (Tomovision). Tissue cross-sectional areas were computed. Analysis was stratified on sex. Comparisons used Chi-2 test, Kruskal-Wallis test and log rank test.
Results
The study population consisted in 64 men and 49 women. The median age was 57.3 years. Drug interruption due to toxicity (DIT) occurred in 15% of the pts. The only factor associated with DIT was low MT: 117 cm2 versus 138 cm2 (p = 0.039). Pts who did not experiment DIT were more likely to have MT > median (56% versus 15%, p = 0.007). None of the following parameters were found to be associated with DIT: weight change >5%, >10%, Albumin< 35g/l, lacticodeshydrogenase > 250 IU/l, transthyretin <0.21mg/dl, C reactive protein > 6mg/l. Additionally, VAT < median was associated with poorer overall survival (OS) (p = 0.01) and shorter progression free survival (PFS) (p = 0.02).
Conclusion
In a heterogeneous population of pts enrolled in various phase I trials, low muscle tissue was associated with the drug interruption due to toxicity. OS and PFS were linked to the VAT. Body composition is linked to toxicities risk and outcome in phase I pts.
Disclosure
All authors have declared no conflicts of interest.}
}
@article{WOCHER2020102219,
title = {RTM-based dynamic absorption integrals for the retrieval of biochemical vegetation traits},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {93},
pages = {102219},
year = {2020},
issn = {0303-2434},
doi = {https://doi.org/10.1016/j.jag.2020.102219},
url = {https://www.sciencedirect.com/science/article/pii/S0303243420305754},
author = {Matthias Wocher and Katja Berger and Martin Danner and Wolfram Mauser and Tobias Hank},
keywords = {Hyperspectral, Spectroscopy, Carotenoid content, Chlorophyll content, Water content, LUT, PROSAIL RTM},
abstract = {Information about pigment and water contents provides comprehensive insights for evaluating photosynthetic potential and activity of agricultural crops. In this study, we present the concept of using spectral integral ratios (SIR) to retrieve three biochemical traits, namely chlorophyll a and b (Cab), carotenoids (Ccx), and water (Cw) content, simultaneously from hyperspectral measurements in the wavelength range 460−1100 nm. The SIR concept is based on automatic separation of respective absorption features through local peak and intercept analysis between log-transformed reflectance and convex hulls. The algorithm was tested on two synthetically established databases using a physiologically constrained look-up-table (LUT) generated by (i) the leaf optical properties model PROSPECT and (ii) the canopy radiative transfer model (RTM) PROSAIL. LUT constraints were realized based on natural Ccx-Cab relations and green peak locations identified in the leaf optical database ANGERS. Linear regression between obtained SIRs and model parameters resulted in coefficients of determination (R²) of 0.66 (i and ii) for Ccx, R2 = 0.85 (i) and 0.53 (ii) for Cab, and R2 = 0.97 (i) and 0.67 (ii) for Cw, respectively. Using the model established from the PROSPECT LUT, leaf level validation was carried out based on ANGERS data with reasonable results both in terms of goodness of fit and root mean square error (RMSE) (Ccx: R2 = 0.86, RMSE = 2.1 μg cm−2; Cab: R2 = 0.67, RMSE = 12.5 μg cm-2; Cw: R2 = 0.89, RMSE = 0.007 cm). The algorithm was applied to airborne spectrometric HyMap data acquired on 12th July 2003 in Barrax, Spain and to AVIRIS-NG data recorded on 2nd July 2018 southwest of Munich, Germany. Mapping of the SIR results as multiband images (3-segment SIR) allows for intuitive visualization of dominant absorptions with respect to the three considered biochemical variables. Barrax in situ validation using linear regression models derived from PROSAIL LUT showed satisfactory results regarding Cab (R2 = 0.84; RMSE = 9.06 μg cm-2) and canopy water content (CWC, R2 = 0.70; RMSE = 0.05 cm). Retrieved Ccx values were reasonable according to Cab-Ccx-dependence plausibility analysis. Hence, the presented SIR algorithm allows for computationally efficient and RTM supported robust retrievals of the two most important vegetation pigments as well as of water content and is ready to be applied on satellite imaging spectroscopy data available in the near future. The algorithm is publicly available as an interface supported tool within the 'Agricultural Applications' of the EnMAP-Box 3 hyperspectral remote sensing software suite.}
}
@article{IORDACHE2013708,
title = {Experimental investigation on the sound pressure level for a high thermal capacity burner during a running cycle},
journal = {Applied Acoustics},
volume = {74},
number = {5},
pages = {708-717},
year = {2013},
issn = {0003-682X},
doi = {https://doi.org/10.1016/j.apacoust.2012.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0003682X12003246},
author = {Vlad Iordache and Tiberiu Catalina},
keywords = {Sound pressure level, Gas burner, Experimental campaign, Running cycle},
abstract = {In many research or technical expertise studies the maximum noise level of a boiler is associated with the maximum thermal load of the burner. However, this type of air injected burners presents a complex running cycle with different functioning periods, where different parts (engine, fan, flame) of the burner are running separately or in the same time. In this study we are focused on the analysis, by experimental measurements, of the entire functioning cycle of a boiler by pointing out the noise differences and their importance when doing an experimental acoustical investigation. The entire 1/3 octave spectrum of the sound pressure level (SPL) was recorded during a complete running cycle by means of logging software associated to the sound meter. The sound equivalent level was calculated for each period of the running cycle and compared to the norms and with two theoretical prediction models that take into account the heating power and the boiler room volume. It was found that the most accurate data are obtained when the measurements are done in one-third octave. The maximum noise level was established to be not for the maximum thermal load period, but for the ventilation period of the boiler (before gas injection) with 82.1dB at 125Hz. A shut down delay was detected at the end of the cycle with 13s for higher frequencies, due to the vibration of the boiler parts. Two 3D graphical representations point out the most important frequencies characterizing each running state of the burner. Compared to the noise curve (NC85) the minimum differences between the admissible values and the ones produced by the burner were found to be around 5.5dB and therefore no acoustical treatment was needed. The results of the SPL prediction models matched the experimental data only for some of the boiler cycle periods and for only some of the frequencies. This type of detailed experiment investigation of the burner noise highlights the periods of the running cycle and the frequencies where the noise level requires acoustical treatment.}
}
@article{KIRKWOOD2015902,
title = {Deterministic effects after fenestrated endovascular aortic aneurysm repair},
journal = {Journal of Vascular Surgery},
volume = {61},
number = {4},
pages = {902-907},
year = {2015},
issn = {0741-5214},
doi = {https://doi.org/10.1016/j.jvs.2014.11.044},
url = {https://www.sciencedirect.com/science/article/pii/S0741521414021557},
author = {Melissa L. Kirkwood and Gary M. Arbique and Jeffrey B. Guild and Carlos Timaran and Jon A. Anderson and R. James Valentine},
abstract = {Background
Endovascular aortic aneurysm repairs (EVARs) with fenestrated (FEVAR) stent grafts are high radiation dose cases, yet no skin injuries were found retrospectively in our 61 cases with a mean peak skin dose (PSD) of 6.8 Gy. We hypothesize that skin injury is under-reported. This study examined deterministic effects in FEVARs after procedural changes implemented to detect skin injury.
Methods
All FEVARs during a 6-month period with a radiation dose of 5 Gy reference air kerma (RAK; National Council on Radiation Protection and Measurements threshold for substantial radiation dose level [SRDL]) were included. Patients were questioned about skin erythema, epilation, and necrosis, with a physical examination of the back completed daily until discharge and then at 2 and 4 weeks and at 3 and 6 months. PSD distributions were calculated with custom software using input data from fluoroscopic machine logs. These calculations have been validated against Gafchromic (Ashland Inc, Covington, Ky) film measurements. Dose was summed for the subset of patients with multiple procedures ≤6 months of the SRDL event, consistent with the joint commission recommendations.
Results
Twenty-two patients, 21 FEVARs and one embolization, reached an RAK of 5 Gy. The embolization procedure was excluded from review. The average RAK was 7.6 ± 2.0 Gy (range, 5.1-11.4 Gy), with a mean PSD of 4.8 ± 2.0 Gy (range, 2.3-10.4 Gy). Fifty-two percent of patients had multiple endovascular procedures ≤6 months of the SRDL event. The mean RAK for this subset was 10.0 ± 2.9 Gy (range, 5.5-15.1 Gy), with a mean PSD of 6.6 ± 1.9 Gy (range, 3.4-9.4 Gy). One patient died before the first postoperative visit. No radiation skin injuries were found. Putative risk factors for skin injury were evaluated and included smoking (32%), diabetes (14%), cytotoxic drugs (9%), and fair skin type (91%). No other risk factors were present (hyperthyroidism, collagen vascular disorders).
Conclusions
Deterministic skin injuries are uncommon after FEVAR, even at high RAK levels, regardless of cumulative dose effects. This study addresses the concern of missed injuries based on the retrospective clinical examination findings that were published in our previous work. Even with more comprehensive postoperative skin examinations and patient questioning, the fact that no skin injuries were found suggests that radiation-induced skin injuries are multifactorial and not solely dose dependent.}
}
@article{OLGIATI2015S56,
title = {Living longer with disability in China and G20 nations: a quantitative model for the Sustainable Development Goal for health post-2015: a reproducible study},
journal = {The Lancet},
volume = {386},
pages = {S56},
year = {2015},
note = {The Lancet-CAMS Health Summit Abstracts booklet},
issn = {0140-6736},
doi = {https://doi.org/10.1016/S0140-6736(15)00637-6},
url = {https://www.sciencedirect.com/science/article/pii/S0140673615006376},
author = {Stefano Olgiati and Alessandro Danovi},
abstract = {Background
Evidence from the Global Burden of Disease Study 2010 is consistent with the expectation that, in China, where the number of healthy years lost to disability at birth in 2010 was 7·95 years compared with the median of 10·51 (median absolute deviation=1·15) of G20 nations, post-2015 a society living longer will lose an increasing number of years to non-fatal disease and injury. We generated the conditional hypothesis that this expansion of morbidity also has economic effects: in 2012, with a per-person total health expenditure of 480 purchasing power parity in international dollars, China was the third lowest spender among the G20 nations (median 2222, median absolute deviation 1723). We reviewed published work on convergence of health status and analysed data from the Global Health Data Exchange (GHDx) of the Institute for Health Metrics and Evaluation, the University of Washington, Seattle, WA, USA, and from the WHO Health Expenditure Statistics Database.
Methods
Reproducibility can improve evidence-based research, engage stakeholders, and promote transparency among health policy makers. For these reasons, this study uses open-access R software and public repositories of the GHDx (454 410 records) and the WHO Health Expenditure Statistics Database (164 220 records); our statistical inference can be downloaded from GitHub, and reproduced and improved by fellow researchers. We tested the multiple linear model that, within the reference population of G20 nations, per-person total health expenditure is related to health-adjusted life expectancy (HALE) and the number of years lived with disability (YLD).
Findings
In our reference population, the log transformation of per-person total health expenditure is positively related to HALE and YLD (R2 0·61; F-statistic 33; p<0·001; variance inflation factor 1·35), with YLD showing the strongest incremental effect: per-person total health expenditure increases exponentially by 1·47 times (95% CI 1·21–1·79) per additional year of disability versus 1·11 times (95% CI 1·05–1·17) per additional year of health-adjusted life expectancy.
Interpretation
We generated the conditional hypothesis that post-2015 in China, if YLD and HALE converge towards the median of G20 nations, per-person total health expenditure will grow from 480 to 2195 purchasing power parity in international dollars (95% CI 1827–2636). Our results lent support to the conclusion that YLD and HALE are relevant quantities for an evidence-based assessment of the sustainability of the health development agenda in China and G20 nations post-2015.
Funding
University of Bergamo, Bergamo, Italy.}
}
@article{CORDIER2018256,
title = {Humour : quels regards portés par les professionnels et étudiants manipulateurs en électroradiologie médicale ?},
journal = {Annales Médico-psychologiques, revue psychiatrique},
volume = {176},
number = {3},
pages = {256-265},
year = {2018},
issn = {0003-4487},
doi = {https://doi.org/10.1016/j.amp.2017.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0003448717302251},
author = {Étienne Cordier and Sophie Lantheaume and Ladislav Motak and Karine Eve},
keywords = {Étudiant, Formation, Humour, Manipulateur d’électroradiologie médicale, Relation soignant–soigné, Humour, Professional–patient relations, Radiographer, Student, Training},
abstract = {Résumé
Objectifs
Recueillir les avis des professionnels et des étudiants manipulateurs en électroradiologie médicale (MER) au sujet de l’emploi de l’humour dans la relation de soin, et évaluer ses effets positifs et/ou négatifs.
Méthode
Échantillon : 1052 personnes provenant de toute la France ont participé à cette étude, dont 641 soignants MER, quelle que soit leur modalité d’exercice (radiologie conventionnelle, radiologie interventionnelle, scanographie, remnographie, médecine nucléaire et/ou radiothérapie) et 411 étudiants de première, deuxième ou troisième année. Outils. Dans une phase de recueil de données quantitatives, tant les professionnels que les étudiants avaient à répondre à quatre échelles de type Likert relatives aux concepts de l’humour et ses fonctions. Cette première phase a été suivie, uniquement auprès des professionnels, par un recueil de données qualitatives visant à approfondir les concepts abordés au préalable. Le verbatim des participants a été recueilli et analysé suite à leurs réponses aux questions ouvertes.
Résultats
Alors que les étudiants évoquent davantage les bienfaits de l’humour au travers du patient (relation de confiance, technique de distraction), les MER en exercice rapportent principalement des bénéfices du point de vue de leurs collègues ou de leur propre personne (bonne ambiance, mécanisme de défense face au stress). Si les fonctions positives prédominent dans les deux groupes, les soignants mettent en garde sur l’importance de l’aspect contextuel de l’humour et son risque à porter atteinte au patient et à sa dignité.
Conclusion
En posant l’humour comme une valeur personnelle et professionnelle aux yeux des soignants et des étudiants, nous élargissons notre réflexion sur une éventuelle formation à l’humour à mettre en place dans le cadre professionnel ou scolaire pour éviter tout risque d’un usage nocif.
Objectives
Although many scientific studies have focused on humour in the care relationship, we thought it would be a good idea to interview radiographers, whose role illustrates the dichotomy between humanity and sophisticated healthcare techniques. We intended to highlight the need to introduce humour – an essential part of communication with the patient – in the technical environment of this profession. We collected data on how practitioners perceive humour in their day-to-day practice, and then compared the results with the abstract opinions the radiographer students have on using humour in their future practice. This study thus reports on the comparison between the students’ expectations and the reality of radiographers in the field, dealing with the extent to which it is relevant to introduce a more personal approach in healthcare through humour and laugher.
Methods
Sample. One thousand and fifty-two people participated in this study, among whom 641 radiographer practitioners (161 men and 480 women) and 411 students (89 men and 322 women). Concerning inclusion criteria for the ‘practitioners’ group, all radiographer graduates from mainland France and the overseas French départements working in conventional radiology, interventional radiology, CT-scan, magnetic resonance imaging, nuclear medicine and/or radiation therapy units, were invited to participate. For the ‘students’ group, the undergraduate first-, second- or third-year students were solicited. No exclusion criteria were specified. Material. This study included an initial phase of quantitative research based on a questionnaire made up of closed-ended questions, conducted from November 12, 2015 to January 31, 2016; and a second qualitative phase based on a questionnaire made up of open-ended questions, conducted from November 27, 2015 to February 17, 2016. The questionnaires were developed using the review of the available literature, as well as the findings of a great number of studies on the place of humour in the care relationship. During the first phase, both practitioners and students had to answer using four Likert-type scales related to the positive impact of humour, the negative impact of humour, the perception of humour as a value, and the acceptance of a training module on using humour. The statistical analysis of the quantitative data was performed using the Statistical Package for the Social Sciences (SPSS.23.0) software, as well as parametric tests (MANOVA, ANOVA, Student's t test), after logarithmic transformations (Log10) of raw data, if necessary. Only the practitioners were involved in the second phase because a minimum professional experience was required in order for the expected answers to be relevant. The respondents’ answers to the open-ended questions were recorded and analysed.
Results
The students were more prone than the practitioners to consider humour as a way of building trust with the patient, as a technique to distract the patient during intimate or invasive care, and as a way of alleviating nervousness inherent in the beneficiary/carer hierarchy. The practitioners more than students tended to consider humour as a way of creating a relaxed ambiance between colleagues and health practitioners, and as a defence mechanism against occupational stress. While students more often mentioned the benefit of humour in relation to the patient, practitioners essentially mentioned the benefits of humour in their relationships to their colleagues or for themselves. When it came to considering humour as inadequate, undignified or as reflecting a denial of the patient's sufferings, this humour practice was judged less negative by the practitioners than by the students. Nonetheless, the practitioners warned about the significant context-dependency of humour, as well as against the risk of affecting the patient. Overall, the opinions of both groups on humour were rather positive. Indeed, they considered it to be of significant value, whether in the private or occupational context. While both groups’ assessments of humour as an important personal value were similar, the practitioners paid more attention than students to humour in the occupational context. As the attitudes towards humour were predominantly positive, both groups claimed they were interested in the introduction of a vocational training programme to improve the therapeutic management of patients, discuss their experiences, overcome their shyness or merely satisfy their curiosity. The students were more inclined than the practitioners to consider an initial training module as more relevant.
Conclusions
The findings of this study have proven the need to include a human factor, namely humour, in the technical environment of the radiography profession. After concluding that humour is of personal and professional value for practitioners and students, we are now considering the possibility of establishing a training module on humour within the initial or ongoing education framework to prevent it being used in a harmful way.}
}
@article{WAGNER2014e41,
title = {Assessment of Resident Operative Performance Using a Real-Time Mobile Web System: Preparing for the Milestone Age},
journal = {Journal of Surgical Education},
volume = {71},
number = {6},
pages = {e41-e46},
year = {2014},
issn = {1931-7204},
doi = {https://doi.org/10.1016/j.jsurg.2014.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S193172041400186X},
author = {Justin P. Wagner and David C. Chen and Timothy R. Donahue and Chi Quach and O. {Joe Hines} and Jonathan R. Hiatt and Areti Tillou},
keywords = {surgical skills, feedback, evaluations, residents, operative performance, Milestones, Patient Care, Interpersonal and Communication Skills, Practice-Based Learning and Improvement, Professionalism},
abstract = {Objective
To satisfy trainees’ operative competency requirements while improving feedback validity and timeliness using a mobile Web-based platform.
Design
The Southern Illinois University Operative Performance Rating Scale (OPRS) was embedded into a website formatted for mobile devices. From March 2013 to February 2014, faculty members were instructed to complete the OPRS form while providing verbal feedback to the operating resident at the conclusion of each procedure. Submitted data were compiled automatically within a secure Web-based spreadsheet. Conventional end-of-rotation performance (CERP) evaluations filed 2006 to 2013 and OPRS performance scores were compared by year of training using serial and independent-samples t tests. The mean CERP scores and OPRS overall resident operative performance scores were directly compared using a linear regression model. OPRS mobile site analytics were reviewed using a Web-based reporting program.
Setting
Large university-based general surgery residency program.
Participants
General Surgery faculty used the mobile Web OPRS system to rate resident performance. Residents and the program director reviewed evaluations semiannually.
Results
Over the study period, 18 faculty members and 37 residents logged 176 operations using the mobile OPRS system. There were 334 total OPRS website visits. Median time to complete an evaluation was 45 minutes from the end of the operation, and faculty spent an average of 134 seconds on the site to enter 1 assessment. In the 38,506 CERP evaluations reviewed, mean performance scores showed a positive linear trend of 2% change per year of training (p = 0.001). OPRS overall resident operative performance scores showed a significant linear (p = 0.001), quadratic (p = 0.001), and cubic (p = 0.003) trend of change per year of clinical training, reflecting the resident operative experience in our training program. Differences between postgraduate year-1 and postgraduate year-5 overall performance scores were greater with the OPRS (mean = 0.96, CI: 0.55-1.38) than with CERP measures (mean = 0.37, CI: 0.34-0.41). Additionally, there were consistent increases in each of the OPRS subcategories.
Conclusions
In contrast to CERPs, the OPRS fully satisfies the Accreditation Council for Graduate Medical Education and American Board of Surgery operative assessment requirements. The mobile Web platform provides a convenient interface, broad accessibility, automatic data compilation, and compatibility with common database and statistical software. Our mobile OPRS system encourages candid feedback dialog and generates a comprehensive review of individual and group-wide operative proficiency in real time.}
}
@article{RIGA201562,
title = {Blood donors – Serious adverse reactions (SAR) 2010–2014 EFS Châteauroux, France},
journal = {Transfusion Clinique et Biologique},
volume = {22},
number = {2},
pages = {62-65},
year = {2015},
issn = {1246-7820},
doi = {https://doi.org/10.1016/j.tracli.2015.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S1246782015000270},
author = {A. Riga and T. Sapey and M. Bacanu and J.-Y. Py and F. Dehaut},
keywords = {Blood donors, Vasovagal syncope, Adverse reactions, Donation time course, Donor vigilance, Donneurs de sang, Malaise vagal, Mauvaise réaction, Moment du don, Vigilance donneur},
abstract = {Background
In 2013, the national French incidence of serious adverse reactions (SAR) was 155.7 per 100,000 donations and 82% of SAR were grade 2 (French classification of SAR related to blood donors)
Aims
The purpose of our study was to describe the profile of blood donator candidate which had a SAR in our center.
Methods
The study contains all the SAR superior to grade 1 occurred on the site EFS Châteauroux (site and mobile blood collection) from January 2010 to October 31, 2014. We analyzed 37 parameters from the e-fit files (e-site French blood vigilance) and In-log software.
Results
We identified 82 SAR for 72,553 blood donations (incidence: 113.02 SAR per 100,000 donations). Forty-one men and 41 women, middle age 39 years (18–66). Average height: 1.68m (1.49–1.85); average weight: 68kg (50–98); body mass index (kg/m2): 24,13(18.6–31.9). All donors were Caucasian and 30% unemployed. We found 74 vasovagal syncope (VVS), 5 hematomas, 2 arterial injuries and an adverse reaction to citrate. In 90%, the SAR was immediate and of grade 2 in 85% of cases. Thirty-seven percent of SAR were first donation in connection with whole blood in 87% of cases. Regarding the seniority of donors, the number of average donations (whole blood, plasma, platelets) was 16.5. An SAR determined the stop of blood donation in 65% of cases with nearly 80% stoppage if it was a first donation. Seventy-three percent of SAR as a VVS took place during blood collection or within 5minutes following the end of the donation. Sixty-one percent were men. Forty-four percent of cases were a first donation and 83% occurred in mobile blood collection. Average age was 36 years. The result was a permanent stop of all type of donations in 76% of cases. Twenty-seven percent of SAR as a VVS took place beyond 5minutes after the end of the donation. Seventy-five percent were women. Thirty percent of cases were a first donation and 95% of SAR occurred in mobile blood collection. Average age was 42 years. The result was a permanent stop of all type of donations in 40% of cases.
Conclusions
When the SAR as a VVS occurs during or within 5minutes following the end of the donation, it leads to a permanent stop of any type of donation in 76% of cases.
Résumé
Introduction
En 2013, l’incidence nationale était de 155,7 EIGD pour 100 000 dons dont 82 % étaient de grade 2. Le but de notre étude était de décrire le profil du candidat au don du sang ayant eu un EIGD dans notre centre.
Matériel et méthode
L’étude contient tous les EIGD supérieurs au grade 1 survenus sur le site de Châteauroux (collecte mobile, collecte en site fixe) de janvier 2010 au 31 octobre 2014. Nous avons analysé 37 paramètres à partir des fiches e-fit et du logiciel In-log.
Résultats
Nous avons recensés 82 EIGD pour 72 553 dons (incidence de 113,02 EIGD pour 100 000 dons). Quarante et un hommes et 41 femmes, d’âge moyen : 39ans (18–66). Nous avons trouvé 74 malaises vagaux, 5 hématomes, 2 blessures artérielles et une réaction au citrate. Dans 90 %, l’EIGD était immédiat et de grade 2 dans 85 % des cas. Dans 37 % des cas, il s’agissait d’un premier don, en rapport avec du sang total dans 87 %. L’apparition d’un EIGD entraînait un arrêt définitif de tout don dans 65 % des cas avec près de 80 % s’il s’agissait d’un premier don. Soixante-treize pour cent des EIGD sous la forme d’un malaise vagal avaient lieu pendant le prélèvement ou dans les 5minutes qui suivaient son arrêt. Soixante et un pour cent étaient des hommes. Dans 44 %, il s’agissait d’un premier don. Cela entraînait un arrêt définitif de tout don dans 76 %. Vingt-sept pour cent des EIGD sous la forme d’un malaise vagal avaient lieu au-delà de 5 min après la fin du prélèvement. Soixante-quinze pour cent étaient des femmes. Dans 30 %, il s’agissait d’un premier don. Cela entraînait un arrêt définitif de tout don dans 30 %.
Conclusion
Lorsque l’EIGD sous la forme d’un malaise vagal se produit pendant le prélèvement ou dans les 5minutes qui suivent son arrêt, il entraîne un arrêt définitif de tout don dans 76 % des cas.}
}
@article{ANDERSON2019S99,
title = {2017 Research Grant: NF1 and KEAP1 mutations are correlated to increased rates of local failure after radiation therapy for spine metastases from non-small cell lung cancer},
journal = {The Spine Journal},
volume = {19},
number = {9, Supplement },
pages = {S99-S100},
year = {2019},
issn = {1529-9430},
doi = {https://doi.org/10.1016/j.spinee.2019.05.590},
url = {https://www.sciencedirect.com/science/article/pii/S1529943019307879},
author = {Erik S. Anderson and Natalie Lockney and Dennis Lockney and Robert Samstein and Daniel Higginson and Mark H. Bilsky and Ilya Laufer and Yoshiya Yamada and Adam M. Schmitt},
abstract = {PURPOSE/OBJECTIVES
The spine is a common site of metastasis from non-small cell lung cancer (NSCLC), a disease hallmarked by heterogeneity in molecular alterations. Traditional spine radiation treatment (RT) with conventionally fractionated external beam radiation therapy (cEBRT) is associated with high local failure (LF) rates in patients surviving more than 12 months. Treatment with advanced techniques including stereotactic body radiation therapy (SBRT) significantly reduces LF. We hypothesized that specific genomic mutations in NSCLC metastases impart increased risk of LF, and that treatment with SBRT would prevent LF in metastases harboring such alterations.
STUDY DESIGN/SETTING
Retrospective review, single institution.
PATIENT SAMPLE
NSCLC patients with tumor molecular profiling data who underwent spine RT with follow-up at a single institution.
OUTCOME MEASURES
Local control of irradiated lesions (radiographic), overall survival.
METHODS
Under an IRB-approved retrospective research protocol, we examined the records of 189 pathologically-confirmed lung adenocarcinoma patients who had undergone radiation therapy for spine metastases at our institution and had tumor molecular profiling via the FDA-authorized MSK-IMPACT tumor-profiling multiplex panel platform. Mutation profiles, radiation treatment details and patient follow-up were recorded. Local control was defined radiographically using spine MRI when available and CT or PET-CT in a minority of cases. cEBRT was defined as a biologically effective dose (BED) <45Gy in 2Gy equivalents (BED2Gy[α/β=10] <45Gy), while SBRT was defined as treatment in ≤5 fractions with BED2Gy >45Gy. Cumulative incidence of LF was calculated using a competing risk analysis with R Statistical Software and examined for significance using Gray's test, stratified by alteration in canonical NSCLC driver genes. Survival after RT was calculated by the Kaplan-Meier method using GraphPad Prism software and examined for significant differences using the Log-rank (Mantel-Cox) test. Finally, radiation sensitivity was tested by clonogenic survival assay in isogenic NSCLC cell lines with CRISPR-mediated disruption of candidate genes.
RESULTS
We identified 189 NSCLC patients with spine metastases who underwent RT (cEBRT = 81, SBRT = 108), had available tumor profiling by MSK-IMPACT, and had at least one follow-up imaging study. There was an overall 12-month LF rate of 13.9% for all patients. When stratified by RT type, LF at 12 months after cEBRT was 24.4% vs 5.7% after SBRT (p<0.001). Median survival after RT was not significantly different between cEBRT and SBRT groups (13.0 vs 19.9 months, p=0.06). We reviewed molecular alteration rates in 20 canonical NSCLC driver genes and found that 14 had alteration rates > 5%. Mutations in NF1 were associated with increased LF after RT (HR 3.63; 95% CI 1.58-8.33; p=0.002), and mutations in KEAP1 trended toward correlation with LF (HR 1.94; 95% CI 0.92-4.10; p=0.08). When stratified by RT type, mutations in NF1 and KEAP1 were significantly associated with LF after cEBRT (NF1: HR 3.72, 95% CI 1.58-8.73, p=0.003; KEAP1: HR 2.29, 95% CI 1.00-5.26, p=0.05), but not after SBRT (NF1: HR 2.52, 95% CI 0.31-20.7, p=0.39; KEAP1: HR 1.31, 95% CI 0.26-6.56, p=0.74). CRISPR-mediated disruption of NF1 with two independent guide sequences resulted in reduced radiation sensitivity in two NSCLC cell lines (H661 and H1299), as assayed by clonogenic survival assay.
CONCLUSIONS
LF after RT for spine metastases from lung adenocarcinoma is common, with 12-month incidence of LF=13.9% in this cohort. Progression at 12 months is more common after cEBRT as compared to SBRT (24.4% vs 5.7%), suggesting that a large subset of patients may benefit from upfront SBRT. Alterations in NF1 and KEAP1 are associated with increased risk of local progression after cEBRT, while failure after SBRT remains low even in tumors harboring these mutations. Importantly, these findings are supported by our novel preclinical data that NF1 loss of function reduces radiation sensitivity in NSCLC cells. Furthermore, it confirms preclinical results from independent laboratories showing a role for KEAP1 in radiation sensitivity. We propose that upfront SBRT for NSCLC patients harboring mutations in NF1 and KEAP1 is appropriate for prospective study. Finally, we suggest that future radiation dose escalation trials contain pre-specified stratification on molecular alteration patterns.
FDA DEVICE/DRUG STATUS OF ALL MEDICAL DEVICES/MEDICATIONS DISCUSSED
MSK-IMPACT Tumor-Profiling Multiplex Panel. FDA-authorized.}
}
@article{ALTHAGAFI2017662,
title = {Spectral characterization, CT-DNA binding, DFT/B3LYP, molecular docking and antitumor studies for new nano-sized VO(II)-hydrazonoyl complexes},
journal = {Journal of Molecular Liquids},
volume = {242},
pages = {662-677},
year = {2017},
issn = {0167-7322},
doi = {https://doi.org/10.1016/j.molliq.2017.06.113},
url = {https://www.sciencedirect.com/science/article/pii/S0167732217317300},
author = {Ismail Althagafi and Marwa G. Elghalban and Fawaz Saad and Jabir H. Al-Fahemi and Nashwa M. El-Metwaly and Samir Bondock and Layla Almazroai and Kamel A. Saleh and Gamil A. Al-Hazmi},
keywords = {VO(II) complexes, Docking, DFT, Antitumor and DNA binding},
abstract = {New hydrazonoyl chloride derivatives were prepared and characterized. Their VO(II) complexes were isolated after adjusting reaction medium to slightly basic by 0.5g acetate. Neutral tridentate mode is a general coordination feature of ligands towards binuclear VO(II) atoms. Square pyramidal is a proposed configuration for all complexes. XRD and SEM analysis offer an assertion about the presence of all compounds excellently in nano-scale for well crystals. TGA and DTA analysis reflect low thermal stability for crystals occluded with water molecules as well as kinetic parameters clarify rigidity of coordination spheres. Molecular modeling using Gaussian09 software was implemented through DFT/B3LYP method by definite base sets. The best configurations were extracted and essential parameters were calculated over visualized structures. Log P was calculated for all hydrazonoyl derivatives and offering distinguish biological feature of HL3. Docking process was executed using AutoDock tools 4.2 over 1sm3, 4uy9 and 5j6a protein receptors (breast, colon and liver cancers, respectively) beside 2ki2 for DNA. This study was concerned to make matching with biological investigation intended in the study. The best interaction was recorded with 1sm3 and 2ki2 receptors. Antitumor activity was screened for all compounds by a comparative view over breast, colon and liver carcinoma cell lines. IC50 values represent promising efficiency of most tested compounds against breast, colon and liver carcinoma cell lines. The binding efficiency of ligands towards CT-DNA was tested. Binding constant (Kb) values are in agreement with electron drawing character for p-substituent which offer high Kb values. Also variable Hammett's relations were drawn.}
}
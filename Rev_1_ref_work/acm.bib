@inproceedings{10.1109/MSR.2019.00081,
author = {Schipper, Daan and Aniche, Maur\'{\i}cio and van Deursen, Arie},
title = {Tracing Back Log Data to Its Log Statement: From Research to Practice},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00081},
doi = {10.1109/MSR.2019.00081},
abstract = {Logs are widely used as a source of information to understand the activity of computer systems and to monitor their health and stability. However, most log analysis techniques require the link between the log messages in the raw log file and the log statements in the source code that produce them. Several solutions have been proposed to solve this non-trivial challenge, of which the approach based on static analysis reaches the highest accuracy. We, at Adyen, implemented the state-of-the-art research on log parsing in our logging environment and evaluated their accuracy and performance. Our results show that, with some adaptation, the current static analysis techniques are highly efficient and performant. In other words, ready for use.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {545–549},
numpages = {5},
keywords = {software engineering, runtime monitoring, log parsing},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3238147.3238193,
author = {He, Pinjia and Chen, Zhuangbin and He, Shilin and Lyu, Michael R.},
title = {Characterizing the Natural Language Descriptions in Software Logging Statements},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238193},
doi = {10.1145/3238147.3238193},
abstract = {Logging is a common programming practice of great importance in modern software development, because software logs have been widely used in various software maintenance tasks. To provide high-quality logs, developers need to design the description text in logging statements carefully. Inappropriate descriptions will slow down or even mislead the maintenance process, such as postmortem analysis. However, there is currently a lack of rigorous guide and specifications on developer logging behaviors, which makes the construction of description text in logging statements a challenging problem. To fill this significant gap, in this paper, we systematically study what developers log, with focus on the usage of natural language descriptions in logging statements. We obtain 6 valuable findings by conducting source code analysis on 10 Java projects and 7 C# projects, which contain 28,532,975 LOC and 115,159 logging statements in total. Furthermore, our study demonstrates the potential of automated description text generation for logging statements by obtaining up to 49.04 BLEU-4 score and 62.1 ROUGE-L score using a simple information retrieval method. To facilitate future research in this field, the datasets have been publicly released.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {178–189},
numpages = {12},
keywords = {Logging, empirical study, natural language processing},
location = {Montpellier, France},
series = {ASE 2018}
}

@inproceedings{10.5555/2819009.2819035,
author = {Pecchia, Antonio and Cinque, Marcello and Carrozza, Gabriella and Cotroneo, Domenico},
title = {Industry Practices and Event Logging: Assessment of a Critical Software Development Process},
year = {2015},
publisher = {IEEE Press},
abstract = {Practitioners widely recognize the importance of event logging for a variety of tasks, such as accounting, system measurements and troubleshooting. Nevertheless, in spite of the importance of the tasks based on the logs collected under real workload conditions, event logging lacks systematic design and implementation practices. The implementation of the logging mechanism strongly relies on the human expertise.This paper proposes a measurement study of event logging practices in a critical industrial domain. We assess a software development process at Selex ES, a leading Finmeccanica company in electronic and information solutions for critical systems. Our study combines source code analysis, inspection of around 2.3 millions log entries, and direct feedback from the development team to gain process-wide insights ranging from programming practices, logging objectives and issues impacting log analysis. The findings of our study were extremely valuable to prioritize event logging reengineering tasks at Selex ES.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
pages = {169–178},
numpages = {10},
keywords = {event logging, coding practices, industry domain, source code analysis, development process},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/2597073.2597101,
author = {Khodabandelou, Ghazaleh and Hug, Charlotte and Deneck\`{e}re, Rebecca and Salinesi, Camille},
title = {Unsupervised Discovery of Intentional Process Models from Event Logs},
year = {2014},
isbn = {9781450328630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597073.2597101},
doi = {10.1145/2597073.2597101},
abstract = { Research on guidance and method engineering has highlighted that many method engineering issues, such as lack of flexibility or adaptation, are solved more effectively when intentions are explicitly specified. However, software engineering process models are most often described in terms of sequences of activities. This paper presents a novel approach, so-called Map Miner Method (MMM), designed to automate the construction of intentional process models from process logs. To do so, MMM uses Hidden Markov Models to model users' activities logs in terms of users' strategies. MMM also infers users' intentions and constructs fine-grained and coarse-grained intentional process models with respect to the Map metamodel syntax (i.e., metamodel that specifies intentions and strategies of process actors). These models are obtained by optimizing a new precision-fitness metric. The result is a software engineering method process specification aligned with state of the art of method engineering approaches. As a case study, the MMM is used to mine the intentional process associated to the Eclipse platform usage. Observations show that the obtained intentional process model offers a new understanding of software processes, and could readily be used for recommender systems. },
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
pages = {282–291},
numpages = {10},
keywords = {Event logs Mining, Intentional Process Modeling, Software Development Process},
location = {Hyderabad, India},
series = {MSR 2014}
}

@article{10.1145/3077583,
author = {Cingolani, Davide and Pellegrini, Alessandro and Quaglia, Francesco},
title = {Transparently Mixing Undo Logs and Software Reversibility for State Recovery in Optimistic PDES},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1049-3301},
url = {https://doi.org/10.1145/3077583},
doi = {10.1145/3077583},
abstract = {The Time Warp synchronization protocol for Parallel Discrete Event Simulation (PDES) is universally considered a viable solution to exploit the intrinsic simulation model parallelism and to provide model execution speedup. Yet it leads the PDES system to execute events in an order that may generate causal inconsistencies that need to be recovered via rollback, which requires restoration of a previous (consistent) simulation state whenever a causality violation is detected. The rollback operation is so critical for the performance of a Time Warp system that it has been extensively studied in the literature for decades to find approaches suitable to optimize it. The proposed solutions can be roughly classified as based on either checkpointing or reverse computing. In this article, we explore the practical design and implementation of a fully new approach based on the runtime generation of so-called undo code blocks, which are blocks of instructions implementing the reverse memory side effects generated by the forward execution of the events. However, this is not done by recomputing the original values to be restored, as instead it occurs in reverse computing schemes. Hence, the philosophy undo code blocks rely on is similar in spirit to that of undo-logs (as a form of checkpointing). Nevertheless, they are not data logs (as instead checkpoints are); rather, they are logs of instructions. Our proposal is fully transparent, thanks to the reliance on static software instrumentation (targeting the x86 architecture and Linux systems). Also, as we show, it can be combined with classical checkpointing to further improve the runtime behavior of the state recoverability support as a function of the workload. We also present experimental results related to our implementation, which is released as free software and fully integrated into the open source ROOT-Sim package. Experimental data support the viability and effectiveness of our proposal.},
journal = {ACM Trans. Model. Comput. Simul.},
month = may,
articleno = {11},
numpages = {26},
keywords = {time warp, Optimistic synchronization, software reversibility}
}

@inproceedings{10.1145/1987856.1987869,
author = {Nagappan, Meiyappan and Robinson, Brian},
title = {Creating Operational Profiles of Software Systems by Transforming Their Log Files to Directed Cyclic Graphs},
year = {2011},
isbn = {9781450305891},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1987856.1987869},
doi = {10.1145/1987856.1987869},
abstract = {Most log files are of one format - a flat file with the events of execution recorded one after the other. Each line in the file contains at least a timestamp, a combination of one or more event identifiers, and the actual log message with information of which event was executed and what the values for the dynamic parameters of that event are. Since log files have this trace information, we can use it for many purposes, such as operational profiling and anomalous execution path detection. However the current flat file format of a log file is very unintuitive to detect the existence of a repeating pattern. In this paper we propose a transformation of the current serial order format of a log file to a directed cyclic graph (such as a non-finite state machine) format and how the operational profile of a system can be built from this representation of the log file. We built a tool (in C++), that transforms a log file with a set of log events in a serial order to an adjacency matrix for the resulting graphical representation. We can then easily apply existing graph theory based algorithms on the adjacency matrix to analyze the log file of the system. The directed cyclic graph and the analysis of it can be visualized by rendering the adjacency matrix with graph visualization tools, like Graphviz.},
booktitle = {Proceedings of the 6th International Workshop on Traceability in Emerging Forms of Software Engineering},
pages = {54–57},
numpages = {4},
keywords = {log files, directed cyclic graphs, operational profiling},
location = {Waikiki, Honolulu, HI, USA},
series = {TEFSE '11}
}

@inproceedings{10.1145/3236024.3236069,
author = {Amar, Hen and Bao, Lingfeng and Busany, Nimrod and Lo, David and Maoz, Shahar},
title = {Using Finite-State Models for Log Differencing},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236069},
doi = {10.1145/3236024.3236069},
abstract = {Much work has been published on extracting various kinds of models from logs that document the execution of running systems. In many cases, however, for example in the context of evolution, testing, or malware analysis, engineers are interested not only in a single log but in a set of several logs, each of which originated from a different set of runs of the system at hand. Then, the difference between the logs is the main target of interest.  In this work we investigate the use of finite-state models for log differencing. Rather than comparing the logs directly, we generate concise models to describe and highlight their differences. Specifically, we present two algorithms based on the classic k-Tails algorithm: 2KDiff, which computes and highlights simple traces containing sequences of k events that belong to one log but not the other, and nKDiff, which extends k-Tails from one to many logs, and distinguishes the sequences of length k that are common to all logs from the ones found in only some of them, all on top of a single, rich model. Both algorithms are sound and complete modulo the abstraction defined by the use of k-Tails.  We implemented both algorithms and evaluated their performance on mutated logs that we generated based on models from the literature. We conducted a user study including 60 participants demonstrating the effectiveness of the approach in log differencing tasks. We have further performed a case study to examine the use of our approach in malware analysis. Finally, we have made our work available in a prototype web-application, for experiments.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {49–59},
numpages = {11},
keywords = {log analysis, model inference},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/1810295.1810405,
author = {Nagappan, Meiyappan},
title = {Analysis of Execution Log Files},
year = {2010},
isbn = {9781605587196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1810295.1810405},
doi = {10.1145/1810295.1810405},
abstract = {Log analysis can be used to find problems, define operational profiles, and even pro-actively prevent issues. The goal of my dissertation research is to investigate log management and analysis techniques suited for very large and very complex logs, such as those we might expect in a computational cloud system.},
booktitle = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 2},
pages = {409–412},
numpages = {4},
keywords = {analysis of textual data, fault isolation, diagnosis, log files},
location = {Cape Town, South Africa},
series = {ICSE '10}
}

@inproceedings{10.1145/3377812.3381385,
author = {Li, Zhenhao},
title = {Towards Providing Automated Supports to Developers on Writing Logging Statements},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3381385},
doi = {10.1145/3377812.3381385},
abstract = {Developers write logging statements to generate logs and record system execution behaviors. Such logs are widely used for a variety of tasks, such as debugging, testing, program comprehension, and performance analysis. However, there exists no practical guidelines on how to write logging statements; hence, making the logging decision a very challenging task. There are two main challenges that developers are facing while making logging decisions: 1) Difficult to accurately and succinctly record execution behaviors; and 2) Hard to decide where to write logging statements. This thesis proposes a series of approaches to address the problems and help developers make logging decisions in two aspects: assist in making decisions on logging contents and on logging locations. Through case studies on large-scale open source and commercial systems, we anticipate that our study will provide useful suggestions and supports to developers for writing better logging statements.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {198–201},
numpages = {4},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/2769458.2769482,
author = {Cingolani, Davide and Pellegrini, Alessandro and Quaglia, Francesco},
title = {Transparently Mixing Undo Logs and Software Reversibility for State Recovery in Optimistic PDES},
year = {2015},
isbn = {9781450335836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2769458.2769482},
doi = {10.1145/2769458.2769482},
abstract = {The rollback operation is a fundamental building block to support the correct execution of a speculative Time Warp-based Parallel Discrete Event Simulation. In the literature, several solutions to reduce the execution cost of this operation have been proposed, either based on the creation of a checkpoint of previous simulation state images, or on the execution of negative copies of simulation events which are able to undo the updates on the state. In this paper, we explore the practical design and implementation of a state recoverability technique which allows to restore a previous simulation state either relying on checkpointing or on the reverse execution of the state updates occurred while processing events in forward mode. Differently from other proposals, we address the issue of executing backward updates in a fully-transparent and event granularity-independent way, by relying on static software instrumentation (targeting the x86 architecture and Linux systems) to generate at runtime reverse update code blocks (not to be confused with reverse events, proper of the reverse computing approach). These are able to undo the effects of a forward execution while minimizing the cost of the undo operation. We also present experimental results related to our implementation, which is released as free software and fully integrated into the open source ROOT-Sim (ROme OpTimistic Simulator) package. The experimental data support the viability and effectiveness of our proposal.},
booktitle = {Proceedings of the 3rd ACM SIGSIM Conference on Principles of Advanced Discrete Simulation},
pages = {211–222},
numpages = {12},
keywords = {speculative processing, code instrumentation, pdes, reversibility},
location = {London, United Kingdom},
series = {SIGSIM PADS '15}
}

@inproceedings{10.1145/3220267.3220269,
author = {Latib, Marlina Abdul and Ismail, Saiful Adli and Yusop, Othman Mohd and Magalingam, Pritheega and Azmi, Azri},
title = {Analysing Log Files For Web Intrusion Investigation Using Hadoop},
year = {2018},
isbn = {9781450364690},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220267.3220269},
doi = {10.1145/3220267.3220269},
abstract = {The process of analyzing large amount of data from the log file helps organization to identify the web intruders' activities as well as the vulnerabilities of the website. However, analyzing them is totally a great challenge as the process is time consuming and sometimes can be inefficient. Existing or traditional log analyzers may not able to analyze such big chunk of data. Therefore, the aim of this research is to produce an analysis result for web intrusion investigation in Big Data environment. In this study, web log was analyzed based on attacks that are captured through web server log files. The web log was cleaned and refined through a log-preprocessing program before it was analyzed. An experimental simulation was conducted using Hadoop framework to produce the required analysis results. The results of this experimental simulation indicate that Hadoop application is able to produce analysis results from large size web log files in order to assist the web intrusion investigation. Besides that, the execution time performance analysis shows that the total execution time will not increase linearly with the size of the data. This study also provides solution on visualizing the analysis result using Power View and Hive.},
booktitle = {Proceedings of the 7th International Conference on Software and Information Engineering},
pages = {12–21},
numpages = {10},
keywords = {Hadoop, Big Data, log pre-processing, web log file, web intrusion},
location = {Cairo, Egypt},
series = {ICSIE '18}
}

@inproceedings{10.1145/3178461.3178462,
author = {Meryem, Amar and Samira, Douzi and Bouabid, El Ouahidi},
title = {Enhancing Cloud Security Using Advanced MapReduce K-Means on Log Files},
year = {2018},
isbn = {9781450354387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178461.3178462},
doi = {10.1145/3178461.3178462},
abstract = {Many customers ranked cloud security as a major challenge that threaten their work and reduces their trust on cloud service's provider. Hence, a significant improvement is required to establish better adaptations of security measures that suit recent technologies and especially distributed architectures.Considering the meaningful recorded data in cloud generated log files, making analysis on them, mines insightful value about hacker's activities. It identifies malicious user behaviors and predicts new suspected events. Not only that, but centralizing log files, prevents insiders from causing damage to system. In this paper, we proposed to take away sensitive log files into a single server provider and combining both MapReduce programming and k-means on the same algorithm to cluster observed events into classes having similar features. To label unknown user behaviors and predict new suspected activities this approach considers cosine distances and deviation metrics.},
booktitle = {Proceedings of the 2018 International Conference on Software Engineering and Information Management},
pages = {63–67},
numpages = {5},
keywords = {log files, Cloud Security, Deviation metric, K-means, MapReduce},
location = {Casablanca, Morocco},
series = {ICSIM2018}
}

@inproceedings{10.5555/2735522.2735542,
author = {Hasankiyadeh, Asef Pourmaoumi and Kahani, Mohsen and Bagheri, Ebrahim and Asadi, Mohsen},
title = {Mining Common Morphological Fragments from Process Event Logs},
year = {2014},
publisher = {IBM Corp.},
address = {USA},
abstract = {Many organizations have implemented their organizational processes within integrated information systems using formal process models. These processes, which have been implemented in different organizations can share significant amount of similarities. Analysis and mining of these processes for identifying similarities can lead to valuable insight for the organizations. There has already been work on mining process models from event logs for an individual organization. The objective of this paper is, however, to detect and extract common process fragments from a family of processes that may not have been executed within the same application/organization. These identified common fragments can be used as building blocks of future applications or be used for refactoring existing applications. To this end, we first provide a precise definition of process fragments. We define morphological fragments as operationally identical fragments. We then propose an algorithm for extracting morphological fragments from process event logs. We discuss the relative performance of our proposed algorithm and its applicability in practice.},
booktitle = {Proceedings of 24th Annual International Conference on Computer Science and Software Engineering},
pages = {179–191},
numpages = {13},
keywords = {event logs, process model fragmentation, common fragments},
location = {Markham, Ontario, Canada},
series = {CASCON '14}
}

@inproceedings{10.1145/3341105.3373845,
author = {Gholamian, Sina and Ward, Paul A. S.},
title = {Logging Statements' Prediction Based on Source Code Clones},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3373845},
doi = {10.1145/3341105.3373845},
abstract = {Log files are widely used to record runtime information of software systems, such as the time-stamp of an event, the unique ID of the source of the log, and a part of the state of task execution. The rich information of logs enables system operators to monitor the runtime behaviors of their systems and further track down system problems in production settings. Although logs are useful, there exists a trade-off between their benefit and cost, and it is a crucial problem to optimize the location and content of log messages in the source code, i.e., "where and what to log?"Prior research has analyzed logging statements in the source code and proposed ways to predict and suggest the location of log statements in order to partially automate log statement addition to the source code. However, there are gaps and unsolved problems in the literature to fully automate the logging process. Thus, in this research, we perform an experimental study on open-source Java projects and apply code-clone detection methods for log statements' prediction. Our work demonstrates the feasibility of logging automation by predicting the location of a log point in a code snippet based on the existence of a logging statement in its corresponding code clone pair. We propose a Log-Aware Code-Clone Detector (LACC) which achieves a higher accuracy of log prediction when compared to state-of-the-art general-purpose clone detectors. Our analysis shows that 98% of clone snippets match in their logging behavior, and LACC can predict the location of logging statements by the accuracy of 90+% for Apache Java projects.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {82–91},
numpages = {10},
keywords = {software engineering, source code, logging statement, code clones, automation},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1145/2600176.2600183,
author = {King, Jason and Williams, Laurie},
title = {Log Your CRUD: Design Principles for Software Logging Mechanisms},
year = {2014},
isbn = {9781450329071},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600176.2600183},
doi = {10.1145/2600176.2600183},
abstract = {According to a 2011 survey in healthcare, the most commonly reported breaches of protected health information involved employees snooping into medical records of friends and relatives. Logging mechanisms can provide a means for forensic analysis of user activity in software systems by proving that a user performed certain actions in the system. However, logging mechanisms often inconsistently capture user interactions with sensitive data, creating gaps in traces of user activity. Explicit design principles and systematic testing of logging mechanisms within the software development lifecycle may help strengthen the overall security of software. The objective of this research is to observe the current state of logging mechanisms by performing an exploratory case study in which we systematically evaluate logging mechanisms by supplementing the expected results of existing functional black-box test cases to include log output. We perform an exploratory case study of four open-source electronic health record (EHR) logging mechanisms: OpenEMR, OSCAR, Tolven eCHR, and WorldVistA. We supplement the expected results of 30 United States government-sanctioned test cases to include log output to track access of sensitive data. We then execute the test cases on each EHR system. Six of the 30 (20%) test cases failed on all four EHR systems because user interactions with sensitive data are not logged. We find that viewing protected data is often not logged by default, allowing unauthorized views of data to go undetected. Based on our results, we propose a set of principles that developers should consider when developing logging mechanisms to ensure the ability to capture adequate traces of user activity.},
booktitle = {Proceedings of the 2014 Symposium and Bootcamp on the Science of Security},
articleno = {5},
numpages = {10},
keywords = {logging mechanism, case study, nonrepudiation, electronic health record software, science, audit, black-box testing, accountability, healthcare},
location = {Raleigh, North Carolina, USA},
series = {HotSoS '14}
}

@inproceedings{10.5555/2486788.2487022,
author = {King, Jason},
title = {Measuring the Forensic-Ability of Audit Logs for Nonrepudiation},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = { Forensic analysis of software log files is used to extract user behavior profiles, detect fraud, and check compliance with policies and regulations. Software systems maintain several types of log files for different purposes. For example, a system may maintain logs for debugging, monitoring application performance, and/or tracking user access to system resources. The objective of my research is to develop and validate a minimum set of log file attributes and software security metrics for user nonrepudiation by measuring the degree to which a given audit log file captures the data necessary to allow for meaningful forensic analysis of user behavior within the software system. For a log to enable user nonrepudiation, the log file must record certain data fields, such as a unique user identifier. The log must also record relevant user activity, such as creating, viewing, updating, and deleting system resources, as well as software security events, such as the addition or revocation of user privileges. Using a grounded theory method, I propose a methodology for observing the current state of activity logging mechanisms in healthcare, education, and finance, then I quantify differences between activity logs and logs not specifically intended to capture user activity. I will then propose software security metrics for quantifying the forensic-ability of log files. I will evaluate my work with empirical analysis by comparing the performance of my metrics on several types of log files, including both activity logs and logs not directly intended to record user activity. My research will help software developers strengthen user activity logs for facilitating forensic analysis for user nonrepudiation. },
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {1419–1422},
numpages = {4},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1109/ICSE-Companion.2019.00055,
author = {Chen, An Ran},
title = {An Empirical Study on Leveraging Logs for Debugging Production Failures},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion.2019.00055},
doi = {10.1109/ICSE-Companion.2019.00055},
abstract = {In modern software development, maintenance is one of the most expensive processes. When end-users encounter software defects, they report the bug to developers by specifying the expected behavior and error messages (e.g., log message). Then, they wait for a bug fix from the developers. However, on the developers' side, it can be very challenging and expensive to debug the problem. To fix the bugs, developers often have to play the role of detectives: seeking clues in the user-reported logs files or stack trace in a snapshot of specific system execution. This debugging process may take several hours or even days.In this paper, we first look at the usefulness of the user-reported logs. Then, we propose an automated approach to assist the debugging process by reconstructing the execution path. Through the analysis, our investigation shows that 31% of the time, developer further requests logs from the reporter. Moreover, our preliminary results show that the reconducted path illustrates the user's execution. We believe that our approach proposes a novel solution in debugging production failures.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings},
pages = {126–128},
numpages = {3},
keywords = {mining software repository data, static analysis, production errors, events log},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3196321.3196340,
author = {Messaoudi, Salma and Panichella, Annibale and Bianculli, Domenico and Briand, Lionel and Sasnauskas, Raimondas},
title = {A Search-Based Approach for Accurate Identification of Log Message Formats},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3196340},
doi = {10.1145/3196321.3196340},
abstract = {Many software engineering activities process the events contained in log files. However, before performing any processing activity, it is necessary to parse the entries in a log file, to retrieve the actual events recorded in the log. Each event is denoted by a log message, which is composed of a fixed part---called (event) template---that is the same for all occurrences of the same event type, and a variable part, which may vary with each event occurrence. The formats of log messages, in complex and evolving systems, have numerous variations, are typically not entirely known, and change on a frequent basis; therefore, they need to be identified automatically.The log message format identification problem deals with the identification of the different templates used in the messages of a log. Any solution to this problem has to generate templates that meet two main goals: generating templates that are not too general, so as to distinguish different events, but also not too specific, so as not to consider different occurrences of the same event as following different templates; however, these goals are conflicting.In this paper, we present the MoLFI approach, which recasts the log message identification problem as a multi-objective problem. MoLFI uses an evolutionary approach to solve this problem, by tailoring the NSGA-II algorithm to search the space of solutions for a Pareto optimal set of message templates. We have implemented MoLFI in a tool, which we have evaluated on six real-world datasets, containing log files with a number of entries ranging from 2K to 300K. The experiments results show that MoLFI extracts by far the highest number of correct log message templates, significantly outperforming two state-of-the-art approaches on all datasets.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {167–177},
numpages = {11},
keywords = {NSGA-II, log parsing, log message format, log analysis},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@inproceedings{10.1145/2451116.2451130,
author = {Viennot, Nicolas and Nair, Siddharth and Nieh, Jason},
title = {Transparent Mutable Replay for Multicore Debugging and Patch Validation},
year = {2013},
isbn = {9781450318709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2451116.2451130},
doi = {10.1145/2451116.2451130},
abstract = {We present Dora, a mutable record-replay system which allows a recorded execution of an application to be replayed with a modified version of the application. This feature, not available in previous record-replay systems, enables powerful new functionality. In particular, Dora can help reproduce, diagnose, and fix software bugs by replaying a version of a recorded application that is recompiled with debugging information, reconfigured to produce verbose log output, modified to include additional print statements, or patched to fix a bug.Dora uses lightweight operating system mechanisms to record an application execution by capturing nondeterministic events to a log without imposing unnecessary timing and ordering constraints. It replays the log using a modified version of the application even in the presence of added, deleted, or modified operations that do not match events in the log. Dora searches for a replay that minimizes differences between the log and the replayed execution of the modified program. If there are no modifications, Dora provides deterministic replay of the unmodified program.We have implemented a Linux prototype which provides transparent mutable replay without recompiling or relinking applications. We show that Dora is useful for reproducing, diagnosing, and fixing software bugs in real-world applications, including Apache and MySQL. Our results show that Dora (1) captures bugs and replays them with applications modified or reconfigured to produce additional debugging output for root cause diagnosis, (2) captures exploits and replays them with patched applications to validate that the patches successfully eliminate vulnerabilities, (3) records production workloads and replays them with patched applications to validate patches with realistic workloads, and (4) maintains low recording overhead on commodity multicore hardware, making it suitable for production systems.},
booktitle = {Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {127–138},
numpages = {12},
keywords = {mutable replay, debugging, multicore, deterministic replay},
location = {Houston, Texas, USA},
series = {ASPLOS '13}
}

@article{10.1145/2499368.2451130,
author = {Viennot, Nicolas and Nair, Siddharth and Nieh, Jason},
title = {Transparent Mutable Replay for Multicore Debugging and Patch Validation},
year = {2013},
issue_date = {April 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2499368.2451130},
doi = {10.1145/2499368.2451130},
abstract = {We present Dora, a mutable record-replay system which allows a recorded execution of an application to be replayed with a modified version of the application. This feature, not available in previous record-replay systems, enables powerful new functionality. In particular, Dora can help reproduce, diagnose, and fix software bugs by replaying a version of a recorded application that is recompiled with debugging information, reconfigured to produce verbose log output, modified to include additional print statements, or patched to fix a bug.Dora uses lightweight operating system mechanisms to record an application execution by capturing nondeterministic events to a log without imposing unnecessary timing and ordering constraints. It replays the log using a modified version of the application even in the presence of added, deleted, or modified operations that do not match events in the log. Dora searches for a replay that minimizes differences between the log and the replayed execution of the modified program. If there are no modifications, Dora provides deterministic replay of the unmodified program.We have implemented a Linux prototype which provides transparent mutable replay without recompiling or relinking applications. We show that Dora is useful for reproducing, diagnosing, and fixing software bugs in real-world applications, including Apache and MySQL. Our results show that Dora (1) captures bugs and replays them with applications modified or reconfigured to produce additional debugging output for root cause diagnosis, (2) captures exploits and replays them with patched applications to validate that the patches successfully eliminate vulnerabilities, (3) records production workloads and replays them with patched applications to validate patches with realistic workloads, and (4) maintains low recording overhead on commodity multicore hardware, making it suitable for production systems.},
journal = {SIGPLAN Not.},
month = mar,
pages = {127–138},
numpages = {12},
keywords = {mutable replay, multicore, deterministic replay, debugging}
}

@article{10.1145/2490301.2451130,
author = {Viennot, Nicolas and Nair, Siddharth and Nieh, Jason},
title = {Transparent Mutable Replay for Multicore Debugging and Patch Validation},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2490301.2451130},
doi = {10.1145/2490301.2451130},
abstract = {We present Dora, a mutable record-replay system which allows a recorded execution of an application to be replayed with a modified version of the application. This feature, not available in previous record-replay systems, enables powerful new functionality. In particular, Dora can help reproduce, diagnose, and fix software bugs by replaying a version of a recorded application that is recompiled with debugging information, reconfigured to produce verbose log output, modified to include additional print statements, or patched to fix a bug.Dora uses lightweight operating system mechanisms to record an application execution by capturing nondeterministic events to a log without imposing unnecessary timing and ordering constraints. It replays the log using a modified version of the application even in the presence of added, deleted, or modified operations that do not match events in the log. Dora searches for a replay that minimizes differences between the log and the replayed execution of the modified program. If there are no modifications, Dora provides deterministic replay of the unmodified program.We have implemented a Linux prototype which provides transparent mutable replay without recompiling or relinking applications. We show that Dora is useful for reproducing, diagnosing, and fixing software bugs in real-world applications, including Apache and MySQL. Our results show that Dora (1) captures bugs and replays them with applications modified or reconfigured to produce additional debugging output for root cause diagnosis, (2) captures exploits and replays them with patched applications to validate that the patches successfully eliminate vulnerabilities, (3) records production workloads and replays them with patched applications to validate patches with realistic workloads, and (4) maintains low recording overhead on commodity multicore hardware, making it suitable for production systems.},
journal = {SIGARCH Comput. Archit. News},
month = mar,
pages = {127–138},
numpages = {12},
keywords = {multicore, debugging, deterministic replay, mutable replay}
}

@inproceedings{10.1145/3102980.3103001,
author = {Zhao, Xu and Rodrigues, Kirk and Luo, Yu and Stumm, Michael and Yuan, Ding and Zhou, Yuanyuan},
title = {The Game of Twenty Questions: Do You Know Where to Log?},
year = {2017},
isbn = {9781450350686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102980.3103001},
doi = {10.1145/3102980.3103001},
abstract = {A production system's printed logs are often the only source of runtime information available for postmortem debugging, performance analysis and profiling, security auditing, and user behavior analytics. Therefore, the quality of this data is critically important. Recent work has attempted to enhance log quality by recording additional variable values, but logging statement placement, i.e., where to place a logging statement, which is the most challenging and fundamental problem for improving log quality, has not been adequately addressed so far. This position paper proposes we automate the placement of logging statements by measuring how much uncertainty, i.e., the expected number of possible execution code paths taken by the software, can be removed by adding a logging statement to a basic block. Guided by ideas from information theory, we describe a simple approach that automates logging statement placement. Preliminary results suggest that our algorithm can effectively cover, and further improve, the existing logging statement placements selected by developers. It can compute an optimal logging statement placement that disambiguates the entire function call path with only 0.218% of slowdown.},
booktitle = {Proceedings of the 16th Workshop on Hot Topics in Operating Systems},
pages = {125–131},
numpages = {7},
location = {Whistler, BC, Canada},
series = {HotOS '17}
}

@inproceedings{10.1145/1985429.1985438,
author = {Panchenko, Oleksandr and Plattner, Hasso and Zeier, Alexander},
title = {What Do Developers Search for in Source Code and Why},
year = {2011},
isbn = {9781450305976},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985429.1985438},
doi = {10.1145/1985429.1985438},
abstract = {Source code search is an important tool used by software engineers. However, until now relatively little is known about what developers search for in source code and why. This paper addresses this knowledge gap. We present the results of a log file analysis of a source code search engine. The data from the log file was analyzed together with the change history of four development and maintenance systems. The results show that most of the search targets were not changed after being downloaded, thus we concluded that the developers conducted searches to find reusable components, to obtain coding examples or to perform impact analysis. In contrast, maintainers often change the code they have downloaded. Moreover, we automatically categorized the search queries. The most popular categories were: method name, structural pattern, and keyword. The major search target was a statement. Although the selected data set was small, the deviations between the systems were negligible, therefore we conclude that our results are valid.},
booktitle = {Proceedings of the 3rd International Workshop on Search-Driven Development: Users, Infrastructure, Tools, and Evaluation},
pages = {33–36},
numpages = {4},
keywords = {log file analysis, development activities, source code search},
location = {Waikiki, Honolulu, HI, USA},
series = {SUITE '11}
}

@article{10.14778/3436905.3436925,
author = {Garcia, Rolando and Liu, Eric and Sreekanti, Vikram and Yan, Bobby and Dandamudi, Anusha and Gonzalez, Joseph E. and Hellerstein, Joseph M. and Sen, Koushik},
title = {Hindsight Logging for Model Training},
year = {2020},
issue_date = {December 2020},
publisher = {VLDB Endowment},
volume = {14},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/3436905.3436925},
doi = {10.14778/3436905.3436925},
abstract = {In modern Machine Learning, model training is an iterative, experimental process that can consume enormous computation resources and developer time. To aid in that process, experienced model developers log and visualize program variables during training runs. Exhaustive logging of all variables is infeasible, so developers are left to choose between slowing down training via extensive conservative logging, or letting training run fast via minimalist optimistic logging that may omit key information. As a compromise, optimistic logging can be accompanied by program checkpoints; this allows developers to add log statements post-hoc, and "replay" desired log statements from checkpoint---a process we refer to as hindsight logging. Unfortunately, hindsight logging raises tricky problems in data management and software engineering. Done poorly, hindsight logging can waste resources and generate technical debt embodied in multiple variants of training code. In this paper, we present methodologies for efficient and effective logging practices for model training, with a focus on techniques for hindsight logging. Our goal is for experienced model developers to learn and adopt these practices. To make this easier, we provide an open-source suite of tools for Fast Low-Overhead Recovery (flor) that embodies our design across three tasks: (i) efficient background logging in Python, (ii) adaptive periodic checkpointing, and (iii) an instrumentation library that codifies hindsight logging for efficient and automatic record-replay of model-training. Model developers can use each flor tool separately as they see fit, or they can use flor in hands-free mode, entrusting it to instrument their code end-to-end for efficient record-replay. Our solutions leverage techniques from physiological transaction logs and recovery in database systems. Evaluations on modern ML benchmarks demonstrate that flor can produce fast checkpointing with small user-specifiable overheads (e.g. 7%), and still provide hindsight log replay times orders of magnitude faster than restarting training from scratch.},
journal = {Proc. VLDB Endow.},
month = dec,
pages = {682–693},
numpages = {12}
}

@inproceedings{10.1145/2287076.2287085,
author = {Ilsche, Thomas and Schuchart, Joseph and Cope, Jason and Kimpe, Dries and Jones, Terry and Kn\"{u}pfer, Andreas and Iskra, Kamil and Ross, Robert and Nagel, Wolfgang E. and Poole, Stephen},
title = {Enabling Event Tracing at Leadership-Class Scale through I/O Forwarding Middleware},
year = {2012},
isbn = {9781450308052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2287076.2287085},
doi = {10.1145/2287076.2287085},
abstract = {Event tracing is an important tool for understanding the performance of parallel applications. As concurrency increases in leadership-class computing systems, the quantity of performance log data can overload the parallel file system, perturbing the application being observed. In this work we present a solution for event tracing at leadership scales. We enhance the I/O forwarding system software to aggregate and reorganize log data prior to writing to the storage system, significantly reducing the burden on the underlying file system for this type of traffic. Furthermore, we augment the I/O forwarding system with a write buffering capability to limit the impact of artificial perturbations from log data accesses on traced applications. To validate the approach, we modify the Vampir tracing toolset to take advantage of this new capability and show that the approach increases the maximum traced application size by a factor of 5x to more than 200,000 processes.},
booktitle = {Proceedings of the 21st International Symposium on High-Performance Parallel and Distributed Computing},
pages = {49–60},
numpages = {12},
keywords = {event tracing, atomic append, I/O forwarding},
location = {Delft, The Netherlands},
series = {HPDC '12}
}

@inproceedings{10.1145/3338906.3338931,
author = {Zhang, Xu and Xu, Yong and Lin, Qingwei and Qiao, Bo and Zhang, Hongyu and Dang, Yingnong and Xie, Chunyu and Yang, Xinsheng and Cheng, Qian and Li, Ze and Chen, Junjie and He, Xiaoting and Yao, Randolph and Lou, Jian-Guang and Chintalapati, Murali and Shen, Furao and Zhang, Dongmei},
title = {Robust Log-Based Anomaly Detection on Unstable Log Data},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338931},
doi = {10.1145/3338906.3338931},
abstract = {Logs are widely used by large and complex software-intensive systems for troubleshooting. There have been a lot of studies on log-based anomaly detection. To detect the anomalies, the existing methods mainly construct a detection model using log event data extracted from historical logs. However, we find that the existing methods do not work well in practice. These methods have the close-world assumption, which assumes that the log data is stable over time and the set of distinct log events is known. However, our empirical study shows that in practice, log data often contains previously unseen log events or log sequences. The instability of log data comes from two sources: 1) the evolution of logging statements, and 2) the processing noise in log data. In this paper, we propose a new log-based anomaly detection approach, called LogRobust. LogRobust extracts semantic information of log events and represents them as semantic vectors. It then detects anomalies by utilizing an attention-based Bi-LSTM model, which has the ability to capture the contextual information in the log sequences and automatically learn the importance of different log events. In this way, LogRobust is able to identify and handle unstable log events and sequences. We have evaluated LogRobust using logs collected from the Hadoop system and an actual online service system of Microsoft. The experimental results show that the proposed approach can well address the problem of log instability and achieve accurate and robust results on real-world, ever-changing log data.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {807–817},
numpages = {11},
keywords = {Anomaly Detection, Deep Learning, Log Instability, Data Quality, Log Analysis},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/2103799.2103811,
author = {Zhang, Cheng and Guo, Zhenyu and Wu, Ming and Lu, Longwen and Fan, Yu and Zhao, Jianjun and Zhang, Zheng},
title = {AutoLog: Facing Log Redundancy and Insufficiency},
year = {2011},
isbn = {9781450311793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2103799.2103811},
doi = {10.1145/2103799.2103811},
abstract = {Logs are valuable for failure diagnosis and software debugging in practice. However, due to the ad-hoc style of inserting logging statements, the quality of logs can hardly be guaranteed. In case of a system failure, the log file may contain a large number of irrelevant logs, while crucial clues to the root cause may still be missing.In this paper, we present an automated approach to log improvement based on the combination of information from program source code and textual logs. It selects the most relevant ones from an ocean of logs to help developers focus and reason along the causality chain, and generates additional informative logs to help developers discover the root causes of failures. We have conducted a preliminary case study using an implementation prototype to demonstrate the usefulness of our approach.},
booktitle = {Proceedings of the Second Asia-Pacific Workshop on Systems},
articleno = {10},
numpages = {5},
location = {Shanghai, China},
series = {APSys '11}
}

@inproceedings{10.1145/3400286.3418261,
author = {Svacina, Jan and Raffety, Jackson and Woodahl, Connor and Stone, Brooklynn and Cerny, Tomas and Bures, Miroslav and Shin, Dongwan and Frajtak, Karel and Tisnovsky, Pavel},
title = {On Vulnerability and Security Log Analysis: A Systematic Literature Review on Recent Trends},
year = {2020},
isbn = {9781450380256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400286.3418261},
doi = {10.1145/3400286.3418261},
abstract = {Log analysis is a technique of deriving knowledge from log files containing records of events in a computer system. A common application of log analysis is to derive critical information about a system's security issues and intrusions, which subsequently leads to being able to identify and potentially stop intruders attacking the system. However, many systems produce a high volume of log data with high frequency, posing serious challenges in analysis. This paper contributes with a systematic literature review and discusses current trends, advancements, and future directions in log security analysis within the past decade. We summarized current research strategies with respect to technology approaches from 34 current publications. We identified limitations that poses challenges to future research and opened discussion on issues towards logging mechanism in the software systems. Findings of this study are relevant for software systems as well as software parts of the Internet of Things (IoT) systems.},
booktitle = {Proceedings of the International Conference on Research in Adaptive and Convergent Systems},
pages = {175–180},
numpages = {6},
keywords = {Intrusion Detection, Log Analysis, Machine Learning, Log Mining, Anomaly Detection},
location = {Gwangju, Republic of Korea},
series = {RACS '20}
}

@inproceedings{10.1145/2908961.2931740,
author = {Sekanina, Lukas and Kapusta, Vlastimil},
title = {Visualisation and Analysis of Genetic Records Produced by Cartesian Genetic Programming},
year = {2016},
isbn = {9781450343237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2908961.2931740},
doi = {10.1145/2908961.2931740},
abstract = {Cartesian genetic programming (CGP) is a branch of genetic programming in which candidate designs are represented using directed acyclic graphs. Evolutionary circuit design is the most typical application of CGP. This paper presents a new software tool---CGPAnalyzer---developed to analyse and visualise a genetic record (i.e. a log file) generated by CGP-based circuit design software. CGPAnalyzer automatically finds key genetic improvements in the genetic record and presents relevant phenotypes. The comparison module of CGPAnalyzer allows the user to select two phenotypes and compare their structure, history and functionality. It thus enables to reconstruct the process of discovering new circuit designs. This feature is demonstrated by means of the analysis of the genetic record from a 9-parity circuit evolution. The CGPAnalyzer tool is a desktop application with a graphical user interface created using Java v.8 and Swing library.},
booktitle = {Proceedings of the 2016 on Genetic and Evolutionary Computation Conference Companion},
pages = {1411–1418},
numpages = {8},
keywords = {digital circuit, cartesian genetic programming, visualisation},
location = {Denver, Colorado, USA},
series = {GECCO '16 Companion}
}

@inproceedings{10.1145/3361242.3361261,
author = {Zhu, Jing and Rong, Guoping and Huang, Guocheng and Gu, Shenghui and Zhang, He and Shao, Dong},
title = {JLLAR: A Logging Recommendation Plug-in Tool for Java},
year = {2019},
isbn = {9781450377010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361242.3361261},
doi = {10.1145/3361242.3361261},
abstract = {Logs are the execution results of logging statements in software systems after being triggered by various events, which is able to capture the dynamic behavior of software systems during runtime and provide important information for software analysis, e.g., issue tracking, performance monitoring, etc. Obviously, to meet this purpose, the quality of the logs is critical, which requires appropriately placement of logging statements. Existing research on this topic reveals that where to log? and what to log? are two most concerns when conducting logging practice in software development, which mainly relies on developers' personal skills, expertise and preference, rendering several problems impacting the quality of the logs inevitably. One of the reasons leading to this phenomenon might be that several recognized best practices(strategies as well) are easily neglected by software developers. Especially in those software projects with relatively large number of participants. To address this issue, we designed and implemented a plug-in tool (i.e., JLLAR) based on the Intellij IDEA, which applied machine learning technology to identify and create a set of rules reflecting commonly recognized logging practices. Based on this rule set, JLLAR can be used to scan existing source code to identify issues regarding the placement of logging statements. Moreover, JLLAR also provides automatic code completion and semi code completion (i.e., to provide recommendations) regarding logging practice to support software developers during coding.},
booktitle = {Proceedings of the 11th Asia-Pacific Symposium on Internetware},
articleno = {16},
numpages = {6},
keywords = {tool, logging practice, machine learning},
location = {Fukuoka, Japan},
series = {Internetware '19}
}

@inproceedings{10.1109/ICPC.2019.00029,
author = {Mizouchi, Tsuyoshi and Shimari, Kazumasa and Ishio, Takashi and Inoue, Katsuro},
title = {PADLA: A Dynamic Log Level Adapter Using Online Phase Detection},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2019.00029},
doi = {10.1109/ICPC.2019.00029},
abstract = {Logging is an important feature for a software system to record its run-time information. Although detailed logs are helpful to identify the cause of a failure in a program execution, constantly recording detailed logs of a long-running system is challenging because of its performance overhead and storage cost. To solve the problem, we propose PADLA (<u>P</u>hase-<u>A</u>ware <u>D</u>ynamic <u>L</u>og Level <u>A</u>dapter) that dynamically adjusts the log level of a running system so that the system can record irregular events such as performance anomalies in detail while recording regular events concisely. PADLA is an extension of Apache Log4j, one of the most popular logging framework for Java. It employs an online phase detection algorithm to recognize irregular events. It monitors run-time performance of a system and learns regular execution phases of a program. If it recognizes a performance anomalies, it automatically changes the log level of a system to record the detailed behavior. In the case study, PADLA successfully recorded a detailed log for performance analysis of a server system under high load while suppressing the amount of log data and performance overhead.},
booktitle = {Proceedings of the 27th International Conference on Program Comprehension},
pages = {135–138},
numpages = {4},
keywords = {Log4j, performance anomaly, phase detection, dynamic analysis, log level},
location = {Montreal, Quebec, Canada},
series = {ICPC '19}
}

@inproceedings{10.1145/2968478.2968486,
author = {Mazumder, Biswajit and Hallstrom, Jason O.},
title = {A Fast, Lightweight, and Reliable File System for Wireless Sensor Networks},
year = {2016},
isbn = {9781450344852},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968478.2968486},
doi = {10.1145/2968478.2968486},
abstract = {Sensor nodes are increasingly used in critical applications. A file storage system that is fast, lightweight, and reliable across device failures is important to safeguard the data that these devices record. A fast and lightweight file system should enable sensed data to be sampled and stored quickly for later transmission, while imposing a small resource footprint. A reliable file system should provide storage integrity in the face of hardware, software, and other failures.We present the design and implementation of LoggerFS, a fast, lightweight, and reliable file system for wireless sensor networks which uses a hybrid memory design consisting of RAM, FRAM, and flash. LoggerFS is engineered to provide fast data storage, have a small memory footprint, and provide data reliability across system failures. Additionally, LoggerFS is designed to be power efficient. LoggerFS adapts a log-structured file system approach, augmented with data persistence and reliability guarantees. A caching mechanism allows for flash wear-leveling and fast data buffering. We present a performance evaluation of LoggerFS using a prototypical in situ sensing platform, and demonstrate between 50% and 800% improvements for various workloads using the FRAM write-back cache.},
booktitle = {Proceedings of the 13th International Conference on Embedded Software},
articleno = {23},
numpages = {10},
location = {Pittsburgh, Pennsylvania},
series = {EMSOFT '16}
}

@inproceedings{10.1109/ICSE.2017.15,
author = {Chen, Boyuan and Jiang, Zhen Ming (Jack)},
title = {Characterizing and Detecting Anti-Patterns in the Logging Code},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.15},
doi = {10.1109/ICSE.2017.15},
abstract = {Snippets of logging code are output statements (e.g., LOG.info or System.out.println) that developers insert into a software system. Although more logging code can provide more execution context of the system's behavior during runtime, it is undesirable to instrument the system with too much logging code due to maintenance overhead. Furthermore, excessive logging may cause unexpected side-effects like performance slow-down or high disk I/O bandwidth. Recent studies show that there are no well-defined coding guidelines for performing effective logging. Previous research on the logging code mainly tackles the problems of where-to-log and what-to-log. There are very few works trying to address the problem of how-to-log (developing and maintaining high-quality logging code).In this paper, we study the problem of how-to-log by characterizing and detecting the anti-patterns in the logging code. As the majority of the logging code is evolved together with the feature code, the remaining set of logging code changes usually contains the fixes to the anti-patterns. We have manually examined 352 pairs of independently changed logging code snippets from three well-maintenance open source systems: ActiveMQ, Hadoop and Maven. Our analysis has resulted in six different anti-patterns in the logging code. To demonstrate the value of our findings, we have encoded these anti-patterns into a static code analysis tool, LCAnalyzer. Case studies show that LCAnalyzer has an average recall of 95% and precision of 60% and can be used to automatically detect previously unknown anti-patterns in the source code. To gather feedback, we have filed 64 representative instances of the logging code anti-patterns from the most recent releases of ten open source software systems. Among them, 46 instances (72%) have already been accepted by their developers.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {71–81},
numpages = {11},
keywords = {software maintenance, anti-patterns, logging practices, logging code, empirical studies},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@inproceedings{10.1145/2442516.2442537,
author = {Chen, Yufei and Chen, Haibo},
title = {Scalable Deterministic Replay in a Parallel Full-System Emulator},
year = {2013},
isbn = {9781450319225},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2442516.2442537},
doi = {10.1145/2442516.2442537},
abstract = {Full-system emulation has been an extremely useful tool in developing and debugging systems software like operating systems and hypervisors. However, current full-system emulators lack the support for deterministic replay, which limits the reproducibility of concurrency bugs that is indispensable for analyzing and debugging the essentially multi-threaded systems software.This paper analyzes the challenges in supporting deterministic replay in parallel full-system emulators and makes a comprehensive study on the sources of non-determinism. Unlike application-level replay systems, our system, called ReEmu, needs to log sources of non-determinism in both the guest software stack and the dynamic binary translator for faithful replay. To provide scalable and efficient record and replay on multicore machines, ReEmu makes several notable refinements to the CREW protocol that replays shared memory systems. First, being aware of the performance bottlenecks in frequent lock operations in the CREW protocol, ReEmu refines the CREW protocol with a seqlock-like design, to avoid serious contention and possible starvation in instrumentation code tracking dependence of racy accesses on a shared memory object. Second, to minimize the required log files, ReEmu only logs minimal local information regarding accesses to a shared memory location, but instead relies on an offline log processing tool to derive precise shared memory dependence for faithful replay. Third, ReEmu adopts an automatic lock clustering mechanism that clusters a set of uncontended memory objects to a bulk to reduce the frequencies of lock operations, which noticeably boost performance.Our prototype ReEmu is based on our open-source COREMU system and supports scalable and efficient record and replay of full-system environments (both x64 and ARM). Performance evaluation shows that ReEmu has very good performance scalability on an Intel multicore machine. It incurs only 68.9% performance overhead on average (ranging from 51.8% to 94.7%) over vanilla COREMU to record five PARSEC benchmarks running on a 16-core emulated system.},
booktitle = {Proceedings of the 18th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {207–218},
numpages = {12},
keywords = {scalable deterministic replay, full-system emulator},
location = {Shenzhen, China},
series = {PPoPP '13}
}

@article{10.1145/2517327.2442537,
author = {Chen, Yufei and Chen, Haibo},
title = {Scalable Deterministic Replay in a Parallel Full-System Emulator},
year = {2013},
issue_date = {August 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/2517327.2442537},
doi = {10.1145/2517327.2442537},
abstract = {Full-system emulation has been an extremely useful tool in developing and debugging systems software like operating systems and hypervisors. However, current full-system emulators lack the support for deterministic replay, which limits the reproducibility of concurrency bugs that is indispensable for analyzing and debugging the essentially multi-threaded systems software.This paper analyzes the challenges in supporting deterministic replay in parallel full-system emulators and makes a comprehensive study on the sources of non-determinism. Unlike application-level replay systems, our system, called ReEmu, needs to log sources of non-determinism in both the guest software stack and the dynamic binary translator for faithful replay. To provide scalable and efficient record and replay on multicore machines, ReEmu makes several notable refinements to the CREW protocol that replays shared memory systems. First, being aware of the performance bottlenecks in frequent lock operations in the CREW protocol, ReEmu refines the CREW protocol with a seqlock-like design, to avoid serious contention and possible starvation in instrumentation code tracking dependence of racy accesses on a shared memory object. Second, to minimize the required log files, ReEmu only logs minimal local information regarding accesses to a shared memory location, but instead relies on an offline log processing tool to derive precise shared memory dependence for faithful replay. Third, ReEmu adopts an automatic lock clustering mechanism that clusters a set of uncontended memory objects to a bulk to reduce the frequencies of lock operations, which noticeably boost performance.Our prototype ReEmu is based on our open-source COREMU system and supports scalable and efficient record and replay of full-system environments (both x64 and ARM). Performance evaluation shows that ReEmu has very good performance scalability on an Intel multicore machine. It incurs only 68.9% performance overhead on average (ranging from 51.8% to 94.7%) over vanilla COREMU to record five PARSEC benchmarks running on a 16-core emulated system.},
journal = {SIGPLAN Not.},
month = feb,
pages = {207–218},
numpages = {12},
keywords = {scalable deterministic replay, full-system emulator}
}

@inproceedings{10.1145/3324884.3416636,
author = {Li, Zhenhao and Chen, Tse-Hsun (Peter) and Shang, Weiyi},
title = {Where Shall We Log? Studying and Suggesting Logging Locations in Code Blocks},
year = {2020},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416636},
doi = {10.1145/3324884.3416636},
abstract = {Developers write logging statements to generate logs and record system execution behaviors to assist in debugging and software maintenance. However, deciding where to insert logging statements is a crucial yet challenging task. On one hand, logging too little may increase the maintenance difficulty due to missing important system execution information. On the other hand, logging too much may introduce excessive logs that mask the real problems and cause significant performance overhead. Prior studies provide recommendations on logging locations, but such recommendations are only for limited situations (e.g., exception logging) or at a coarse-grained level (e.g., method level). Thus, properly helping developers decide finer-grained logging locations for different situations remains an unsolved challenge. In this paper, we tackle the challenge by first conducting a comprehensive manual study on the characteristics of logging locations in seven open-source systems. We uncover six categories of logging locations and find that developers usually insert logging statements to record execution information in various types of code blocks. Based on the observed patterns, we then propose a deep learning framework to automatically suggest logging locations at the block level. We model the source code at the code block level using the syntactic and semantic information. We find that: 1) our models achieve an average of 80.1% balanced accuracy when suggesting logging locations in blocks; 2) our cross-system logging suggestion results reveal that there might be an implicit logging guideline across systems. Our results show that we may accurately provide finer-grained suggestions on logging locations, and such suggestions may be shared across systems.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {361–372},
numpages = {12},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/2070942.2070972,
author = {Tancreti, Matthew and Hossain, Mohammad Sajjad and Bagchi, Saurabh and Raghunathan, Vijay},
title = {Aveksha: A Hardware-Software Approach for Non-Intrusive Tracing and Profiling of Wireless Embedded Systems},
year = {2011},
isbn = {9781450307185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070942.2070972},
doi = {10.1145/2070942.2070972},
abstract = {It is important to get an idea of the events occurring in an embedded wireless node when it is deployed in the field, away from the convenience of an interactive debugger. Such visibility can be useful for post-deployment testing, replay-based debugging, and for performance and energy profiling of various software components. Prior software-based solutions to address this problem have incurred high execution overhead and intrusiveness. The intrusiveness changes the intrinsic timing behavior of the application, thereby reducing the fidelity of the collected profile. Prior hardware-based solutions have involved the use of dedicated ASICs or other tightly coupled changes to the embedded node's processor, which significantly limits their applicability.In this paper, we present Aveksha, a hardware-software approach for achieving the above goals in a non-intrusive manner. Our approach is based on the key insight that most embedded processors have an on-chip debug module (which has traditionally been used for interactive debugging) that provides significant visibility into the internal state of the processor. We design a debug board that interfaces with the on-chip debug module of an embedded node's processor through the JTAG port and provides three modes of event logging and tracing: breakpoint, watchpoint, and program counter polling. Using expressive triggers that the on-chip debug module supports, Aveksha can watch for, and record, a variety of programmable events of interest. A key feature of Aveksha is that the target processor does not have to be stopped during event logging (in the last two of the three modes), subject to a limit on the rate at which logged events occur. Aveksha also performs power monitoring of the embedded wireless node and, importantly, enables power consumption data to be correlated to events of interest.Aveksha is an operating system-agnostic solution. We demonstrate its functionality and performance using three applications running on Telos motes; two in TinyOS and one in Contiki. We show that Aveksha can trace tasks and other generic events at the function and task-level granularity. We also describe how we used Aveksha to find a subtle bug in the TinyOS low power listening protocol.},
booktitle = {Proceedings of the 9th ACM Conference on Embedded Networked Sensor Systems},
pages = {288–301},
numpages = {14},
keywords = {wireless sensor network, tracing, debugging, JTAG},
location = {Seattle, Washington},
series = {SenSys '11}
}

@article{10.1145/3448976,
author = {Chen, Boyuan and Jiang, Zhen Ming (Jack)},
title = {A Survey of Software Log Instrumentation},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3448976},
doi = {10.1145/3448976},
abstract = {Log messages have been used widely in many software systems for a variety of purposes during software development and field operation. There are two phases in software logging: log instrumentation and log management. Log instrumentation refers to the practice that developers insert logging code into source code to record runtime information. Log management refers to the practice that operators collect the generated log messages and conduct data analysis techniques to provide valuable insights of runtime behavior. There are many open source and commercial log management tools available. However, their effectiveness highly depends on the quality of the instrumented logging code, as log messages generated by high-quality logging code can greatly ease the process of various log analysis tasks (e.g., monitoring, failure diagnosis, and auditing). Hence, in this article, we conducted a systematic survey on state-of-the-art research on log instrumentation by studying 69 papers between 1997 and 2019. In particular, we have focused on the challenges and proposed solutions used in the three steps of log instrumentation: (1) logging approach; (2) logging utility integration; and (3) logging code composition. This survey will be useful to DevOps practitioners and researchers who are interested in software logging.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {90},
numpages = {34},
keywords = {instrumentation, software logging, Systematic survey}
}

@inproceedings{10.1109/ISCA.2018.00043,
author = {Bae, Duck-Ho and Jo, Insoon and Choi, Youra Adel and Hwang, Joo-Young and Cho, Sangyeun and Lee, Dong-Gi and Jeong, Jaeheon},
title = {2B-SSD: The Case for Dual, Byte- and Block-Addressable Solid-State Drives},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00043},
doi = {10.1109/ISCA.2018.00043},
abstract = {Performance critical transaction and storage systems require fast persistence of write data. Typically, a non-volatile RAM (NVRAM) is employed on the datapath to the permanent storage, to temporarily and quickly store write data before the system acknowledges the write request. NVRAM is commonly implemented with battery-backed DRAM. Unfortunately, battery-backed DRAM is small and costly, and occupies a precious DIMM slot. In this paper, we make a case for dual, byte- and block-addressable solid-state drive (2B-SSD), a novel NAND flash SSD architecture designed to offer a dual view of byte addressability and traditional block addressability at the same time. Unlike a conventional storage device, 2B-SSD allows accessing the same file with two independent byte-and block- I/O paths. It controls the data transfer between its internal DRAM and NAND flash memory through an intuitive software interface, and manages the mapping of the two address spaces. 2B-SSD realizes a wholly different way and speed of accessing files on a storage device; applications can access them directly using memory-mapped I/O, and moreover write with a DRAM-like latency. To quantify the benefits of 2B-SSD, we modified logging subsystems of major database engines to store log records directly on it without buffering them in the host memory. When running popular workloads, we measured throughput gains in the range of 1.2X and 2.8X with no risk of data loss.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {425–438},
numpages = {14},
keywords = {non-volatile memory, 2B-SSD, WAL},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1145/1982185.1982229,
author = {Kontogiannis, Kostas and Wasfy, Ahmed and Mankovskii, Serge},
title = {Event Clustering for Log Reduction and Run Time System Understanding},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982229},
doi = {10.1145/1982185.1982229},
abstract = {Large software systems are constantly monitored so that audits can be initiated, once a failure occurs or when maintenance operations are performed. However, the log files are usually sizeable, and require filtering and reduction in order to be processed efficiently. In this paper, we define the concept of the Event Dependency Graph, and we discuss an event filtering and a use case identification technique, that is based on event clustering. This technique can be used to reduce the size of system logs and assist on system analysis and, program understanding.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {191–192},
numpages = {2},
keywords = {use case determination, log reduction, log analysis, software systems, clustering},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@inproceedings{10.1145/3377812.3382168,
author = {Li, Zhenhao},
title = {Studying and Suggesting Logging Locations in Code Blocks},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3382168},
doi = {10.1145/3377812.3382168},
abstract = {Developers write logging statements to generate logs and record system execution behaviors to assist in debugging and software maintenance. However, there exists no practical guidelines on where to write logging statements. On one hand, adding too many logging statements may introduce superfluously trivial logs and performance overheads. On the other hand, logging too little may miss necessary runtime information. Thus, properly deciding the logging location is a challenging task and a finer-grained understanding of where to write logging statements is needed to assist developers in making logging decisions. In this paper, we conduct a comprehensive study to uncover guidelines on logging locations at the code block level. We analyze logging statements and their surrounding code by combining both deep learning techniques and manual investigations. From our preliminary results, we find that our deep learning models achieve over 90% in precision and recall when trained using the syntactic (e.g., nodes in abstract syntax tree) and semantic (e.g., variable names) features. However, cross-system models trained using semantic features only have 45.6% in precision and 73.2% in recall, while models trained using syntactic features still have over 90% precision and recall. Our current progress highlights that there is an implicit syntactic logging guideline across systems, and such information may be leveraged to uncover general logging guidelines.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {125–127},
numpages = {3},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/2839509.2844590,
author = {Nandi, Arnab and Mandernach, Meris},
title = {Hackathons as an Informal Learning Platform},
year = {2016},
isbn = {9781450336857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2839509.2844590},
doi = {10.1145/2839509.2844590},
abstract = {Hackathons are fast-paced events where competitors work in teams to go from an idea to working software or hardware within a single day or a weekend and demonstrate their creation to a live audience of peers. Due to the "fun" and informal nature of such events, they make for excellent informal learning platforms that attract a diverse spectrum of students, especially those typically uninterested in traditional classroom settings. In this paper, we investigate the informal learning aspects of Ohio State's annual hackathon events over the past two years, with over 100 student participants in 2013 and over 200 student participants in 2014. Despite the competitive nature of such events, we observed a significant amount of peer-learning -- students teaching each other how to solve specific challenges and learn new skills. The events featured mentors from both the university and industry, who provided round-the-clock hands-on support, troubleshooting and advice. Due to the gamified format of the events, students were heavily motivated to learn new skills due to practical applicability and peer effects, rather than merely academic metrics. Some teams continued their hacks as long-term projects, while others formed new student groups to host lectures and practice building prototypes on a regular basis. Using a combined analysis of post-event surveys, student academic records and source-code commit log data from the event, we share insights, demographics, statistics and anecdotes from hosting these hackathons.},
booktitle = {Proceedings of the 47th ACM Technical Symposium on Computing Science Education},
pages = {346–351},
numpages = {6},
keywords = {informal learning, hackathon},
location = {Memphis, Tennessee, USA},
series = {SIGCSE '16}
}

@inproceedings{10.1145/2797022.2797028,
author = {Ren, Shiru and Li, Chunqi and Tan, Le and Xiao, Zhen},
title = {Samsara: Efficient Deterministic Replay with Hardware Virtualization Extensions},
year = {2015},
isbn = {9781450335546},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797022.2797028},
doi = {10.1145/2797022.2797028},
abstract = {Deterministic replay, which provides the ability to travel backward in time and reconstructs the past execution flow of a multi-processor system, has many prominent applications including cyclic debugging, intrusion detection, malware analysis, and fault tolerance. Previous software-only schemes cannot take advantage of modern hardware support for replay and suffer from excessive performance overhead. They also produce huge log sizes due to the inherent draw-backs of the point-to-point logging approach used. In this paper, we propose a novel approach, called Samsara, which uses hardware-assisted virtualization (HAV) extensions to achieve an efficient software-based replay system. Unlike previous software-only schemes that record dependences between individual instructions, we record processors' execution as a series of chunks. By leveraging HAV extensions, we avoid the large number of memory access detections which are a major source of overhead in the previous work and instead perform a single extended page table (EPT) traversal at the end of each chunk. We have implemented and evaluated our system on KVM with Intel's Haswell processor. Evaluation results show that our system incurs less than 3X overhead when compared to native execution with two processors while the overhead in other state-of-the-art work is much more than 10X. Our system improves recording performance dramatically with a log size even smaller than that in hardware-based scheme.},
booktitle = {Proceedings of the 6th Asia-Pacific Workshop on Systems},
articleno = {9},
numpages = {7},
location = {Tokyo, Japan},
series = {APSys '15}
}

@inproceedings{10.1145/3095786.3095788,
author = {Abe, Hiroshi and Shima, Keiichi and Sekiya, Yuji and Miyamoto, Daisuke and Ishihara, Tomohiro and Okada, Kazuya},
title = {Hayabusa: Simple and Fast Full-Text Search Engine for Massive System Log Data},
year = {2017},
isbn = {9781450353328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3095786.3095788},
doi = {10.1145/3095786.3095788},
abstract = {In this study, we introduce a simple and high-speed search engine for large-scale system logs, called Hayabusa. Hayabusa uses SQLite, standard lightweight database software with GNU Parallel and general Linux commands, such that it can run efficiently without complex components. Network administrators can use Hayabusa to accumulate and store log information at high speeds and to search the logs quickly.In our experiments, Hayabusa required only 8 seconds to convert 1.2 M log messages into a database file. Moreover, Hayabusa required only 5 seconds to search a keyword from 1.7 billion records. Hayabusa achieved high-performance search speed in a stand-alone environment without a complex distributed environment. Compared with the distributed environment, Spark, the proposed stand-alone Hayabusa was approximately 27 times faster.},
booktitle = {Proceedings of the 12th International Conference on Future Internet Technologies},
articleno = {2},
numpages = {7},
keywords = {Parallel System, SQL, Data Processing, Parallel Processing},
location = {Fukuoka, Japan},
series = {CFI'17}
}

@inproceedings{10.1145/3184407.3184440,
author = {Lyu, Michael R.},
title = {AI Techniques in Software Engineering Paradigm},
year = {2018},
isbn = {9781450350952},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3184407.3184440},
doi = {10.1145/3184407.3184440},
abstract = {In the next decade, Artificial Intelligent (AI) techniques can see wide adoption in our daily life to release human burden. In our recent Software Engineering research, we investigated on the design of novel AI methods to facilitate all three major phases in software engineering: development, operation, and analysis. In this talk, I will first introduce the AI techniques we employed, including machine learning framework, classification, clustering, matrix factorization, topic modeling, deep learning, and parallel computing platform. Then I will explain the challenges in each phase and describe our recently proposed methodologies. First in development phase, we suggested an automated code completion technique via deep learning. Our technique learns the code style from lots of existing code bases, and recommends the most suitable token based on the trained deep learning model and current coding context. Besides, to help developers in conducting effective logging, we designed a tool named LogAdvisor, which tells developers whether they should write a logging statement in the current code block or not. Secondly, in operation phase, we implemented a continuous and passive authentication method for mobile phones based on user touch biometrics. Different from the traditional password authentication scheme, our method can recognize malicious attackers based on abnormal user behaviors. Moreover, we developed PAID, which automatically prioritizes app issues by mining user reviews. Finally, in analysis phase, we designed systematic data analytics techniques for software reliability prediction. Besides, to make full use of the crucial runtime information, we proposed effective methods for every step in log analysis, including log parsing, feature extraction, and log mining. Furthermore, we developed a CNN-based defect prediction method to help developers find the buggy code. In the end, we expect to establish a comprehensive framework for systematic employment of AI techniques in the Software Engineering paradigm.},
booktitle = {Proceedings of the 2018 ACM/SPEC International Conference on Performance Engineering},
pages = {2},
numpages = {1},
keywords = {software engineering, artificial intelligence},
location = {Berlin, Germany},
series = {ICPE '18}
}

@inproceedings{10.1109/ICSSP.2019.00020,
author = {Ardimento, Pasquale and Bernardi, Mario Luca and Cimitile, Marta and Maggi, Fabrizio Maria},
title = {Evaluating Coding Behavior in Software Development Processes: A Process Mining Approach},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSSP.2019.00020},
doi = {10.1109/ICSSP.2019.00020},
abstract = {Process mining is a family of techniques that aim at analyzing business process execution data recorded in event logs. Conformance checking is a branch of this discipline embracing approaches for verifying whether the behavior of a process, as recorded in a log, is in line with some expected behavior provided in the form of a process model. In the literature, process mining techniques have already been used to study software development processes starting from logs derived from version management systems or from document management systems. In this paper, we use conformance checking to test coding behaviors starting from event logs generated from IDE usage. Understanding how developers carry out coding activities and what hurdles they usually face should provide useful tips for improving and supporting software development processes. In particular, through conformance checking, we can compare different process executions, and identify behavioral similarities and differences. In our experimentation, we evaluated the activities performed by 40 novice developers performing coding activities in 5 development sessions. We assessed the developers to distinguish the ones obtaining the best performance. We then compared the behavior extracted from this group of developers with the others. The results show different IDE usage patterns for developers with different skills and performance.},
booktitle = {Proceedings of the International Conference on Software and System Processes},
pages = {84–93},
numpages = {10},
keywords = {process mining, conformance checking, IDE logging, software development process},
location = {Montreal, Quebec, Canada},
series = {ICSSP '19}
}

@inproceedings{10.1109/ICSE-Companion.2019.00062,
author = {Li, Zhenhao},
title = {Characterizing and Detecting Duplicate Logging Code Smells},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion.2019.00062},
doi = {10.1109/ICSE-Companion.2019.00062},
abstract = {Software logs are widely used by developers to assist in various tasks. Despite the importance of logs, prior studies show that there is no industrial standard on how to write logging statements. Recent research on logs often only considers the appropriateness of a log as an individual item (e.g., one single logging statement); while logs are typically analyzed in tandem. In this paper, we focus on studying duplicate logging statements, which are logging statements that have the same static text message. Such duplications in the text message are potential indications of logging code smells, which may affect developers' understanding of the dynamic view of the system. We manually studied over 3K duplicate logging statements and their surrounding code in four large-scale open source systems and uncovered five patterns of duplicate logging code smells. For each instance of the problematic code smell, we contact developers in order to verify our manual study result. We integrated our manual study result and developers' feedback into our automated static analysis tool, DLFinder, which automatically detects problematic duplicate logging code smells. We evaluated DLFinder on the manually studied systems and two additional systems. In total, combining the results of DLFinder and our manual analysis, DLFinder is able to detect over 85% of the instances which were reported to developers and then fixed.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings},
pages = {147–149},
numpages = {3},
keywords = {static analysis, code smell, duplicate log, log},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3328905.3329511,
author = {Lin, Wei-Tsung and Bakir, Fatih and Krintz, Chandra and Wolski, Rich and Mock, Markus},
title = {Data Repair for Distributed, Event-Based IoT Applications},
year = {2019},
isbn = {9781450367943},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328905.3329511},
doi = {10.1145/3328905.3329511},
abstract = {Motivated by the growth of Internet of Things (IoT) technologies and the volumes and velocity of data that they can and will produce, we investigate automated data repair for event-driven, IoT applications. IoT devices are heterogeneous in their hardware architectures, software, size, cost, capacity, network capabilities, power requirements, etc. They must execute in a wide range of operating environments where failures and degradations of service due to hardware malfunction, software bugs, network partitions, etc. cannot be immediately remediated. Further, many of these failure modes cause corruption in the data that these devices produce and in the computations "downstream" that depend on this data.To "repair" corrupted data from its origin through its computational dependencies in a distributed IoT setting, we explore SANS-SOUCI--a system for automatically tracking causal data dependencies and re-initiating dependent computations in event-driven IoT deployment frameworks. SANS-SOUCI presupposes an event-driven programming model based on cloud functions, which we extend for portable execution across IoT tiers (device, edge, cloud). We add fast, persistent, append-only storage and versioning for efficient data robustness and durability. SANS-SOUCI records events and their causal dependencies using a distributed event log and repairs applications dynamically, across tiers via replay. We evaluate SANS-SOUCI using a portable, open source, distributed IoT platform, example applications, and microbenchmarks. We find that SANS-SOUCI is able to perform repair for both software (function) and sensor produced data corruption with very low overhead.},
booktitle = {Proceedings of the 13th ACM International Conference on Distributed and Event-Based Systems},
pages = {139–150},
numpages = {12},
keywords = {cloud functions, serverless, IoT, replay},
location = {Darmstadt, Germany},
series = {DEBS '19}
}

@inproceedings{10.1145/3167486.3167553,
author = {El abdelkhalki, Jamal and Ben ahmed, Mohamed and Anouar, Boudhir Hakim},
title = {Classification and Exploration of TSM Log File Based on Datamining Algorithms},
year = {2017},
isbn = {9781450353069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167486.3167553},
doi = {10.1145/3167486.3167553},
abstract = {analyzing the log file for software or device provides a focal point for making incremental improvements; it is the performed step to start the incident analysis. Although, log messages format or contents may not always be fully documented, and described in many different formats. It makes the log analysis task more difficult, affects the correction deadline of incidents and therefore involves a high financial risk. In this paper, we survey the log file analysis and the existing systems elaborated to resolve current issue. Then, we propose a methodology to support the log analysis in the complex environment related to big data issues. Finally, we illustrate our proposal on the file log of the Tivoli Storage Manager (TSM) and provide a discussion of the result clusters.},
booktitle = {Proceedings of the 2nd International Conference on Computing and Wireless Communication Systems},
articleno = {64},
numpages = {7},
keywords = {Knowledge extraction, Clustering, Classification, Data analysis, Big Data, Data Mining, TSM},
location = {Larache, Morocco},
series = {ICCWCS'17}
}

@inproceedings{10.1145/3368756.3369069,
author = {Abdelkhalki, J. El and Ahmed, M. Ben and Slimani, A.},
title = {Incident Prediction through Logging Management and Machine Learning},
year = {2019},
isbn = {9781450362894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368756.3369069},
doi = {10.1145/3368756.3369069},
abstract = {Analyzing the log file for software or device provides a focal point for making incremental improvements; it is the performed step to start the incident analysis. Although, log messages format or contents may not always be fully documented, and described in many different formats. It makes the log analysis task more difficult, affects the correction deadline of incidents and therefore involves a high financial risk.In this paper, we survey the log file analysis and the existing systems elaborated to resolve current issue. Then, we propose a methodology to support the log analysis in the complex environment. The KN-K-Nearest-Neighbor (KNN) classification method was chosed to be used online by weka to predict the error. Therefore, a program was developed in python to extract, clean and format the log file before comparing the different algorithms of the classifiation method KNN, J48 and Bayes - NaiveBayes in the context of dataset.API was used in order to process Weka.Finally, we illustrate our proposal in the Tivoli Storage Manager (TSM) file log and provide a description of the results obtained.},
booktitle = {Proceedings of the 4th International Conference on Smart City Applications},
articleno = {83},
numpages = {8},
keywords = {KNN, Bayes-Na\"{\i}ve Bayes, data analysis, data mining, TSM, J48, knowledge extraction, classification, clustering},
location = {Casablanca, Morocco},
series = {SCA '19}
}

@inproceedings{10.1109/ICSE.2019.00032,
author = {Li, Zhenhao and Chen, Tse-Hsun (Peter) and Yang, Jinqiu and Shang, Weiyi},
title = {Dlfinder: Characterizing and Detecting Duplicate Logging Code Smells},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00032},
doi = {10.1109/ICSE.2019.00032},
abstract = {Developers rely on software logs for a wide variety of tasks, such as debugging, testing, program comprehension, verification, and performance analysis. Despite the importance of logs, prior studies show that there is no industrial standard on how to write logging statements. Recent research on logs often only considers the appropriateness of a log as an individual item (e.g., one single logging statement); while logs are typically analyzed in tandem. In this paper, we focus on studying duplicate logging statements, which are logging statements that have the same static text message. Such duplications in the text message are potential indications of logging code smells, which may affect developers' understanding of the dynamic view of the system. We manually studied over 3K duplicate logging statements and their surrounding code in four large-scale open source systems: Hadoop, CloudStack, ElasticSearch, and Cassandra. We uncovered five patterns of duplicate logging code smells. For each instance of the code smell, we further manually identify the problematic (i.e., require fixes) and justifiable (i.e., do not require fixes) cases. Then, we contact developers in order to verify our manual study result. We integrated our manual study result and developers' feedback into our automated static analysis tool, DLFinder, which automatically detects problematic duplicate logging code smells. We evaluated DLFinder on the four manually studied systems and two additional systems: Camel and Wicket. In total, combining the results of DLFinder and our manual analysis, we reported 82 problematic code smell instances to developers and all of them have been fixed.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {152–163},
numpages = {12},
keywords = {log, duplicate log, static analysis, empirical study, code smell},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.5555/2888619.2888731,
author = {Rai, Sudhendu and Daniels, Marc},
title = {An Event-Log Analysis and Simulation-Based Approach for Quantifying Sustainability Metrics in Production Facilities},
year = {2015},
isbn = {9781467397414},
publisher = {IEEE Press},
abstract = {This paper describes a discrete-event simulation and event-log analysis based approach for computing sustainability metrics in production environments to perform various types of comparative analysis and assessments. Event logs collected from the production environment are analyzed to compute current state sustainability metrics such as energy usage, carbon footprint and heating/cooling requirements. Bootstrapping based forecasting leveraging expert input is utilized to estimate future demand. The forecasted demand is then simulated to predict sustainability metrics. The discrete-event simulation results from the forecasted data and computation of heat produced is combined with thermodynamic models of heat transfer through the thermal envelope of the facility to provide more accurate estimates of true carbon footprint associated with the production operations while also enabling cross-comparative studies of setting operations in different geographical locations. The framework and software tool enables the integration of productivity metrics and sustainability metrics in decision-making process for designing and operating production environments.},
booktitle = {Proceedings of the 2015 Winter Simulation Conference},
pages = {1033–1043},
numpages = {11},
location = {Huntington Beach, California},
series = {WSC '15}
}

@inproceedings{10.1145/2925426.2926264,
author = {Qian, Xuehai and Sen, Koushik and Hargrove, Paul and Iancu, Costin},
title = {SReplay: Deterministic Sub-Group Replay for One-Sided Communication},
year = {2016},
isbn = {9781450343619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2925426.2926264},
doi = {10.1145/2925426.2926264},
abstract = {Replay of parallel execution is required by HPC debuggers and resilience mechanisms. Up-to-date, there is no existing deterministic replay solution for one-sided communication. The essential problem is that the readers of updated data do not have any information on which remote threads produced the updates, the conventional happens-before based ordering tracking techniques are challenging to work at scale. This paper presents SReplay, the first software tool for sub-group deterministic record and replay for one-sided communication. SReplay allows the user to specify and record the execution of a set of threads of interest (sub-group), and then deterministically replays the execution of the sub-group on a local machine without starting the remaining threads. SReplay ensures sub-group determinism using a hybrid data- and order-replay technique. SReplay maintains scalability by a combination of local logging and approximative event order tracking within sub-group. Our evaluation on deterministic and nondeterministic UPC programs shows that SReplay introduces an overhead ranging from 1.3x to 29x, when running on 1,024 cores and tracking up to 16 threads.},
booktitle = {Proceedings of the 2016 International Conference on Supercomputing},
articleno = {17},
numpages = {13},
keywords = {Debugging Tools, One-Sided Communication, PGAS},
location = {Istanbul, Turkey},
series = {ICS '16}
}

@inproceedings{10.5555/2820518.2820560,
author = {Gupta, Monika and Sureka, Ashish and Padmanabhuni, Srinivas and Asadullah, Allahbaksh Mohammedali},
title = {Identifying Software Process Management Challenges: Survey of Practitioners in a Large Global IT Company},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {Process mining consists of mining event logs generated from business process execution supported by Information Systems (IS). Process mining of software repositories has diverse applications because vast data is generated during Software Development Life Cycle (SDLC) and archived in IS such as Version Control System (VCS), Peer Code Review (PCR) System, Issue Tracking System (ITS), and mail archives. There is need to explore its applications on different repositories to aid managers in process management. We conduct two phase surveys and interviews with managers in a large, global, IT company. The first survey and in-person interviews identify the process challenges encountered by them that can be addressed by novel applications of process mining. We filter, group and abstract responses formulating 30 generic problem statements. On the basis of process mining type, we classify identified problems to eight categories such as control analysis, organizational analysis, conformance analysis, and preventive analysis. The second survey asks distinct participants the importance of solving identified problems. We calculate proposed Net Importance Metric (NIM) using 1262 ratings from 43 participants. Combined analysis of NIM and first survey responses reveals that the problems mentioned by few practitioners in first survey are considered important by majority in the second survey. We elaborate on possible solutions and challenges for most frequent and important problems. We believe solving these validated problems will help managers in improving project quality and productivity.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {346–356},
numpages = {11},
keywords = {software development life cycle, qualitative study, process mining, software repositories},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1145/3106237.3106266,
author = {Yu, Tingting and Zaman, Tarannum S. and Wang, Chao},
title = {DESCRY: Reproducing System-Level Concurrency Failures},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106266},
doi = {10.1145/3106237.3106266},
abstract = { Concurrent systems may fail in the field due to various elusive faults such as race conditions. Reproducing such failures is hard because (1) concurrency failures at the system level often involve multiple processes or event handlers (e.g., software signals), which cannot be handled by existing tools for reproducing intra-process (thread-level) failures; (2) detailed field data, such as user input, file content and interleaving schedule, may not be available to developers; and (3) the debugging environment may differ from the deployed environment, which further complicates failure reproduction. To address these problems, we present DESCRY, the first fully automated tool for reproducing system-level concurrency failures based only on default log messages collected from the field. DESCRY uses a combination of static and dynamic analysis techniques, together with symbolic execution, to synthesize both the failure-inducing data input and the interleaving schedule, and leverages them to deterministically replay the failed execution using existing virtual platforms. We have evaluated DESCRY on 22 real-world multi-process Linux applications with a total of 236,875 lines of code to demonstrate both its effectiveness and its efficiency in reproducing failures that no other tool can reproduce. },
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {694–704},
numpages = {11},
keywords = {Concurrency Failures, Multi-Process Applications, Failure Reproduction, Debugging},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/3184407.3184416,
author = {Yao, Kundi and B. de P\'{a}dua, Guilherme and Shang, Weiyi and Sporea, Steve and Toma, Andrei and Sajedi, Sarah},
title = {Log4Perf: Suggesting Logging Locations for Web-Based Systems' Performance Monitoring},
year = {2018},
isbn = {9781450350952},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3184407.3184416},
doi = {10.1145/3184407.3184416},
abstract = {Performance assurance activities are an essential step in the release cycle of software systems. Logs have become one of the most important sources of information that is used to monitor, understand and improve software performance. However, developers often face the challenge of making logging decisions, i.e., neither logging too little and logging too much is desirable. Although prior research has proposed techniques to assist in logging decisions, those automated logging guidance techniques are rather general, without considering a particular goal, such as monitoring software performance. In this paper, we present Log4Perf, an automated approach that provides suggestions of where to insert logging statement with the goal of monitoring web-based systems» software performance. In particular, our approach builds and manipulates a statistical performance model to identify the locations in the source code that statistically significantly influences software performance. To evaluate Log4Perf, we conduct case studies on open source system, i.e., CloudStore and OpenMRS, and one large-scale commercial system. Our evaluation results show that Log4Perf can build well-fit statistical performance models, indicating that such models can be leveraged to investigate the influence of locations in the source code on performance. Also, the suggested logging locations are often small and simple methods that do not have logging statements and that are not performance hotspots, making our approach an ideal complement to traditional approaches that are based on software metrics or performance hotspots. Log4Perf is integrated into the release engineering process of the commercial software to provide logging suggestions on a regular basis.},
booktitle = {Proceedings of the 2018 ACM/SPEC International Conference on Performance Engineering},
pages = {127–138},
numpages = {12},
keywords = {performance modeling, logging suggestion, performance monitoring, performance engineering, software logs},
location = {Berlin, Germany},
series = {ICPE '18}
}

@inproceedings{10.1145/3196321.3196328,
author = {Li, Shanshan and Niu, Xu and Jia, Zhouyang and Wang, Ji and He, Haochen and Wang, Teng},
title = {Logtracker: Learning Log Revision Behaviors Proactively from Software Evolution History},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3196328},
doi = {10.1145/3196321.3196328},
abstract = {Log statements are widely used for postmortem debugging. Despite the importance of log messages, it is difficult for developers to establish good logging practices. There are two main reasons for this. First, there are no rigorous specifications or systematic processes to guide the practices of software logging. Second, logging code co-evolves with bug fixes or feature updates. While previous works on log enhancement have successfully focused on the first problem, they are hard to solve the latter. For taking the first step towards solving the second problem, this paper is inspired by code clones and assumes that logging code with similar context is pervasive in software and deserves similar modifications. To verify our assumptions, we conduct an empirical study on eight open-source projects. Based on the observation, we design and implement LogTracker, an automatic tool that can predict log revisions by mining the correlation between logging context and modifications. With an enhanced modeling of logging context, LogTracker is able to guide more intricate log revisions that cannot be covered by existing tools. We evaluate the effectiveness of LogTracker by applying it to the latest version of subject projects. The results of our experiments show that LogTracker can detect 199 instances of log revisions. So far, we have reported 25 of them, and 6 have been accepted.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {178–188},
numpages = {11},
keywords = {software evolution, log revision, failure diagnose},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@inproceedings{10.1145/3351095.3372841,
author = {Borradaile, Glencora and Burkhardt, Brett and LeClerc, Alexandria},
title = {Whose Tweets Are Surveilled for the Police: An Audit of a Social-Media Monitoring Tool via Log Files},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372841},
doi = {10.1145/3351095.3372841},
abstract = {Social media monitoring by law enforcement is becoming commonplace, but little is known about what software packages for it do. Through public records requests, we obtained log files from the Corvallis (Oregon) Police Department's use of social media monitoring software called DigitalStakeout. These log files include the results of proprietary searches by DigitalStakeout that were running over a period of 13 months and include 7240 social media posts. In this paper, we focus on the Tweets logged in this data and consider the racial and ethnic identity (through manual coding) of the users that are therein flagged by DigitalStakeout. We observe differences in the demographics of the users whose Tweets are flagged by DigitalStakeout compared to the demographics of the Twitter users in the region, however, our sample size is too small to determine significance. Further, the demographics of the Twitter users in the region do not seem to reflect that of the residents of the region, with an apparent higher representation of Black and Hispanic people. We also reconstruct the keywords related to a Narcotics report set up by DigitalStakeout for the Corvallis Police Department and find that these keywords flag Tweets unrelated to narcotics or flag Tweets related to marijuana, a drug that is legal for recreational use in Oregon. Almost all of the keywords have a common meaning unrelated to narcotics (e.g. broken, snow, hop, high) that call into question the utility that such a keyword based search could have to law enforcement.As social media monitoring is increasingly used for law enforcement purposes, racial biases in surveillance may contribute to existing racial disparities in law enforcement practices. We are hopeful that log files obtainable through public records request will shed light on the operation of these surveillance tools. There are challenges in auditing these tools: public records requests may go unfulfilled even if the data is available, social media platforms may not provide comparable data for comparison with surveillance data, demographics can be difficult to ascertain from social media and Institutional Review Boards may not understand how to weigh the ethical considerations involved in this type of research. We include in this paper a discussion of our experience in navigating these issues.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {570–580},
numpages = {11},
keywords = {keywords, audit, surveillance, police, demographics, social media monitoring},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/2746194.2746200,
author = {King, Jason and Pandita, Rahul and Williams, Laurie},
title = {Enabling Forensics by Proposing Heuristics to Identify Mandatory Log Events},
year = {2015},
isbn = {9781450333764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2746194.2746200},
doi = {10.1145/2746194.2746200},
abstract = {Software engineers often implement logging mechanisms to debug software and diagnose faults. As modern software manages increasingly sensitive data, logging mechanisms also need to capture detailed traces of user activity to enable forensics and hold users accountable. Existing techniques for identifying what events to log are often subjective and produce inconsistent results. The objective of this study is to help software engineers strengthen forensic-ability and user accountability by 1) systematically identifying mandatory log events through processing of unconstrained natural language software artifacts; and 2) proposing empirically-derived heuristics to help determine whether an event must be logged. We systematically extract each verb and object being acted upon from natural language software artifacts for three open-source software systems. We extract 3,513 verb-object pairs from 2,128 total sentences studied. Two raters classify each verb-object pair as either a mandatory log event or not. Through grounded theory analysis of discussions to resolve disagreements between the two raters, we develop 12 heuristics to help determine whether a verb-object pair describes an action that must be logged. Our heuristics help resolve 882 (96%) of 919 disagreements between the two raters. In addition, our results demonstrate that the proposed heuristics facilitate classification of 3,372 (96%) of 3,513 extracted verb-object pairs as either mandatory log events or not.},
booktitle = {Proceedings of the 2015 Symposium and Bootcamp on the Science of Security},
articleno = {6},
numpages = {11},
keywords = {natural language, forensics, logging, nonrepudiation, software requirements, security, accountability},
location = {Urbana, Illinois},
series = {HotSoS '15}
}

@inproceedings{10.1145/3400286.3418263,
author = {Das, Dipta and Schiewe, Micah and Brighton, Elizabeth and Fuller, Mark and Cerny, Tomas and Bures, Miroslav and Frajtak, Karel and Shin, Dongwan and Tisnovsky, Pavel},
title = {Failure Prediction by Utilizing Log Analysis: A Systematic Mapping Study},
year = {2020},
isbn = {9781450380256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400286.3418263},
doi = {10.1145/3400286.3418263},
abstract = {In modern computing, log files provide a wealth of information regarding the past of a system, including the system failures and security breaches that cost companies and developers a fortune in both time and money. While this information can be used to attempt to recover from a problem, such an approach merely mitigates the damage that has already been done. Detecting problems, however, is not the only information that can be gathered from log files. It is common knowledge that segments of log files, if analyzed correctly, can yield a good idea of what the system is likely going to do next in real-time, allowing a system to take corrective action before any negative actions occur. In this paper, the authors put forth a systematic map of this field of log prediction, screening several hundred papers and finally narrowing down the field to approximately 30 relevant papers. These papers, when broken down, give a good idea of the state of the art, methodologies employed, and future challenges that still must be overcome. Findings and conclusions of this study can be applied to a variety of software systems and components, including classical software systems, as well as software parts of control, or the Internet of Things (IoT) systems.},
booktitle = {Proceedings of the International Conference on Research in Adaptive and Convergent Systems},
pages = {188–195},
numpages = {8},
keywords = {Error Logs, Failure Prediction, Log Analysis, Mapping Study, Defect Prediction},
location = {Gwangju, Republic of Korea},
series = {RACS '20}
}

@inproceedings{10.1145/2737095.2741839,
author = {Tancreti, Matthew and Sundaram, Vinaitheerthan and Bagchi, Saurabh and Eugster, Patrick},
title = {Software-Only System-Level Record and Replay in Wireless Sensor Networks},
year = {2015},
isbn = {9781450334754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737095.2741839},
doi = {10.1145/2737095.2741839},
abstract = {Wireless sensor networks (WSNs) are plagued by the possibility of bugs manifesting only at deployment. However, debugging deployed WSNs is challenging for several reasons---the remote location of deployed sensor nodes, the non- determinism of execution that can make it difficult to replicate a buggy run, and the limited hardware resources available on a node. In particular, existing solutions to record and replay debugging in WSNs fail to capture the complete code execution, thus negating the possibility of a faithful replay and causing a large class of bugs to go unnoticed. In short, record and replay logs a trace of predefined events while a deployed application is executing, enabling replaying of events later using debugging tools. Existing recording methods fail due to the many sources of non-determinism and the scarcity of resources on nodes.In this demo, we present Trace And Replay Debugging In Sensornets (Tardis), a software-only approach for deterministic record and replay of WSN nodes. Tardis is able to record all sources of non-determinism, based on the observation that such information is compressible using a combination of techniques specialized for respective sources. Despite their domain-specific nature, the techniques presented are applicable to the broader class of resource-constrained embedded systems.},
booktitle = {Proceedings of the 14th International Conference on Information Processing in Sensor Networks},
pages = {400–401},
numpages = {2},
keywords = {replay, debugging, wireless sensor networks, tracing},
location = {Seattle, Washington},
series = {IPSN '15}
}

@inproceedings{10.1145/3276945.3276952,
author = {Marron, Mark},
title = {Log++ Logging for a Cloud-Native World},
year = {2018},
isbn = {9781450360302},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3276945.3276952},
doi = {10.1145/3276945.3276952},
abstract = {Logging is a fundamental part of the software development and deployment lifecycle but logging support is often provided as an afterthought via limited library APIs or third-party modules. Given the critical nature of logging in modern cloud, mobile, and IoT development workflows, the unique needs of the APIs involved, and the opportunities for optimization using semantic knowledge, we argue logging should be included as a central part of the language and runtime designs. This paper presents a rethinking of the logger for modern cloud-native workflows.  Based on a set of design principles for modern logging we build a logging system, that supports near zero-cost for disabled log statements, low cost lazy-copying for enabled log statements, selective persistence of logging output, unified control of logging output across different libraries, and DevOps integration for use with modern cloud-based deployments. To evaluate these concepts we implemented the Log++ logger for Node.js hosted JavaScript applications.},
booktitle = {Proceedings of the 14th ACM SIGPLAN International Symposium on Dynamic Languages},
pages = {25–36},
numpages = {12},
keywords = {Logging, Runtime Monitoring, JavaScript},
location = {Boston, MA, USA},
series = {DLS 2018}
}

@article{10.1145/3393673.3276952,
author = {Marron, Mark},
title = {Log++ Logging for a Cloud-Native World},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3393673.3276952},
doi = {10.1145/3393673.3276952},
abstract = {Logging is a fundamental part of the software development and deployment lifecycle but logging support is often provided as an afterthought via limited library APIs or third-party modules. Given the critical nature of logging in modern cloud, mobile, and IoT development workflows, the unique needs of the APIs involved, and the opportunities for optimization using semantic knowledge, we argue logging should be included as a central part of the language and runtime designs. This paper presents a rethinking of the logger for modern cloud-native workflows.  Based on a set of design principles for modern logging we build a logging system, that supports near zero-cost for disabled log statements, low cost lazy-copying for enabled log statements, selective persistence of logging output, unified control of logging output across different libraries, and DevOps integration for use with modern cloud-based deployments. To evaluate these concepts we implemented the Log++ logger for Node.js hosted JavaScript applications.},
journal = {SIGPLAN Not.},
month = oct,
pages = {25–36},
numpages = {12},
keywords = {Runtime Monitoring, Logging, JavaScript}
}

@inproceedings{10.1145/2737095.2737096,
author = {Tancreti, Matthew and Sundaram, Vinaitheerthan and Bagchi, Saurabh and Eugster, Patrick},
title = {TARDIS: Software-Only System-Level Record and Replay in Wireless Sensor Networks},
year = {2015},
isbn = {9781450334754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737095.2737096},
doi = {10.1145/2737095.2737096},
abstract = {Wireless sensor networks (WSNs) are plagued by the possibility of bugs manifesting only at deployment. However, debugging deployed WSNs is challenging for several reasons---the remote location of deployed sensor nodes, the non-determinism of execution that can make it difficult to replicate a buggy run, and the limited hardware resources available on a node. In particular, existing solutions to record and replay debugging in WSNs fail to capture the complete code execution, thus negating the possibility of a faithful replay and causing a large class of bugs to go unnoticed. In short, record and replay logs a trace of predefined events while a deployed application is executing, enabling replaying of events later using debugging tools. Existing recording methods fail due to the many sources of non-determinism and the scarcity of resources on nodes.In this paper we introduce Trace And Replay Debugging In Sensornets (Tardis), a software-only approach for deterministic record and replay of WSN nodes. Tardis is able to record all sources of non-determinism, based on the observation that such information is compressible using a combination of techniques specialized for respective sources. Despite their domain-specific nature, the techniques presented are applicable to the broader class of resource-constrained embedded systems. We empirically demonstrate the viability of our approach and its effectiveness in diagnosing a newly discovered bug in a widely used routing protocol.},
booktitle = {Proceedings of the 14th International Conference on Information Processing in Sensor Networks},
pages = {286–297},
numpages = {12},
keywords = {tracing, replay, wireless sensor networks, debugging},
location = {Seattle, Washington},
series = {IPSN '15}
}

@inproceedings{10.1145/2499393.2499405,
author = {Zimmermann, Thomas},
title = {Software Analytics = Sharing Information},
year = {2013},
isbn = {9781450320160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499393.2499405},
doi = {10.1145/2499393.2499405},
abstract = {Software and its development generates an inordinate amount of data. Development activities such as check-ins, work items, bug reports, code reviews, and test executions are recorded in software repositories. User interactions that reflect how customers experience software are recorded in telemetry data, run-time traces, and log files and helps to track application and feature usage and expose performance and reliability. Software analytics takes this data and turns it into actionable insight to better inform decisions related to software. To a large extent, software analytics is about what we can learn and share about software. This include our own projects but also projects by others. Looking back at decades of research in empirical software engineering and mining software repositories, software analytics lets us share different things: insights, models, methods, and data. In this talk, I will present successful efforts from Microsoft and academia on software analytics and try to convince you that analytics is all about sharing information.},
booktitle = {Proceedings of the 9th International Conference on Predictive Models in Software Engineering},
articleno = {1},
numpages = {1},
location = {Baltimore, Maryland, USA},
series = {PROMISE '13}
}

@inproceedings{10.1145/2724660.2728688,
author = {Foltz, Peter W. and Rosenstein, Mark},
title = {Analysis of a Large-Scale Formative Writing Assessment System with Automated Feedback},
year = {2015},
isbn = {9781450334112},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2724660.2728688},
doi = {10.1145/2724660.2728688},
abstract = {Formative writing systems with automated scoring provide opportunities for students to write, receive feedback, and then revise essays in a timely iterative cycle. This paper describes ongoing investigations of a formative writing tool through mining student data in order to understand how the system performs and to measure improvement in student writing. The sampled data included over 1.3M student essays written in response to approximately 200 pre-defined prompts as well as a log of all student actions and computer generated feedback. Analyses both measured and modeled changes in student performance over revisions, the effects of system responses and the amount of time students spent working on assignments. Implications are discussed for employing large-scale data analytics to improve educational outcomes, to understand the role of feedback in writing, to drive improvements in formative technology and to aid in designing better kinds of feedback and scaffolding to support students in the writing process.},
booktitle = {Proceedings of the Second (2015) ACM Conference on Learning @ Scale},
pages = {339–342},
numpages = {4},
keywords = {educational data mining, data mining, log file analysis, automated essay scoring, large scale data analytics, educational software},
location = {Vancouver, BC, Canada},
series = {L@S '15}
}

@inproceedings{10.1145/2652524.2652583,
author = {Rubin, Vladimir A. and Mitsyuk, Alexey A. and Lomazova, Irina A. and van der Aalst, Wil M. P.},
title = {Process Mining Can Be Applied to Software Too!},
year = {2014},
isbn = {9781450327749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2652524.2652583},
doi = {10.1145/2652524.2652583},
abstract = {Modern information systems produce tremendous amounts of event data. The area of process mining deals with extracting knowledge from this data. Real-life processes can be effectively discovered, analyzed and optimized with the help of mature process mining techniques. There is a variety of process mining case studies and experience reports from such business areas as healthcare, public, transportation and education. Although nowadays, these techniques are mostly used for discovering business processes.The goal of this industrial paper is to show that process mining can be applied to software too. Here we present and analyze our experiences on applying process mining in different productive software systems used in the touristic domain. Process models and user interface workflows underlie the functional specifications of the systems we experiment with. When the systems are utilized, user interaction is recorded in event logs. After applying process mining methods to these logs, process and user interface flow models are automatically derived. These resulting models provide insight regarding the real usage of the software, motivate the changes in the functional specifications, enable usability improvements and software redesign.Thus, with the help of our examples we demonstrate that process mining facilitates new forms of software analysis. The user interaction with almost every software system can be mined in order to improve the software and to monitor and measure its real usage.},
booktitle = {Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {57},
numpages = {8},
keywords = {client technology, process mining, software process mining, user interface design},
location = {Torino, Italy},
series = {ESEM '14}
}

@inproceedings{10.5555/2818754.2818807,
author = {Zhu, Jieming and He, Pinjia and Fu, Qiang and Zhang, Hongyu and Lyu, Michael R. and Zhang, Dongmei},
title = {Learning to Log: Helping Developers Make Informed Logging Decisions},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {Logging is a common programming practice of practical importance to collect system runtime information for postmortem analysis. Strategic logging placement is desired to cover necessary runtime information without incurring unintended consequences (e.g., performance overhead, trivial logs). However, in current practice, there is a lack of rigorous specifications for developers to govern their logging behaviours. Logging has become an important yet tough decision which mostly depends on the domain knowledge of developers. To reduce the effort on making logging decisions, in this paper, we propose a "learning to log" framework, which aims to provide informative guidance on logging during development. As a proof of concept, we provide the design and implementation of a logging suggestion tool, LogAdvisor, which automatically learns the common logging practices on where to log from existing logging instances and further leverages them for actionable suggestions to developers. Specifically, we identify the important factors for determining where to log and extract them as structural features, textual features, and syntactic features. Then, by applying machine learning techniques (e.g., feature selection and classifier learning) and noise handling techniques, we achieve high accuracy of logging suggestions. We evaluate LogAdvisor on two industrial software systems from Microsoft and two open-source software systems from GitHub (totally 19.1M LOC and 100.6K logging statements). The encouraging experimental results, as well as a user study, demonstrate the feasibility and effectiveness of our logging suggestion tool. We believe our work can serve as an important first step towards the goal of "learning to log".},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {415–425},
numpages = {11},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/2947626.2951964,
author = {Bronte, Robert and Shahriar, Hossain and Haddad, Hisham M.},
title = {A Signature-Based Intrusion Detection System for Web Applications Based on Genetic Algorithm},
year = {2016},
isbn = {9781450347648},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2947626.2951964},
doi = {10.1145/2947626.2951964},
abstract = {Web application attacks are an extreme threat to the world's information technology infrastructure. A web application is generally defined as a client-server software application where the client uses a user interface within a web browser. Most users are familiar with web application attacks. For instance, a user may have received a link in an email that led the user to a malicious website. The most widely accepted solution to this threat is to deploy an Intrusion Detection System (IDS). Such a system currently relies on signatures of the predefined set of events matching with attacks. Issues still arise as all possible attack signatures may not be defined before deploying an IDS. Attack events may not fit with the pre-defined signatures. Thus, there is a need to detect new types of attacks with a mutated signature based detection approach. Most traditional literature works describe signature based IDSs for application layer attacks, but several works mention that not all attacks can be detected. It is well known that many security threats can be related to software or application development and design or implementation flaws. Given that fact, this work expands a new method for signature based web application layer attack detection. We apply a genetic algorithm to analyze web server and database logs and the log entries. The work contributes to the development of a mutated signature detection framework. The initial results show that the suggested approach can detect specific application layer attacks such as Cross-Site Scripting, SQL Injection and Remote File Inclusion attacks.},
booktitle = {Proceedings of the 9th International Conference on Security of Information and Networks},
pages = {32–39},
numpages = {8},
keywords = {mutation, selection, application layer attacks signatures, cross over, log analysis, Intrusion detection system, genetic algorithm},
location = {Newark, NJ, USA},
series = {SIN '16}
}

@inproceedings{10.1145/3238147.3238214,
author = {Chen, Boyuan and Song, Jian and Xu, Peng and Hu, Xing and Jiang, Zhen Ming (Jack)},
title = {An Automated Approach to Estimating Code Coverage Measures via Execution Logs},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238214},
doi = {10.1145/3238147.3238214},
abstract = {Software testing is a widely used technique to ensure the quality of software systems. Code coverage measures are commonly used to evaluate and improve the existing test suites. Based on our industrial and open source studies, existing state-of-the-art code coverage tools are only used during unit and integration testing due to issues like engineering challenges, performance overhead, and incomplete results. To resolve these issues, in this paper we have proposed an automated approach, called LogCoCo, to estimating code coverage measures using the readily available execution logs. Using program analysis techniques, LogCoCo matches the execution logs with their corresponding code paths and estimates three different code coverage criteria: method coverage, statement coverage, and branch coverage. Case studies on one open source system (HBase) and five commercial systems from Baidu and systems show that: (1) the results of LogCoCo are highly accurate (&gt;96% in seven out of nine experiments) under a variety of testing activities (unit testing, integration testing, and benchmarking); and (2) the results of LogCoCo can be used to evaluate and improve the existing test suites. Our collaborators at Baidu are currently considering adopting LogCoCo and use it on a daily basis.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {305–316},
numpages = {12},
keywords = {logging code, test coverage, software maintenance, empirical studies, software testing},
location = {Montpellier, France},
series = {ASE 2018}
}

@inproceedings{10.5555/2665671.2665737,
author = {Honarmand, Nima and Torrellas, Josep},
title = {Replay Debugging: Leveraging Record and Replay for Program Debugging},
year = {2014},
isbn = {9781479943944},
publisher = {IEEE Press},
abstract = {Hardware-assisted Record and Deterministic Replay (RnR) of programs has been proposed as a primitive for debugging hard-to-repeat software bugs. However, simply providing support for repeatedly stumbling on the same bug does not help diagnose it. For bug diagnosis, developers typically want to modify the code, e.g., by creating and operating on new variables, or printing state. Unfortunately, this renders the RnR log inconsistent and makes Replay Debugging (i.e., debugging while using an RnR log for replay) dicey at bestThis paper presents rdb, the first scheme for replay debugging that guarantees exact replay. rdb relies on two mechanisms. The first one is compiler support to split the instrumented application into two executables: one that is identical to the original program binary, and another that encapsulates all the added debug code. The second mechanism is a runtime infrastructure that replays the application and, without affecting it in any way, invokes the appropriate debug code at the appropriate locations. We describe an implementation of rdb based on LLVM and Pin, and show an example of how rdb's replay debugging helps diagnose a real bug},
booktitle = {Proceeding of the 41st Annual International Symposium on Computer Architecuture},
pages = {445–456},
numpages = {12},
location = {Minneapolis, Minnesota, USA},
series = {ISCA '14}
}

@article{10.1145/2678373.2665737,
author = {Honarmand, Nima and Torrellas, Josep},
title = {Replay Debugging: Leveraging Record and Replay for Program Debugging},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {3},
issn = {0163-5964},
url = {https://doi.org/10.1145/2678373.2665737},
doi = {10.1145/2678373.2665737},
abstract = {Hardware-assisted Record and Deterministic Replay (RnR) of programs has been proposed as a primitive for debugging hard-to-repeat software bugs. However, simply providing support for repeatedly stumbling on the same bug does not help diagnose it. For bug diagnosis, developers typically want to modify the code, e.g., by creating and operating on new variables, or printing state. Unfortunately, this renders the RnR log inconsistent and makes Replay Debugging (i.e., debugging while using an RnR log for replay) dicey at bestThis paper presents rdb, the first scheme for replay debugging that guarantees exact replay. rdb relies on two mechanisms. The first one is compiler support to split the instrumented application into two executables: one that is identical to the original program binary, and another that encapsulates all the added debug code. The second mechanism is a runtime infrastructure that replays the application and, without affecting it in any way, invokes the appropriate debug code at the appropriate locations. We describe an implementation of rdb based on LLVM and Pin, and show an example of how rdb's replay debugging helps diagnose a real bug},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {445–456},
numpages = {12}
}

@inproceedings{10.1145/3452383.3452385,
author = {Dasgupta, Gargi B.},
title = {AI and Its Applications in the Cloud Strategy},
year = {2021},
isbn = {9781450390460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452383.3452385},
doi = {10.1145/3452383.3452385},
abstract = { The fourth industrial revolution identifies cloud computing, data, and artificial intelligence (AI) as opportunity clusters with double digit growth in the next couple of years. As part of the cloud and digital transformation, the role of AI is crucial in enabling that transformation as well as creating the new breed of applications on top. AI mechanisms can help accelerate the modernization of applications, their management, and the testing on cloud architectures. I will focus on two sub-problems: 1) Refactoring of massive monolith applications using AI techniques. This problem statement is particularly relevant in understanding legacy un-optimized code and transforming them to be more cloud-ready. Microservices are indeed becoming the de-facto design choice for software architecture. It involves partitioning the software components into finer modules such that the development can happen independently [2]. It also provides natural benefits when deployed on the cloud since resources can be allocated dynamically to necessary components based on demand. We are exploring how AI can help accelerate the transformation of existing applications to microservices. 2) Detecting faults in application behavior at runtime from operational data. This problem statement is particularly relevant in understanding how to manage this new architecture of multiple microservices across the cloud stack [1], [3]. Operational data artifacts span across logs, metrics, tickets, and traces. Looking at signals across the artifacts and across the stack presents a challenging data correlation problem. AI mechanisms can help accelerate problem determination in these complex environments. I will also share my thoughts on how fundamental breakthroughs in AI Research will be needed as we address some of the core problems of cloud computing. },
booktitle = {14th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {2},
numpages = {1},
keywords = {code refactoring, AI Ops, log anomalies, hybrid cloud, modernization},
location = {Bhubaneswar, Odisha, India},
series = {ISEC 2021}
}

@inproceedings{10.1145/3121252.3127582,
author = {Williams, Laurie},
title = {Building Forensics in: Supporting the Investigation of Digital Criminal Activities (Invited Talk)},
year = {2017},
isbn = {9781450351560},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3121252.3127582},
doi = {10.1145/3121252.3127582},
abstract = {Logging mechanisms that capture detailed traces of user activity, including creating, reading, updating, and deleting (CRUD) data, facilitate meaningful forensic analysis following a security or privacy breach. However, software requirements often inadequately and inconsistently state 'what' user actions should be logged, thus hindering meaningful forensic analysis. In this talk, we will explore a variety of techniques for building a software system that supports forensic analysis. We will discuss systematic heuristics-driven and patterns-driven processes for identifying log events that must be logged based on user actions and potential accidental and malicious use, as described in natural language software artifacts. We then discuss systematic process for creating a black-box test suite for verifying the identified log events are logged. Using the results of executing the black-box test suite, we propose and evaluate a security metric for measuring the forensic-ability of user activity logs. },
booktitle = {Proceedings of the 1st ACM SIGSOFT International Workshop on Software Engineering and Digital Forensics},
pages = {1},
numpages = {1},
keywords = {Forensics, Software Engineering, Security},
location = {Paderborn, Germany},
series = {SERF 2017}
}

@inproceedings{10.1145/2851581.2892388,
author = {Frisson, Christian and Malacria, Sylvain and Bailly, Gilles and Dutoit, Thierry},
title = {InspectorWidget: A System to Analyze Users Behaviors in Their Applications},
year = {2016},
isbn = {9781450340823},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851581.2892388},
doi = {10.1145/2851581.2892388},
abstract = {We propose InspectorWidget, an opensource application to track and analyze users' behaviors in interactive software. The key contributions of our application are: 1) it works with closed applications that do not provide source code nor scripting capabilities; 2) it covers the whole pipeline of software analysis from logging input events to visual statistics through browsing and programmable annotation; 3) it allows post-recording logging; and 4) it does not require programming skills. To achieve this, InspectorWidget combines low-level event logging (e.g. mouse and keyboard events) and high-level screen features (e.g. interface widgets) captured though computer vision techniques. InspectorWidget benefits end users, usability experts and HCI researchers.},
booktitle = {Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {1548–1554},
numpages = {7},
keywords = {logging, automatic annotation, computer vision},
location = {San Jose, California, USA},
series = {CHI EA '16}
}

@inproceedings{10.5555/2820144.2820146,
author = {Zimmermann, Thomas},
title = {Software Analytics for Digital Games: Keynote Abstract},
year = {2015},
publisher = {IEEE Press},
abstract = {Software and its development generates an inordinate amount of data. Development activities such as check-ins, work items, bug reports, code reviews, and test executions are recorded in software repositories. User interactions that reflect how customers experience software are recorded in telemetry data, run-time traces, and log files and helps to track application and feature usage and expose performance and reliability. Software analytics takes this data and turns it into actionable insight to better inform decisions related to software.In this talk, I will summarize our efforts in the area of software analytics with a special focus on digital games. I will present several examples of games studies, which we have worked on at Microsoft Research such as how players are engaged in Project Gotham Racing, how skill develops over time in Halo Reach and Forza Motorsports, and the initial experience of game play. I will also point out important differences between games development and traditional software development. The work presented in this talk has been done by Nachi Nagappan, myself, and many others who have visited our group over the past years.},
booktitle = {Proceedings of the Fourth International Workshop on Games and Software Engineering},
pages = {1–2},
numpages = {2},
keywords = {digital games, software analytics},
location = {Florence, Italy},
series = {GAS '15}
}

@inproceedings{10.1145/2961111.2962605,
author = {Sun, Yan and Wang, Qing and Li, Mingshu},
title = {Understanding the Contribution of Non-Source Documents in Improving Missing Link Recovery: An Empirical Study},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962605},
doi = {10.1145/2961111.2962605},
abstract = {Background: Links between issue reports and their fixing commits play an important role in software maintenance. Such link data are often missing in practice and many approaches have been proposed in order to recover them automatically. Most of existing approaches focus on comparing log messages and source code files in commits with issues reports. Besides the two kinds of data in commits, non-source documents (NSDs) such as change logs usually record the fixing activities and sometimes share similar texts as those in issue reports. However, few discussions have been made on the role of NSDs in designing link recovery approaches.Aims: This paper aims at understanding whether and how NSDs affect the performance of link recovery approaches.Method: An empirical study is conducted to evaluate the role of NSDs in link recovery approaches in 18 open source projects with 6370 issues and 22761 commits.Results: With the inclusion of NSDs, link recovery approaches can get an average increase in F-Measure ranging from 2.76% - 25.63%. Further examinations show NSDs contribute to the performance improvement in 15 projects and have exceptions in 3 projects. The performance improvement in the 15 projects is mainly from the filtering of noisy links. On average, 23.59% - 76.30% false links can be excluded by exploiting NSDs in the link recovery approach. We also analyze the 3 projects in which NSDs cannot improve the performance. Our finding shows sophisticated data selection in NSDs is necessary.Conclusions: Our preliminary findings demonstrate that involving NSDs can improve the performance of link recovery approaches in most cases.},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {39},
numpages = {10},
keywords = {Missing Link Recovery, Mining Software Repositories, Software Maintenance, Non-Source Documents},
location = {Ciudad Real, Spain},
series = {ESEM '16}
}

@inproceedings{10.5555/2337223.2337490,
author = {Shang, Weiyi},
title = {Bridging the Divide between Software Developers and Operators Using Logs},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = { There is a growing gap between the software development and operation worlds. Software developers rarely divulge development knowledge about the software to operators, while operators rarely communicate field knowledge to developers. To improve the quality and reduce the operational cost of large-scale software systems, bridging the gap between these two worlds is essential. This thesis proposes the use of logs as mechanism to bridge the gap between these two worlds. Logs are messages generated from statements inserted by developers in the source code and are often used by operators for monitoring the field operation of a system. However, the rich knowledge in logs has not yet been fully used because of their non-structured nature, their large scale, and the use of the ad hoc log analysis techniques. Through case studies on large commercial and open source systems, we plan to demonstrate the value of logs as a tool to support developers and operators. },
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {1583–1586},
numpages = {4},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1109/ICSE-SEIP.2019.00021,
author = {Zhu, Jieming and He, Shilin and Liu, Jinyang and He, Pinjia and Xie, Qi and Zheng, Zibin and Lyu, Michael R.},
title = {Tools and Benchmarks for Automated Log Parsing},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP.2019.00021},
doi = {10.1109/ICSE-SEIP.2019.00021},
abstract = {Logs are imperative in the development and maintenance process of many software systems. They record detailed runtime information that allows developers and support engineers to monitor their systems and dissect anomalous behaviors and errors. The increasing scale and complexity of modern software systems, however, make the volume of logs explodes. In many cases, the traditional way of manual log inspection becomes impractical. Many recent studies, as well as industrial tools, resort to powerful text search and machine learning-based analytics solutions. Due to the unstructured nature of logs, a first crucial step is to parse log messages into structured data for subsequent analysis. In recent years, automated log parsing has been widely studied in both academia and industry, producing a series of log parsers by different techniques. To better understand the characteristics of these log parsers, in this paper, we present a comprehensive evaluation study on automated log parsing and further release the tools and benchmarks for easy reuse. More specifically, we evaluate 13 log parsers on a total of 16 log datasets spanning distributed systems, supercomputers, operating systems, mobile systems, server applications, and standalone software. We report the benchmarking results in terms of accuracy, robustness, and efficiency, which are of practical importance when deploying automated log parsing in production. We also share the success stories and lessons learned in an industrial application at Huawei. We believe that our work could serve as the basis and provide valuable guidance to future research and deployment of automated log parsing.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Software Engineering in Practice},
pages = {121–130},
numpages = {10},
keywords = {log analysis, anomaly detection, AIOps, log parsing, log management},
location = {Montreal, Quebec, Canada},
series = {ICSE-SEIP '19}
}

@inproceedings{10.1145/2818000.2818029,
author = {Gelernter, Nethanel and Grinstein, Yoel and Herzberg, Amir},
title = {Cross-Site Framing Attacks},
year = {2015},
isbn = {9781450336826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818000.2818029},
doi = {10.1145/2818000.2818029},
abstract = {We identify the threat of cross-site framing attacks, which involves planting false evidence that incriminates computer users, without requiring access to their computer. We further show that a variety of framing-evidence can be planted using only modest framing-attacker capabilities. The attacker can plant evidence in both the logs of popular reputable sites and in the computer of the victim, without requiring client-side malware and without leaving traces.To infect the records of several of the most popular sites, we identified operations that are often considered benign and hence not protected from cross-site request forgery (CSRF) attacks. We demonstrate the attacks on the largest search engines: Google, Bing, and Yahoo!, on Youtube and Facebook, and on the e-commerce sites: Amazon, eBay, and Craigslist.To plant pieces of framing evidence on the computer, we abused the vulnerabilities of browsers and weaknesses in the examination procedure done by forensic software. Specifically, we show that it is possible to manipulate the common NTFS file system and to plant files on the hard disk of the victim, without leaving any traces indicating that these files were created via the browser.We validated the effectiveness of the framing evidence with the assistance of law authorities, in addition to using prominent forensic software. This work also discusses tactics for defense against cross-site framing and its applicability to web-services, browsers, and forensic software.},
booktitle = {Proceedings of the 31st Annual Computer Security Applications Conference},
pages = {161–170},
numpages = {10},
keywords = {Web attacks, Security, Framing, Forensic},
location = {Los Angeles, CA, USA},
series = {ACSAC 2015}
}

@inproceedings{10.1145/2597073.2597081,
author = {Gupta, Monika and Sureka, Ashish and Padmanabhuni, Srinivas},
title = {Process Mining Multiple Repositories for Software Defect Resolution from Control and Organizational Perspective},
year = {2014},
isbn = {9781450328630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597073.2597081},
doi = {10.1145/2597073.2597081},
abstract = { Issue reporting and resolution is a software engineering process supported by tools such as Issue Tracking System (ITS), Peer Code Review (PCR) system and Version Control System (VCS). Several open source software projects such as Google Chromium and Android follow process in which a defect or feature enhancement request is reported to an issue tracker followed by source-code change or patch review and patch commit using a version control system. We present an application of process mining three software repositories (ITS, PCR and VCS) from control flow and organizational perspective for effective process management. ITS, PCR and VCS are not explicitly linked so we implement regular expression based heuristics to integrate data from three repositories for Google Chromium project. We define activities such as bug reporting, bug fixing, bug verification, patch submission, patch review, and source code commit and create an event log of the bug resolution process. The extracted event log contains audit trail data such as caseID, timestamp, activity name and performer. We discover runtime process model for bug resolution process spanning three repositories using process mining tool, Disco, and conduct process performance and efficiency analysis. We identify bottlenecks, define and detect basic and composite anti-patterns. In addition to control flow analysis, we mine event log to perform organizational analysis and discover metrics such as handover of work, subcontracting, joint cases and joint activities. },
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
pages = {122–131},
numpages = {10},
keywords = {Peer Code Review System, Empirical Software Engineering and Measurements, Social Network Analysis, Process Mining, Issue Tracking System, Software Maintenance},
location = {Hyderabad, India},
series = {MSR 2014}
}

@inproceedings{10.1145/2837614.2837648,
author = {Koskinen, Eric and Yang, Junfeng},
title = {Reducing Crash Recoverability to Reachability},
year = {2016},
isbn = {9781450335492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837614.2837648},
doi = {10.1145/2837614.2837648},
abstract = { Software applications run on a variety of platforms (filesystems, virtual slices, mobile hardware, etc.) that do not provide 100% uptime. As such, these applications may crash at any unfortunate moment losing volatile data and, when re-launched, they must be able to correctly recover from potentially inconsistent states left on persistent storage. From a verification perspective, crash recovery bugs can be particularly frustrating because, even when it has been formally proved for a program that it satisfies a property, the proof is foiled by these external events that crash and restart the program. In this paper we first provide a hierarchical formal model of what it means for a program to be crash recoverable. Our model captures the recoverability of many real world programs, including those in our evaluation which use sophisticated recovery algorithms such as shadow paging and write-ahead logging. Next, we introduce a novel technique capable of automatically proving that a program correctly recovers from a crash via a reduction to reachability. Our technique takes an input control-flow automaton and transforms it into an encoding that blends the capture of snapshots of pre-crash states into a symbolic search for a proof that recovery terminates and every recovered execution simulates some crash-free execution. Our encoding is designed to enable one to apply existing abstraction techniques in order to do the work that is necessary to prove recoverability. We have implemented our technique in a tool called Eleven82, capable of analyzing C programs to detect recoverability bugs or prove their absence. We have applied our tool to benchmark examples drawn from industrial file systems and databases, including GDBM, LevelDB, LMDB, PostgreSQL, SQLite, VMware and ZooKeeper. Within minutes, our tool is able to discover bugs or prove that these fragments are crash recoverable. },
booktitle = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
pages = {97–108},
numpages = {12},
keywords = {formal verification, Crash recovery, program analysis},
location = {St. Petersburg, FL, USA},
series = {POPL '16}
}

@article{10.1145/2914770.2837648,
author = {Koskinen, Eric and Yang, Junfeng},
title = {Reducing Crash Recoverability to Reachability},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0362-1340},
url = {https://doi.org/10.1145/2914770.2837648},
doi = {10.1145/2914770.2837648},
abstract = { Software applications run on a variety of platforms (filesystems, virtual slices, mobile hardware, etc.) that do not provide 100% uptime. As such, these applications may crash at any unfortunate moment losing volatile data and, when re-launched, they must be able to correctly recover from potentially inconsistent states left on persistent storage. From a verification perspective, crash recovery bugs can be particularly frustrating because, even when it has been formally proved for a program that it satisfies a property, the proof is foiled by these external events that crash and restart the program. In this paper we first provide a hierarchical formal model of what it means for a program to be crash recoverable. Our model captures the recoverability of many real world programs, including those in our evaluation which use sophisticated recovery algorithms such as shadow paging and write-ahead logging. Next, we introduce a novel technique capable of automatically proving that a program correctly recovers from a crash via a reduction to reachability. Our technique takes an input control-flow automaton and transforms it into an encoding that blends the capture of snapshots of pre-crash states into a symbolic search for a proof that recovery terminates and every recovered execution simulates some crash-free execution. Our encoding is designed to enable one to apply existing abstraction techniques in order to do the work that is necessary to prove recoverability. We have implemented our technique in a tool called Eleven82, capable of analyzing C programs to detect recoverability bugs or prove their absence. We have applied our tool to benchmark examples drawn from industrial file systems and databases, including GDBM, LevelDB, LMDB, PostgreSQL, SQLite, VMware and ZooKeeper. Within minutes, our tool is able to discover bugs or prove that these fragments are crash recoverable. },
journal = {SIGPLAN Not.},
month = jan,
pages = {97–108},
numpages = {12},
keywords = {Crash recovery, program analysis, formal verification}
}

@article{10.14778/2735496.2735502,
author = {Huang, Jian and Schwan, Karsten and Qureshi, Moinuddin K.},
title = {NVRAM-Aware Logging in Transaction Systems},
year = {2014},
issue_date = {December 2014},
publisher = {VLDB Endowment},
volume = {8},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/2735496.2735502},
doi = {10.14778/2735496.2735502},
abstract = {Emerging byte-addressable, non-volatile memory technologies (NVRAM) like phase-change memory can increase the capacity of future memory systems by orders of magnitude. Compared to systems that rely on disk storage, NVRAM-based systems promise significant improvements in performance for key applications like online transaction processing (OLTP). Unfortunately, NVRAM systems suffer from two drawbacks: their asymmetric read-write performance and the notable higher cost of the new memory technologies compared to disk. This paper investigates the cost-effective use of NVRAM in transaction systems. It shows that using NVRAM only for the logging subsystem (NV-Logging) provides much higher transactions per dollar than simply replacing all disk storage with NVRAM. Specifically, for NV-Logging, we show that the software overheads associated with centralized log buffers cause performance bottlenecks and limit scaling. The per-transaction logging methods described in the paper help avoid these overheads, enabling concurrent logging for multiple transactions. Experimental results with a faithful emulation of future NVRAM-based servers using the TPCC, TATP, and TPCB benchmarks show that NV-Logging improves throughput by 1.42 - 2.72x over the costlier option of replacing all disk storage with NVRAM. Results also show that NV-Logging performs 1.21 - 6.71x better than when logs are placed into the PMFS NVRAM-optimized file system. Compared to state-of-the-art distributed logging, NV-Logging delivers 20.4% throughput improvements.},
journal = {Proc. VLDB Endow.},
month = dec,
pages = {389–400},
numpages = {12}
}

@inproceedings{10.1145/2451116.2451138,
author = {Honarmand, Nima and Dautenhahn, Nathan and Torrellas, Josep and King, Samuel T. and Pokam, Gilles and Pereira, Cristiano},
title = {Cyrus: Unintrusive Application-Level Record-Replay for Replay Parallelism},
year = {2013},
isbn = {9781450318709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2451116.2451138},
doi = {10.1145/2451116.2451138},
abstract = {Architectures for deterministic record-replay (R&amp;R) of multithreaded code are attractive for program debugging, intrusion analysis, and fault-tolerance uses. However, very few of the proposed designs have focused on maximizing replay speed -- a key enabling property of these systems. The few efforts that focus on replay speed require intrusive hardware or software modifications, or target whole-system R&amp;R rather than the more useful application-level R&amp;R.This paper presents the first hardware-based scheme for unintrusive, application-level R&amp;R that explicitly targets high replay speed. Our scheme, called Cyrus, requires no modification to commodity snoopy cache coherence. It introduces the concept of an on-the-fly software Backend Pass during recording which, as the log is being generated, transforms it for high replay parallelism. This pass also fixes-up the log, and can flexibly trade-off replay parallelism for log size. We analyze the performance of Cyrus using full system (OS plus hardware) simulation. Our results show that Cyrus has negligible recording overhead. In addition, for 8-processor runs of SPLASH-2, Cyrus attains an average replay parallelism of 5, and a replay speed that is, on average, only about 50% lower than the recording speed.},
booktitle = {Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {193–206},
numpages = {14},
keywords = {backend log processing, application-level parallel replay, source-only recording, deterministic replay, unintrusive hardware-assisted recording},
location = {Houston, Texas, USA},
series = {ASPLOS '13}
}

@article{10.1145/2499368.2451138,
author = {Honarmand, Nima and Dautenhahn, Nathan and Torrellas, Josep and King, Samuel T. and Pokam, Gilles and Pereira, Cristiano},
title = {Cyrus: Unintrusive Application-Level Record-Replay for Replay Parallelism},
year = {2013},
issue_date = {April 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2499368.2451138},
doi = {10.1145/2499368.2451138},
abstract = {Architectures for deterministic record-replay (R&amp;R) of multithreaded code are attractive for program debugging, intrusion analysis, and fault-tolerance uses. However, very few of the proposed designs have focused on maximizing replay speed -- a key enabling property of these systems. The few efforts that focus on replay speed require intrusive hardware or software modifications, or target whole-system R&amp;R rather than the more useful application-level R&amp;R.This paper presents the first hardware-based scheme for unintrusive, application-level R&amp;R that explicitly targets high replay speed. Our scheme, called Cyrus, requires no modification to commodity snoopy cache coherence. It introduces the concept of an on-the-fly software Backend Pass during recording which, as the log is being generated, transforms it for high replay parallelism. This pass also fixes-up the log, and can flexibly trade-off replay parallelism for log size. We analyze the performance of Cyrus using full system (OS plus hardware) simulation. Our results show that Cyrus has negligible recording overhead. In addition, for 8-processor runs of SPLASH-2, Cyrus attains an average replay parallelism of 5, and a replay speed that is, on average, only about 50% lower than the recording speed.},
journal = {SIGPLAN Not.},
month = mar,
pages = {193–206},
numpages = {14},
keywords = {application-level parallel replay, source-only recording, unintrusive hardware-assisted recording, backend log processing, deterministic replay}
}

@article{10.1145/2490301.2451138,
author = {Honarmand, Nima and Dautenhahn, Nathan and Torrellas, Josep and King, Samuel T. and Pokam, Gilles and Pereira, Cristiano},
title = {Cyrus: Unintrusive Application-Level Record-Replay for Replay Parallelism},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2490301.2451138},
doi = {10.1145/2490301.2451138},
abstract = {Architectures for deterministic record-replay (R&amp;R) of multithreaded code are attractive for program debugging, intrusion analysis, and fault-tolerance uses. However, very few of the proposed designs have focused on maximizing replay speed -- a key enabling property of these systems. The few efforts that focus on replay speed require intrusive hardware or software modifications, or target whole-system R&amp;R rather than the more useful application-level R&amp;R.This paper presents the first hardware-based scheme for unintrusive, application-level R&amp;R that explicitly targets high replay speed. Our scheme, called Cyrus, requires no modification to commodity snoopy cache coherence. It introduces the concept of an on-the-fly software Backend Pass during recording which, as the log is being generated, transforms it for high replay parallelism. This pass also fixes-up the log, and can flexibly trade-off replay parallelism for log size. We analyze the performance of Cyrus using full system (OS plus hardware) simulation. Our results show that Cyrus has negligible recording overhead. In addition, for 8-processor runs of SPLASH-2, Cyrus attains an average replay parallelism of 5, and a replay speed that is, on average, only about 50% lower than the recording speed.},
journal = {SIGARCH Comput. Archit. News},
month = mar,
pages = {193–206},
numpages = {14},
keywords = {unintrusive hardware-assisted recording, deterministic replay, backend log processing, application-level parallel replay, source-only recording}
}

@inproceedings{10.1145/3025171.3025184,
author = {Dev, Himel and Liu, Zhicheng},
title = {Identifying Frequent User Tasks from Application Logs},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025184},
doi = {10.1145/3025171.3025184},
abstract = {In the light of continuous growth in log analytics, application logs remain a valuable source to understand and analyze patterns in user behavior. Today, almost every major software company employs analysts to reveal user insights from log data. To understand the tasks and challenges of the analysts, we conducted a background study with a group of analysts from a major software company. A fundamental analytics objective that we recognized through this study involves identifying frequent user tasks from application logs. More specifically, analysts are interested in identifying operation groups that represent meaningful tasks performed by many users inside applications. This is challenging, primarily because of the nature of modern application logs, which are long, noisy and consist of events from high-cardinality set. In this paper, we address these challenges to design a novel frequent pattern ranking technique that extracts frequent user tasks from application logs. Our experimental study shows that our proposed technique significantly outperforms state of the art for real-world data.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {263–273},
numpages = {11},
keywords = {application log, pattern ranking, frequent pattern mining, user task},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3297280.3297609,
author = {Krismayer, Thomas and Rabiser, Rick and Gr\"{u}nbacher, Paul},
title = {Mining Constraints for Monitoring Systems of Systems},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297609},
doi = {10.1145/3297280.3297609},
abstract = {Complex software-intensive systems are often systems of systems whose full behavior emerges during operation only, when the involved systems interact with each other and the environment. Runtime monitoring approaches are thus used to detect deviations from the expected behavior. Most approaches assume that engineers define the expected behavior as constraints, however, the deep domain knowledge required to specify constraints is often not available. We describe an approach that automatically mines constraint candidates for runtime monitoring from event logs recorded from systems of systems. Our approach extracts different types of constraints on event occurrence, timing, and data and offers users filtering and ranking strategies for the mined candidates.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {1864–1866},
numpages = {3},
keywords = {constraint mining, automated constraint extraction, runtime monitoring, dynamic analysis, systems of systems},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.5555/1924943.1924956,
author = {Bergan, Tom and Hunt, Nicholas and Ceze, Luis and Gribble, Steven D.},
title = {Deterministic Process Groups in DOS},
year = {2010},
publisher = {USENIX Association},
address = {USA},
abstract = {Current multiprocessor systems execute parallel and concurrent software nondeterministically: even when given precisely the same input, two executions of the same program may produce different output. This severely complicates debugging, testing, and automatic replication for fault-tolerance. Previous efforts to address this issue have focused primarily on record and replay, but making execution actually deterministic would address the problem at the root.Our goals in this work are twofold: (1) to provide fully deterministic execution of arbitrary, unmodified, multithreaded programs as an OS service; and (2) to make all sources of intentional nondeterminism, such as network I/O, be explicit and controllable. To this end we propose a new OS abstraction, the Deterministic Process Group (DPG). All communication between threads and processes internal to a DPG happens deterministically, including implicit communication via shared-memory accesses, as well as communication via OS channels such as pipes, signals, and the filesystem. To deal with fundamentally nondeterministic external events, our abstraction includes the shim layer, a programmable interface that interposes on all interaction between a DPG and the external world, making determinism useful even for reactive applications.We implemented the DPG abstraction as an extension to Linux and demonstrate its benefits with three use cases: plain deterministic execution; replicated execution; and record and replay by logging just external input. We evaluated our implementation on both parallel and reactive workloads, including Apache, Chromium, and PARSEC.},
booktitle = {Proceedings of the 9th USENIX Conference on Operating Systems Design and Implementation},
pages = {177–191},
numpages = {15},
location = {Vancouver, BC, Canada},
series = {OSDI'10}
}

@inproceedings{10.5555/2387880.2387909,
author = {Yuan, Ding and Park, Soyeon and Huang, Peng and Liu, Yang and Lee, Michael M. and Tang, Xiaoming and Zhou, Yuanyuan and Savage, Stefan},
title = {Be Conservative: Enhancing Failure Diagnosis with Proactive Logging},
year = {2012},
isbn = {9781931971966},
publisher = {USENIX Association},
address = {USA},
abstract = {When systems fail in the field, logged error or warning messages are frequently the only evidence available for assessing and diagnosing the underlying cause. Consequently, the efficacy of such logging--how often and how well error causes can be determined via postmortem log messages--is a matter of significant practical importance. However, there is little empirical data about how well existing logging practices work and how they can yet be improved. We describe a comprehensive study characterizing the efficacy of logging practices across five large and widely used software systems. Across 250 randomly sampled reported failures, we first identify that more than half of the failures could not be diagnosed well using existing log data. Surprisingly, we find that majority of these unreported failures are manifested via a common set of generic error patterns (e.g., system call return errors) that, if logged, can significantly ease the diagnosis of these unreported failure cases. We further mechanize this knowledge in a tool called Errlog, that proactively adds appropriate logging statements into source code while adding only 1.4% performance overhead. A controlled user study suggests that Errlog can reduce diagnosis time by 60.7%.},
booktitle = {Proceedings of the 10th USENIX Conference on Operating Systems Design and Implementation},
pages = {293–306},
numpages = {14},
location = {Hollywood, CA, USA},
series = {OSDI'12}
}

@inproceedings{10.1145/2304510.2304521,
author = {Graefe, Goetz and Kuno, Harumi and Seeger, Bernhard},
title = {Self-Diagnosing and Self-Healing Indexes},
year = {2012},
isbn = {9781450314299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2304510.2304521},
doi = {10.1145/2304510.2304521},
abstract = {Transactional storage and indexing is the heart of every database, not only for performance and functionality but also for reliability and availability. For high concurrency, these components must be programmed carefully with short critical sections, a variety of consistent states with short transitions, etc. Many-core CPUs exacerbate these requirements. Testing software with 100s or 1, 000s of threads is very difficult.In order to test such code, we suggest verifying the B-tree structure in each traversal. With carefully designed tree structure and node contents, a root-to-leaf pass can verify all nodes along its path comprehensively, i. e., it can verify all B-tree invariants including its consistency constraints with respect to its siblings and cousins (defined below). Thus, instead of testing the index implementation by running a stress-test and verifying the B-tree structure and contents afterwards, i. e., instead of the traditional approach, the test execution itself verifies the structure frequently and efficiently.During testing prior to a software release, frequent comprehensive verification combined with fast access to all relevant log records permits efficient root cause analysis of test failures. In deployments after software release, frequent verification and fast access to log records permits automatic, reliable, and efficient recovery of the correct, up-to-date page contents. Our contribution is an index structure that gives reliable and efficient access to all relevant log records and thus enables root cause analysis during testing and automatic recovery after deployment.},
booktitle = {Proceedings of the Fifth International Workshop on Testing Database Systems},
articleno = {8},
numpages = {8},
keywords = {recovery, quality assurance, failure, B-tree},
location = {Scottsdale, Arizona},
series = {DBTest '12}
}

@inproceedings{10.1145/2785592.2785593,
author = {Aalst, Wil van der},
title = {Big Software on the Run: In Vivo Software Analytics Based on Process Mining (Keynote)},
year = {2015},
isbn = {9781450333467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785592.2785593},
doi = {10.1145/2785592.2785593},
abstract = { Software-related problems have an incredible impact on society, organizations, and users that increasingly rely on information technology. Specification, verification and testing techniques aim to avoid such problems. However, the growing complexity, scale, and diversity of software complicate matters. Since software is evolving and operates in a changing environment, one cannot anticipate all problems at design-time. Hence, we propose to analyze software "in vivo", i.e., we study systems in their natural habitat rather than through testing or software design. We propose to observe running systems, collect and analyze data on them, generate descriptive models, and use these to respond to failures. We focus on process mining as a tool for in vivo software analytics. Process discovery techniques can be used to capture the real behavior of software. Conformance checking techniques can be used to spot deviations. The alignment of models and real software behavior can be used to predict problems related to performance or conformance. Recent developments in process mining and instrumentation of software make this possible. This keynote paper provides pointers to process mining literature and introduces the "Big Software on the Run" (BSR) research program that just started. },
booktitle = {Proceedings of the 2015 International Conference on Software and System Process},
pages = {1–5},
numpages = {5},
keywords = {software analytics, event logs, process discovery, software engineering, Process mining, conformance checking},
location = {Tallinn, Estonia},
series = {ICSSP 2015}
}

@inproceedings{10.1109/ASE.2019.00085,
author = {Liu, Jinyang and Zhu, Jieming and He, Shilin and He, Pinjia and Zheng, Zibin and Lyu, Michael R.},
title = {Logzip: Extracting Hidden Structures via Iterative Clustering for Log Compression},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00085},
doi = {10.1109/ASE.2019.00085},
abstract = {System logs record detailed runtime information of software systems and are used as the main data source for many tasks around software engineering. As modern software systems are evolving into large scale and complex structures, logs have become one type of fast-growing big data in industry. In particular, such logs often need to be stored for a long time in practice (e.g., a year), in order to analyze recurrent problems or track security issues. However, archiving logs consumes a large amount of storage space and computing resources, which in turn incurs high operational cost. Data compression is essential to reduce the cost of log storage. Traditional compression tools (e.g., gzip) work well for general texts, but are not tailed for system logs. In this paper, we propose a novel and effective log compression method, namely logzip. Logzip is capable of extracting hidden structures from raw logs via fast iterative clustering and further generating coherent intermediate representations that allow for more effective compression. We evaluate logzip on five large log datasets of different system types, with a total of 63.6 GB in size. The results show that logzip can save about half of the storage space on average over traditional compression tools. Meanwhile, the design of logzip is highly parallel and only incurs negligible overhead. In addition, we share our industrial experience of applying logzip to Huawei's real products.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {863–873},
numpages = {11},
keywords = {log management, logs, log compression, structure extraction, iterative clustering},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/2976767.2987688,
author = {Kalenkova, Anna A. and van der Aalst, Wil M. P. and Lomazova, Irina A. and Rubin, Vladimir A.},
title = {Process Mining Using BPMN: Relating Event Logs and Process Models},
year = {2016},
isbn = {9781450343213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976767.2987688},
doi = {10.1145/2976767.2987688},
abstract = {Process mining is an emerging discipline incorporating methods and tools for the analysis of system/process executions captured in the form of event logs [1]. Traditionally process mining can be divided into three research areas: discovery (construction of process models from event logs), conformance checking (finding log and model deviations), and enhancement of existing process models with additional event log data.BPMN (Business Process Model and Notation) 2.0 [2] is a widely used process modeling notation, supported by various process modeling and analysis tools, and is a de-facto process modeling standard. Using BPMN within process mining opens perspectives for applicability of the existing process mining techniques: for instance discovered process models can be analyzed or enacted using existing BPMN-based software, and vice versa, manually created models can be imported to a process mining tool, verified against event logs, and enhanced with additional data.In this work we bridge the gap between conventional process modeling formalisms used in the context of process mining (e.g., Petri nets, causal nets, process trees) and BPMN. For that purpose we developed a suite of conversion algorithms and provide formal guarantees relating the behavior of Petri nets (including non-free-choice nets) to the corresponding BPMN models (and vice versa). The derived relations are used to enhance the BPMN models with information learned from the event logs.The developed conversion techniques are described in detail in [3] and have been implemented [4] as a part of ProM (Process Mining Framework) [5] -- an open source tool for process mining and verified on real event log data. Moreover, cases for which conversion algorithms give more compact process models in comparison with the initial models are identified. Although the developed algorithms deal with basic control flow constructs only, they can be applied in the discovery of advanced BPMN modeling elements [2], including subprocesses [6-7], cancellations [8], conditional branching and data objects, swimlanes, message flows, and others.},
booktitle = {Proceedings of the ACM/IEEE 19th International Conference on Model Driven Engineering Languages and Systems},
pages = {123},
numpages = {1},
location = {Saint-malo, France},
series = {MODELS '16}
}

@article{10.1145/3162614,
author = {Yoo, Jinsoo and Oh, Joontaek and Lee, Seongjin and Won, Youjip and Ha, Jin-Yong and Lee, Jongsung and Shim, Junseok},
title = {OrcFS: Orchestrated File System for Flash Storage},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1553-3077},
url = {https://doi.org/10.1145/3162614},
doi = {10.1145/3162614},
abstract = {In this work, we develop the Orchestrated File System (OrcFS) for Flash storage. OrcFS vertically integrates the log-structured file system and the Flash-based storage device to eliminate the redundancies across the layers. A few modern file systems adopt sophisticated append-only data structures in an effort to optimize the behavior of the file system with respect to the append-only nature of the Flash memory. While the benefit of adopting an append-only data structure seems fairly promising, it makes the stack of software layers full of unnecessary redundancies, leaving substantial room for improvement. The redundancies include (i) redundant levels of indirection (address translation), (ii) duplicate efforts to reclaim the invalid blocks (i.e.,&nbsp;segment cleaning in the file system and garbage collection in the storage device), and (iii) excessive over-provisioning (i.e.,&nbsp;separate over-provisioning areas in each layer). OrcFS eliminates these redundancies via distributing the address translation, segment cleaning (or garbage collection), bad block management, and wear-leveling across the layers. Existing solutions suffer from high segment cleaning overhead and cause significant write amplification due to mismatch between the file system block size and the Flash page size. To optimize the I/O stack while avoiding these problems, OrcFS adopts three key technical elements.First, OrcFS uses disaggregate mapping, whereby it partitions the Flash storage into two areas, managed by a file system and Flash storage, respectively, with different granularity. In OrcFS, the metadata area and data area are maintained by 4Kbyte page granularity and 256Mbyte superblock granularity. The superblock-based storage management aligns the file system section size, which is a unit of segment cleaning, with the superblock size of the underlying Flash storage. It can fully exploit the internal parallelism of the underlying Flash storage, exploiting the sequential workload characteristics of the log-structured file system. Second, OrcFS adopts quasi-preemptive segment cleaning to prohibit the foreground I/O operation from being interfered with by segment cleaning. The latency to reclaim the free space can be prohibitive in OrcFS due to its large file system section size, 256Mbyte. OrcFS effectively addresses this issue via adopting a polling-based segment cleaning scheme. Third, the OrcFS introduces block patching to avoid unnecessary write amplification in the partial page program. OrcFS is the enhancement of the F2FS file system. We develop a prototype OrcFS based on F2FS and server class SSD with modified firmware (Samsung 843TN). OrcFS reduces the device mapping table requirement to 1/465 and 1/4 compared with the page mapping and the smallest mapping scheme known to the public, respectively. Via eliminating the redundancy in the segment cleaning and garbage collection, the OrcFS reduces 1/3 of the write volume under heavy random write workload. OrcFS achieves 56% performance gain against EXT4 in varmail workload.},
journal = {ACM Trans. Storage},
month = apr,
articleno = {17},
numpages = {26},
keywords = {Flash memories, Garbage Collection, Log-structured File System}
}

@inproceedings{10.1145/3208040.3208061,
author = {Fernando, Pradeep and Gavrilovska, Ada and Kannan, Sudarsun and Eisenhauer, Greg},
title = {NVStream: Accelerating HPC Workflows with NVRAM-Based Transport for Streaming Objects},
year = {2018},
isbn = {9781450357852},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3208040.3208061},
doi = {10.1145/3208040.3208061},
abstract = {Nonvolatile memory technologies (NVRAM) with larger capacity relative to DRAM and faster persistence relative to block-based storage technologies are expected to play a crucial role in accelerating I/O performance for HPC scientific workflows. Typically, a scientific workflow includes a simulation process (producer of data) and an analytics application process (consumer of data) that stream, share, and exchange data supported by an underlying OS-level file system. However, using an OS-level file system for data sharing adds substantial software overheads due to frequent system calls, journaling (for crash-consistency) cost, and file-system metadata update cost. To overcome these challenges, we design NVStream- a lightweight user-level data management system that exploits NVRAMs byte addressability and fast persistence to support streaming I/O in scientific workflows. First, NVStream reduces I/O-related software overheads by designing a memory-based persistent object store and log-structured heap manager that exploit NVRAM's large capacity. Second, NVStream incorporates a hardware-assisted non-temporal stores for crash-consistent updates at near hardware data copy (memory copy) speeds. Finally, NVStream reduces data written to NVRAM with a delta compression, which further reduces I/O cost for workflows with higher write locality. The evaluation of NVStream using I/O benchmarks and scientific applications demonstrates 10X reduction in I/O compared to NVRAM-optimized file systems and also guaranteeing crash-consistent data movement.},
booktitle = {Proceedings of the 27th International Symposium on High-Performance Parallel and Distributed Computing},
pages = {231–242},
numpages = {12},
keywords = {HPC I/O, streaming data, NVM, crash-consistent updates},
location = {Tempe, Arizona},
series = {HPDC '18}
}

@inproceedings{10.1109/ASE.2019.00055,
author = {Zaman, Tarannum Shaila and Han, Xue and Yu, Tingting},
title = {SCMiner: Localizing System-Level Concurrency Faults from Large System Call Traces},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00055},
doi = {10.1109/ASE.2019.00055},
abstract = {Localizing concurrency faults that occur in production is hard because, (1) detailed field data, such as user input, file content and interleaving schedule, may not be available to developers to reproduce the failure; (2) it is often impractical to assume the availability of multiple failing executions to localize the faults using existing techniques; (3) it is challenging to search for buggy locations in an application given limited runtime data; and, (4) concurrency failures at the system level often involve multiple processes or event handlers (e.g., software signals), which cannot be handled by existing tools for diagnosing intra-process (thread-level) failures. To address these problems, we present SCMiner, a practical online bug diagnosis tool to help developers understand how a system-level concurrency fault happens based on the logs collected by the default system audit tools. SCMiner achieves online bug diagnosis to obviate the need for offline bug reproduction. SCMiner does not require code instrumentation on the production system or rely on the assumption of the availability of multiple failing executions. Specifically, after the system call traces are collected, SCMiner uses data mining and statistical anomaly detection techniques to identify the failure-inducing system call sequences. It then maps each abnormal sequence to specific application functions. We have conducted an empirical study on 19 real-world benchmarks. The results show that SCMiner is both effective and efficient at localizing system-level concurrency faults.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {515–526},
numpages = {12},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/2897695.2897697,
author = {Destefanis, Giuseppe and Ortu, Marco and Porru, Simone and Swift, Stephen and Marchesi, Michele},
title = {A Statistical Comparison of Java and Python Software Metric Properties},
year = {2016},
isbn = {9781450341776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897695.2897697},
doi = {10.1145/2897695.2897697},
abstract = {This paper presents a statistical analysis of 20 opens ource object-oriented systems with the purpose of detecting differences in metrics distribution between Java and Python projects. We selected ten Java projects from the Java Qualitas Corpus and ten projects written in Python. For each system, we considered 10 class-level software metrics.We performed a best fit procedure on the empirical distributions through the log-normal distribution and the double Pareto distribution to identify differences between the two languages. Even though the statistical distributions for projects written in Java and Python may appear the same for lower values of the metric, performing the procedure with the double Pareto distribution for the Number of Local Methods metric reveals that major differences can be noticed along the queue of the distributions. On the contrary, the same analysis performed with the Number of Statements metric reveals that only the initial portion of the double Pareto distribution shows differences between the two languages. In addition, the dispersion parameter associated to the log-normal distribution fit for the total Number Of Methods can be used for distinguishing Java projects from Python projects.},
booktitle = {Proceedings of the 7th International Workshop on Emerging Trends in Software Metrics},
pages = {22–28},
numpages = {7},
location = {Austin, Texas},
series = {WETSoM '16}
}

@inproceedings{10.1145/2818000.2818039,
author = {Ma, Shiqing and Lee, Kyu Hyung and Kim, Chung Hwan and Rhee, Junghwan and Zhang, Xiangyu and Xu, Dongyan},
title = {Accurate, Low Cost and Instrumentation-Free Security Audit Logging for Windows},
year = {2015},
isbn = {9781450336826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818000.2818039},
doi = {10.1145/2818000.2818039},
abstract = {Audit logging is an important approach to cyber attack investigation. However, traditional audit logging either lacks accuracy or requires expensive and complex binary instrumentation. In this paper, we propose a Windows based audit logging technique that features accuracy and low cost. More importantly, it does not require instrumenting the applications, which is critical for commercial software with IP protection. The technique is build on Event Tracing for Windows (ETW). By analyzing ETW log and critical parts of application executables, a model can be constructed to parse ETW log to units representing independent sub-executions in a process. Causality inferred at the unit level renders much higher accuracy, allowing us to perform accurate attack investigation and highly effective log reduction.},
booktitle = {Proceedings of the 31st Annual Computer Security Applications Conference},
pages = {401–410},
numpages = {10},
location = {Los Angeles, CA, USA},
series = {ACSAC 2015}
}

@inproceedings{10.5555/2151054.2151075,
author = {Pellegrini, Alessandro and Vitali, Roberto and Quaglia, Francesco},
title = {The ROme OpTimistic Simulator: Core Internals and Programming Model},
year = {2011},
isbn = {9781936968008},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
abstract = {In this article we overview the ROme OpTimistic Simulator (ROOT-Sim), an open source C/MPI-based simulation package targeted at POSIX systems, which implements a general-purpose parallel/distributed simulation environment relying on the optimistic (i.e., rollback based) synchronization paradigm. It offers a very simple programming model based on the classical notion of simulation-event handlers, to be implemented according to the ANSI-C standard, and transparently supports all the services required to parallelize the execution. It also offers a set of optimized protocols (e.g. CPU scheduling and state log/restore protocols) aimed at minimizing the run-time overhead of the platform, thus allowing for high performance and scalability. Here we overview the core internal mechanisms provided by ROOT-Sim, together with the offered APIs and the programming model that is expected to be agreed in order to produce simulation software that can be transparently run, in a concurrent fashion, on top of the ROOT-Sim layer.},
booktitle = {Proceedings of the 4th International ICST Conference on Simulation Tools and Techniques},
pages = {96–98},
numpages = {3},
location = {Barcelona, Spain},
series = {SIMUTools '11}
}

@article{10.1145/2110356.2110359,
author = {Veeraraghavan, Kaushik and Lee, Dongyoon and Wester, Benjamin and Ouyang, Jessica and Chen, Peter M. and Flinn, Jason and Narayanasamy, Satish},
title = {DoublePlay: Parallelizing Sequential Logging and Replay},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {1},
issn = {0734-2071},
url = {https://doi.org/10.1145/2110356.2110359},
doi = {10.1145/2110356.2110359},
abstract = {Deterministic replay systems record and reproduce the execution of a hardware or software system. In contrast to replaying execution on uniprocessors, deterministic replay on multiprocessors is very challenging to implement efficiently because of the need to reproduce the order of or the values read by shared memory operations performed by multiple threads. In this paper, we present DoublePlay, a new way to efficiently guarantee replay on commodity multiprocessors. Our key insight is that one can use the simpler and faster mechanisms of single-processor record and replay, yet still achieve the scalability offered by multiple cores, by using an additional execution to parallelize the record and replay of an application. DoublePlay timeslices multiple threads on a single processor, then runs multiple time intervals (epochs) of the program concurrently on separate processors. This strategy, which we call uniparallelism, makes logging much easier because each epoch runs on a single processor (so threads in an epoch never simultaneously access the same memory) and different epochs operate on different copies of the memory. Thus, rather than logging the order of shared-memory accesses, we need only log the order in which threads in an epoch are timesliced on the processor. DoublePlay runs an additional execution of the program on multiple processors to generate checkpoints so that epochs run in parallel. We evaluate DoublePlay on a variety of client, server, and scientific parallel benchmarks; with spare cores, DoublePlay reduces logging overhead to an average of 15% with two worker threads and 28% with four threads.},
journal = {ACM Trans. Comput. Syst.},
month = feb,
articleno = {3},
numpages = {24},
keywords = {Deterministic replay, uniparallelism}
}

@inproceedings{10.1145/1950365.1950370,
author = {Veeraraghavan, Kaushik and Lee, Dongyoon and Wester, Benjamin and Ouyang, Jessica and Chen, Peter M. and Flinn, Jason and Narayanasamy, Satish},
title = {DoublePlay: Parallelizing Sequential Logging and Replay},
year = {2011},
isbn = {9781450302661},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1950365.1950370},
doi = {10.1145/1950365.1950370},
abstract = {Deterministic replay systems record and reproduce the execution of a hardware or software system. In contrast to replaying execution on uniprocessors, deterministic replay on multiprocessors is very challenging to implement efficiently because of the need to reproduce the order or values read by shared memory operations performed by multiple threads. In this paper, we present DoublePlay, a new way to efficiently guarantee replay on commodity multiprocessors. Our key insight is that one can use the simpler and faster mechanisms of single-processor record and replay, yet still achieve the scalability offered by multiple cores, by using an additional execution to parallelize the record and replay of an application. DoublePlay timeslices multiple threads on a single processor, then runs multiple time intervals (epochs) of the program concurrently on separate processors. This strategy, which we call uniparallelism, makes logging much easier because each epoch runs on a single processor (so threads in an epoch never simultaneously access the same memory) and different epochs operate on different copies of the memory. Thus, rather than logging the order of shared-memory accesses, we need only log the order in which threads in an epoch are timesliced on the processor. DoublePlay runs an additional execution of the program on multiple processors to generate checkpoints so that epochs run in parallel. We evaluate DoublePlay on a variety of client, server, and scientific parallel benchmarks; with spare cores, DoublePlay reduces logging overhead to an average of 15% with two worker threads and 28% with four threads.},
booktitle = {Proceedings of the Sixteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {15–26},
numpages = {12},
keywords = {uniparallelism, deterministic replay},
location = {Newport Beach, California, USA},
series = {ASPLOS XVI}
}

@article{10.1145/1961295.1950370,
author = {Veeraraghavan, Kaushik and Lee, Dongyoon and Wester, Benjamin and Ouyang, Jessica and Chen, Peter M. and Flinn, Jason and Narayanasamy, Satish},
title = {DoublePlay: Parallelizing Sequential Logging and Replay},
year = {2011},
issue_date = {March 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/1961295.1950370},
doi = {10.1145/1961295.1950370},
abstract = {Deterministic replay systems record and reproduce the execution of a hardware or software system. In contrast to replaying execution on uniprocessors, deterministic replay on multiprocessors is very challenging to implement efficiently because of the need to reproduce the order or values read by shared memory operations performed by multiple threads. In this paper, we present DoublePlay, a new way to efficiently guarantee replay on commodity multiprocessors. Our key insight is that one can use the simpler and faster mechanisms of single-processor record and replay, yet still achieve the scalability offered by multiple cores, by using an additional execution to parallelize the record and replay of an application. DoublePlay timeslices multiple threads on a single processor, then runs multiple time intervals (epochs) of the program concurrently on separate processors. This strategy, which we call uniparallelism, makes logging much easier because each epoch runs on a single processor (so threads in an epoch never simultaneously access the same memory) and different epochs operate on different copies of the memory. Thus, rather than logging the order of shared-memory accesses, we need only log the order in which threads in an epoch are timesliced on the processor. DoublePlay runs an additional execution of the program on multiple processors to generate checkpoints so that epochs run in parallel. We evaluate DoublePlay on a variety of client, server, and scientific parallel benchmarks; with spare cores, DoublePlay reduces logging overhead to an average of 15% with two worker threads and 28% with four threads.},
journal = {SIGARCH Comput. Archit. News},
month = mar,
pages = {15–26},
numpages = {12},
keywords = {uniparallelism, deterministic replay}
}

@article{10.1145/1961296.1950370,
author = {Veeraraghavan, Kaushik and Lee, Dongyoon and Wester, Benjamin and Ouyang, Jessica and Chen, Peter M. and Flinn, Jason and Narayanasamy, Satish},
title = {DoublePlay: Parallelizing Sequential Logging and Replay},
year = {2011},
issue_date = {March 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {3},
issn = {0362-1340},
url = {https://doi.org/10.1145/1961296.1950370},
doi = {10.1145/1961296.1950370},
abstract = {Deterministic replay systems record and reproduce the execution of a hardware or software system. In contrast to replaying execution on uniprocessors, deterministic replay on multiprocessors is very challenging to implement efficiently because of the need to reproduce the order or values read by shared memory operations performed by multiple threads. In this paper, we present DoublePlay, a new way to efficiently guarantee replay on commodity multiprocessors. Our key insight is that one can use the simpler and faster mechanisms of single-processor record and replay, yet still achieve the scalability offered by multiple cores, by using an additional execution to parallelize the record and replay of an application. DoublePlay timeslices multiple threads on a single processor, then runs multiple time intervals (epochs) of the program concurrently on separate processors. This strategy, which we call uniparallelism, makes logging much easier because each epoch runs on a single processor (so threads in an epoch never simultaneously access the same memory) and different epochs operate on different copies of the memory. Thus, rather than logging the order of shared-memory accesses, we need only log the order in which threads in an epoch are timesliced on the processor. DoublePlay runs an additional execution of the program on multiple processors to generate checkpoints so that epochs run in parallel. We evaluate DoublePlay on a variety of client, server, and scientific parallel benchmarks; with spare cores, DoublePlay reduces logging overhead to an average of 15% with two worker threads and 28% with four threads.},
journal = {SIGPLAN Not.},
month = mar,
pages = {15–26},
numpages = {12},
keywords = {uniparallelism, deterministic replay}
}

@inproceedings{10.1145/2503210.2503230,
author = {Browne, James C. and DeLeon, Robert L. and Lu, Charng-Da and Jones, Matthew D. and Gallo, Steven M. and Ghadersohi, Amin and Patra, Abani K. and Barth, William L. and Hammond, John and Furlani, Thomas R. and McLay, Robert T.},
title = {Enabling Comprehensive Data-Driven System Management for Large Computational Facilities},
year = {2013},
isbn = {9781450323789},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2503210.2503230},
doi = {10.1145/2503210.2503230},
abstract = {This paper presents a tool chain, based on the open source tool TACC_Stats, for systematic and comprehensive job level resource use measurement for large cluster computers, and its incorporation into XDMoD, a reporting and analytics framework for resource management that targets meeting the information needs of users, application developers, systems administrators, systems management and funding managers. Accounting, scheduler and event logs are integrated with system performance data from TACC_Stats. TACC_Stats periodically records resource use including many hardware counters for each job running on each node. Furthermore, system level metrics are obtained through aggregation of the node (job) level data. Analysis of this data generates many types of standard and custom reports and even a limited predictive capability that has not previously been available for open-source, Linux-based software systems. This paper presents case studies of information that can be applied for effective resource management. We believe this system to be the first fully comprehensive system for supporting the information needs of all stakeholders in open-source software based HPC systems.},
booktitle = {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
articleno = {86},
numpages = {11},
location = {Denver, Colorado},
series = {SC '13}
}

@article{10.1145/3092697,
author = {Li, Tao and Zeng, Chunqiu and Jiang, Yexi and Zhou, Wubai and Tang, Liang and Liu, Zheng and Huang, Yue},
title = {Data-Driven Techniques in Computing System Management},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3092697},
doi = {10.1145/3092697},
abstract = {Modern forms of computing systems are becoming progressively more complex, with an increasing number of heterogeneous hardware and software components. As a result, it is quite challenging to manage these complex systems and meet the requirements in manageability, dependability, and performance that are demanded by enterprise customers. This survey presents a variety of data-driven techniques and applications with a focus on computing system management. In particular, the survey introduces intelligent methods for event generation that can transform diverse log data sources into structured events, reviews different types of event patterns and the corresponding event-mining techniques, and summarizes various event summarization methods and data-driven approaches for problem diagnosis in system management. We hope this survey will provide a good overview for data-driven techniques in computing system management.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {45},
numpages = {43},
keywords = {application, Computing system management, data mining}
}

@article{10.1109/TNET.2019.2930040,
author = {Satpathi, Siddhartha and Deb, Supratim and Srikant, R. and Yan, He},
title = {Learning Latent Events From Network Message Logs},
year = {2019},
issue_date = {August 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2019.2930040},
doi = {10.1109/TNET.2019.2930040},
abstract = {We consider the problem of separating error messages generated in large distributed data center networks into error events. In such networks, each error event leads to a stream of messages generated by hardware and software components affected by the event. These messages are stored in a giant message log. We consider the unsupervised learning problem of identifying the signatures of events that generated these messages; here, the signature of an error event refers to the mixture of messages generated by the event. One of the main contributions of the paper is a novel mapping of our problem which transforms it into a problem of topic discovery in documents. Events in our problem correspond to topics and messages in our problem correspond to words in the topic discovery problem. However, there is no direct analog of documents. Therefore, we use a non-parametric change-point detection algorithm, which has linear computational complexity in the number of messages, to divide the message log into smaller subsets called episodes, which serve as the equivalents of documents. After this mapping has been done, we use a well-known algorithm for topic discovery, called LDA, to solve our problem. We theoretically analyze the change-point detection algorithm, and show that it is consistent and has low sample complexity. We also demonstrate the scalability of our algorithm on a real data set consisting of 97 million messages collected over a period of 15 days, from a distributed data center network which supports the operations of a large wireless service provider.},
journal = {IEEE/ACM Trans. Netw.},
month = aug,
pages = {1728–1741},
numpages = {14}
}

@inproceedings{10.1145/3401025.3401740,
author = {Scrocca, Mario and Tommasini, Riccardo and Margara, Alessandro and Valle, Emanuele Della and Sakr, Sherif},
title = {The Kaiju Project: Enabling Event-Driven Observability},
year = {2020},
isbn = {9781450380287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3401025.3401740},
doi = {10.1145/3401025.3401740},
abstract = {Microservices architectures are getting momentum. Even small and medium-size companies are migrating towards cloud-based distributed solutions supported by lightweight virtualization techniques, containers, and orchestration systems. In this context, understanding the system behavior at runtime is critical to promptly react to errors. Unfortunately, traditional monitoring techniques are not adequate for such complex and dynamic environments. Therefore, a new challenge, namely observability, emerged from precise industrial needs: expose and make sense of the system behavior at runtime. In this paper, we investigate observability as a research problem. We discuss the benefits of events as a unified abstraction for metrics, logs, and trace data, and the advantages of employing event stream processing techniques and tools in this context. We show that an event-based approach enables understanding the system behavior in near real-time more effectively than state-of-the-art solutions in the field. We implement our model in the Kaiju system and we validate it against a realistic deployment supported by a software company.},
booktitle = {Proceedings of the 14th ACM International Conference on Distributed and Event-Based Systems},
pages = {85–96},
numpages = {12},
keywords = {event stream processing, observability, event-based systems, orchestration systems},
location = {Montreal, Quebec, Canada},
series = {DEBS '20}
}

@inproceedings{10.1145/2668332.2668337,
author = {Brouwers, Niels and Zuniga, Marco and Langendoen, Koen},
title = {NEAT: A Novel Energy Analysis Toolkit for Free-Roaming Smartphones},
year = {2014},
isbn = {9781450331432},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668332.2668337},
doi = {10.1145/2668332.2668337},
abstract = {Analyzing the power consumption of smartphones is difficult because of the complex interplay between soft- and hardware. Currently, researchers rely on mainly two options: external measurement tools, which are precise but constrain the mobility of the device and require the annotation of power traces; or modelling methods, which allow mobility and consider explicitly the state of events, but have less accuracy and lower sampling rates than external tools.We address the challenges of mobile power analysis with a novel power metering toolkit, called NEAT, which comprises a coin-sized power measurement board that fits inside a typical smartphone, and analysis software that automatically fuses the event logs taken from the phone with the obtained power trace. The combination of high-fidelity power measurements and detailed information about the state of the phone's hardware and software components allows for fine-grained analysis of complex and short-lived energy patterns.We equipped smartphones with NEAT and conducted various experiments to highlight (i) its accuracy with respect to model-based approaches, showing errors upwards of 20%; (ii) its ability to gather accurate and well annotated user-data "in the wild", which would be hard to do with current external meters; and (iii) the importance of having fine-granular and expressive traces by resolving kernel energy bugs.},
booktitle = {Proceedings of the 12th ACM Conference on Embedded Network Sensor Systems},
pages = {16–30},
numpages = {15},
keywords = {trace visualization, mobility, power monitor, accuracy},
location = {Memphis, Tennessee},
series = {SenSys '14}
}

@inproceedings{10.1145/1736020.1736031,
author = {Lee, Dongyoon and Wester, Benjamin and Veeraraghavan, Kaushik and Narayanasamy, Satish and Chen, Peter M. and Flinn, Jason},
title = {Respec: Efficient Online Multiprocessor Replayvia Speculation and External Determinism},
year = {2010},
isbn = {9781605588391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1736020.1736031},
doi = {10.1145/1736020.1736031},
abstract = {Deterministic replay systems record and reproduce the execution of a hardware or software system. While it is well known how to replay uniprocessor systems, replaying shared memory multiprocessor systems at low overhead on commodity hardware is still an open problem. This paper presents Respec, a new way to support deterministic replay of shared memory multithreaded programs on commodity multiprocessor hardware. Respec targets online replay in which the recorded and replayed processes execute concurrently.Respec uses two strategies to reduce overhead while still ensuring correctness: speculative logging and externally deterministic replay. Speculative logging optimistically logs less information about shared memory dependencies than is needed to guarantee deterministic replay, then recovers and retries if the replayed process diverges from the recorded process. Externally deterministic replay relaxes the degree to which the two executions must match by requiring only their system output and final program states match. We show that the combination of these two techniques results in low recording and replay overhead for the common case of data-race-free execution intervals and still ensures correct replay for execution intervals that have data races.We modified the Linux kernel to implement our techniques. Our software system adds on average about 18% overhead to the execution time for recording and replaying programs with two threads and 55% overhead for programs with four threads.},
booktitle = {Proceedings of the Fifteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {77–90},
numpages = {14},
keywords = {external determinism, speculative execution, replay},
location = {Pittsburgh, Pennsylvania, USA},
series = {ASPLOS XV}
}

@article{10.1145/1735971.1736031,
author = {Lee, Dongyoon and Wester, Benjamin and Veeraraghavan, Kaushik and Narayanasamy, Satish and Chen, Peter M. and Flinn, Jason},
title = {Respec: Efficient Online Multiprocessor Replayvia Speculation and External Determinism},
year = {2010},
issue_date = {March 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {3},
issn = {0362-1340},
url = {https://doi.org/10.1145/1735971.1736031},
doi = {10.1145/1735971.1736031},
abstract = {Deterministic replay systems record and reproduce the execution of a hardware or software system. While it is well known how to replay uniprocessor systems, replaying shared memory multiprocessor systems at low overhead on commodity hardware is still an open problem. This paper presents Respec, a new way to support deterministic replay of shared memory multithreaded programs on commodity multiprocessor hardware. Respec targets online replay in which the recorded and replayed processes execute concurrently.Respec uses two strategies to reduce overhead while still ensuring correctness: speculative logging and externally deterministic replay. Speculative logging optimistically logs less information about shared memory dependencies than is needed to guarantee deterministic replay, then recovers and retries if the replayed process diverges from the recorded process. Externally deterministic replay relaxes the degree to which the two executions must match by requiring only their system output and final program states match. We show that the combination of these two techniques results in low recording and replay overhead for the common case of data-race-free execution intervals and still ensures correct replay for execution intervals that have data races.We modified the Linux kernel to implement our techniques. Our software system adds on average about 18% overhead to the execution time for recording and replaying programs with two threads and 55% overhead for programs with four threads.},
journal = {SIGPLAN Not.},
month = mar,
pages = {77–90},
numpages = {14},
keywords = {speculative execution, external determinism, replay}
}

@article{10.1145/1735970.1736031,
author = {Lee, Dongyoon and Wester, Benjamin and Veeraraghavan, Kaushik and Narayanasamy, Satish and Chen, Peter M. and Flinn, Jason},
title = {Respec: Efficient Online Multiprocessor Replayvia Speculation and External Determinism},
year = {2010},
issue_date = {March 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/1735970.1736031},
doi = {10.1145/1735970.1736031},
abstract = {Deterministic replay systems record and reproduce the execution of a hardware or software system. While it is well known how to replay uniprocessor systems, replaying shared memory multiprocessor systems at low overhead on commodity hardware is still an open problem. This paper presents Respec, a new way to support deterministic replay of shared memory multithreaded programs on commodity multiprocessor hardware. Respec targets online replay in which the recorded and replayed processes execute concurrently.Respec uses two strategies to reduce overhead while still ensuring correctness: speculative logging and externally deterministic replay. Speculative logging optimistically logs less information about shared memory dependencies than is needed to guarantee deterministic replay, then recovers and retries if the replayed process diverges from the recorded process. Externally deterministic replay relaxes the degree to which the two executions must match by requiring only their system output and final program states match. We show that the combination of these two techniques results in low recording and replay overhead for the common case of data-race-free execution intervals and still ensures correct replay for execution intervals that have data races.We modified the Linux kernel to implement our techniques. Our software system adds on average about 18% overhead to the execution time for recording and replaying programs with two threads and 55% overhead for programs with four threads.},
journal = {SIGARCH Comput. Archit. News},
month = mar,
pages = {77–90},
numpages = {14},
keywords = {external determinism, speculative execution, replay}
}

@inproceedings{10.1145/2245276.2231949,
author = {P\'{e}rez-Castillo, Ricardo and de Guzm\'{a}n, Ignacio Garc\'{\i}a-Rodr\'{\i}guez and Piattini, Mario and Weber, Barbara},
title = {Integrating Event Logs into KDM Repositories},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2231949},
doi = {10.1145/2245276.2231949},
abstract = {Business knowledge embedded in legacy information systems is a valuable asset that must be recovered and preserved when these systems are modernized. Event logs register the execution of business activities supported by existing information systems, thus they entail a key artifact to be used for recovering the actual business processes. There exists a wide variety of techniques to discover business processes by reversing event logs. Unfortunately, event logs are typically represented with particular notations such as Mining XML (MXML) rather than the recent software modernization standard Knowledge Discovery Metamodel (KDM). Process mining techniques consequently cannot be effectively reused within software modernization projects. This paper proposes an automatic technique to transform MXML event logs into event models to be integrated into KDM repositories. Its main implication is the exploitation of valuable event logs by well-proven software modernization techniques. The model transformation has been validated through a case study involving several benchmark event logs.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {1095–1102},
numpages = {8},
keywords = {model-driven reengineering, business process, event logs, knowledge discovery metamodel},
location = {Trento, Italy},
series = {SAC '12}
}

@inproceedings{10.1145/3196321.3196345,
author = {Castro, Diego and Schots, Marcelo},
title = {Analysis of Test Log Information through Interactive Visualizations},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3196345},
doi = {10.1145/3196321.3196345},
abstract = {A fundamental activity to achieve software quality is software testing, whose results are typically stored in log files. These files contain the richest and more detailed source of information for developers trying to understand failures and identify their potential causes. Analyzing and understanding the information presented in log files, however, can be a complex task, depending on the amount of errors and the variety of information. Some previously proposed tools try to visualize test information, but they have limited interaction and present a single perspective of such data. This paper presents ASTRO, an infrastructure that extracts information from a number of log files and presents it in multi-perspective interactive visualizations that aim at easing and improving the developers' analysis process. A study carried out with practitioners from 3 software test factories indicated that ASTRO helps to analyze information of interest, with less accuracy in tasks that involved assimilation of information from different perspectives. Based on their difficulties, participants also provided feedback for improving the tool.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {156–166},
numpages = {11},
keywords = {test log analysis, information visualization, software testing},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@article{10.1145/3309987,
author = {Bergman, Shai and Brokhman, Tanya and Cohen, Tzachi and Silberstein, Mark},
title = {SPIN: Seamless Operating System Integration of Peer-to-Peer DMA Between SSDs and GPUs},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {2},
issn = {0734-2071},
url = {https://doi.org/10.1145/3309987},
doi = {10.1145/3309987},
abstract = {Recent GPUs enable Peer-to-Peer Direct Memory Access (p2p) from fast peripheral devices like NVMe SSDs to exclude the CPU from the data path between them for efficiency. Unfortunately, using p2p to access files is challenging because of the subtleties of low-level non-standard interfaces, which bypass the OS file I/O layers and may hurt system performance. Developers must possess intimate knowledge of low-level interfaces to manually handle the subtleties of data consistency and misaligned accesses.We present SPIN, which integrates p2p into the standard OS file I/O stack, dynamically activating p2p where appropriate, transparently to the user. It combines p2p with page cache accesses, re-enables read-ahead for sequential reads, all while maintaining standard POSIX FS consistency, portability across GPUs and SSDs, and compatibility with virtual block devices such as software RAID.We evaluate SPIN on NVIDIA and AMD GPUs using standard file I/O benchmarks, application traces, and end-to-end experiments. SPIN achieves significant performance speedups across a wide range of workloads, exceeding p2p throughput by up to an order of magnitude. It also boosts the performance of an aerial imagery rendering application by 2.6\texttimes{} by dynamically adapting to its input-dependent file access pattern, enables 3.3\texttimes{} higher throughput for a GPU-accelerated log server, and enables 29% faster execution for the highly optimized GPU-accelerated image collage with only 30 changed lines of code.},
journal = {ACM Trans. Comput. Syst.},
month = apr,
articleno = {5},
numpages = {26},
keywords = {I/O subsystem, GPU, operating systems, file systems, Accelerators}
}

@inproceedings{10.1145/2591062.2591172,
author = {Di Martino, Catello and Kalbarczyk, Zbigniew and Iyer, Ravishankar K. and Goel, Geetika and Sarkar, Santonu and Ganesan, Rajeshwari},
title = {Characterization of Operational Failures from a Business Data Processing SaaS Platform},
year = {2014},
isbn = {9781450327688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591062.2591172},
doi = {10.1145/2591062.2591172},
abstract = { This paper characterizes operational failures of a production Custom Package Good Software-as-a-Service (SaaS) platform. Events log collected over 283 days of in-field operation are used to characterize platform failures. The characterization is performed by estimating (i) common failure types of the platform, (ii) key factors impacting platform failures, (iii) failure rate, and (iv) how user workload (files submitted for processing) impacts on the failure rate. The major findings are: (i) 34.1% of failures are caused by unexpected values in customers' data, (ii) nearly 33% of the failures are because of timeout, and (iii) the failure rate increases if the workload intensity (transactions/second) increases, while there is no statistical evidence of being influenced by the workload volume (size of users' data). Finally, the paper presents the lessons learned and how the findings and the implemented analysis tool allow platform developers to improve platform code, system settings and customer management. },
booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
pages = {195–204},
numpages = {10},
keywords = {Logs, Cloud Computing, Robustness, Failure Analysis, SaaS},
location = {Hyderabad, India},
series = {ICSE Companion 2014}
}

@inproceedings{10.5555/3026877.3026924,
author = {Zhao, Xu and Rodrigues, Kirk and Luo, Yu and Yuan, Ding and Stumm, Michael},
title = {Non-Intrusive Performance Profiling for Entire Software Stacks Based on the Flow Reconstruction Principle},
year = {2016},
isbn = {9781931971331},
publisher = {USENIX Association},
address = {USA},
abstract = {Understanding the performance behavior of distributed server stacks at scale is non-trivial. The servicing of just a single request can trigger numerous sub-requests across heterogeneous software components; and many similar requests are serviced concurrently and in parallel. When a user experiences poor performance, it is extremely difficult to identify the root cause, as well as the software components and machines that are the culprits.This paper describes Stitch, a non-intrusive tool capable of profiling the performance of an entire distributed software stack solely using the unstructured logs output by heterogeneous software components. Stitch is substantially different from all prior related tools in that it is capable of constructing a system model of an entire software stack without building any domain knowledge into Stitch. Instead, it automatically reconstructs the extensive domain knowledge of the programmers who wrote the code; it does this by relying on the Flow Reconstruction Principle which states that programmers log events such that one can reliably reconstruct the execution flow a posteriori.},
booktitle = {Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation},
pages = {603–618},
numpages = {16},
location = {Savannah, GA, USA},
series = {OSDI'16}
}

@inproceedings{10.1145/2735711.2735788,
author = {Shimozuru, Kota and Terada, Tsutomu and Tsukamoto, Masahiko},
title = {A Life Log System That Recognizes the Objects in a Pocket},
year = {2015},
isbn = {9781450333498},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2735711.2735788},
doi = {10.1145/2735711.2735788},
abstract = {A novel approach has been developed for recognizing objects in pockets and for recording the events related to the objects. Information on putting an object into or taking it out of a pocket is closely related to user contexts. For example, when a house key is taken out from a pocket, the owner of the key is likely just getting home. We implemented a objects-in-pocket recognition device, which has a pair of infrared sensors arranged in a matrix, and life log software to obtain the time stamp of events happening. We evaluated whether or not the system could deal with one of five objects (a smartphone, ticket, hand, key, and lip balm) using template matching. When one registered object (the smartphone, ticket, or key) was put in the pocket, our system recognized the object correctly 91% of the time on average. We also evaluated our system in one action scenario. With our system's time stamps, user could easily remember what he took on that day and when he used the items.},
booktitle = {Proceedings of the 6th Augmented Human International Conference},
pages = {81–88},
numpages = {8},
keywords = {wearable computing, pockets, life log, context awareness},
location = {Singapore, Singapore},
series = {AH '15}
}

@article{10.14778/1920841.1920928,
author = {Johnson, Ryan and Pandis, Ippokratis and Stoica, Radu and Athanassoulis, Manos and Ailamaki, Anastasia},
title = {Aether: A Scalable Approach to Logging},
year = {2010},
issue_date = {September 2010},
publisher = {VLDB Endowment},
volume = {3},
number = {1–2},
issn = {2150-8097},
url = {https://doi.org/10.14778/1920841.1920928},
doi = {10.14778/1920841.1920928},
abstract = {The shift to multi-core hardware brings new challenges to database systems, as the software parallelism determines performance. Even though database systems traditionally accommodate simultaneous requests, a multitude of synchronization barriers serialize execution. Write-ahead logging is a fundamental, omnipresent component in ARIES-style concurrency and recovery, and one of the most important yet-to-be addressed potential bottlenecks, especially in OLTP workloads making frequent small changes to data.In this paper, we identify four logging-related impediments to database system scalability. Each issue challenges different level in the software architecture: (a) the high volume of small-sized I/O requests may saturate the disk, (b) transactions hold locks while waiting for the log flush, (c) extensive context switching overwhelms the OS scheduler with threads executing log I/Os, and (d) contention appears as transactions serialize accesses to in-memory log data structures. We demonstrate these problems and address them with techniques that, when combined, comprise a holistic, scalable approach to logging. Our solution achieves a 20%-69% speedup over a modern database system when running log-intensive workloads, such as the TPC-B and TATP benchmarks. Moreover, it achieves log insert throughput over 1.8GB/s for small log records on a single socket server, an order of magnitude higher than the traditional way of accessing the log using a single mutex.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {681–692},
numpages = {12}
}

@inproceedings{10.1145/1736020.1736038,
author = {Yuan, Ding and Mai, Haohui and Xiong, Weiwei and Tan, Lin and Zhou, Yuanyuan and Pasupathy, Shankar},
title = {SherLog: Error Diagnosis by Connecting Clues from Run-Time Logs},
year = {2010},
isbn = {9781605588391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1736020.1736038},
doi = {10.1145/1736020.1736038},
abstract = {Computer systems often fail due to many factors such as software bugs or administrator errors. Diagnosing such production run failures is an important but challenging task since it is difficult to reproduce them in house due to various reasons: (1) unavailability of users' inputs and file content due to privacy concerns; (2) difficulty in building the exact same execution environment; and (3) non-determinism of concurrent executions on multi-processors.Therefore, programmers often have to diagnose a production run failure based on logs collected back from customers and the corresponding source code. Such diagnosis requires expert knowledge and is also too time-consuming, tedious to narrow down root causes. To address this problem, we propose a tool, called SherLog, that analyzes source code by leveraging information provided by run-time logs to infer what must or may have happened during the failed production run. It requires neither re-execution of the program nor knowledge on the log's semantics. It infers both control and data value information regarding to the failed execution.We evaluate SherLog with 8 representative real world software failures (6 software bugs and 2 configuration errors) from 7 applications including 3 servers. Information inferred by SherLog are very useful for programmers to diagnose these evaluated failures. Our results also show that SherLog can analyze large server applications such as Apache with thousands of logging messages within only 40 minutes.},
booktitle = {Proceedings of the Fifteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {143–154},
numpages = {12},
keywords = {static analysis, log, failure diagnostics},
location = {Pittsburgh, Pennsylvania, USA},
series = {ASPLOS XV}
}

@article{10.1145/1735970.1736038,
author = {Yuan, Ding and Mai, Haohui and Xiong, Weiwei and Tan, Lin and Zhou, Yuanyuan and Pasupathy, Shankar},
title = {SherLog: Error Diagnosis by Connecting Clues from Run-Time Logs},
year = {2010},
issue_date = {March 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/1735970.1736038},
doi = {10.1145/1735970.1736038},
abstract = {Computer systems often fail due to many factors such as software bugs or administrator errors. Diagnosing such production run failures is an important but challenging task since it is difficult to reproduce them in house due to various reasons: (1) unavailability of users' inputs and file content due to privacy concerns; (2) difficulty in building the exact same execution environment; and (3) non-determinism of concurrent executions on multi-processors.Therefore, programmers often have to diagnose a production run failure based on logs collected back from customers and the corresponding source code. Such diagnosis requires expert knowledge and is also too time-consuming, tedious to narrow down root causes. To address this problem, we propose a tool, called SherLog, that analyzes source code by leveraging information provided by run-time logs to infer what must or may have happened during the failed production run. It requires neither re-execution of the program nor knowledge on the log's semantics. It infers both control and data value information regarding to the failed execution.We evaluate SherLog with 8 representative real world software failures (6 software bugs and 2 configuration errors) from 7 applications including 3 servers. Information inferred by SherLog are very useful for programmers to diagnose these evaluated failures. Our results also show that SherLog can analyze large server applications such as Apache with thousands of logging messages within only 40 minutes.},
journal = {SIGARCH Comput. Archit. News},
month = mar,
pages = {143–154},
numpages = {12},
keywords = {static analysis, log, failure diagnostics}
}

@article{10.1145/1735971.1736038,
author = {Yuan, Ding and Mai, Haohui and Xiong, Weiwei and Tan, Lin and Zhou, Yuanyuan and Pasupathy, Shankar},
title = {SherLog: Error Diagnosis by Connecting Clues from Run-Time Logs},
year = {2010},
issue_date = {March 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {3},
issn = {0362-1340},
url = {https://doi.org/10.1145/1735971.1736038},
doi = {10.1145/1735971.1736038},
abstract = {Computer systems often fail due to many factors such as software bugs or administrator errors. Diagnosing such production run failures is an important but challenging task since it is difficult to reproduce them in house due to various reasons: (1) unavailability of users' inputs and file content due to privacy concerns; (2) difficulty in building the exact same execution environment; and (3) non-determinism of concurrent executions on multi-processors.Therefore, programmers often have to diagnose a production run failure based on logs collected back from customers and the corresponding source code. Such diagnosis requires expert knowledge and is also too time-consuming, tedious to narrow down root causes. To address this problem, we propose a tool, called SherLog, that analyzes source code by leveraging information provided by run-time logs to infer what must or may have happened during the failed production run. It requires neither re-execution of the program nor knowledge on the log's semantics. It infers both control and data value information regarding to the failed execution.We evaluate SherLog with 8 representative real world software failures (6 software bugs and 2 configuration errors) from 7 applications including 3 servers. Information inferred by SherLog are very useful for programmers to diagnose these evaluated failures. Our results also show that SherLog can analyze large server applications such as Apache with thousands of logging messages within only 40 minutes.},
journal = {SIGPLAN Not.},
month = mar,
pages = {143–154},
numpages = {12},
keywords = {log, failure diagnostics, static analysis}
}

@inproceedings{10.1145/3141128.3141135,
author = {Carhuamaca, Danny and Aire, Frank and Cornejo, Juan and Chauca, Mario},
title = {Machine Dispensing With Payment System Digital and Local Database},
year = {2017},
isbn = {9781450353434},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3141128.3141135},
doi = {10.1145/3141128.3141135},
abstract = {This article offers a new alternative to buy at vending machines.The system will have a software where the customer will log to get access to a virtual list of products of the different vending machines that the user chooses; In addition to this, a money record will be enabled for the respective client, it means client can charge money per month or whenever he wants.The client, once registered in the application, will be able to buy the desired product. Or in case the product is not found, the machine will show a map to the nearest point of supply.},
booktitle = {Proceedings of the 2017 International Conference on Cloud and Big Data Computing},
pages = {50–54},
numpages = {5},
keywords = {Traffic building, Trade marketing, PayPal, Merchandising},
location = {London, United Kingdom},
series = {ICCBDC 2017}
}

@inproceedings{10.1145/3067695.3082473,
author = {Garc\'{\i}a-Valdez, Mario and Merelo, JJ},
title = {Evospace-Js: Asynchronous Pool-Based Execution of Heterogeneous Metaheuristics},
year = {2017},
isbn = {9781450349390},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3067695.3082473},
doi = {10.1145/3067695.3082473},
abstract = {This paper is part of a continuing effort in the field of EC to develop algorithms that follow an opportunistic approach to computing, allowing the exploitation of freely available services over the Internet by using free tiers of cloud services or volunteer computing resources; the EvoSpace model is able to tap from both kind of resources, using asynchronous evolutionary algorithms. We present its design, which follows an an event-driven architecture and asynchronous I/O model, and its implementation, with a server-side tier programmed in Node.js that uses Redis as an in-memory and high performance data store for the population. This population store is exposed to clients running population-based and nature-inspired metaheuristics through a REST API. Additional capabilities where implemented in this version to allow the logging of experiments where heterogeneous algorithms are executed in parallel. These logs can then be transformed to other formats. As a case study an hybrid global optimization algorithm has been implemented mixing two algorithms: a PSO algorithm from the EvoloPy library and a GA using the DEAP framework. The result was transformed to files compatible to the Comparing Continuous Optimizer platform in order to use their post-processing code. Clients in this case have been developed in the Python language, the language used to implement both libraries. The results from this case study suggest, first, that EvoSpace can be used as a paradigm- and language-agnostic platform for population-based optimization algorithms, and also that this software can yield performance improvements and a viable platform to execute and compare asynchronous pool-based metaheuristics.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1202–1208},
numpages = {7},
keywords = {distributed evolutionary algorithms, nature-inspired metaheuristics},
location = {Berlin, Germany},
series = {GECCO '17}
}

@inproceedings{10.1145/2485922.2485977,
author = {Pokam, Gilles and Danne, Klaus and Pereira, Cristiano and Kassa, Rolf and Kranich, Tim and Hu, Shiliang and Gottschlich, Justin and Honarmand, Nima and Dautenhahn, Nathan and King, Samuel T. and Torrellas, Josep},
title = {QuickRec: Prototyping an Intel Architecture Extension for Record and Replay of Multithreaded Programs},
year = {2013},
isbn = {9781450320795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2485922.2485977},
doi = {10.1145/2485922.2485977},
abstract = {There has been significant interest in hardware-assisted deterministic Record and Replay (RnR) systems for multithreaded programs on multiprocessors. However, no proposal has implemented this technique in a hardware prototype with full operating system support. Such an implementation is needed to assess RnR practicality.This paper presents QuickRec, the first multicore Intel Architecture (IA) prototype of RnR for multithreaded programs. QuickRec is based on QuickIA, an Intel emulation platform for rapid prototyping of new IA extensions. QuickRec is composed of a Xeon server platform with FPGA-emulated second-generation Pentium cores, and Capo3, a full software stack for managing the recording hardware from within a modified Linux kernel.This paper's focus is understanding and evaluating the implementation issues of RnR on a real platform. Our effort leads to some lessons learned, as well as to some pointers for future research. We demonstrate that RnR can be implemented efficiently on a real multicore IA system. In particular, we show that the rate of memory log generation is insignificant, and that the recording hardware has negligible performance overhead. However, the software stack incurs an average recording overhead of nearly 13%, which must be reduced to enable always-on use of RnR.},
booktitle = {Proceedings of the 40th Annual International Symposium on Computer Architecture},
pages = {643–654},
numpages = {12},
keywords = {hardware-software interface, deterministic record and replay, shared memory multiprocessors, FPGA prototype},
location = {Tel-Aviv, Israel},
series = {ISCA '13}
}

@article{10.1145/2508148.2485977,
author = {Pokam, Gilles and Danne, Klaus and Pereira, Cristiano and Kassa, Rolf and Kranich, Tim and Hu, Shiliang and Gottschlich, Justin and Honarmand, Nima and Dautenhahn, Nathan and King, Samuel T. and Torrellas, Josep},
title = {QuickRec: Prototyping an Intel Architecture Extension for Record and Replay of Multithreaded Programs},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0163-5964},
url = {https://doi.org/10.1145/2508148.2485977},
doi = {10.1145/2508148.2485977},
abstract = {There has been significant interest in hardware-assisted deterministic Record and Replay (RnR) systems for multithreaded programs on multiprocessors. However, no proposal has implemented this technique in a hardware prototype with full operating system support. Such an implementation is needed to assess RnR practicality.This paper presents QuickRec, the first multicore Intel Architecture (IA) prototype of RnR for multithreaded programs. QuickRec is based on QuickIA, an Intel emulation platform for rapid prototyping of new IA extensions. QuickRec is composed of a Xeon server platform with FPGA-emulated second-generation Pentium cores, and Capo3, a full software stack for managing the recording hardware from within a modified Linux kernel.This paper's focus is understanding and evaluating the implementation issues of RnR on a real platform. Our effort leads to some lessons learned, as well as to some pointers for future research. We demonstrate that RnR can be implemented efficiently on a real multicore IA system. In particular, we show that the rate of memory log generation is insignificant, and that the recording hardware has negligible performance overhead. However, the software stack incurs an average recording overhead of nearly 13%, which must be reduced to enable always-on use of RnR.},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {643–654},
numpages = {12},
keywords = {deterministic record and replay, hardware-software interface, FPGA prototype, shared memory multiprocessors}
}

@article{10.1007/s00778-011-0260-8,
author = {Johnson, Ryan and Pandis, Ippokratis and Stoica, Radu and Athanassoulis, Manos and Ailamaki, Anastasia},
title = {Scalability of Write-Ahead Logging on Multicore and Multisocket Hardware},
year = {2012},
issue_date = {April     2012},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {2},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-011-0260-8},
doi = {10.1007/s00778-011-0260-8},
abstract = {The shift to multi-core and multi-socket hardware brings new challenges to database systems, as the software parallelism determines performance. Even though database systems traditionally accommodate simultaneous requests, a multitude of synchronization barriers serialize execution. Write-ahead logging is a fundamental, omnipresent component in ARIES-style concurrency and recovery, and one of the most important yet-to-be addressed potential bottlenecks, especially in OLTP workloads making frequent small changes to data. In this paper, we identify four logging-related impediments to database system scalability. Each issue challenges different level in the software architecture: (a) the high volume of small-sized I/O requests may saturate the disk, (b) transactions hold locks while waiting for the log flush, (c) extensive context switching overwhelms the OS scheduler with threads executing log I/Os, and (d) contention appears as transactions serialize accesses to in-memory log data structures. We demonstrate these problems and address them with techniques that, when combined, comprise a holistic, scalable approach to logging. Our solution achieves a 20---69% speedup over a modern database system when running log-intensive workloads, such as the TPC-B and TATP benchmarks, in a single-socket multiprocessor server. Moreover, it achieves log insert throughput over 2.2 GB/s for small log records on the single-socket server, roughly 20 times higher than the traditional way of accessing the log using a single mutex. Furthermore, we investigate techniques on scaling the performance of logging to multi-socket servers. We present a set of optimizations which partly ameliorate the latency penalty that comes with multi-socket hardware, and then we investigate the feasibility of applying a distributed log buffer design at the socket level.},
journal = {The VLDB Journal},
month = apr,
pages = {239–263},
numpages = {25},
keywords = {Log manager, Consolidation array, Log buffer contention, Scaling to multisockets, Flush pipelining, Early lock release}
}

@inproceedings{10.1145/3374664.3375743,
author = {Long, Yunhui and Xu, Le and Gunter, Carl A.},
title = {A Hypothesis Testing Approach to Sharing Logs with Confidence},
year = {2020},
isbn = {9781450371070},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3374664.3375743},
doi = {10.1145/3374664.3375743},
abstract = {Logs generated by systems and applications contain a wide variety of heterogeneous information that is important for performance profiling, failure detection, and security analysis. There is a strong need for sharing the logs among different parties to outsource the analysis or to improve system and security research. However, sharing logs may inadvertently leak confidential or proprietary information. Besides sensitive information that is directly saved in logs, such as user-identifiers and software versions, indirect evidence like performance metrics can also lead to the leakage of sensitive information about the physical machines and the system. In this work, we introduce a game-based definition of the risk of exposing sensitive information through released logs. We propose log indistinguishability, a property that is met only when the logs leak little information about the protected sensitive attributes. We design an end-to-end framework that allows a user to identify risk of information leakage in logs, to protect the exposure with log redaction and obfuscation, and to release the logs with a much lower risk of exposing the sensitive attribute. Our framework contains a set of statistical tests to identify violations of the log indistinguishability property and a variety of obfuscation methods to prevent the leakage of sensitive information. The framework views the log-generating process as a black-box and can therefore be applied to different systems and processes. We perform case studies on two different types of log datasets: Spark event log and hardware counters. We show that our framework is effective in preventing the leakage of the sensitive attribute with a reasonable testing time and an acceptable utility loss in logs.},
booktitle = {Proceedings of the Tenth ACM Conference on Data and Application Security and Privacy},
pages = {307–318},
numpages = {12},
keywords = {log obfuscation, hypothesis test, indistinguishability, privacy},
location = {New Orleans, LA, USA},
series = {CODASPY '20}
}

@inproceedings{10.5555/2685048.2685068,
author = {Yuan, Ding and Luo, Yu and Zhuang, Xin and Rodrigues, Guilherme Renna and Zhao, Xu and Zhang, Yongle and Jain, Pranay U. and Stumm, Michael},
title = {Simple Testing Can Prevent Most Critical Failures: An Analysis of Production Failures in Distributed Data-Intensive Systems},
year = {2014},
isbn = {9781931971164},
publisher = {USENIX Association},
address = {USA},
abstract = {Large, production quality distributed systems still fail periodically, and do so sometimes catastrophically, where most or all users experience an outage or data loss. We present the result of a comprehensive study investigating 198 randomly selected, user-reported failures that occurred on Cassandra, HBase, Hadoop Distributed File System (HDFS), Hadoop MapReduce, and Redis, with the goal of understanding how one or multiple faults eventually evolve into a user-visible failure. We found that from a testing point of view, almost all failures require only 3 or fewer nodes to reproduce, which is good news considering that these services typically run on a very large number of nodes. However, multiple inputs are needed to trigger the failures with the order between them being important. Finally, we found the error logs of these systems typically contain sufficient data on both the errors and the input events that triggered the failure, enabling the diagnose and the reproduction of the production failures.We found the majority of catastrophic failures could easily have been prevented by performing simple testing on error handling code - the last line of defense - even without an understanding of the software design. We extracted three simple rules from the bugs that have lead to some of the catastrophic failures, and developed a static checker, Aspirator, capable of locating these bugs. Over 30% of the catastrophic failures would have been prevented had Aspirator been used and the identified bugs fixed. Running Aspirator on the code of 9 distributed systems located 143 bugs and bad practices that have been fixed or confirmed by the developers.},
booktitle = {Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation},
pages = {249–265},
numpages = {17},
location = {Broomfield, CO},
series = {OSDI'14}
}

@inproceedings{10.1145/2818346.2830604,
author = {Barmaki, Roghayeh and Hughes, Charles E.},
title = {Providing Real-Time Feedback for Student Teachers in a Virtual Rehearsal Environment},
year = {2015},
isbn = {9781450339124},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818346.2830604},
doi = {10.1145/2818346.2830604},
abstract = {Research in learning analytics and educational data mining has recently become prominent in the fields of computer science and education. Most scholars in the field emphasize student learning and student data analytics; however, it is also important to focus on teaching analytics and teacher preparation because of their key roles in student learning, especially in K-12 learning environments. Nonverbal communication strategies play an important role in successful interpersonal communication of teachers with their students. In order to assist novice or practicing teachers with exhibiting open and affirmative nonverbal cues in their classrooms, we have designed a multimodal teaching platform with provisions for online feedback. We used an interactive teaching rehearsal software, TeachLivE, as our basic research environment. TeachLivE employs a digital puppetry paradigm as its core technology. Individuals walk into this virtual environment and interact with virtual students displayed on a large screen. They can practice classroom management, pedagogy and content delivery skills with a teaching plan in the TeachLivE environment. We have designed an experiment to evaluate the impact of an online nonverbal feedback application. In this experiment, different types of multimodal data have been collected during two experimental settings. These data include talk-time and nonverbal behaviors of the virtual students, captured in log files; talk time and full body tracking data of the participant; and video recording of the virtual classroom with the participant. 34 student teachers participated in this 30-minute experiment. In each of the settings, the participants were provided with teaching plans from which they taught. All the participants took part in both of the experimental settings. In order to have a balanced experiment design, half of the participants received nonverbal online feedback in their first session and the other half received this feedback in the second session. A visual indication was used for feedback each time the participant exhibited a closed, defensive posture. Based on recorded full-body tracking data, we observed that only those who received feedback in their first session demonstrated a significant number of open postures in the session containing no feedback. However, the post-questionnaire information indicated that all participants were more mindful of their body postures while teaching after they had participated in the study.},
booktitle = {Proceedings of the 2015 ACM on International Conference on Multimodal Interaction},
pages = {531–537},
numpages = {7},
keywords = {gesture recognition, reflection, immersive rehearsal environment, teacher preparation, mutimodal analytics},
location = {Seattle, Washington, USA},
series = {ICMI '15}
}

@inproceedings{10.1145/1810295.1810314,
author = {Groce, Alex and Havelund, Klaus and Smith, Margaret},
title = {From Scripts to Specifications: The Evolution of a Flight Software Testing Effort},
year = {2010},
isbn = {9781605587196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1810295.1810314},
doi = {10.1145/1810295.1810314},
abstract = {This paper describes the evolution of a software testing effort during a critical period for the flagship Mars Science Laboratory rover project at the Jet Propulsion Laboratory. Formal specification for post-run analysis of log files, using a domain-specific language, LogScope, replaced scripted real-time analysis. Log analysis addresses the key problems of on-the-fly approaches and cleanly separates specification and execution. Mining the test repository suggested the inadequacy of the scripted approach, and encouraged a partly engineer-driven development. LogScope development should hold insights for others facing the tight deadlines and reactionary nature of testing for critical projects. LogScope received a JPL Mariner Award for "improving productivity and quality of the MSL Flight Software" and has been discussed as an approach for other flight missions. We note LogScope features that most contributed to ease of adoption and effectiveness. LogScope is general and can be applied to any software producing logs.},
booktitle = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 2},
pages = {129–138},
numpages = {10},
keywords = {space flight software, temporal logic, Python, testing, development practices, test infrastructure, runtime verification, logs},
location = {Cape Town, South Africa},
series = {ICSE '10}
}

@inproceedings{10.1145/1809223.1809234,
author = {Meneely, Andrew and Corcoran, Mackenzie and Williams, Laurie},
title = {Improving Developer Activity Metrics with Issue Tracking Annotations},
year = {2010},
isbn = {9781605589763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1809223.1809234},
doi = {10.1145/1809223.1809234},
abstract = {Understanding and measuring how groups of developers collaborate on software projects can provide valuable insight into software quality and the software development process. Current practices of measuring developer collaboration (e.g. with social network analysis) usually employ metrics based on version control change log data to determine who is working on which part of the system. Version control change logs, however, do not tell the whole story. Information about the collaborative problem-solving process is also documented in the issue tracking systems that record solutions to failures, feature requests, or other development tasks. To enrich the data gained from version control change logs, we propose two annotations to be used in issue tracking systems: solution originator and solution approver. We examined the online discussions of 602 issues from the OpenMRS healthcare web application, annotating which developers were the originators of the solution to the issue, or were the approvers of the solution. We used these annotations to augment the version control change logs and found 47 more contributors to the OpenMRS project than the original 40 found in the version control change logs. Applying social network analysis to the data, we found that central developers in a developer network have a high likelihood of being approvers. These results indicate that using our two issue tracking annotations identify project collaborators that version control change logs miss. However, in the absence of our annotations, developer network centrality can be used as an estimate of the project's solution approvers. This improvement in developer activity metrics provides a valuable connection between what we can measure in the project development artifacts and the team's problem-solving process.},
booktitle = {Proceedings of the 2010 ICSE Workshop on Emerging Trends in Software Metrics},
pages = {75–80},
numpages = {6},
keywords = {network analysis, collaboration, developer, metric},
location = {Cape Town, South Africa},
series = {WETSoM '10}
}

@inproceedings{10.1145/3239372.3239396,
author = {Dou, Wei and Bianculli, Domenico and Briand, Lionel},
title = {Model-Driven Trace Diagnostics for Pattern-Based Temporal Specifications},
year = {2018},
isbn = {9781450349499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239372.3239396},
doi = {10.1145/3239372.3239396},
abstract = {Offline trace checking tools check whether a specification holds on a log of events recorded at run time; they yield a verification verdict (typically a boolean value) when the checking process ends. When the verdict is false, a software engineer needs to diagnose the property violations found in the trace in order to understand their cause and, if needed, decide for corrective actions to be performed on the system. However, a boolean verdict may not be informative enough to perform trace diagnostics, since it does not provide any useful information about the cause of the violation and because a property can be violated for multiple reasons.The goal of this paper is to provide a practical and scalable solution to solve the trace diagnostics problem, in the settings of model-driven trace checking of temporal properties expressed in TemPsy, a pattern-based specification language. The main contributions of the paper are: a model-driven approach for trace diagnostics of pattern-based temporal properties expressed in TemPsy, which relies on the evaluation of OCL queries on an instance of a trace metamodel; the implementation of this trace diagnostics procedure in the TemPsy-Report tool; the evaluation of the scalability of TemPsy-Report, when used for the diagnostics of violations of real properties derived from a case study of our industrial partner. The results show that TemPsy-Report is able to collect diagnostic information from large traces (with one million events) in less than ten seconds; TemPsy-Report scales linearly with respect to the length of the trace and keeps approximately constant performance as the number of violations increases.},
booktitle = {Proceedings of the 21th ACM/IEEE International Conference on Model Driven Engineering Languages and Systems},
pages = {278–288},
numpages = {11},
keywords = {run-time verification, offline trace checking, temporal constraints, OCL, specification patterns, trace diagnostics},
location = {Copenhagen, Denmark},
series = {MODELS '18}
}

@inproceedings{10.1145/3147704.3147720,
author = {Sousa, Tiago Boldt and Ferreira, Hugo Sereno and Correia, Filipe Figueiredo and Aguiar, Ademar},
title = {Engineering Software for the Cloud: Messaging Systems and Logging},
year = {2017},
isbn = {9781450348485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3147704.3147720},
doi = {10.1145/3147704.3147720},
abstract = {Software business continues to expand globally, highly motivated by the reachability of the Internet and possibilities of Cloud Computing. While widely adopted, development for the cloud has some intrinsic properties to it, making it complex to any newcomer. This research is capturing those intricacies using a pattern catalog, with this paper contributing with three of those patterns: Messaging System, a message bus for abstracting service placement in a cluster and orchestrating messages between multiple services; Preemptive Logging, a design principle where services and servers continuously output relevant information to log files, making them available for later debugging failures; and Log Aggregation, a technique to aggregate logs from multiple services and servers in a centralized location, which indexes and provides them in a queryable, user friendly format. These patterns are useful for anyone designing software for the cloud, either to guide or validate their design decisions.},
booktitle = {Proceedings of the 22nd European Conference on Pattern Languages of Programs},
articleno = {14},
numpages = {14},
keywords = {Cloud Computing, Design Patterns, Software Engineering},
location = {Irsee, Germany},
series = {EuroPLoP '17}
}

@inproceedings{10.1109/WI-IAT.2010.51,
author = {Han, Xu and Shi, Zhongzhi and Niu, Wenjia and Chen, Kunrong and Yang, Xinghua},
title = {Similarity-Based Bayesian Learning from Semi-Structured Log Files for Fault Diagnosis of Web Services},
year = {2010},
isbn = {9780769541914},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2010.51},
doi = {10.1109/WI-IAT.2010.51},
abstract = {With the rapid development of XML language which has good flexibility and interoperability, more and more log files of software running information are represented in XML format, especially for Web services. Fault diagnosis by analyzing semi-structured and XML like log files is becoming an important issue in this area. For most related learning methods, there is a basic assumption that training data should be in identical structure, which does not hold in many situations in practice. In order to learn from training data in different structures, we propose a similarity-based Bayesian learning approach for fault diagnosis in this paper. Our method is to first estimate similarity degrees of structural elements from different log files. Then the basic structure of combined Bayesian network (CBN) is constructed, and the similarity-based learning algorithm is used to compute probabilities in CBN. Finally, test log data can be classified into possible fault categories based on the generated CBN. Experimental results show our approach outperforms other learning approaches on those training datasets which have different structures.},
booktitle = {Proceedings of the 2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 01},
pages = {589–596},
numpages = {8},
keywords = {Bayesian learning, fault diagnosis, similarity, CBN, Web service},
series = {WI-IAT '10}
}

@inproceedings{10.1145/1993498.1993528,
author = {Lee, Kyu Hyung and Zheng, Yunhui and Sumner, Nick and Zhang, Xiangyu},
title = {Toward Generating Reducible Replay Logs},
year = {2011},
isbn = {9781450306638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993498.1993528},
doi = {10.1145/1993498.1993528},
abstract = {Logging and replay is important to reproducing software failures and recovering from failures. Replaying a long execution is time consuming, especially when replay is further integrated with runtime techniques that require expensive instrumentation, such as dependence detection. In this paper, we propose a technique to reduce a replay log while retaining its ability to reproduce a failure. While traditional logging records only system calls and signals, our technique leverages the compiler to selectively collect additional information on the fly. Upon a failure, the log can be reduced by analyzing itself. The collection is highly optimized. The additional runtime overhead of our technique, compared to a plain logging tool, is trivial (2.61% average) and the size of additional log is comparable to the original log. Substantial reduction can be cost-effectively achieved through a search based algorithm. The reduced log is guaranteed to reproduce the failure.},
booktitle = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {246–257},
numpages = {12},
keywords = {software reliability, instrumentation, debugging, log reduction, replay},
location = {San Jose, California, USA},
series = {PLDI '11}
}

@article{10.1145/1993316.1993528,
author = {Lee, Kyu Hyung and Zheng, Yunhui and Sumner, Nick and Zhang, Xiangyu},
title = {Toward Generating Reducible Replay Logs},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/1993316.1993528},
doi = {10.1145/1993316.1993528},
abstract = {Logging and replay is important to reproducing software failures and recovering from failures. Replaying a long execution is time consuming, especially when replay is further integrated with runtime techniques that require expensive instrumentation, such as dependence detection. In this paper, we propose a technique to reduce a replay log while retaining its ability to reproduce a failure. While traditional logging records only system calls and signals, our technique leverages the compiler to selectively collect additional information on the fly. Upon a failure, the log can be reduced by analyzing itself. The collection is highly optimized. The additional runtime overhead of our technique, compared to a plain logging tool, is trivial (2.61% average) and the size of additional log is comparable to the original log. Substantial reduction can be cost-effectively achieved through a search based algorithm. The reduced log is guaranteed to reproduce the failure.},
journal = {SIGPLAN Not.},
month = jun,
pages = {246–257},
numpages = {12},
keywords = {software reliability, debugging, log reduction, instrumentation, replay}
}

@inproceedings{10.1145/1880071.1880089,
author = {Goggins, Sean P. and Galyen, Krista and Laffey, James},
title = {Network Analysis of Trace Data for the Support of Group Work: Activity Patterns in a Completely Online Course},
year = {2010},
isbn = {9781450303873},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1880071.1880089},
doi = {10.1145/1880071.1880089},
abstract = {A 16-student, completely online software design course was studied using social network analysis and grounded theory techniques. Bi-directional (read and post) log data of user activity was recorded to understand how small group networks change over time with activity type (individual, peer-to-peer, and small group). Network structure was revealed through sociograms and triangulated with discussion board topics and interview data on group experience. Results show significant differences in network structure across activity types, which are supported by open coding and axial coding of the text of member discussions and editing patterns of member work products. It is also indicated that bi-directional log data, contextualized to specific activities and artifacts, revealed a more accurate and complete description of small group activity than ordinary, uni-directional log data would have. Our findings have implications for tool development revealing group structure and software design for completely online group work.},
booktitle = {Proceedings of the 16th ACM International Conference on Supporting Group Work},
pages = {107–116},
numpages = {10},
keywords = {networks of practice, cscw, sna, groups, communities, human capital, cscl, teams, social capital, group development},
location = {Sanibel Island, Florida, USA},
series = {GROUP '10}
}

@inproceedings{10.1145/2592798.2592814,
author = {Dulloor, Subramanya R. and Kumar, Sanjay and Keshavamurthy, Anil and Lantz, Philip and Reddy, Dheeraj and Sankaran, Rajesh and Jackson, Jeff},
title = {System Software for Persistent Memory},
year = {2014},
isbn = {9781450327046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2592798.2592814},
doi = {10.1145/2592798.2592814},
abstract = {Emerging byte-addressable, non-volatile memory technologies offer performance within an order of magnitude of DRAM, prompting their inclusion in the processor memory subsystem. However, such load/store accessible Persistent Memory (PM) has implications on system design, both hardware and software. In this paper, we explore system software support to enable low-overhead PM access by new and legacy applications. To this end, we implement PMFS, a light-weight POSIX file system that exploits PM's byte-addressability to avoid overheads of block-oriented storage and enable direct PM access by applications (with memory-mapped I/O). PMFS exploits the processor's paging and memory ordering features for optimizations such as fine-grained logging (for consistency) and transparent large page support (for faster memory-mapped I/O). To provide strong consistency guarantees, PMFS requires only a simple hardware primitive that provides software enforceable guarantees of durability and ordering of stores to PM. Finally, PMFS uses the processor's existing features to protect PM from stray writes, thereby improving reliability.Using a hardware emulator, we evaluate PMFS's performance with several workloads over a range of PM performance characteristics. PMFS shows significant (up to an order of magnitude) gains over traditional file systems (such as ext4) on a RAMDISK-like PM block device, demonstrating the benefits of optimizing system software for PM.},
booktitle = {Proceedings of the Ninth European Conference on Computer Systems},
articleno = {15},
numpages = {15},
location = {Amsterdam, The Netherlands},
series = {EuroSys '14}
}

@inproceedings{10.1145/3299869.3328523,
author = {Arulraj, Joy},
title = {Data Management on Non-Volatile Memory},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3328523},
doi = {10.1145/3299869.3328523},
abstract = {We are at an exciting point in the evolution of memory technology. Device manufacturers have created a new non- volatile memory (NVM) technology that can serve as both system memory and storage. NVM supports fast reads and writes similar to volatile memory, but all writes to it are persistent like a solid-state disk. The advent of NVM invalidates decades of design decisions that are deeply embedded in today's database management systems (DBMSs). These systems are unable to take full advantage of NVM because their internal architectures are predicated on the assumption that memory is volatile. With NVM, many of the components of today's DBMSs are unnecessary and will degrade the performance of data-intensive applications. Thus, the best way to resolve these shortcomings is by designing a new system explicitly tailored for NVM. In this talk, I will present our research on the design and development of an NVM DBMS, called Peloton. Peloton's architecture shows that the impact of NVM spans across all the layers of the DBMS. I will first introduce write-behind logging, an NVM-centric protocol that improves the availability of the database system by two orders-of-magnitude compared to the widely-used write- ahead logging protocol. I will then present the BzTree, an NVM-centric index data structure that illustrates how to simplify programming on NVM. In drawing broader lessons from this work, I will argue that all types of software systems, including file systems, machine-learning systems, and key-value stores, are amenable to similar architectural changes to achieve high performance and availability on NVM.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {1114},
numpages = {1},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@inproceedings{10.1145/3422392.3422455,
author = {Lima, M\'{a}rcia and Oliveira, Edson and Conte, Tayana and Gadelha, Bruno},
title = {Clouds Are Heavy! A Storm of Relevant Project-Related Terms to Support Newcomers' Onboarding},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422455},
doi = {10.1145/3422392.3422455},
abstract = {Both distributed and collocated software teams use collaborative communication channels, such as instant messaging (IM) tools, to support software development and management tasks. Furthermore, when teams use IM tools, relevant software-related discussions end up stored in these tools' log files. This research aims to investigate how we can extract and use projects' knowledge from developers' IM logs to support newcomers' onboarding. We propose an approach based on data-mining techniques to automatically obtain frequent software-related discussions from the developer's IM logs, extracting what we call the Project's Frequent Knowledge (PFK). We assessed the proposed approach evaluating three newcomers' knowledge acquisition regarding a software project. The results demonstrated that, on average, 70% of the frequent software-related discussions identified in this study were useful to determine the PFK, and newcomers were able to comprehend software related issues by analyzing the PFK. Our findings indicate the usefulness of the proposed approach to extract software knowledge from developers' IM logs and support newcomers' onboarding. Moreover, a conducted follow-up interview involving newcomers and the team's project manager revealed the feasibility of using PFK on knowledge transfer and acquisition.},
booktitle = {Proceedings of the 34th Brazilian Symposium on Software Engineering},
pages = {319–324},
numpages = {6},
keywords = {Communication Tools, Newcomer Members, Data-mining, Software Knowledge},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.1109/ESEM.2017.47,
author = {Gadler, Daniele and Mairegger, Michael and Janes, Andrea and Russo, Barbara},
title = {Mining Logs to Model the Use of a System},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.47},
doi = {10.1109/ESEM.2017.47},
abstract = {Background. Process mining is a technique to build process models from "execution logs" (i.e., events triggered by the execution of a process). State-of-the-art tools can provide process managers with different graphical representations of such models. Managers use these models to compare them with an ideal process model or to support process improvement. They typically select the representation based on their experience and knowledge of the system.Aim. This work studies how to automatically build process models representing the actual intents (or uses) of users while interacting with a software system. Such intents are expressed as a set of actions performed by a user to a system to achieve specific use goals.Method. This work applies the theory of Hidden Markov Models to mine use logs and automatically model the use of a system.Results. Unlike the models generated with process mining tools, the Hidden Markov Models automatically generated in this study provide the intents of a user and can be used to recommend managers with a faithful representation of the use of their systems.Conclusions. The automatic generation of the Hidden Markov Models can achieve a good level of accuracy in representing the actual user's intents provided the log dataset is carefully chosen. In our study, the information contained in one-month set of logs helped automatically build Hidden Markov Models with superior accuracy and similar expressiveness of the models built together with the company's stakeholder.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {334–343},
numpages = {10},
keywords = {log analysis, hidden markov chain, process modelling},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@inproceedings{10.1145/3199478.3199490,
author = {Mavroeidis, Vasileios and J\o{}sang, Audun},
title = {Data-Driven Threat Hunting Using Sysmon},
year = {2018},
isbn = {9781450363617},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3199478.3199490},
doi = {10.1145/3199478.3199490},
abstract = {Threat actors can be persistent, motivated and agile, and they leverage a diversified and extensive set of tactics, techniques, and procedures to attain their goals. In response to that, organizations establish threat intelligence programs to improve their defense capabilities and mitigate risk. Actionable threat intelligence is integrated into security information and event management systems (SIEM) forming a threat intelligence platform. A threat intelligence platform aggregates log data from multiple disparate sources by deploying numerous collection agents and provides centralized analysis and reporting of an organization's security events for identifying malicious activity. Sysmon logs is a data source that has received considerable attention for endpoint visibility. Approaches for threat detection using Sysmon have been proposed mainly focusing on search engines (NoSQL database systems). This paper presents a new automated threat assessment system that relies on the analysis of continuous incoming feeds of Sysmon logs. The system is based on a cyber threat intelligence ontology and analyses Sysmon logs to classify software in different threat levels and augment cyber defensive capabilities through situational awareness, prediction, and automated courses of action.},
booktitle = {Proceedings of the 2nd International Conference on Cryptography, Security and Privacy},
pages = {82–88},
numpages = {7},
keywords = {sysmon, threat hunting, cyber threat intelligence, threat assessment},
location = {Guiyang, China},
series = {ICCSP 2018}
}

@inproceedings{10.1145/2960310.2960342,
author = {Ericson, Barbara J.},
title = {Dynamically Adaptive Parsons Problems},
year = {2016},
isbn = {9781450344494},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960310.2960342},
doi = {10.1145/2960310.2960342},
abstract = {Parsons problems are code segments that must be placed in the correct order with the correct indention. Research on Parsons problems suggests that they might be a more effective and efficient learning approach than writing equivalent code, especially for time-strapped secondary teachers. I am exploring this hypothesis with empirical experiments, observations, and log file analyses. Our research team also plans to modify the open-source js-parsons software to allow Parsons problems to be dynamically adaptive, which means that the difficulty of the problem will change based on the user's past performance. I plan to compare dynamically adaptive Parsons problems to the current non-adaptive Parsons problems to determine which learners prefer and to see if solving dynamically adaptive Parsons problems leads to more efficient and effective learning than solving non-adaptive Parsons problems.},
booktitle = {Proceedings of the 2016 ACM Conference on International Computing Education Research},
pages = {269–270},
numpages = {2},
keywords = {online learning, adaptive learning, ebook, learning programming, parsons problems},
location = {Melbourne, VIC, Australia},
series = {ICER '16}
}

@inproceedings{10.1109/ISCA.2018.00045,
author = {Joshi, Arpit and Nagarajan, Vijay and Cintra, Marcelo and Viglas, Stratis},
title = {DHTM: Durable Hardware Transactional Memory},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00045},
doi = {10.1109/ISCA.2018.00045},
abstract = {The emergence of byte-addressable persistent (non-volatile) memory provides a low latency and high bandwidth path to durability. However, programmers need guarantees on what will remain in persistent memory in the event of a system crash. A widely accepted model for crash consistent programming is ACID transactions, in which updates within a transaction are made visible as well as durable in an atomic manner. However, existing software based proposals suffer from significant performance overheads.In this paper, we support both atomic visibility and durability in hardware. We propose DHTM (durable hardware transactional memory) that leverages a commercial HTM to provide atomic visibility and extends it with hardware support for redo logging to provide atomic durability. Furthermore, we leverage the same logging infrastructure to extend the supported transaction size (from being L1-limited to LLC-limited) with only minor changes to the coherence protocol. Our evaluation shows that DHTM outperforms the state-of-the-art by an average of 21% to 25% on TATP, TPC-C and a set of microbenchmarks. We believe DHTM is the first complete and practical hardware based solution for ACID transactions that has the potential to significantly ease the burden of crash consistent programming.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {452–465},
numpages = {14},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1145/2393596.2393671,
author = {Nguyen, Anh Tuan and Nguyen, Tung Thanh and Nguyen, Hoan Anh and Nguyen, Tien N.},
title = {Multi-Layered Approach for Recovering Links between Bug Reports and Fixes},
year = {2012},
isbn = {9781450316149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393596.2393671},
doi = {10.1145/2393596.2393671},
abstract = {The links between the bug reports in an issue-tracking system and the corresponding fixing changes in a version repository are not often recorded by developers. Such linking information is crucial for research in mining software repositories in measuring software defects and maintenance efforts. However, the state-of-the-art bug-to-fix link recovery approaches still rely much on textual matching between bug reports and commit/change logs and cannot handle well the cases where their contents are not textually similar.This paper introduces MLink, a multi-layered approach that takes into account not only textual features but also source code features of the changed code corresponding to the commit logs. It is also capable of learning the association relations between the terms in bug reports and the names of entities/components in the changed source code of the commits from the established bug-to-fix links, and uses them for link recovery between the reports and commits that do not share much similar texts. Our empirical evaluation on real-world projects shows that MLink can improve the state-of-the-art bug-to-fix link recovery methods by 11--18%, 13--17%, and 8--17% in F-score, recall, and precision, respectively.},
booktitle = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering},
articleno = {63},
numpages = {11},
keywords = {bug-to-fix links, fixes, bugs, mining software repository},
location = {Cary, North Carolina},
series = {FSE '12}
}

@inproceedings{10.1145/2568225.2568234,
author = {Ghezzi, Carlo and Pezz\`{e}, Mauro and Sama, Michele and Tamburrelli, Giordano},
title = {Mining Behavior Models from User-Intensive Web Applications},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568234},
doi = {10.1145/2568225.2568234},
abstract = { Many modern user-intensive applications, such as Web applications, must satisfy the interaction requirements of thousands if not millions of users, which can be hardly fully understood at design time. Designing applications that meet user behaviors, by efficiently supporting the prevalent navigation patterns, and evolving with them requires new approaches that go beyond classic software engineering solutions. We present a novel approach that automates the acquisition of user-interaction requirements in an incremental and reflective way. Our solution builds upon inferring a set of probabilistic Markov models of the users' navigational behaviors, dynamically extracted from the interaction history given in the form of a log file. We annotate and analyze the inferred models to verify quantitative properties by means of probabilistic model checking. The paper investigates the advantages of the approach referring to a Web application currently in use. },
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {277–287},
numpages = {11},
keywords = {User Profiles, Web Application, Markov Chains, Log Analysis, Probabilistic Model Checking},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/3292500.3330889,
author = {Tao, Zhiqiang and Li, Sheng and Wang, Zhaowen and Fang, Chen and Yang, Longqi and Zhao, Handong and Fu, Yun},
title = {Log2Intent: Towards Interpretable User Modeling via Recurrent Semantics Memory Unit},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330889},
doi = {10.1145/3292500.3330889},
abstract = {Modeling user behavior from unstructured software log-trace data is critical in providing personalized service (emphe.g., cross-platform recommendation). Existing user modeling approaches cannot well handle the long-term temporal information in log data, or produce semantically meaningful results for interpreting user logs. To address these challenges, we propose a Log2Intent framework for interpretable user modeling in this paper. Log2Intent adopts a deep sequential modeling framework that contains a temporal encoder, a semantic encoder and a log action decoder, and it fully captures the long-term temporal information in user sessions. Moreover, to bridge the semantic gap between log-trace data and human language, a recurrent semantics memory unit (RSMU) is proposed to encode the annotation sentences from an auxiliary software tutorial dataset, and the output of RSMU is fed into the semantic encoder of Log2Intent. Comprehensive experiments on a real-world Photoshop log-trace dataset with an auxiliary Photoshop tutorial dataset demonstrate the effectiveness of the proposed Log2Intent framework over the state-of-the-art log-trace user modeling method in three different tasks, including log annotation retrieval, user interest detection and user next action prediction.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1055–1063},
numpages = {9},
keywords = {semantics attention, user modeling, log-trace data, sequential modeling, recurrent memory network},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/2857546.2857574,
author = {Yamaguchi, Saneyasu and Nakao, Akihiro and Oguchi, Masato and Goto, Atsuhiro and Yamamoto, Shu},
title = {Monitoring Dynamic Modification of Routing Information in OpenFlow Networks},
year = {2016},
isbn = {9781450341424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2857546.2857574},
doi = {10.1145/2857546.2857574},
abstract = {Network routing information can be dynamically modified with ease using software-defined networking (SDN). However, it is not possible to simultaneously change the routing information in all the switches in a network. Thus, there must exist a short period in which the routing information is not completely synchronized. In such a period, some switches have the new routing information and the others have the old information. This may cause routing loops and packet losses. For discussing such issues, an integrated observation that covers all network elements is required. In current SDN networks, each network element has its own operating system and works independently. Thus, integrated monitoring and analyses is not achieved. In this paper, we focus on SDN networks using OpenFlow and propose a method for monitoring the behavior of the entire network during a period in which the network routing information changes. We construct an experimental OpenFlow network with open-source OpenFlow software. Then, we modify the software source code and implement event log storing functions for profound monitoring. Further, we create an integrated analysis system that gathers all monitored events in all the network elements and visualizes these events. We apply the proposed method and analyze the packet losses for a period in which the routing information is modified in OpenFlow SDN. The obtained results demonstrate that the cause of packet losses can be identified by our monitoring system.},
booktitle = {Proceedings of the 10th International Conference on Ubiquitous Information Management and Communication},
articleno = {27},
numpages = {7},
keywords = {Network analyses, OpenFlow, SDN},
location = {Danang, Viet Nam},
series = {IMCOM '16}
}

@inproceedings{10.1145/2555243.2555274,
author = {Lee, Kyu Hyung and Kim, Dohyeong and Zhang, Xiangyu},
title = {Infrastructure-Free Logging and Replay of Concurrent Execution on Multiple Cores},
year = {2014},
isbn = {9781450326568},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2555243.2555274},
doi = {10.1145/2555243.2555274},
abstract = {We develop a logging and replay technique for real concurrent execution on multiple cores. Our technique directly works on binaries and does not require any hardware or complex software infrastructure support. We focus on minimizing logging overhead as it only logs a subset of system calls and thread spawns. Replay is on a single core. During replay, our technique first tries to follow only the event order in the log. However, due to schedule differences, replay may fail. An exploration process is then triggered to search for a schedule that allows the replay to make progress. Exploration is performed within a window preceding the point of replay failure. During exploration, our technique first tries to reorder synchronized blocks. If that does not lead to progress, it further reorders shared variable accesses. The exploration is facilitated by a sophisticated caching mechanism. Our experiments on real world programs and real workload show that the proposed technique has very low logging overhead (2.6% on average) and fast schedule reconstruction.},
booktitle = {Proceedings of the 19th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {371–372},
numpages = {2},
keywords = {software reliability, replay, debugging},
location = {Orlando, Florida, USA},
series = {PPoPP '14}
}

@article{10.1145/2692916.2555274,
author = {Lee, Kyu Hyung and Kim, Dohyeong and Zhang, Xiangyu},
title = {Infrastructure-Free Logging and Replay of Concurrent Execution on Multiple Cores},
year = {2014},
issue_date = {August 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/2692916.2555274},
doi = {10.1145/2692916.2555274},
abstract = {We develop a logging and replay technique for real concurrent execution on multiple cores. Our technique directly works on binaries and does not require any hardware or complex software infrastructure support. We focus on minimizing logging overhead as it only logs a subset of system calls and thread spawns. Replay is on a single core. During replay, our technique first tries to follow only the event order in the log. However, due to schedule differences, replay may fail. An exploration process is then triggered to search for a schedule that allows the replay to make progress. Exploration is performed within a window preceding the point of replay failure. During exploration, our technique first tries to reorder synchronized blocks. If that does not lead to progress, it further reorders shared variable accesses. The exploration is facilitated by a sophisticated caching mechanism. Our experiments on real world programs and real workload show that the proposed technique has very low logging overhead (2.6% on average) and fast schedule reconstruction.},
journal = {SIGPLAN Not.},
month = feb,
pages = {371–372},
numpages = {2},
keywords = {debugging, software reliability, replay}
}

@inproceedings{10.1145/3123939.3124539,
author = {Shin, Seunghee and Tirukkovalluri, Satish Kumar and Tuck, James and Solihin, Yan},
title = {Proteus: A Flexible and Fast Software Supported Hardware Logging Approach for NVM},
year = {2017},
isbn = {9781450349529},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123939.3124539},
doi = {10.1145/3123939.3124539},
abstract = {Emerging non-volatile memory (NVM) technologies, such as phase-change memory, spin-transfer torque magnetic memory, memristor, and 3D Xpoint, are encouraging the development of new architectures that support the challenges of persistent programming. An important remaining challenge is dealing with the high logging overheads introduced by durable transactions.In this paper, we propose a new logging approach, Proteus for durable transactions that achieves the favorable characteristics of both prior software and hardware approaches. Like software, it has no hardware constraint limiting the number of transactions or logs available to it, and like hardware, it has very low overhead. Our approach introduces two new instructions: log-load creates a log entry by loading the original data, and log-flush writes the log entry into the log. We add hardware support, primarily within the core, to manage the execution of these instructions and critical ordering requirements between logging operations and updates to data. We also propose a novel optimization at the memory controller that is enabled by a persistent write pending queue in the memory controller. We drop log updates that have not yet written back to NVMM by the time a transaction is considered durable.We implemented our design on a cycle accurate simulator, MarssX86, and compared it against state-of-the-art hardware logging, ATOM [19], and a software only approach. Our experiments show that Proteus improves performance by 1.44--1.47\texttimes{} depending on configuration, on average, compared to a system without hardware logging and 9--11% faster than ATOM. A significant advantage of our approach is dropping writes to the log when they are not needed. On average, ATOM makes 3.4\texttimes{} more writes to memory than our design.},
booktitle = {Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {178–190},
numpages = {13},
keywords = {software supported hardware logging, non-volatile main memory, failure safety},
location = {Cambridge, Massachusetts},
series = {MICRO-50 '17}
}

@inproceedings{10.1145/2970276.2970359,
author = {Wen, Ming and Wu, Rongxin and Cheung, Shing-Chi},
title = {Locus: Locating Bugs from Software Changes},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970359},
doi = {10.1145/2970276.2970359},
abstract = {Various information retrieval (IR) based techniques have been proposed recently to locate bugs automatically at the file level. However, their usefulness is often compromised by the coarse granularity of files and the lack of contextual information. To address this, we propose to locate bugs using software changes, which offer finer granularity than files and provide important contextual clues for bug-fixing. We observe that bug inducing changes can facilitate the bug fixing process. For example, it helps triage the bug fixing task to the developers who committed the bug inducing changes or enables developers to fix bugs by reverting these changes. Our study further identifies that change logs and the naturally small granularity of changes can help boost the performance of IR-based bug localization. Motivated by these observations, we propose an IR-based approach Locus to locate bugs from software changes, and evaluate it on six large open source projects. The results show that Locus outperforms existing techniques at the source file level localization significantly. MAP and MRR in particular have been improved, on average, by 20.1% and 20.5%, respectively. Locus is also capable of locating the inducing changes within top 5 for 41.0% of the bugs. The results show that Locus can significantly reduce the number of lines needing to be scanned to locate the bug compared with existing techniques.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {262–273},
numpages = {12},
keywords = {software analytics, bug localization, information retrieval, software changes},
location = {Singapore, Singapore},
series = {ASE 2016}
}

@inproceedings{10.1145/2751504.2751506,
author = {Goudarzi, Alireza and Arnold, Dorian and Stefanovic, Darko and Ferreira, Kurt B. and Feldman, Guy},
title = {A Principled Approach to HPC Event Monitoring},
year = {2015},
isbn = {9781450335690},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2751504.2751506},
doi = {10.1145/2751504.2751506},
abstract = {As high-performance computing (HPC) systems become larger and more complex, fault tolerance becomes a greater concern. At the same time, the data volume collected to help in understanding and mitigating hardware and software faults and failures also becomes prohibitively large. We argue that the HPC community must adopt more systematic approaches to system event logging as opposed to the current, ad hoc, strategies based on practitioner intuition and experience. Specifically, we show that event correlation and prediction can increase our understanding of fault behavior and can become critical components of effective fault tolerance strategies. While event correlation and prediction have been used in HPC contexts, we offer new insights about their potential capabilities. Using event logs from the computer failure data repository (cfdr) (1) we use cross and partial correlations to observe conditional correlations in HPC event data; (2) we use information theory to understand the fundamental predictive power of HPC failure data; (3) we study neural networks for failure prediction; and (4) finally, we use principal component analysis to understand to what extent dimensionality reduction can apply to HPC event data. This work results in the following insights that can inform HPC event monitoring: ad hoc correlations or ones based on direct correlations can be deficient or even misleading; highly accurate failure prediction may only require small windows of failure event history; and principal component analysis can significantly reduce HPC event data without loss of relevant information.},
booktitle = {Proceedings of the 5th Workshop on Fault Tolerance for HPC at EXtreme Scale},
pages = {3–10},
numpages = {8},
keywords = {failure prediction, event analysis, resource monitoring, fault-tolerance},
location = {Portland, Oregon, USA},
series = {FTXS '15}
}

@inproceedings{10.1145/3382494.3410684,
author = {Rosenberg, Carl Martin and Moonen, Leon},
title = {Spectrum-Based Log Diagnosis},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3410684},
doi = {10.1145/3382494.3410684},
abstract = {Background: Continuous Engineering practices are increasingly adopted in modern software development. However, a frequently reported need is for more effective methods to analyze the massive amounts of data resulting from the numerous build and test runs. Aims: We present and evaluate Spectrum-Based Log Diagnosis (SBLD), a method to help developers quickly diagnose problems found in complex integration and deployment runs. Inspired by Spectrum-Based Fault Localization, SBLD leverages the differences in event occurrences between logs for failing and passing runs, to highlight events that are stronger associated with failing runs.Method: Using data provided by Cisco Norway, we empirically investigate the following questions: (i) How well does SBLD reduce the effort needed to identify all failure-relevant events in the log for a failing run? (ii) How is the performance of SBLD affected by available data? (iii) How does SBLD compare to searching for simple textual patterns that often occur in failure-relevant events? We answer (i) and (ii) using summary statistics and heatmap visualizations, and for (iii) we compare three configurations of SBLD (with resp. minimum, median and maximum data) against a textual search using Wilcoxon signed-rank tests and the Vargha-Delaney measure of stochastic superiority.Results: Our evaluation shows that (i) SBLD achieves a significant effort reduction for the dataset used, (ii) SBLD benefits from additional logs for passing runs in general, and it benefits from additional logs for failing runs when there is a proportional amount of logs for passing runs in the data. Finally, (iii) SBLD and textual search are roughly equally effective at effort-reduction, while textual search has slightly better recall. We investigate the cause, and discuss how it is due to characteristics of a specific part of our data.Conclusions: We conclude that SBLD shows promise as a method for diagnosing failing runs, that its performance is positively affected by additional data, but that it does not outperform textual search on the dataset considered. Future work includes investigating SBLD's generalizability on additional datasets.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {18},
numpages = {12},
keywords = {failure diagnosis, continuous engineering, log mining, log analysis},
location = {Bari, Italy},
series = {ESEM '20}
}

@inproceedings{10.5555/2872965.2872986,
author = {Bhowmik, Priyasree and Osgood, Nathaniel and Dutchyn, Christopher},
title = {Improving the Flexibility of Simulation Modeling with Aspects},
year = {2015},
isbn = {9781510801059},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {While simulation and modeling serve as increasingly popular tools in addressing complex policy challenges, modeling projects are often encumbered by significant complexity within the model itself. This includes complexity extending from software engineering challenges, implementation, management of the model execution, difficulty in maintaining metadata to cross-link models, scenario results, associated simulation results, and a dependence of knowledge-users on modelers to modify model output and visualization mechanisms to explore patterns of interest. Furthermore, debugging of and developing confidence in a model often requires enabling/disabling tracing output of various model quantities. We present techniques to enhance flexibility, transparency, usefulness and effectiveness of simulation modeling by using Aspect-Oriented Programming to automatically manage the high-level execution results (Run Log) and, separately, low-level details (Trace Log) associated with model executions. With an eye towards enabling scenario reproducibility, Run Log documents the scenarios run for a given model, and records the associated model version, scenario assumptions and elements of output. The Aspect framework for Trace Log eliminates boilerplate logging code within models, supports flexibly enabling/disabling logging, improves the robustness of the model by providing easy mechanisms of debugging, and supports knowledge-users in exploring model output. We describe the framework, experiments conducted, and feedback received.},
booktitle = {Proceedings of the Symposium on Theory of Modeling &amp; Simulation: DEVS Integrative M&amp;S Symposium},
pages = {149–156},
numpages = {8},
keywords = {reproducibility, tracing, aspects, flexibility, logging},
location = {Alexandria, Virginia},
series = {DEVS '15}
}

@inproceedings{10.1145/3236024.3236083,
author = {He, Shilin and Lin, Qingwei and Lou, Jian-Guang and Zhang, Hongyu and Lyu, Michael R. and Zhang, Dongmei},
title = {Identifying Impactful Service System Problems via Log Analysis},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236083},
doi = {10.1145/3236024.3236083},
abstract = {Logs are often used for troubleshooting in large-scale software systems. For a cloud-based online system that provides 24/7 service, a huge number of logs could be generated every day. However, these logs are highly imbalanced in general, because most logs indicate normal system operations, and only a small percentage of logs reveal impactful problems. Problems that lead to the decline of system KPIs (Key Performance Indicators) are impactful and should be fixed by engineers with a high priority. Furthermore, there are various types of system problems, which are hard to be distinguished manually. In this paper, we propose Log3C, a novel clustering-based approach to promptly and precisely identify impactful system problems, by utilizing both log sequences (a sequence of log events) and system KPIs. More specifically, we design a novel cascading clustering algorithm, which can greatly save the clustering time while keeping high accuracy by iteratively sampling, clustering, and matching log sequences. We then identify the impactful problems by correlating the clusters of log sequences with system KPIs. Log3C is evaluated on real-world log data collected from an online service system at Microsoft, and the results confirm its effectiveness and efficiency. Furthermore, our approach has been successfully applied in industrial practice.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {60–70},
numpages = {11},
keywords = {Log Analysis, Service Systems, Problem Identification, Clustering},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/2785592.2785605,
author = {Blum, Fabian Rojas and Simmonds, Jocelyn and Bastarrica, Mar\'{\i}a Cecilia},
title = {Software Process Line Discovery},
year = {2015},
isbn = {9781450333467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785592.2785605},
doi = {10.1145/2785592.2785605},
abstract = { Companies define software processes for planning and guiding projects. Since process definition is expensive, and in practice, no one process "fits all" projects, the current trend is to define a Software Process Line (SPrL): a base process that represents the common process elements, along with its potential variability. Specifying a SPrL is more expensive than just specifying one process, but the SPrL can be adapted to specific project contexts, minimizing the amount of extra work carried out by employees. Mining project logs has proven to be a promising approach for discovering the process that is applied in practice. However, considering all the possible variations that may be logged, the mined process may be overly complex. Some algorithms deal with this by filtering infrequent relations between log events, but they may discard relevant relations. In this paper we propose the v-algorithm that uses two thresholds to set up a SPrL: highly frequent relations are used to build the base process, variable relations define process variability, and rare relations are discarded as noise. We applied the $v$-$algorithm$ to the project log of Mobius, a small Chilean software company. We obtained a SPrL where we identified unexpected alternative ways of performing certain activities, as well as an optional activity that was originally specified as mandatory. },
booktitle = {Proceedings of the 2015 International Conference on Software and System Process},
pages = {127–136},
numpages = {10},
keywords = {process discovery, noise in logs, variability, Software process lines},
location = {Tallinn, Estonia},
series = {ICSSP 2015}
}

@inproceedings{10.1145/2834996.2835001,
author = {Birngruber, Erich and Forai, Petar and Zauner, Aaron},
title = {Total Recall: Holistic Metrics for Broad Systems Performance and User Experience Visibility in a Data-Intensive Computing Environment},
year = {2015},
isbn = {9781450340007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2834996.2835001},
doi = {10.1145/2834996.2835001},
abstract = {User support personnel, systems engineers, and administrators of HPC installations need to be aware of log and telemetry information from different systems in order to perform routine tasks ranging from systems management to user inquiries. We present an integrated, distributed HPC tailored monitoring system, based on a current generation software stack from the DevOps community, with integration into the work load management system. The goal of this system is to provide a quicker turnaround time for user inquiries in response to errors. Dashboards provide an overlay of system and node level events on top of correlated metrics data. This information is directly available for querying, manipulation, and filtering, allowing statistical analysis and aggregation of collected data. Furthermore, additional dashboards offer in-sight into how users are interacting with available resources and pin-point fluctuations in utilization. The system can integrate sources of information from other monitoring solutions and event-based sources.},
booktitle = {Proceedings of the Second International Workshop on HPC User Support Tools},
articleno = {5},
numpages = {12},
keywords = {performance analysis, DevOps, metrics, user support tools, distributed systems monitoring, telemetry, HPC, systems performance, job scheduling, event correlation, time-series databases},
location = {Austin, Texas},
series = {HUST '15}
}

@inproceedings{10.1145/2808797.2809281,
author = {Burnap, Pete and Javed, Amir and Rana, Omer F. and Awan, Malik S.},
title = {Real-Time Classification of Malicious URLs on Twitter Using Machine Activity Data},
year = {2015},
isbn = {9781450338547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808797.2809281},
doi = {10.1145/2808797.2809281},
abstract = {Massive online social networks with hundreds of millions of active users are increasingly being used by Cyber criminals to spread malicious software (malware) to exploit vulnerabilities on the machines of users for personal gain. Twitter is particularly susceptible to such activity as, with its 140 character limit, it is common for people to include URLs in their tweets to link to more detailed information, evidence, news reports and so on. URLs are often shortened so the endpoint is not obvious before a person clicks the link. Cyber criminals can exploit this to propagate malicious URLs on Twitter, for which the endpoint is a malicious server that performs unwanted actions on the person's machine. This is known as a drive-by-download. In this paper we develop a machine classification system to distinguish between malicious and benign URLs within seconds of the URL being clicked (i.e. 'real-time'). We train the classifier using machine activity logs created while interacting with URLs extracted from Twitter data collected during a large global event -- the Superbowl -- and test it using data from another large sporting event -- the Cricket World Cup. The results show that machine activity logs produce precision performances of up to 0.975 on training data from the first event and 0.747 on a test data from a second event. Furthermore, we examine the properties of the learned model to explain the relationship between machine activity and malicious software behaviour, and build a learning curve for the classifier to illustrate that very small samples of training data can be used with only a small detriment to performance.},
booktitle = {Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015},
pages = {970–977},
numpages = {8},
location = {Paris, France},
series = {ASONAM '15}
}

@inproceedings{10.1145/3220228.3220248,
author = {Tian, Yongfeng and Tan, Huobin and Lin, Guangyan},
title = {Statistical Properties Analysis of File Modification in Open-Source Software Repositories},
year = {2018},
isbn = {9781450364454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220228.3220248},
doi = {10.1145/3220228.3220248},
abstract = {Mining the evolutionary rules of source code files can be conducted by analyzing the data generated in the development of open source software. In this paper, the development log information of two famous open source projects is collected and the statistical distribution of the number of developers corresponding to class files modification is analyzed by statistical method. As a result, we discover that the statistical distribution of the number of developers corresponding to class files modification fellows approximately an exponential distribution. In addition, we analyze the features of function and structure of two kinds of class files and discover that both class files developed by developers who have too many modification behaviors to their projects and class files modified by too many developers tend to be more complex. The statistical analysis of these two projects may provide new insights into the research on studying the evolution of open source software, choosing appropriate programmers to refactor open source software and allocating task of maintenance for open source software.},
booktitle = {Proceedings of the International Conference on Geoinformatics and Data Analysis},
pages = {62–66},
numpages = {5},
keywords = {developers, evolutionary rules, class files, file modification, open source software},
location = {Prague, Czech Republic},
series = {ICGDA '18}
}

@inproceedings{10.1109/ICPC.2019.00039,
author = {Bai, Gina R. and Clee, Brian and Shrestha, Nischal and Chapman, Carl and Wright, Cimone and Stolee, Kathryn T.},
title = {Exploring Tools and Strategies Used during Regular Expression Composition Tasks},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2019.00039},
doi = {10.1109/ICPC.2019.00039},
abstract = {Regular expressions are frequently found in programming projects. Studies have found that developers can accurately determine whether a string matches a regular expression. However, we still do not know the challenges associated with composing regular expressions.We conduct an exploratory case study to reveal the tools and strategies developers use during regular expression composition. In this study, 29 students are tasked with composing regular expressions that pass unit tests illustrating the intended behavior. The tasks are in Java and the Eclipse IDE was set up with JUnit tests. Participants had one hour to work and could use any Eclipse tools, web search, or web-based tools they desired. Screen-capture software recorded all interactions with browsers and the IDE. We analyzed the videos quantitatively by transcribing logs and extracting personas. Our results show that participants were 30% successful (28 of 94 attempts) at achieving a 100% pass rate on the unit tests. When participants used tools frequently, as in the case of the novice tester and the knowledgeable tester personas, or when they guess at a solution prior to searching, they are more likely to pass all the unit tests. We also found that compile errors often arise when participants searched for a result and copy/pasted the regular expression from another language into their Java files. These results point to future research into making regular expression composition easier for programmers, such as integrating visualization into the IDE to reduce context switching or providing language migration support when reusing regular expressions written in another language to reduce compile errors.},
booktitle = {Proceedings of the 27th International Conference on Program Comprehension},
pages = {197–208},
numpages = {12},
keywords = {personas, regular expressions, exploratory study, problem solving strategies},
location = {Montreal, Quebec, Canada},
series = {ICPC '19}
}

@inproceedings{10.1145/3027385.3027450,
author = {Holstein, Kenneth and McLaren, Bruce M. and Aleven, Vincent},
title = {SPACLE: Investigating Learning across Virtual and Physical Spaces Using Spatial Replays},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027450},
doi = {10.1145/3027385.3027450},
abstract = {Classroom experiments that evaluate the effectiveness of educational technologies do not typically examine the effects of classroom contextual variables (e.g., out-of-software help-giving and external distractions). Yet these variables may influence students' instructional outcomes. In this paper, we introduce the Spatial Classroom Log Explorer (SPACLE): a prototype tool that facilitates the rapid discovery of relationships between within-software and out-of-software events. Unlike previous tools for retrospective analysis, SPACLE replays moment-by-moment analytics about student and teacher behaviors in their original spatial context. We present a data analysis workflow using SPACLE and demonstrate how this workflow can support causal discovery. We share the results of our initial replay analyses using SPACLE, which highlight the importance of considering spatial factors in the classroom when analyzing ITS log data. We also present the results of an investigation into the effects of student-teacher interactions on student learning in K-12 blended classrooms, using our workflow, which combines replay analysis with SPACLE and causal modeling. Our findings suggest that students' awareness of being monitored by their teachers may promote learning, and that "gaming the system" behaviors may extend outside of educational software use.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {358–367},
numpages = {10},
keywords = {intelligent tutoring systems, causal modeling, classroom, blended learning, teachers, visualizations},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@article{10.1145/3208104,
author = {Mace, Jonathan and Roelke, Ryan and Fonseca, Rodrigo},
title = {Pivot Tracing: Dynamic Causal Monitoring for Distributed Systems},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {0734-2071},
url = {https://doi.org/10.1145/3208104},
doi = {10.1145/3208104},
abstract = {Monitoring and troubleshooting distributed systems is notoriously difficult; potential problems are complex, varied, and unpredictable. The monitoring and diagnosis tools commonly used today—logs, counters, and metrics—have two important limitations: what gets recorded is defined a priori, and the information is recorded in a component- or machine-centric way, making it extremely hard to correlate events that cross these boundaries. This article presents Pivot Tracing, a monitoring framework for distributed systems that addresses both limitations by combining dynamic instrumentation with a novel relational operator: the happened-before join. Pivot Tracing gives users, at runtime, the ability to define arbitrary metrics at one point of the system, while being able to select, filter, and group by events meaningful at other parts of the system, even when crossing component or machine boundaries. We have implemented a prototype of Pivot Tracing for Java-based systems and evaluate it on a heterogeneous Hadoop cluster comprising HDFS, HBase, MapReduce, and YARN. We show that Pivot Tracing can effectively identify a diverse range of root causes such as software bugs, misconfiguration, and limping hardware. We show that Pivot Tracing is dynamic, extensible, and enables cross-tier analysis between inter-operating applications, with low execution overhead.},
journal = {ACM Trans. Comput. Syst.},
month = dec,
articleno = {11},
numpages = {28},
keywords = {end-to-end tracing, Distributed systems monitoring}
}

@article{10.1145/2557833.2557849,
author = {Malhotra, Ruchika and Agrawal, Anushree},
title = {CMS Tool: Calculating Defect and Change Data from Software Project Repositories},
year = {2014},
issue_date = {January 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2557833.2557849},
doi = {10.1145/2557833.2557849},
abstract = {Defect and change prediction is a very important activity in software development. Predicting erroneous classes of the system early in the software development life cycle will enable early identification of risky classes in the initial phases. This will assist software practitioners in designing and developing software systems of better quality with focused resources and hence take necessary corrective design actions. In this work we describe a framework to develop and calculate the defect fixes and changes made during various versions of a software system. We develop a tool, Configuration Management System (CMS), which uses log files obtained from a Concurrent Versioning System (CVS) repository in order to collect the number of defects from each class. The tool also calculates the number of changes made during each version of the software. This tool will also assist software practitioners and researchers in collecting defect and change data for software systems.},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–5},
numpages = {5},
keywords = {defect prediction, CVS, change prediction, software project repositories}
}

@article{10.1145/2557833.2560585,
author = {Nguyen, Tung Thanh and Nguyen, Anh Tuan and Nguyen, Tien N.},
title = {Topic-Based, Time-Aware Bug Assignment},
year = {2014},
issue_date = {January 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2557833.2560585},
doi = {10.1145/2557833.2560585},
abstract = {Bugs are prevalent in software systems and improving time efficiency in bug fixing is desired. We performed an analysis on 11,115 bug records of Eclipse JDT and found that bug resolution time is log-normally distributed and varies across fixers, technical topics, and bug severity levels. We then propose FixTime, a novel method for bug assignment. The key of FixTime is a topicbased, log-normal regression model for predicting defect resolution time on which FixTime is based to make fixing assignment recommendations. Preliminary results suggest that FixTime has higher prediction accuracy than existing approaches.},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–4},
numpages = {4},
keywords = {bug assignment, bug resolution time, topic modeling}
}

@inproceedings{10.1145/2342441.2342453,
author = {Handigol, Nikhil and Heller, Brandon and Jeyakumar, Vimalkumar and Mazi\'{e}res, David and McKeown, Nick},
title = {Where is the Debugger for My Software-Defined Network?},
year = {2012},
isbn = {9781450314770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2342441.2342453},
doi = {10.1145/2342441.2342453},
abstract = {The behavior of a Software-Defined Network is controlled by programs, which like all software, will have bugs - but this programmatic control also enables new ways to debug networks. This paper introduces ndb, a prototype network debugger inspired by gdb, which implements two primitives useful for debugging an SDN: breakpoints and packet backtraces. We show how ndb modifies forwarding state and logs packet digests to rebuild the sequence of events leading to an errant packet, providing SDN programmers and operators with a valuable tool for tracking down the root cause of a bug.},
booktitle = {Proceedings of the First Workshop on Hot Topics in Software Defined Networks},
pages = {55–60},
numpages = {6},
keywords = {interactive, network debugging, software-defined networks},
location = {Helsinki, Finland},
series = {HotSDN '12}
}

@proceedings{10.5555/3355301,
title = {SoHeal '19: Proceedings of the 2nd International Workshop on Software Health},
year = {2019},
publisher = {IEEE Press},
abstract = {SoHeal 2019, the second edition of the International Workshop on Software Health has taken place in Montreal, Canada on May 28. As its previous edition, it was held in conjunction with the IEEE International Conference on Software Engineering (ICSE). As such, SoHeal is establishing itself as an annual venue gathering researchers, industrials, practitioners and open source software community members to share their vision, experience and opinion on what constitutes software health and how such health should be supported, both at the level of software ecosystems and software communities.To date, there is no clear-cut definition of what constitutes software health, since it encompasses many different aspects of software design, development, evolution, deployment and operation, including success, longevity, growth, resilience, survival, diversity, sustainability, transparency, privacy, security, etc. Factors impacting software health can vary depending on the viewpoint of the involved stakeholders: process factors, technical factors concerning software artefacts (such as the source code, its documentation, log files, reviews, issue reports), social and cultural factors concerning the communities of software contributors and users, business factors concerning commercial and financial issues, legal factors such as licensing issues, ethical factors such as privacy preservation, and many more.},
location = {Montreal, Quebec, Canada}
}

@article{10.1145/1740390.1740412,
author = {De Pauw, Wim and Heisig, Stephen},
title = {Visual and Algorithmic Tooling for System Trace Analysis: A Case Study},
year = {2010},
issue_date = {January 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {1},
issn = {0163-5980},
url = {https://doi.org/10.1145/1740390.1740412},
doi = {10.1145/1740390.1740412},
abstract = {Despite advances in the application of automated statistical and machine learning techniques to system log and trace data there will always be a need for human analysis of machine traces, because trace information on unstable systems may be incomplete, or incorrect. In addition, false positives from automated analysis will not likely disappear, and remediation measures and candidate fix tests will need to be evaluated. We present Zinsight, a visual and analytic tool that supports performance analysts and debugging, using large event traces to understand complex systems. This tool enables analysts to quickly create and manipulate high-level structural representations linked with statistical analysis derived from the underlying event trace data. The original raw trace is annotated with module names and a domain specific database is incorporated to relate software functions to module names. Navigable sequence context graph views present automatically extracted execution flow patterns from arbitrarily definable sets of events and are linked to frequency, distribution, and response time views. The goal is to reduce the cognitive and computational load on the analyst while providing answers to the most natural questions in a problem determination session. We present a case study of the tool in use on field problems from the recently shipped (late 2008) IBM z10 mainframe. As a result of the industry trend toward higher parallelism and memory latency, many issues were encountered with legacy code. The tool was applied successfully to diagnose these problems.},
journal = {SIGOPS Oper. Syst. Rev.},
month = mar,
pages = {97–102},
numpages = {6},
keywords = {problem determination, trace analysis, visualization, pattern extraction}
}

@inproceedings{10.1145/3287324.3287505,
author = {Maicus, Evan and Peveler, Matthew and Patterson, Stacy and Cutler, Barbara},
title = {Autograding Distributed Algorithms in Networked Containers},
year = {2019},
isbn = {9781450358903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287324.3287505},
doi = {10.1145/3287324.3287505},
abstract = {We present a container-based system to automatically run and evaluate networked applications that implement distributed algorithms. Our implementation of this design leverages lightweight, networked Docker containers to provide students with fast, accurate, and helpful feedback about the correctness of their submitted code. We provide a simple, easy-to-use interface for instructors to specify networks, deploy and run instances of student and instructor code, and to log and collect statistics concerning node connection types and message content. Instructors further have the ability to control network features such as message delay, drop, and reorder. Running student programs can be interfaced with via stream-controlled standard input or through additional containers running custom instructor software. Student program behavior can be automatically evaluated by analyzing console or file output and instructor-specified rules regarding network communications. Program behavior, including logs of all messages passed within the system, can optionally be displayed to the student to aid in development and debugging. We evaluate the utility of this design and implementation for managing the submission and robust and secure testing of programming projects in a large enrollment theory of distributed systems course. This research has been implemented as an extension to Submitty, an open source, language-agnostic course management platform with automated testing and automated grading of student programming assignments. Submitty supports all levels of courses, from introductory to advanced special topics, and includes features for manual grading by TAs, version control, team submission, discussion forums, and plagiarism detection.},
booktitle = {Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
pages = {133–138},
numpages = {6},
keywords = {testing, autograding, distributed systems, container},
location = {Minneapolis, MN, USA},
series = {SIGCSE '19}
}

@inproceedings{10.1145/3041021.3054183,
author = {Yang, Longqi and Fang, Chen and Jin, Hailin and Hoffman, Matthew D. and Estrin, Deborah},
title = {Personalizing Software and Web Services by Integrating Unstructured Application Usage Traces},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3054183},
doi = {10.1145/3041021.3054183},
abstract = {Users of software applications generate vast amounts of unstructured log-trace data. These traces contain clues to the intentions and interests of those users, but service providers may find it difficult to uncover and exploit those clues. In this paper, we propose a framework for personalizing software and web services by leveraging such unstructured traces. We use 6 months of Photoshop usage history and 7 years of interaction records from 67K Behance users to design, develop, and validate a user-modeling technique that discovers highly discriminative representations of Photoshop users; we refer to the model as cutilization-to-vector, util2vec. We demonstrate the promise of this approach for three sample applications: (1) a practical user-tagging system that automatically predicts areas of focus for millions of Photoshop users; (2) a two-phase recommendation model that enables cold-start personalized recommendations for many new Behance users who have Photoshop usage data, improving recommendation quality (Recall@100) by 21.2% over a popularity-based recommender; and (3) a novel inspiration engine that provides real-time personalized inspirations to artists. We believe that this work demonstrates the potential impact of unstructured usage-log data for personalization.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {485–493},
numpages = {9},
keywords = {user modeling, application usage, recommendation},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@inproceedings{10.1145/2590748.2590749,
author = {Gupta, Monika and Sureka, Ashish},
title = {Nirikshan: Mining Bug Report History for Discovering Process Maps, Inefficiencies and Inconsistencies},
year = {2014},
isbn = {9781450327763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2590748.2590749},
doi = {10.1145/2590748.2590749},
abstract = {Issue tracking systems such as Bugzilla, Mantis and JIRA are Process Aware Information Systems to support business process of issue (defect and feature enhancement) reporting and resolution. The process of issue reporting to resolution consists of several steps or activities performed by various roles (bug reporter, bug triager, bug fixer, developers, and quality assurance manager) within the software maintenance team. Project teams define a workflow or a business process (design time process model and guidelines) to streamline and structure the issue management activities. However, the runtime process (reality) may not conform to the design time model and can have imperfections or inefficiencies. We apply business process mining tools and techniques to analyze the event log data (bug report history) generated by an issue tracking system with the objective of discovering runtime process maps, inefficiencies and inconsistencies. We conduct a case-study on data extracted from Bugzilla issue tracking system of the popular open-source Firefox browser project. We present and implement a process mining framework, Nirikshan, consisting of various steps: data extraction, data transformation, process discovery, performance analysis and conformance checking. We conduct a series of process mining experiments to study self-loops, back-and-forth, issue reopen, unique traces, event frequency, activity frequency, bottlenecks and present an algorithm and metrics to compute the degree of conformance between the design time and the runtime process.},
booktitle = {Proceedings of the 7th India Software Engineering Conference},
articleno = {1},
numpages = {10},
keywords = {open-source software, software maintenance, mining software repositories, issue tracking system, process mining, empirical software engineering and measurements},
location = {Chennai, India},
series = {ISEC '14}
}

@inproceedings{10.1145/3239235.3239248,
author = {Rosenberg, Carl Martin and Moonen, Leon},
title = {Improving Problem Identification via Automated Log Clustering Using Dimensionality Reduction},
year = {2018},
isbn = {9781450358231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239235.3239248},
doi = {10.1145/3239235.3239248},
abstract = {Background: Continuous engineering practices, such as continuous integration and continuous deployment, see increased adoption in modern software development. A frequently reported challenge for adopting these practices is the need to make sense of the large amounts of data that they generate.Goal: We consider the problem of automatically grouping logs of runs that failed for the same underlying reasons, so that they can be treated more effectively, and investigate the following questions: (1) Does an approach developed to identify problems in system logs generalize to identifying problems in continuous deployment logs? (2) How does dimensionality reduction affect the quality of automated log clustering? (3) How does the criterion used for merging clusters in the clustering algorithm affect clustering quality?Method: We replicate and extend earlier work on clustering system log files to assess its generalization to continuous deployment logs. We consider the optional inclusion of one of these dimensionality reduction techniques: Principal Component Analysis (PCA), Latent Semantic Indexing (LSI), and Non-negative Matrix Factorization (NMF). Moreover, we consider three alternative cluster merge criteria (Single Linkage, Average Linkage, and Weighted Linkage), in addition to the Complete Linkage criterion used in earlier work. We empirically evaluate the 16 resulting configurations on continuous deployment logs provided by our industrial collaborator.Results: Our study shows that (1) identifying problems in continuous deployment logs via clustering is feasible, (2) including NMF significantly improves overall accuracy and robustness, and (3) Complete Linkage performs best of all merge criteria analyzed.Conclusions: We conclude that problem identification via automated log clustering is improved by including dimensionality reduction, as it decreases the pipeline's sensitivity to parameter choice, thereby increasing its robustness for handling different inputs.},
booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {16},
numpages = {10},
keywords = {log mining, log analysis, failure diagnosis, continuous engineering},
location = {Oulu, Finland},
series = {ESEM '18}
}

@inproceedings{10.1109/MSR.2019.00070,
author = {Gote, Christoph and Scholtes, Ingo and Schweitzer, Frank},
title = {Git2net: Mining Time-Stamped Co-Editing Networks from Large Git Repositories},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00070},
doi = {10.1109/MSR.2019.00070},
abstract = {Data from software repositories have become an important foundation for the empirical study of software engineering processes. A recurring theme in the repository mining literature is the inference of developer networks capturing e.g. collaboration, coordination, or communication from the commit history of projects. Most of the studied networks are based on the co-authorship of software artefacts defined at the level of files, modules, or packages. While this approach has led to insights into the social aspects of software development, it neglects detailed information on code changes and code ownership, e.g. which exact lines of code have been authored by which developers, that is contained in the commit log of software projects.Addressing this issue, we introduce git2net, a scalable python software that facilitates the extraction of fine-grained co-editing networks in large git repositories. It uses text mining techniques to analyse the detailed history of textual modifications within files. This information allows us to construct directed, weighted, and time-stamped networks, where a link signifies that one developer has edited a block of source code originally written by another developer. Our tool is applied in case studies of an Open Source and a commercial software project. We argue that it opens up a massive new source of high-resolution data on human collaboration patterns.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {433–444},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/2426656.2426715,
author = {Lim, Roman and Walser, Christoph and Ferrari, Federico and Zimmerling, Marco and Beutel, Jan},
title = {Distributed and Synchronized Measurements with FlockLab},
year = {2012},
isbn = {9781450311694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2426656.2426715},
doi = {10.1145/2426656.2426715},
abstract = {Developing, testing, debugging, and evaluating communication protocols for low-power wireless networks is a long and cumbersome task. Simulators can be helpful in the early stages of development, but their models of hardware components and the wireless channel are often rather simplistic and hence cannot substitute experiments on real sensor node platforms. The resources available on common platforms are however very limited, and so are the possibilities for non-intrusive debugging and testing. With most existing testbeds it is only possible to collect information from the serial port, which requires adding highly intrusive logging statements that alter the timing behavior of the software running on the nodes. This is particularly detrimental to the operation of time-critical components, such as radio drivers, media access control (MAC) protocols, and certain flooding protocols [2], hindering their testbed-assisted development.},
booktitle = {Proceedings of the 10th ACM Conference on Embedded Network Sensor Systems},
pages = {373–374},
numpages = {2},
location = {Toronto, Ontario, Canada},
series = {SenSys '12}
}

@proceedings{10.1145/3378904,
title = {BDET 2020: Proceedings of the 2020 2nd International Conference on Big Data Engineering and Technology},
year = {2020},
isbn = {9781450376839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Big data is an emerging paradigm applied to datasets whose size is beyond the ability of commonly used software tools to capture, manage, and process the data within a tolerable elapsed time. Such datasets are often from various sources (Variety) yet unstructured such as social media, sensors, scientific applications, surveillance, video and image archives, Internet texts and documents, Internet search indexing, medical records, business transactions and web logs; and are of large size (Volume) with fast data in/out (Velocity). 2020 2nd International Conference on Big Data Engineering and Technology is one of the premier events to network and learn from colleagues and other leading international scientific voices from across the world, who is actively engaged in advancing research and raising awareness of the many challenges in the diverse field of Big Data Engineering and Technology.},
location = {Singapore, China}
}

@inproceedings{10.1145/2872362.2872381,
author = {Kolli, Aasheesh and Pelley, Steven and Saidi, Ali and Chen, Peter M. and Wenisch, Thomas F.},
title = {High-Performance Transactions for Persistent Memories},
year = {2016},
isbn = {9781450340915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2872362.2872381},
doi = {10.1145/2872362.2872381},
abstract = {Emerging non-volatile memory (NVRAM) technologies offer the durability of disk with the byte-addressability of DRAM. These devices will allow software to access persistent data structures directly in NVRAM using processor loads and stores, however, ensuring consistency of persistent data across power failures and crashes is difficult. Atomic, durable transactions are a widely used abstraction to enforce such consistency. Implementing transactions on NVRAM requires the ability to constrain the order of NVRAM writes, for example, to ensure that a transaction's log record is complete before it is marked committed. Since NVRAM write latencies are expected to be high, minimizing these ordering constraints is critical for achieving high performance. Recent work has proposed programming interfaces to express NVRAM write ordering constraints to hardware so that NVRAM writes may be coalesced and reordered while preserving necessary constraints. Unfortunately, a straightforward implementation of transactions under these interfaces imposes unnecessary constraints. We show how to remove these dependencies through a variety of techniques, notably, deferring commit until after locks are released. We present a comprehensive analysis contrasting two transaction designs across three NVRAM programming interfaces, demonstrating up to 2.5x speedup.},
booktitle = {Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {399–411},
numpages = {13},
keywords = {memory persistency, transactions, recoverability, non-volatile memory},
location = {Atlanta, Georgia, USA},
series = {ASPLOS '16}
}

@article{10.1145/2980024.2872381,
author = {Kolli, Aasheesh and Pelley, Steven and Saidi, Ali and Chen, Peter M. and Wenisch, Thomas F.},
title = {High-Performance Transactions for Persistent Memories},
year = {2016},
issue_date = {May 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {2},
issn = {0163-5964},
url = {https://doi.org/10.1145/2980024.2872381},
doi = {10.1145/2980024.2872381},
abstract = {Emerging non-volatile memory (NVRAM) technologies offer the durability of disk with the byte-addressability of DRAM. These devices will allow software to access persistent data structures directly in NVRAM using processor loads and stores, however, ensuring consistency of persistent data across power failures and crashes is difficult. Atomic, durable transactions are a widely used abstraction to enforce such consistency. Implementing transactions on NVRAM requires the ability to constrain the order of NVRAM writes, for example, to ensure that a transaction's log record is complete before it is marked committed. Since NVRAM write latencies are expected to be high, minimizing these ordering constraints is critical for achieving high performance. Recent work has proposed programming interfaces to express NVRAM write ordering constraints to hardware so that NVRAM writes may be coalesced and reordered while preserving necessary constraints. Unfortunately, a straightforward implementation of transactions under these interfaces imposes unnecessary constraints. We show how to remove these dependencies through a variety of techniques, notably, deferring commit until after locks are released. We present a comprehensive analysis contrasting two transaction designs across three NVRAM programming interfaces, demonstrating up to 2.5x speedup.},
journal = {SIGARCH Comput. Archit. News},
month = mar,
pages = {399–411},
numpages = {13},
keywords = {transactions, memory persistency, non-volatile memory, recoverability}
}

@article{10.1145/2954679.2872381,
author = {Kolli, Aasheesh and Pelley, Steven and Saidi, Ali and Chen, Peter M. and Wenisch, Thomas F.},
title = {High-Performance Transactions for Persistent Memories},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2954679.2872381},
doi = {10.1145/2954679.2872381},
abstract = {Emerging non-volatile memory (NVRAM) technologies offer the durability of disk with the byte-addressability of DRAM. These devices will allow software to access persistent data structures directly in NVRAM using processor loads and stores, however, ensuring consistency of persistent data across power failures and crashes is difficult. Atomic, durable transactions are a widely used abstraction to enforce such consistency. Implementing transactions on NVRAM requires the ability to constrain the order of NVRAM writes, for example, to ensure that a transaction's log record is complete before it is marked committed. Since NVRAM write latencies are expected to be high, minimizing these ordering constraints is critical for achieving high performance. Recent work has proposed programming interfaces to express NVRAM write ordering constraints to hardware so that NVRAM writes may be coalesced and reordered while preserving necessary constraints. Unfortunately, a straightforward implementation of transactions under these interfaces imposes unnecessary constraints. We show how to remove these dependencies through a variety of techniques, notably, deferring commit until after locks are released. We present a comprehensive analysis contrasting two transaction designs across three NVRAM programming interfaces, demonstrating up to 2.5x speedup.},
journal = {SIGPLAN Not.},
month = mar,
pages = {399–411},
numpages = {13},
keywords = {recoverability, memory persistency, transactions, non-volatile memory}
}

@inproceedings{10.1145/3331184.3331187,
author = {White, Ryen W. and Hassan Awadallah, Ahmed and Sim, Robert},
title = {Task Completion Detection: A Study in the Context of Intelligent Systems},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331187},
doi = {10.1145/3331184.3331187},
abstract = {People can record their pending tasks using to-do lists, digital assistants, and other task management software. In doing so, users of these systems face at least two challenges: (1) they must manually mark their tasks as complete, and (2) when systems proactively remind them about their pending tasks, say, via interruptive notifications, they lack information on task completion status. As a result, people may not realize the full benefits of to-do lists (since these lists can contain both completed and pending tasks) and they may be reminded about tasks they have already done (wasting time and causing frustration). In this paper, we present methods to automatically detect task completion. These inferences can be used to deprecate completed tasks and/or suppress notifications for these tasks (or for other purposes, e.g., task prioritization). Using log data from a popular digital assistant, we analyze temporal dynamics in the completion of tasks and train machine-learned models to detect completion with accuracy exceeding 80% using a variety of features (time elapsed since task creation, task content, email, notifications, user history). The findings have implications for the design of intelligent systems to help people manage their tasks.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {405–414},
numpages = {10},
keywords = {task completion detection, intelligent systems},
location = {Paris, France},
series = {SIGIR'19}
}

@inproceedings{10.1145/3196321.3196343,
author = {Feng, Yang and Dreef, Kaj and Jones, James A. and van Deursen, Arie},
title = {Hierarchical Abstraction of Execution Traces for Program Comprehension},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3196343},
doi = {10.1145/3196321.3196343},
abstract = {Understanding the dynamic behavior of a software system is one of the most important and time-consuming tasks for today's software maintainers. In practice, understanding the inner workings of software requires studying the source code and documentation and inserting logging code in order to map high-level descriptions of the program behavior with low-level implementation, i.e., the source code. Unfortunately, for large codebases and large log files, such cognitive mapping can be quite challenging. To bridge the cognitive gap between the source code and detailed models of program behavior, we propose a fully automatic approach to present a semantic abstraction with different levels of functional granularity from full execution traces. Our approach builds multi-level abstractions and identifies frequent behaviors at each level based on a number of execution traces, and then, it labels phases within individual execution traces according to the identified major functional behaviors of the system. To validate our approach, we conducted a case study on a large-scale subject program, Javac, to demonstrate the effectiveness of the mining result. Furthermore, the results of a user study demonstrate that our approach is capable of presenting users a high-level comprehensible abstraction of execution behavior. Based on a real world subject program the participants in our user study were able to achieve a mean accuracy of 70%.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {86–96},
numpages = {11},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@inproceedings{10.1145/2637002.2637051,
author = {Wei, Xing and Zhang, Yinglong and Gwizdka, Jacek},
title = {YASFIIRE: Yet Another System for IIR Evaluation},
year = {2014},
isbn = {9781450329767},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2637002.2637051},
doi = {10.1145/2637002.2637051},
abstract = {We present a system that supports Interactive Information Retrieval user studies on the Web. Our system provides support for user and task management, for processing web-based task specific interfaces and for Web-event logging. It also offers functionality useful to IIR studies that capture eye-movement on Web page elements. The system complements logging functionality offered by a typical usability/eye-tracking software packages and is designed to act in concert with such software.},
booktitle = {Proceedings of the 5th Information Interaction in Context Symposium},
pages = {316–319},
numpages = {4},
keywords = {experiment support, evaluation, interactive information retrieval},
location = {Regensburg, Germany},
series = {IIiX '14}
}

@inproceedings{10.1145/3324884.3416616,
author = {Chen, Bihuan and Chen, Linlin and Zhang, Chen and Peng, Xin},
title = {BuildFast: History-Aware Build Outcome Prediction for Fast Feedback and Reduced Cost in Continuous Integration},
year = {2020},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416616},
doi = {10.1145/3324884.3416616},
abstract = {Long build times in continuous integration (CI) can greatly increase the cost in human and computing resources, and thus become a common barrier faced by software organizations adopting CI. Build outcome prediction has been proposed as one of the remedies to reduce such cost. However, the state-of-the-art approaches have a poor prediction performance for failed builds, and are not designed for practical usage scenarios. To address the problems, we first conduct an empirical study on 2,590,917 builds to characterize build times in real-world projects, and a survey with 75 developers to understand their perceptions about build outcome prediction. Then, motivated by our study and survey results, we propose a new history-aware approach, named BuildFast, to predict CI build outcomes cost-efficiently and practically. We develop multiple failure-specific features from closely related historical builds via analyzing build logs and changed files, and propose an adaptive prediction model to switch between two models based on the build outcome of the previous build. We investigate a practical online usage scenario of BuildFast, where builds are predicted in chronological order, and measure the benefit from correct predictions and the cost from incorrect predictions. Our experiments on 20 projects have shown that BuildFast improved the state-of-the-art by 47.5% in F1-score for failed builds.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {42–53},
numpages = {12},
keywords = {failure prediction, build failures, continuous integration},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@article{10.1145/1945023.1945034,
author = {Chen, Shimin and Gibbons, Phillip B. and Kozuch, Michael and Mowry, Todd C.},
title = {Log-Based Architectures: Using Multicore to Help Software Behave Correctly},
year = {2011},
issue_date = {January 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {1},
issn = {0163-5980},
url = {https://doi.org/10.1145/1945023.1945034},
doi = {10.1145/1945023.1945034},
abstract = {While application performance and power-efficiency are both important, application correctness is even more important. In other words, if the application is misbehaving, it is little consolation that it is doing so quickly or power-efficiently. In the Log-Based Architectures (LBA) project, we are focusing on a challenging source of application misbehavior: software bugs, including obscure bugs that only cause problems during security attacks. To help detect and fix software bugs, we have been exploring techniques for accelerating dynamic program monitoring tools, which we call "lifeguards". Lifeguards are typically written today using dynamic binary instrumentation frameworks such as Valgrind or Pin. Due to the overheads of binary instrumentation, lifeguards that require instructiongrain information typically experience 30X-100X slowdowns, and hence it is only practical to use them during explicit debug cycles. The goal in the LBA project is to reduce these overheads to the point where lifeguards can run continuously on deployed code. To accomplish this, we propose hardware mechanisms to create a dynamic log of instruction-level events in the monitored application and stream this information to one or more software lifeguards running on separate cores on the same multicore processor. In this paper, we highlight techniques and features of LBA that reduce the slowdown to just 2%--51% for sequential programs and 28%--51% for parallel programs.},
journal = {SIGOPS Oper. Syst. Rev.},
month = feb,
pages = {84–91},
numpages = {8},
keywords = {program monitoring, parallel monitoring, software bugs, log-based architectures, lifeguards}
}

@inproceedings{10.1145/1842993.1843104,
author = {Scheuner, Barbara},
title = {Evaluating the Utilization of Clustering Methods Connected with Multivariate Visualizations},
year = {2010},
isbn = {9781450300766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842993.1843104},
doi = {10.1145/1842993.1843104},
abstract = {Visualization software in all fields becomes increasingly interactive with more and more elements to support users. Elements that help an observer to interpret given data sets by computing certain properties of the data to enrich their visualization we call "visual scouts".Many visualization programs implement visual scouts, but few is known about how users apply them in a real setting. In order to learn more about how users work with interactive visualization software we provide them with clustering methods (k-mean and hierarchical clustering), as one possibility to manipulate the data. Subsequently we evaluate how natural science students use these clustering methods as provided by an interactive comparative visualization software for high dimensional data (VisuLab®). The students are introduced to four different visualization methods (figure 1) and given the possibility to apply four clustering methods to help them when interpreting two sample datasets (table 1 first two rows). This is part of a guided instruction after which they independently interpret two new but related real data sets (table 1 last two rows). In total they work for about six hours.During our evaluation we were able to record the clustering activities of ~250 students, by directly logging the user's activities within the software. Based on this data we computed how many times the data was clustered and with which visualization methods the results were visualized. In total we registered over 57'000 clustering activities of which 30% where recorded during the instruction period and the remaining 70% during the time they worked independently.Based on the data gained we investigate relationships between clustering methods, the type of data analyzed and the visualization method chosen. This is shown in figure 2, where the number of clustering in regard to the visualization method is presented in a boxplot.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
pages = {427},
numpages = {1},
location = {Roma, Italy},
series = {AVI '10}
}

@inproceedings{10.1145/3287624.3287758,
author = {Tahghighi, Mohammad and Zhang, Wei},
title = {Accelerate Pattern Recognition for Cyber Security Analysis},
year = {2019},
isbn = {9781450360074},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287624.3287758},
doi = {10.1145/3287624.3287758},
abstract = {Network security analysis is about processing the network equipment's log records to capture malicious and anomalous traffic. Scrutinizing huge amount of records to capture complex patterns is time consuming and difficult to parallelize. In this paper, we proposed a hardware/software co-designed system to address this problem for specific IP chaining patterns.},
booktitle = {Proceedings of the 24th Asia and South Pacific Design Automation Conference},
pages = {23–24},
numpages = {2},
location = {Tokyo, Japan},
series = {ASPDAC '19}
}

@inproceedings{10.1145/3180155.3180224,
author = {Ren, Zhilei and Jiang, He and Xuan, Jifeng and Yang, Zijiang},
title = {Automated Localization for Unreproducible Builds},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180224},
doi = {10.1145/3180155.3180224},
abstract = {Reproducibility is the ability of recreating identical binaries under pre-defined build environments. Due to the need of quality assurance and the benefit of better detecting attacks against build environments, the practice of reproducible builds has gained popularity in many open-source software repositories such as Debian and Bitcoin. However, identifying the unreproducible issues remains a labour intensive and time consuming challenge, because of the lacking of information to guide the search and the diversity of the causes that may lead to the unreproducible binaries.In this paper we propose an automated framework called RepLoc to localize the problematic files for unreproducible builds. RepLoc features a query augmentation component that utilizes the information extracted from the build logs, and a heuristic rule-based filtering component that narrows the search scope. By integrating the two components with a weighted file ranking module, RepLoc is able to automatically produce a ranked list of files that are helpful in locating the problematic files for the unreproducible builds. We have implemented a prototype and conducted extensive experiments over 671 real-world unreproducible Debian packages in four different categories. By considering the topmost ranked file only, RepLoc achieves an accuracy rate of 47.09%. If we expand our examination to the top ten ranked files in the list produced by RepLoc, the accuracy rate becomes 79.28%. Considering that there are hundreds of source code, scripts, Makefiles, etc., in a package, RepLoc significantly reduces the scope of localizing problematic files. Moreover, with the help of RepLoc, we successfully identified and fixed six new unreproducible packages from Debian and Guix.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {71–81},
numpages = {11},
keywords = {software maintenance, unreproducible build, localization},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@article{10.1145/2229156.2229157,
author = {van der Aalst, Wil},
title = {Process Mining: Overview and Opportunities},
year = {2012},
issue_date = {July 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2229156.2229157},
doi = {10.1145/2229156.2229157},
abstract = {Over the last decade, process mining emerged as a new research field that focuses on the analysis of processes using event data. Classical data mining techniques such as classification, clustering, regression, association rule learning, and sequence/episode mining do not focus on business process models and are often only used to analyze a specific step in the overall process. Process mining focuses on end-to-end processes and is possible because of the growing availability of event data and new process discovery and conformance checking techniques.Process models are used for analysis (e.g., simulation and verification) and enactment by BPM/WFM systems. Previously, process models were typically made by hand without using event data. However, activities executed by people, machines, and software leave trails in so-called event logs. Process mining techniques use such logs to discover, analyze, and improve business processes.Recently, the Task Force on Process Mining released the Process Mining Manifesto. This manifesto is supported by 53 organizations and 77 process mining experts contributed to it. The active involvement of end-users, tool vendors, consultants, analysts, and researchers illustrates the growing significance of process mining as a bridge between data mining and business process modeling. The practical relevance of process mining and the interesting scientific challenges make process mining one of the “hot” topics in Business Process Management (BPM). This article introduces process mining as a new research field and summarizes the guiding principles and challenges described in the manifesto.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {7},
numpages = {17},
keywords = {data mining, business intelligence, business process management, Process mining}
}

@inproceedings{10.1145/2025113.2025135,
author = {Cheung, Alvin and Solar-Lezama, Armando and Madden, Samuel},
title = {Partial Replay of Long-Running Applications},
year = {2011},
isbn = {9781450304436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2025113.2025135},
doi = {10.1145/2025113.2025135},
abstract = {Bugs in deployed software can be extremely difficult to track down. Invasive logging techniques, such as logging all non-deterministic inputs, can incur substantial runtime overheads. This paper shows how symbolic analysis can be used to re-create path equivalent executions for very long running programs such as databases and web servers. The goal is to help developers debug such long-running programs by allowing them to walk through an execution of the last few requests or transactions leading up to an error. The challenge is to provide this functionality without the high runtime overheads associated with traditional replay techniques based on input logging or memory snapshots. Our approach achieves this by recording a small amount of information about program execution, such as the direction of branches taken, and then using symbolic analysis to reconstruct the execution of the last few inputs processed by the application, as well as the state of memory before these inputs were executed.We implemented our technique in a new tool called bbr. In this paper, we show that it can be used to replay bugs in long-running single-threaded programs starting from the middle of an execution. We show that bbr incurs low recording overhead (avg. of 10%) during program execution, which is much less than existing replay schemes. We also show that it can reproduce real bugs from web servers, database systems, and other common utilities.},
booktitle = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
pages = {135–145},
numpages = {11},
keywords = {symbolic execution, symbolic debugging, software instrumentation},
location = {Szeged, Hungary},
series = {ESEC/FSE '11}
}

@article{10.1145/3375633,
author = {Beschastnikh, Ivan and Liu, Perry and Xing, Albert and Wang, Patty and Brun, Yuriy and Ernst, Michael D.},
title = {Visualizing Distributed System Executions},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3375633},
doi = {10.1145/3375633},
abstract = {Distributed systems pose unique challenges for software developers. Understanding the system’s communication topology and reasoning about concurrent activities of system hosts can be difficult. The standard approach, analyzing system logs, can be a tedious and complex process that involves reconstructing a system log from multiple hosts’ logs, reconciling timestamps among hosts with non-synchronized clocks, and understanding what took place during the execution encoded by the log. This article presents a novel approach for tackling three tasks frequently performed during analysis of distributed system executions: (1)&nbsp;understanding the relative ordering of events, (2)&nbsp;searching for specific patterns of interaction between hosts, and (3)&nbsp;identifying structural similarities and differences between pairs of executions. Our approach consists of XVector, which instruments distributed systems to capture partial ordering information that encodes the happens-before relation between events, and ShiViz, which processes the resulting logs and presents distributed system executions as interactive time-space diagrams. Two user studies with a total of 109 students and a case study with 2 developers showed that our method was effective, helping participants answer statistically significantly more system-comprehension questions correctly, with a very large effect size.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {9},
numpages = {38},
keywords = {program comprehension, log analysis, Distributed systems}
}

@inproceedings{10.1145/1982185.1982226,
author = {Marty, Raffael},
title = {Cloud Application Logging for Forensics},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982226},
doi = {10.1145/1982185.1982226},
abstract = {Logs are one of the most important pieces of analytical data in a cloud-based service infrastructure. At any point in time, service owners and operators need to understand the status of each infrastructure component for fault monitoring, to assess feature usage, and to monitor business processes. Application developers, as well as security personnel, need access to historic information for debugging and forensic investigations.This paper discusses a logging framework and guidelines that provide a proactive approach to logging to ensure that the data needed for forensic investigations has been generated and collected. The standardized framework eliminates the need for logging stakeholders to reinvent their own standards. These guidelines make sure that critical information associated with cloud infrastructure and software as a service (SaaS) use-cases are collected as part of a defense in depth strategy. In addition, they ensure that log consumers can effectively and easily analyze, process, and correlate the emitted log records. The theoretical foundations are emphasized in the second part of the paper that covers the implementation of the framework in an example SaaS offering running on a public cloud service.While the framework is targeted towards and requires the buy-in from application developers, the data collected is critical to enable comprehensive forensic investigations. In addition, it helps IT architects and technical evaluators of logging architectures build a business oriented logging framework.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {178–184},
numpages = {7},
keywords = {cloud, logging guidelines, computer forensics, log forensics, logging, software as a service},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@inproceedings{10.1145/3196398.3196462,
author = {Joonbakhsh, Alireza and Sami, Ashkan},
title = {Mining and Extraction of Personal Software Process Measures through IDE Interaction Logs},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196462},
doi = {10.1145/3196398.3196462},
abstract = {The Personal Software Process (PSP) is an effective software process improvement method that heavily relies on manual collection of software development data. This paper describes a semi-automated method that reduces the burden of PSP data collection by extracting the required time and size of PSP measurements from IDE interaction logs. The tool mines enriched event data streams so can be easily generalized to other developing environment also. In addition, the proposed method is adaptable to phase definition changes and creates activity visualizations and summarizations that are helpful for software project management. Tools and processed data used for this paper are available on GitHub at: https://github.com/unknowngithubuser1/data.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {78–81},
numpages = {4},
keywords = {personal software process, IDE},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1145/2808797.2808894,
author = {Nunes, Eric and Buto, Casey and Shakarian, Paulo and Lebiere, Christian and Bennati, Stefano and Thomson, Robert and Jaenisch, Holger},
title = {Malware Task Identification: A Data Driven Approach},
year = {2015},
isbn = {9781450338547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808797.2808894},
doi = {10.1145/2808797.2808894},
abstract = {Identifying the tasks a given piece of malware was designed to perform (e.g. logging keystrokes, recording video, establishing remote access, etc.) is a difficult and time-consuming operation that is largely human-driven in practice. In this paper, we present an automated method to identify malware tasks. Using two different malware collections, we explore various circumstances for each - including cases where the training data differs significantly from test; where the malware being evaluated employs packing to thwart analytical techniques; and conditions with sparse training data. We find that this approach consistently out-performs the current state-of-the art software for malware task identification as well as standard machine learning approaches - often achieving an unbiased F1 score of over 0.9. In the near future, we look to deploy our approach for use by analysts in an operational cyber-security environment.},
booktitle = {Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015},
pages = {978–985},
numpages = {8},
location = {Paris, France},
series = {ASONAM '15}
}

@inproceedings{10.1145/3012430.3012575,
author = {Palomo-Duarte, Manuel and Berns, Anke and Isla-Montes, Jos\'{e}-Luis and Dodero, Juan-Manuel and Kabtoul, Owayss},
title = {A Collaborative Mobile Learning System to Facilitate Foreign Language Learning and Assessment Processes},
year = {2016},
isbn = {9781450347471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3012430.3012575},
doi = {10.1145/3012430.3012575},
abstract = {The growing spread of mobile technologies provides educators and developers with new opportunities for creating a wider range of language learning and assessment tools. In this paper we present a multi-user mobile learning system---specifically designed for this study. The system delivers an app version of a paper-based collaborative learning task successfully used for several years with undergraduate students in an A1-level German language course (CEFR). The collaborative learning task, called Terminkalender, requires students to work together to plan different activities and record them in an appointment calendar. While the paper-based version of the Terminkalender already had a great potential for engaging students to communicate and negotiate in the target language, the app-based mobile learning system facilitates students' language learning process---while at the same time allowing teachers to easily trace back learners' interaction in the target language. To this end, specific software implementing several features was developed: a user-friendly web portal interface for teachers, computer-generated anonymous learner identities, an in-app text chat function, real-time task performance feedback to learners as well as a log function for storage and assessment purposes. The experiment provides us with some interesting insights into language learner behaviour and interaction patterns, allowing to draw a number of preliminary conclusions regarding how task design and using a collaborative mobile learning system impacts both student interaction and task performance.},
booktitle = {Proceedings of the Fourth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {567–572},
numpages = {6},
keywords = {MALL, CALL, foreign language learning, computer-assisted assessment, collaborative mobile learning system},
location = {Salamanca, Spain},
series = {TEEM '16}
}

@inproceedings{10.1145/1985500.1985503,
author = {Nagappan, Meiyappan and Peeler, Aaron and Vouk, Mladen},
title = {Modeling Cloud Failure Data: A Case Study of the Virtual Computing Lab},
year = {2011},
isbn = {9781450305822},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985500.1985503},
doi = {10.1145/1985500.1985503},
abstract = {Virtual Computing Lab is a higher education cloud computing environment that on demand, allocates a chosen software stack on the required hardware and gives access to the customers, in this case NCSU students, faculty and staff. VCL has been in operation since 2004. An important component of the quality of the services provided by a cloud is the reliability and availability. For example, typical availability of the system exceeds 0.999, and reservation reliability is in the 0.99 range. VCL provides comprehensive information (provenance, logs, etc.) about its execution, its resources, and its performance. We mined the VCL log files to find out more about its reliability and availability, and the character of its faults and failures. This paper presents some of these results.},
booktitle = {Proceedings of the 2nd International Workshop on Software Engineering for Cloud Computing},
pages = {8–14},
numpages = {7},
keywords = {cloud computing, software reliability and availability, vcl production software failures},
location = {Waikiki, Honolulu, HI, USA},
series = {SECLOUD '11}
}

@inproceedings{10.1145/1978942.1979334,
author = {Karlson, Amy K. and Smith, Greg and Lee, Bongshin},
title = {Which Version is This? Improving the Desktop Experience within a Copy-Aware Computing Ecosystem},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979334},
doi = {10.1145/1978942.1979334},
abstract = {Computers today make it easy for people to scatter copies and versions of digital items across their file systems, but do little to help people manage the resulting mess. In this paper, we introduce the concept of a copy-aware computing ecosystem, inspired by a vision of computing when systems track and surface copy relationships between files. Based on two deployments of a copy-aware software prototype and in-depth interviews with individuals in collaborative relationships, we present our findings on the origins of copies and the barriers to eliminating them, but offer a promising solution based on the set of files that together represent a user's conceptual view of a document - the versionset. We show that the versionset is viable to infer, and we draw upon user activity logs and feedback on personalized views of versionsets to distill guidelines for the factors that define a versionset. We conclude by enumerating the many PIM user experiences that could be transformed as a result.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2669–2678},
numpages = {10},
keywords = {PIM, versioning, copy-aware computing, file management},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/3377811.3380361,
author = {Hoang, Thong and Kang, Hong Jin and Lo, David and Lawall, Julia},
title = {CC2Vec: Distributed Representations of Code Changes},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380361},
doi = {10.1145/3377811.3380361},
abstract = {Existing work on software patches often use features specific to a single task. These works often rely on manually identified features, and human effort is required to identify these features for each task. In this work, we propose CC2Vec, a neural network model that learns a representation of code changes guided by their accompanying log messages, which represent the semantic intent of the code changes. CC2Vec models the hierarchical structure of a code change with the help of the attention mechanism and uses multiple comparison functions to identify the differences between the removed and added code.To evaluate if CC2Vec can produce a distributed representation of code changes that is general and useful for multiple tasks on software patches, we use the vectors produced by CC2Vec for three tasks: log message generation, bug fixing patch identification, and just-in-time defect prediction. In all tasks, the models using CC2Vec outperform the state-of-the-art techniques.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {518–529},
numpages = {12},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3159450.3162241,
author = {Zhang, Ke and Chen, Mingyu and Bao, Yungang},
title = {ZyForce: An FPGA-Based Cloud Platform for Experimental Curriculum of Computer System in University of Chinese Academy of Sciences (Abstract Only)},
year = {2018},
isbn = {9781450351034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159450.3162241},
doi = {10.1145/3159450.3162241},
abstract = {To cultivate students" capability of computer system thinking and software/hardware programming, experimental curriculum of computer system is regarded as one of the most effective methods. Some universities have set up hardware labs equipped with several or dozens of FPGA (Field Programmable Gate Array) boards for these courses. However, these lab kits are always in a relatively low utilization rate and how the students" capability is improved by these assets is not easy to be evaluated. Inspired by the merits of FPGA public cloud (e.g. Amazon AWS F1 instance), an in-house-designed FPGA-based online cloud platform (named ZyForce) is proposed to deploy in UCAS. This platform is equipped with 40 custom designed boards using Xilinx Zynq UltraScale+ MPSoC FPGAs and the utilization rate of these education resources is boosted by means of advanced cloud computing technology. With ZyForce, students remotely carry out lab assignments (e.g. MIPS, RISC-V or domain-specific architecture processor design with cache/memory, DMA, accelerator and performance counter) as using local FPGA boards; instructors can analyze the downloaded operation log file for each student and know how these kits are being used. It"s believed that this kind of online hardware lab appliances provides a novel pay-as-you-go service model for those universities in remote regions who cannot afford to set up their own hardware laboratories, and also facilitates our students, the future scientists and engineers, with this promising cloud development approach.},
booktitle = {Proceedings of the 49th ACM Technical Symposium on Computer Science Education},
pages = {1091},
numpages = {1},
keywords = {experimental curriculum, fpga cloud, virtual lab, online learning, hardware platform},
location = {Baltimore, Maryland, USA},
series = {SIGCSE '18}
}

@inproceedings{10.1145/2662253.2662310,
author = {Giraldo, William J. and Tob\'{o}n, M\'{o}nica Lorena and Giraldo, F\'{a}ber D. and Villegas, Mar\'{\i}a L. and Guerrero, Alexandra and Cort\'{e}s, M\'{o}nica Yulieth and Ruiz, Alexandra and Collazos, C\'{e}sar A.},
title = {HCI Incorporation: A Case for Colombia},
year = {2014},
isbn = {9781450328807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2662253.2662310},
doi = {10.1145/2662253.2662310},
abstract = {This paper shows a practical case of HCI incorporation in Colombian software development companies, mainly in the Colombian coffee region. This case is supported by a self-sustaining ecosystem model that emerges from the academy approach to software development companies. This ecosystem model is a response for the need to translate scientific knowledge into practical actions that can be performed by development engineers. This ecosystem should give coverage to all the necessary elements to generate real services of HCI incorporation, regardless of the nature of projects that software development companies are doing. We have been concretizing, one by one, all the necessary components of this ecosystem which have been registered into a log full of events that take place in various capacities which a self-sustaining business model must deal with.},
booktitle = {Proceedings of the XV International Conference on Human Computer Interaction},
articleno = {57},
numpages = {8},
keywords = {HCI incorporating model in real software projects, real software projects, ecosystem, usability},
location = {Puerto de la Cruz, Tenerife, Spain},
series = {Interacci\'{o}n '14}
}

@inproceedings{10.1145/3239235.3268921,
author = {Strandberg, Per Erik and Afzal, Wasif and Sundmark, Daniel},
title = {Decision Making and Visualizations Based on Test Results},
year = {2018},
isbn = {9781450358231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239235.3268921},
doi = {10.1145/3239235.3268921},
abstract = {Background: Testing is one of the main methods for quality assurance in the development of embedded software, as well as in software engineering in general. Consequently, test results (and how they are reported and visualized) may substantially influence business decisions in software-intensive organizations. Aims: This case study examines the role of test results from automated nightly software testing and the visualizations for decision making they enable at an embedded systems company in Sweden. In particular, we want to identify the use of the visualizations for supporting decisions from three aspects: in daily work, at feature branch merge, and at release time. Method: We conducted an embedded case study with multiple units of analysis by conducting interviews, questionnaires, using archival data and participant observations. Results: Several visualizations and reports built on top of the test results database are utilized in supporting daily work, merging a feature branch to the master and at release time. Some important visualizations are: lists of failing test cases, easy access to log files, and heatmap trend plots. The industrial practitioners perceived the visualizations and reporting as valuable, however they also mentioned several areas of improvement such as better ways of visualizing test coverage in a functional area as well as better navigation between different views. Conclusions: We conclude that visualizations of test results are a vital decision making tool for a variety of roles and tasks in embedded software development, however the visualizations need to be continuously improved to keep their value for its stakeholders.},
booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {34},
numpages = {10},
keywords = {visualizations, software testing, decision making},
location = {Oulu, Finland},
series = {ESEM '18}
}

@inproceedings{10.1145/3416012.3424627,
author = {Mukherjee, Avishek and Zhong, Yaoguang and Zhang, Zhenghao and Zhao, Tingting and Zhang, Jinfeng},
title = {Vecsim: Carrier-Based, Privacy-Preserving Cellphone Contact Tracing},
year = {2020},
isbn = {9781450381192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416012.3424627},
doi = {10.1145/3416012.3424627},
abstract = {In this paper, Vecsim, a novel contact tracing method, is proposed. Vecsim determines the user proximity based on the existing log data already collected by the cellphone network carrier for network management purposes, and is transparent to the users. Compared to the existing methods that require user involvement, such as downloading an application and turning on Bluetooth, Vecsim is easier to deploy and may cover a larger population. Vecsim protects user privacy by focusing on user proximity detection, which is different from localization. In addition, proximity detection is a much easier problem than localization and can achieve higher accuracy. The key novelties of Vecsim include a simple method for distance estimation based on the similarity of two data records, as well as exploiting the massive log data to learn the discontinuity of the signal field. Vecsim has been tested with the signal field generated by a commercial ray tracing software program in a 2 km by 2 km urban area. The results show that Vecsim alerts over 96% of cellphones within 50 m to each other, while alerting less than 4.5% of cellphones beyond 150 meters.},
booktitle = {Proceedings of the 18th ACM Symposium on Mobility Management and Wireless Access},
pages = {47–55},
numpages = {9},
keywords = {cellphone log data, contact tracing},
location = {Alicante, Spain},
series = {MobiWac '20}
}

@inproceedings{10.1145/3400286.3418262,
author = {Bushong, Vincent and Sanders, Russell and Curtis, Jacob and Du, Mark and Cerny, Tomas and Frajtak, Karel and Bures, Miroslav and Tisnovsky, Pavel and Shin, Dongwan},
title = {On Matching Log Analysis to Source Code: A Systematic Mapping Study},
year = {2020},
isbn = {9781450380256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400286.3418262},
doi = {10.1145/3400286.3418262},
abstract = {Logging is a vital part of the software development process. Developers use program logging to monitor program execution and identify errors and anomalies. These errors may also cause uncaught exceptions and generate stack traces that help identify the point of error. Both of these sources contain information that can be matched to points in the source code, but manual log analysis is challenging for large systems that create large volumes of logs and have large codebases. In this paper, we contribute a systematic mapping study to determine the state-of-the-art tools and methods used to perform automatic log analysis and stack trace analysis and match the extracted information back to the program's source code. We analyzed 16 publications that address this issue, summarizing their strategies and goals, and we identified open research directions from this body of work.},
booktitle = {Proceedings of the International Conference on Research in Adaptive and Convergent Systems},
pages = {181–187},
numpages = {7},
keywords = {log analysis, log mining, static code analysis, program slicing, anomaly detection},
location = {Gwangju, Republic of Korea},
series = {RACS '20}
}

@inproceedings{10.1145/3208040.3208051,
author = {Das, Anwesha and Mueller, Frank and Siegel, Charles and Vishnu, Abhinav},
title = {Desh: Deep Learning for System Health Prediction of Lead Times to Failure in HPC},
year = {2018},
isbn = {9781450357852},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3208040.3208051},
doi = {10.1145/3208040.3208051},
abstract = {Today's large-scale supercomputers encounter faults on a daily basis. Exascale systems are likely to experience even higher fault rates due to increased component count and density. Triggering resilience-mitigating techniques remains a challenge due to the absence of well defined failure indicators. System logs consist of unstructured text that obscures essential system health information contained within. In this context, efficient failure prediction via log mining can enable proactive recovery mechanisms to increase reliability.This work aims to predict node failures that occur in supercomputing systems via long short-term memory (LSTM) networks that exploit recurrent neural networks (RNNs). Our framework, Desh1 (<u>De</u>ep Learning for <u>S</u>ystem <u>H</u>ealth), diagnoses and predicts failures with short lead times. Desh identifies failure indicators with enhanced training and classification for generic applicability to logs from operating systems and software components without the need to modify any of them. Desh uses a novel three-phase deep learning approach to (1) train to recognize chains of log events leading to a failure, (2) re-train chain recognition of events augmented with expected lead times to failure, and (3) predict lead times during testing/inference deployment to predict which specific node fails in how many minutes. Desh obtains as high as 3 minutes average lead time with no less than 85% recall and 83% accuracy to take proactive actions on the failing nodes, which could be used to migrate computation to healthy nodes.},
booktitle = {Proceedings of the 27th International Symposium on High-Performance Parallel and Distributed Computing},
pages = {40–51},
numpages = {12},
keywords = {failure prediction, deep learning, lead times, log mining, HPC, anomaly detection, node failures, LSTM},
location = {Tempe, Arizona},
series = {HPDC '18}
}

@inproceedings{10.1145/3180155.3180181,
author = {Hassan, Foyzul and Wang, Xiaoyin},
title = {HireBuild: An Automatic Approach to History-Driven Repair of Build Scripts},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180181},
doi = {10.1145/3180155.3180181},
abstract = {Advancements in software build tools such as Maven reduce build management effort, but developers still need specialized knowledge and long time to maintain build scripts and resolve build failures. More recent build tools such as Gradle give developers greater extent of customization flexibility, but can be even more difficult to maintain. According to the TravisTorrent dataset of open-source software continuous integration, 22% of code commits include changes in build script files to maintain build scripts or to resolve build failures. Automated program repair techniques have great potential to reduce cost of resolving software failures, but the existing techniques mostly focus on repairing source code so that they cannot directly help resolving software build failures. To address this limitation, we propose HireBuild: <u>Hi</u>story-Driven <u>Rep</u>air of <u>Build</u> Scripts, the first approach to automatic patch generation for build scripts, using fix patterns automatically generated from existing build script fixes and recommending fix patterns based on build log similarity. From TravisTorrent dataset, we extracted 175 build failures and their corresponding fixes which revise Gradle build scripts. Among these 175 build failures, we used the 135 earlier build fixes for automatic fix-pattern generation and the more recent 40 build failures (fixes) for evaluation of our approach. Our experiment shows that our approach can fix 11 of 24 reproducible build failures, or 45% of the reproducible build failures, within comparable time of manual fixes.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {1078–1089},
numpages = {12},
keywords = {build logs, patch generation, software build scripts},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/2889160.2889166,
author = {Ohmann, Tony and Stanley, Ryan and Beschastnikh, Ivan and Brun, Yuriy},
title = {Visually Reasoning about System and Resource Behavior},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2889166},
doi = {10.1145/2889160.2889166},
abstract = {Understanding how software utilizes resources is an important software engineering task. Existing software comprehension approaches rarely consider how resource utilization affects system behavior. We present Perfume, a general-purpose tool to help developers understand how resource utilization impacts their systems' control flow. Perfume is broadly applicable, as it is configurable to parse a wide variety of execution log formats and applies to all resource types that can be represented numerically. Perfume mines temporal properties that hold over the logged executions and represents system behavior in a resource finite state automaton that satisfies the mined properties. Perfume's interactive interface allows the developers to understand system behavior and to formulate and test hypotheses about system executions. A controlled experiment with 40 students shows that Perfume effectively supports understanding and debugging tasks. Students using Perfume answered 8.3% more questions correctly than those using execution logs alone and did so 15.5% more quickly. Perfume is open source and deployed at http://perfume.cs.umass.edu/.Perfume demo video: http://perfume.cs.umass.edu/demo},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {601–604},
numpages = {4},
keywords = {system understanding, resource modeling, software comprehension, model inference, specification mining},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/2600821.2600842,
author = {Rubin, Vladimir and Lomazova, Irina and Aalst, Wil M. P. van der},
title = {Agile Development with Software Process Mining},
year = {2014},
isbn = {9781450327541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600821.2600842},
doi = {10.1145/2600821.2600842},
abstract = { Modern companies continue investing more and more in the creation, maintenance and change of software systems, but the proper specification and design of such systems continues to be a challenge. The majority of current approaches either ignore real user and system runtime behavior or consider it only informally. This leads to a rather prescriptive top-down approach to software development.  In this paper, we propose a bottom-up approach, which takes event logs (e.g., trace data) of a software system for the analysis of the user and system runtime behavior and for improving the software. We use well-established methods from the area of process mining for this analysis. Moreover, we suggest embedding process mining into the agile development lifecycle.  The goal of this position paper is to motivate the need for foundational research in the area of software process mining (applying process mining to software analysis) by showing the relevance and listing open challenges. Our proposal is based on our experiences with analyzing a big productive touristic system. This system was developed using agile methods and process mining could be effectively integrated into the development lifecycle. },
booktitle = {Proceedings of the 2014 International Conference on Software and System Process},
pages = {70–74},
numpages = {5},
keywords = {Software Process, Process Mining, Agile Methods},
location = {Nanjing, China},
series = {ICSSP 2014}
}

@article{10.1145/3178854,
author = {Antonelli, Humberto Lidio and Igawa, Rodrigo Augusto and Fortes, Renata Pontin De Mattos and Rizo, Eduardo Henrique and Watanabe, Willian Massami},
title = {Drop-Down Menu Widget Identification Using HTML Structure Changes Classification},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1936-7228},
url = {https://doi.org/10.1145/3178854},
doi = {10.1145/3178854},
abstract = {Widgets have been deployed in rich internet applications for more than 10 years. However, many of the widgets currently available on the web do not implement current accessibility design solutions standardized in ARIA (Accessible Rich Internet Applications) specification, hence are not accessible to disabled users. This article sets out an approach for automatically identifying widgets on the basis of machine-learning algorithms and the classification of mutation records; it is an HTML5 technology that logs all changes that occur in the structure of a web application. Automatic widget identification is an essential component for the elaboration of automatic ARIA evaluation and adaptation strategies. Thus, the aim of this article is to take steps toward easing the software-engineering process of ARIA widgets. The proposed approach focuses on the identification of drop-down menu widgets. An experiment with real-world web applications was conducted and the results showed evidence that this approach is capable of identifying these widgets and can outperform previous state-of-the-art techniques based on an F-measure analysis conducted during the experiment.},
journal = {ACM Trans. Access. Comput.},
month = jun,
articleno = {10},
numpages = {23},
keywords = {fly-out menus, web accessibility, widgets, ARIA, Automatic identification of widgets, drop-down menu widget}
}

@inproceedings{10.1145/2090181.2090182,
author = {Yin, Jian and Kulkarni, Anand and Purohit, Sumit and Gorton, Ian and Akyol, Bora},
title = {Scalable Real Time Data Management for Smart Grid},
year = {2011},
isbn = {9781450310741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090181.2090182},
doi = {10.1145/2090181.2090182},
abstract = {This paper presents GridMW, a scalable and reliable data middleware layer for smart grids. Smart grids promise to improve the efficiency of power grid systems and reduce green house emissions through incorporating power generation from renewable sources and shaping demands to match the supply. As a result, power grid will become much more dynamic and require constant adjustments, which requires analysis and decision making applications to improve the efficiency and reliability of smart grid systems. However, these applications rely on large amounts of data gathered from power generation, transmission, and consumption. To this end, millions of sensors, including phasor measurement units (PMU) and smart meters, are being deployed across the smart grid system. Existing data middleware does not have the capability to collect, store, retrieve, and deliver the enormous amount of data from these sensors to analysis and control applications. Most existing data middleware approaches utilize general software systems for flexibility so that the solutions can provide general functionality for a range of applications. However, overheads incurred by generalized APIs cause high latencies and unpredictability in performance, which in turn prevents achieving near real time latencies and high throughput. In our work, by tailoring the system specifically to smart grids, we are able to eliminate much of these overheads while still keeping the implementation effort reasonable. This is achieved by using a log structure inspired architecture to directly access the block device layer, eliminating the indirection incurred by high level file system interfaces. Preliminary results show our system can significantly improve performance compared to traditional systems.},
booktitle = {Proceedings of the Middleware 2011 Industry Track Workshop},
articleno = {1},
numpages = {6},
keywords = {data management middleware, performance, efficiency, near real time, smart grids},
location = {Lisbon, Portugal},
series = {Middleware '11}
}

@article{10.1145/3428447,
author = {Villa, Umberto and Petra, Noemi and Ghattas, Omar},
title = {HIPPYlib: An Extensible Software Framework for Large-Scale Inverse Problems Governed by PDEs: Part I: Deterministic Inversion and Linearized Bayesian Inference},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {2},
issn = {0098-3500},
url = {https://doi.org/10.1145/3428447},
doi = {10.1145/3428447},
abstract = {We present an extensible software framework, hIPPYlib, for solution of large-scale deterministic and Bayesian inverse problems governed by partial differential equations (PDEs) with (possibly) infinite-dimensional parameter fields (which are high-dimensional after discretization). hIPPYlib overcomes the prohibitively expensive nature of Bayesian inversion for this class of problems by implementing state-of-the-art scalable algorithms for PDE-based inverse problems that exploit the structure of the underlying operators, notably the Hessian of the log-posterior. The key property of the algorithms implemented in hIPPYlib is that the solution of the inverse problem is computed at a cost, measured in linearized forward PDE solves, that is independent of the parameter dimension. The mean of the posterior is approximated by the MAP point, which is found by minimizing the negative log-posterior with an inexact matrix-free Newton-CG method. The posterior covariance is approximated by the inverse of the Hessian of the negative log posterior evaluated at the MAP point. The construction of the posterior covariance is made tractable by invoking a low-rank approximation of the Hessian of the log-likelihood. Scalable tools for sample generation are also discussed. hIPPYlib makes all of these advanced algorithms easily accessible to domain scientists and provides an environment that expedites the development of new algorithms.},
journal = {ACM Trans. Math. Softw.},
month = apr,
articleno = {16},
numpages = {34},
keywords = {inexact Newton-CG method, generic PDE toolkit, Bayesian inference, sampling, low-rank approximation, uncertainty quantification, adjoint-based methods, Infinite-dimensional inverse problems}
}

@inproceedings{10.1145/2523649.2523670,
author = {Yen, Ting-Fang and Oprea, Alina and Onarlioglu, Kaan and Leetham, Todd and Robertson, William and Juels, Ari and Kirda, Engin},
title = {Beehive: Large-Scale Log Analysis for Detecting Suspicious Activity in Enterprise Networks},
year = {2013},
isbn = {9781450320153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2523649.2523670},
doi = {10.1145/2523649.2523670},
abstract = {As more and more Internet-based attacks arise, organizations are responding by deploying an assortment of security products that generate situational intelligence in the form of logs. These logs often contain high volumes of interesting and useful information about activities in the network, and are among the first data sources that information security specialists consult when they suspect that an attack has taken place. However, security products often come from a patchwork of vendors, and are inconsistently installed and administered. They generate logs whose formats differ widely and that are often incomplete, mutually contradictory, and very large in volume. Hence, although this collected information is useful, it is often dirty.We present a novel system, Beehive, that attacks the problem of automatically mining and extracting knowledge from the dirty log data produced by a wide variety of security products in a large enterprise. We improve on signature-based approaches to detecting security incidents and instead identify suspicious host behaviors that Beehive reports as potential security incidents. These incidents can then be further analyzed by incident response teams to determine whether a policy violation or attack has occurred. We have evaluated Beehive on the log data collected in a large enterprise, EMC, over a period of two weeks. We compare the incidents identified by Beehive against enterprise Security Operations Center reports, antivirus software alerts, and feedback from enterprise security specialists. We show that Beehive is able to identify malicious events and policy violations which would otherwise go undetected.},
booktitle = {Proceedings of the 29th Annual Computer Security Applications Conference},
pages = {199–208},
numpages = {10},
location = {New Orleans, Louisiana, USA},
series = {ACSAC '13}
}

@inproceedings{10.1145/2591062.2591152,
author = {Mittal, Megha and Sureka, Ashish},
title = {Process Mining Software Repositories from Student Projects in an Undergraduate Software Engineering Course},
year = {2014},
isbn = {9781450327688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591062.2591152},
doi = {10.1145/2591062.2591152},
abstract = { An undergraduate level Software Engineering courses generally consists of a team-based semester long project and emphasizes on both technical and managerial skills. Software Engineering is a practice-oriented and applied discipline and hence there is an emphasis on hands-on development, process, usage of tools in addition to theory and basic concepts. We present an approach for mining the process data (process mining) from software repositories archiving data generated as a result of constructing software by student teams in an educational setting. We present an application of mining three software repositories: team wiki (used during requirement engineering), version control system (development and maintenance) and issue tracking system (corrective and adaptive maintenance) in the context of an undergraduate Software Engineering course. We propose visualizations, metrics and algorithms to provide an insight into practices and procedures followed during various phases of a software development life-cycle. The proposed visualizations and metrics (learning analytics) provide a multi-faceted view to the instructor serving as a feedback tool on development process and quality by students. We mine the event logs produced by software repositories and derive insights such as degree of individual contributions in a team, quality of commit messages, intensity and consistency of commit activities, bug fixing process trend and quality, component and developer entropy, process compliance and verification. We present our empirical analysis on a software repository dataset consisting of 19 teams of 5 members each and discuss challenges, limitations and recommendations. },
booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
pages = {344–353},
numpages = {10},
keywords = {Learning Analytic, Mining Software Repositories, Software Engineering Education, Process Mining, Education Data Mining},
location = {Hyderabad, India},
series = {ICSE Companion 2014}
}

@inproceedings{10.5555/2460416.2460421,
author = {Corney, Malcolm and Mohay, George and Clark, Andrew},
title = {Detection of Anomalies from User Profiles Generated from System Logs},
year = {2011},
isbn = {9781920682965},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {We describe research into the identification of anomalous events and event patterns as manifested in computer system logs. Prototype software has been developed with a capability that identifies anomalous events based on usage patterns or user profiles, and alerts administrators when such events are identified. To reduce the number of false positive alerts we have investigated the use of different user profile training techniques and introduce the use of abstractions to group together applications which are related. Our results suggest that the number of false alerts that are generated is significantly reduced when a growing time window is used for user profile training and when abstraction into groups of applications is used.},
booktitle = {Proceedings of the Ninth Australasian Information Security Conference - Volume 116},
pages = {23–32},
numpages = {10},
keywords = {user profiling, abstraction, insider misuse},
location = {Perth, Australia},
series = {AISC '11}
}

@inproceedings{10.5555/1854509.1854529,
author = {Baker, Ryan S. J. d. and de Carvalho, Adriana M. J. B. and Raspat, Jay and Aleven, Vincent and Corbett, Albert T. and Koedinger, Kenneth R. and Cocea, Mihaela and Hershkovitz, Arnon},
title = {Educational Data Mining Methods for Studying Student Behaviors Minute by Minute across an Entire School Year},
year = {2010},
publisher = {International Society of the Learning Sciences},
abstract = {In this talk, we discuss how educational data mining methods (cf. Baker &amp; Yacef, 2009), conducted using log files of student use of Cognitive Tutor software for mathematics (Koedinger &amp; Corbett, 2006), have significantly increased our scientific understanding of two behaviors that students engage in. The two behaviors studied are gaming the system and off-task behavior. Gaming the system is defined as attempting to succeed in an interactive learning environment by exploiting properties of the system rather than by learning the material (cf. Baker, Corbett, Koedinger, &amp; Wagner, 2004). Examples of gaming within Cognitive Tutors include systematically guessing or abusing hints. Beyond Cognitive Tutors, gaming the system has been observed in assessment software (Walonoski &amp; Heffernan, 2006), graded-participation newsgroups (Cheng &amp; Vassileva, 2005), and educational games (Miller, Lehman, &amp; Koedinger, 1999; Rodrigo et al, 2007). Off-task behavior (engaging in behavior that does not involve the system or the learning task) has been shown to occur with comparable frequency in Cognitive Tutors and traditional classrooms (cf. Baker et al, 2004). Both off-task behavior and gaming the system have been shown to be associated with poorer learning in Cognitive Tutors (Baker et al, 2004; Baker, 2007).},
booktitle = {Proceedings of the 9th International Conference of the Learning Sciences - Volume 2},
pages = {47–48},
numpages = {2},
location = {Chicago, Illinois},
series = {ICLS '10}
}

@inproceedings{10.1145/2470654.2481404,
author = {Birnbaum, Benjamin and Borriello, Gaetano and Flaxman, Abraham D. and DeRenzi, Brian and Karlin, Anna R.},
title = {Using Behavioral Data to Identify Interviewer Fabrication in Surveys},
year = {2013},
isbn = {9781450318990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2470654.2481404},
doi = {10.1145/2470654.2481404},
abstract = {Surveys conducted by human interviewers are one of the principal means of gathering data from all over the world, but the quality of this data can be threatened by interviewer fabrication. In this paper, we investigate a new approach to detecting interviewer fabrication automatically. We instrument electronic data collection software to record logs of low-level behavioral data and show that supervised classification, when applied to features extracted from these logs, can identify interviewer fabrication with an accuracy of up to 96%. We show that even when interviewers know that our approach is being used, have some knowledge of how it works, and are incentivized to avoid detection, it can still achieve an accuracy of 86%. We also demonstrate the robustness of our approach to a moderate amount of label noise and provide practical recommendations, based on empirical evidence, on how much data is needed for our approach to be effective.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2911–2920},
numpages = {10},
keywords = {data quality, curbstoning, supervised classification, behavioral data, user logging, hci4d, data collection, surveys},
location = {Paris, France},
series = {CHI '13}
}

@article{10.1145/2567936,
author = {Abdel-Khalek, Rawan and Bertacco, Valeria},
title = {Post-Silicon Platform for the Functional Diagnosis and Debug of Networks-on-Chip},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3s},
issn = {1539-9087},
url = {https://doi.org/10.1145/2567936},
doi = {10.1145/2567936},
abstract = {The increasing number of units in today's systems-on-chip and multicore processors has led to complex intra-chip communication solutions. Specifically, Networks-on-Chip (NoCs) have emerged as a favorable fabric to provide high bandwidth and low latency in connecting many units in a same chip. To achieve these goals, the NoC often includes complex components and advanced features, leading to the development of large and highly complex interconnect subsystems. One of the biggest challenges in these designs is to ensure the correct functionality of this communication infrastructure. To support this goal, an increasing fraction of the validation effort has shifted to post-silicon validation, because it permits exercising network activities that are too complex to be validated in pre-silicon. However, post-silicon validation is hindered by the lack of observability of the network's internal operations and thus, diagnosing functional errors during this phase is very difficult.In this work, we propose a post-silicon validation platform that improves observability of network operations by taking periodic snapshots of the traffic traversing the network. Each node's local cache is configured to temporarily store the snapshot logs in a designated area reserved for post-silicon validation and relinquished after product release. Each snapshot log is analyzed locally by a software algorithm running on its corresponding core, in order to detect functional errors. Upon error detection, all snapshot logs are aggregated at a central location to extract additional debug data, including an overview of network traffic surrounding the error event, as well as a partial reconstruction of the routes followed by packets in flight at the time. In our experiments, we found that this approach allows us to detect several types of functional errors, as well as observe, on average, over 50% of the network's traffic and reconstruct at least half of each of their routes through the network.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = mar,
articleno = {112},
numpages = {25},
keywords = {Networks-on-chip, functional correctness, performance monitoring, post-silicon validation}
}

@inproceedings{10.1145/3053600.3053634,
author = {Walter, J\"{u}rgen and Stier, Christian and Koziolek, Heiko and Kounev, Samuel},
title = {An Expandable Extraction Framework for Architectural Performance Models},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053634},
doi = {10.1145/3053600.3053634},
abstract = {Providing users with Quality of Service (QoS) guarantees and the prevention of performance problems are challenging tasks for software systems. Architectural performance models can be applied to explore performance properties of a software system at design time and run time. At design time, architectural performance models support reasoning on effects of design decisions. At run time, they enable automatic reconfigurations by reasoning on the effects of changing user behavior. In this paper, we present a framework for the extraction of architectural performance models based on monitoring log files generalizing over the targeted architectural modeling language. Using the presented framework, the creation of a performance model extraction tool for a specific modeling formalism requires only the implementation of a key set of object creation routines specific to the formalism. Our framework integrates them with extraction techniques that apply to many architectural performance models, e.g., resource demand estimation techniques. This lowers the effort to implement performance model extraction tools tremendously through a high level of reuse. We evaluate our framework presenting builders for the Descartes Modeling Language (DML) and the Palladio Component Model(PCM). For the extracted models we compare simulation results with measurements receiving accurate results.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {165–170},
numpages = {6},
keywords = {builder pattern, palladio component model, automated performance model extraction, descartes modeling language},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@article{10.14778/2733004.2733079,
author = {Zhang, Shiming and Yang, Yin and Fan, Wei and Winslett, Marianne},
title = {Design and Implementation of a Real-Time Interactive Analytics System for Large Spatio-Temporal Data},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733004.2733079},
doi = {10.14778/2733004.2733079},
abstract = {In real-time interactive data analytics, the user expects to receive the results of each query within a short time period such as seconds. This is especially challenging when the data is big (e.g., on the scale of petabytes), and the analytics system runs on top of cloud infrastructure (e.g., thousands of interconnected commodity servers). We have been building such a system, called OceanRT, for managing large spatio-temporal data such as call logs and mobile web browsing records collected by a telecommunication company. Although there already exist systems for querying big data in real time, OceanRT's performance stands out due to several novel designs and components that address key efficiency and scalability issues that were largely overlooked in existing systems. First, OceanRT makes extensive use of software RDMA one-sided operations, which reduce networking costs without requiring specialized hardware. Second, OceanRT exploits the parallel computing capabilities of each node in the cloud through a novel architecture consisting of Access-Query Engines (AQEs) connected with minimal overhead. Third, OceanRT contains a novel storage scheme that optimizes for queries with joins and multi-dimensional selections, which are common for large spatio-temporal data. Experiments using the TPC-DS benchmark show that OceanRT is usually more than an order of magnitude faster than the current state-of-the-art systems.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1754–1759},
numpages = {6}
}

@inproceedings{10.5555/2821445.2821454,
author = {Nhlabatsi, Armstrong and Yu, Yijun and Zisman, Andrea and Tun, Thein and Khan, Niamul and Bandara, Arosha and Khan, Khaled M. and Nuseibeh, Bashar},
title = {Managing Security Control Assumptions Using Causal Traceability},
year = {2015},
publisher = {IEEE Press},
abstract = {Security control specifications of software systems are designed to meet their security requirements. It is difficult to know both the value of assets and the malicious intention of attackers at design time, hence assumptions about the operational environment often reveal unexpected flaws. To diagnose the causes of violations in security requirements it is necessary to check these design-time assumptions. Otherwise, the system could be vulnerable to potential attacks. Addressing such vulnerabilities requires an explicit understanding of how the security control specifications were defined from the original security requirements. However, assumptions are rarely explicitly documented and monitored during system operation. This paper proposes a systematic approach to monitoring design-time assumptions explicitly as logs, by using traceability links from requirements to specifications. The work also helps identify which alternative specifications of security control can be used to satisfy a security requirement that has been violated based on the logs. The work is illustrated by an example of an electronic patient record system.},
booktitle = {Proceedings of the 8th International Symposium on Software and Systems Traceability},
pages = {43–49},
numpages = {7},
keywords = {assumptions, traceability, security},
location = {Florence, Italy},
series = {SST '15}
}

@inproceedings{10.1145/2597073.2597074,
author = {Kalliamvakou, Eirini and Gousios, Georgios and Blincoe, Kelly and Singer, Leif and German, Daniel M. and Damian, Daniela},
title = {The Promises and Perils of Mining GitHub},
year = {2014},
isbn = {9781450328630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597073.2597074},
doi = {10.1145/2597073.2597074},
abstract = { With over 10 million git repositories, GitHub is becoming one of the most important source of software artifacts on the Internet. Researchers are starting to mine the information stored in GitHub's event logs, trying to understand how its users employ the site to collaborate on software. However, so far there have been no studies describing the quality and properties of the data available from GitHub. We document the results of an empirical study aimed at understanding the characteristics of the repositories in GitHub and how users take advantage of GitHub's main features---namely commits, pull requests, and issues. Our results indicate that, while GitHub is a rich source of data on software development, mining GitHub for research purposes should take various potential perils into consideration. We show, for example, that the majority of the projects are personal and inactive; that GitHub is also being used for free storage and as a Web hosting service; and that almost 40% of all pull requests do not appear as merged, even though they were. We provide a set of recommendations for software engineering researchers on how to approach the data in GitHub. },
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
pages = {92–101},
numpages = {10},
keywords = {bias, github, Mining software repositories, git, code reviews},
location = {Hyderabad, India},
series = {MSR 2014}
}

@inproceedings{10.1145/3017680.3017716,
author = {Neyem, Andres and Diaz-Mosquera, Juan and Munoz-Gama, Jorge and Navon, Jaime},
title = {Understanding Student Interactions in Capstone Courses to Improve Learning Experiences},
year = {2017},
isbn = {9781450346986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3017680.3017716},
doi = {10.1145/3017680.3017716},
abstract = {Project-based courses can provide valuable learning experiences for computing majors as well as for faculty and community partners. However, proper coordination between students, stakeholders and the academic team is very difficult to achieve. We present an integral study consisting of a twofold approach. First, we propose a proven capstone course framework implementation in conjunction with an educational software tool to support and ensure proper fulfillment of most academic and engineering needs. Second, we propose an approach for mining process data from the information generated by this tool as a way of understanding these courses and improving software engineering education. Moreover, we propose visualizations, metrics and algorithms using Process Mining to provide an insight into practices and procedures followed during various phases of a software development life cycle. We mine the event logs produced by the educational software tool and derive aspects such as cooperative behaviors in a team, component and student entropy, process compliance and verification. The proposed visualizations and metrics (learning analytics) provide a multi-faceted view to the academic team serving as a tool for feedback on development process and quality by students},
booktitle = {Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education},
pages = {423–428},
numpages = {6},
keywords = {education, project-based learning, data science, process mining, computing majors, capstone, empirical software engineering, cloud-based mobile system},
location = {Seattle, Washington, USA},
series = {SIGCSE '17}
}

@article{10.1145/2663348,
author = {G\"{o}tzfried, Johannes and M\"{u}ller, Tilo},
title = {Mutual Authentication and Trust Bootstrapping towards Secure Disk Encryption},
year = {2014},
issue_date = {November 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1094-9224},
url = {https://doi.org/10.1145/2663348},
doi = {10.1145/2663348},
abstract = {The weakest link in software-based full disk encryption is the authentication procedure. Since the master boot record must be present unencrypted in order to launch the decryption of remaining system parts, it can easily be manipulated and infiltrated by bootkits that perform keystroke logging; consequently, password-based authentication schemes become attackable. The current technological response, as enforced by BitLocker, verifies the integrity of the boot process by use of the trusted platform module. But, as we show, this countermeasure is insufficient in practice. We present STARK, the first tamperproof authentication scheme that mutually authenticates the computer and the user in order to resist keylogging during boot. To achieve this, STARK implements trust bootstrapping from a secure token to the whole PC. The secure token is an active USB drive that verifies the integrity of the PC and indicates the verification status by an LED to the user. This way, users can ensure the authenticity of the PC before entering their passwords.},
journal = {ACM Trans. Inf. Syst. Secur.},
month = nov,
articleno = {6},
numpages = {23},
keywords = {mutual authentication, trust bootstrapping, Full disk encryption}
}

@inproceedings{10.1145/2818048.2819972,
author = {Evans, Abigail C. and Wobbrock, Jacob O. and Davis, Katie},
title = {Modeling Collaboration Patterns on an Interactive Tabletop in a Classroom Setting},
year = {2016},
isbn = {9781450335928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818048.2819972},
doi = {10.1145/2818048.2819972},
abstract = {Interaction logs generated by educational software can provide valuable insights into the collaborative learning process and identify opportunities for technology to provide adaptive assistance. Modeling collaborative learning processes at tabletop computers is challenging, as the computer is only able to log a portion of the collaboration, namely the touch events on the table. Our previous lab study with adults showed that patterns in a group's touch interactions with a tabletop computer can reveal the quality of aspects of their collaborative process. We extend this understanding of the relationship between touch interactions and the collaborative process to adolescent learners in a field setting and demonstrate that the touch patterns reflect the quality of collaboration more broadly than previously thought, with accuracies up to 84.2%. We also present an approach to using the touch patterns to model the quality of collaboration in real-time.},
booktitle = {Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work &amp; Social Computing},
pages = {860–871},
numpages = {12},
keywords = {tabletop, modeling, Collaborative learning},
location = {San Francisco, California, USA},
series = {CSCW '16}
}

@inproceedings{10.1145/3345252.3345262,
author = {Sulova, Snezhana},
title = {Models for Web Applications Data Analysis},
year = {2019},
isbn = {9781450371490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345252.3345262},
doi = {10.1145/3345252.3345262},
abstract = {Data analysis is a data inspecting and transforming process for the purpose of making conclusions and obtaining useful information to help make a decision about disclosed forecasts. There are numerous technologies and methods for analyzing data that are mainly determined by the type of data.The paper presents models for organizing web applications' data based on the integrated use of structured data from databases of web applications and unstructured data derived from web pages and server log files. Bu studying the many existing approaches to data analysis from web applications, a summary of the process of data analysis from web applications has been made. An example of using this approach in e-commerce applications is given. The software tools that can be used to perform analyses have been pointed out.},
booktitle = {Proceedings of the 20th International Conference on Computer Systems and Technologies},
pages = {246–250},
numpages = {5},
keywords = {Data Lake, Web applications, Data Integration, Data analysis},
location = {Ruse, Bulgaria},
series = {CompSysTech '19}
}

@article{10.1145/2790077,
author = {Chen, Yunji and Zhang, Shijin and Guo, Qi and Li, Ling and Wu, Ruiyang and Chen, Tianshi},
title = {Deterministic Replay: A Survey},
year = {2015},
issue_date = {November 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/2790077},
doi = {10.1145/2790077},
abstract = {Deterministic replay is a type of emerging technique dedicated to providing deterministic executions of computer programs in the presence of nondeterministic factors. The application scopes of deterministic replay are very broad, making it an important research topic in domains such as computer architecture, operating systems, parallel computing, distributed computing, programming languages, verification, and hardware testing.In this survey, we comprehensively review existing studies on deterministic replay by introducing a taxonomy. Basically, existing deterministic replay schemes can be classified into two categories, single-processor (SP) schemes and multiprocessor (MP) schemes. By reviewing the details of these two categories of schemes respectively, we summarize and compare how existing schemes address technical issues such as log size, record slowdown, replay slowdown, implementation cost, and probe effect, which may shed some light on future studies on deterministic replay.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {17},
numpages = {47},
keywords = {System-on-Chip, Deterministic replay, order, chip multiprocessor, parallel system, debugging, operating system, data race, distributed system}
}

@inproceedings{10.1145/2002951.2002954,
author = {Hura, Dominik and Dimmich, Micha\l{}},
title = {A Method Facilitating Integration Testing of Embedded Software},
year = {2011},
isbn = {9781450308113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2002951.2002954},
doi = {10.1145/2002951.2002954},
abstract = {In this paper a method of supporting integration testing based on logging the operation of an embedded system's software written in the C language is outlined. Its purpose is to facilitate the process of integration testing and partially automate it. It enables automatic verification of tests described with UML language's sequence diagrams by means of a log analyzer based on state machines running in parallel. A class of UML diagrams is also defined to which the abovementioned method is applicable. A short overview of the proposed method's advantages and disadvantages is given. Finally, an example is provided of an embedded system for which the described method can be used.},
booktitle = {Proceedings of the Ninth International Workshop on Dynamic Analysis},
pages = {7–11},
numpages = {5},
keywords = {test oracles, integration testing, embedded software, logging, test automation, automotive multimedia testing},
location = {Toronto, Ontario, Canada},
series = {WODA '11}
}

@inproceedings{10.1145/2207676.2208733,
author = {Gajos, Krzysztof and Reinecke, Katharina and Herrmann, Charles},
title = {Accurate Measurements of Pointing Performance from in Situ Observations},
year = {2012},
isbn = {9781450310154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2207676.2208733},
doi = {10.1145/2207676.2208733},
abstract = {We present a method for obtaining lab-quality measurements of pointing performance from unobtrusive observations of natural in situ interactions. Specifically, we have developed a set of user-independent classifiers for discriminating between deliberate, targeted mouse pointer movements and those movements that were affected by any extraneous factors. To develop and validate these classifiers, we developed logging software to unobtrusively record pointer trajectories as participants naturally interacted with their computers over the course of several weeks. Each participant also performed a set of pointing tasks in a formal study set-up. For each movement, we computed a set of measures capturing nuances of the trajectory and the speed, acceleration, and jerk profiles. Treating the observations from the formal study as positive examples of deliberate, targeted movements and the in situ observations as unlabeled data with an unknown mix of deliberate and distracted interactions, we used a recent advance in machine learning to develop the classifiers. Our results show that, on four distinct metrics, the data collected in-situ and filtered with our classifiers closely matches the results obtained from the formal experiment.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {3157–3166},
numpages = {10},
keywords = {motor performance, machine learning, pointing, in situ studies},
location = {Austin, Texas, USA},
series = {CHI '12}
}

@inproceedings{10.1145/2485732.2485739,
author = {Wei, Michael and Davis, John D. and Wobber, Ted and Balakrishnan, Mahesh and Malkhi, Dahlia},
title = {Beyond Block I/O: Implementing a Distributed Shared Log in Hardware},
year = {2013},
isbn = {9781450321167},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2485732.2485739},
doi = {10.1145/2485732.2485739},
abstract = {The basic block I/O interface used for interacting with storage devices hasn't changed much in 30 years. With the advent of very fast I/O devices based on solid-state memory, it becomes increasingly attractive to make many devices directly and concurrently available to many clients. However, when multiple clients share media at fine grain, retaining data consistency is problematic: SCSI, IDE, and their descendants don't offer much help. We propose an interface to networked storage that reduces an existing software implementation of a distributed shared log to hardware. Our system achieves both scalable throughput and strong consistency, while obtaining significant benefits in cost and power over the software implementation.},
booktitle = {Proceedings of the 6th International Systems and Storage Conference},
articleno = {21},
numpages = {11},
location = {Haifa, Israel},
series = {SYSTOR '13}
}

@inproceedings{10.1145/3445814.3446736,
author = {Chen, Daming D. and Lim, Wen Shih and Bakhshalipour, Mohammad and Gibbons, Phillip B. and Hoe, James C. and Parno, Bryan},
title = {HerQules: Securing Programs via Hardware-Enforced Message Queues},
year = {2021},
isbn = {9781450383172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445814.3446736},
doi = {10.1145/3445814.3446736},
abstract = {Many computer programs directly manipulate memory using unsafe pointers, which may introduce memory safety bugs. In response, past work has developed various runtime defenses, including memory safety checks, as well as mitigations like no-execute memory, shadow stacks, and control-flow integrity (CFI), which aim to prevent attackers from obtaining program control. However, software-based designs often need to update in-process runtime metadata to maximize accuracy, which is difficult to do precisely, efficiently, and securely. Hardware-based fine-grained instruction monitoring avoids this problem by maintaining metadata in special-purpose hardware, but suffers from high design complexity and requires significant microarchitectural changes. In this paper, we present an alternative solution by adding a fast hardware-based append-only inter-process communication (IPC) primitive, named AppendWrite, which enables a monitored program to transmit a log of execution events to a verifier running in a different process, relying on inter-process memory protections for isolation. We show how AppendWrite can be implemented using an FPGA or in hardware at very low cost. Using this primitive, we design HerQules (HQ), a framework for automatically enforcing integrity-based execution policies through compiler instrumentation. HQ reduces overhead on the critical path by decoupling program execution from policy checking via concurrency, without affecting security. We perform a case study on control-flow integrity against multiple benchmark suites, and demonstrate that HQ-CFI achieves a significant improvement in correctness, effectiveness, and performance compared to prior work.},
booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {773–788},
numpages = {16},
keywords = {compiler, kernel, memory safety, inter-process communication, shared memory, control-flow integrity, pointer integrity, microarchitecture, FPGA},
location = {Virtual, USA},
series = {ASPLOS 2021}
}

@inproceedings{10.5555/3351736.3351746,
author = {Leemans, Maikel and van der Aalst, Wil M. P.},
title = {Process Mining in Software Systems: Discovering Real-Life Business Transactions and Process Models from Distributed Systems},
year = {2015},
isbn = {9781467369084},
publisher = {IEEE Press},
abstract = {This paper presents a novel reverse engineering technique for obtaining real-life event logs from distributed systems. This allows us to analyze the operational processes of software systems under real-life conditions, and use process mining techniques to obtain precise and formal models. Hence, the work can be positioned in-between reverse engineering and process mining.We present a formal definition, implementation and an instrumentation strategy based the joinpoint-pointcut model. Two case studies are used to evaluate our approach. These concrete examples demonstrate the feasibility and usefulness of our approach.},
booktitle = {Proceedings of the 18th International Conference on Model Driven Engineering Languages and Systems},
pages = {44–53},
numpages = {10},
keywords = {aspect-oriented programming, joinpoint-pointcut model, performance analysis, process discovery, process mining, distributed systems, event log, reverse engineering},
location = {Ottawa, Ontario, Canada},
series = {MODELS '15}
}

@inproceedings{10.1145/3242587.3242633,
author = {Mysore, Alok and Guo, Philip J.},
title = {Porta: Profiling Software Tutorials Using Operating-System-Wide Activity Tracing},
year = {2018},
isbn = {9781450359481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242587.3242633},
doi = {10.1145/3242587.3242633},
abstract = {It can be hard for tutorial creators to get fine-grained feedback about how learners are actually stepping through their tutorials and which parts lead to the most struggle. To provide such feedback for technical software tutorials, we introduce the idea of tutorial profiling, which is inspired by software code profiling. We prototyped this idea in a system called Porta that automatically tracks how users navigate through a tutorial webpage and what actions they take on their computer such as running shell commands, invoking compilers, and logging into remote servers. Porta surfaces this trace data in the form of profiling visualizations that augment the tutorial with heatmaps of activity hotspots and markers that expand to show event details, error messages, and embedded screencast videos of user actions. We found through a user study of 3 tutorial creators and 12 students who followed their tutorials that Porta enabled both the tutorial creators and the students to provide more specific, targeted, and actionable feedback about how to improve these tutorials. Porta opens up possibilities for performing user testing of technical documentation in a more systematic and scalable way.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},
pages = {201–212},
numpages = {12},
keywords = {tutorial profiling, activity tracing, software tutorials},
location = {Berlin, Germany},
series = {UIST '18}
}

@inproceedings{10.1145/2597073.2597112,
author = {Merten, Thorsten and Mager, Bastian and B\"{u}rsner, Simone and Paech, Barbara},
title = {Classifying Unstructured Data into Natural Language Text and Technical Information},
year = {2014},
isbn = {9781450328630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597073.2597112},
doi = {10.1145/2597073.2597112},
abstract = { Software repository data, for example in issue tracking systems, include natural language text and technical information, which includes anything from log files via code snippets to stack traces.  However, data mining is often only interested in one of the two types e.g. in natural language text when looking at text mining. Regardless of which type is being investigated, any techniques used have to deal with noise caused by fragments of the other type i.e. methods interested in natural language have to deal with technical fragments and vice versa.  This paper proposes an approach to classify unstructured data, e.g. development documents, into natural language text and technical information using a mixture of text heuristics and agglomerative hierarchical clustering.  The approach was evaluated using 225 manually annotated text passages from developer emails and issue tracker data. Using white space tokenization as a basis, the overall precision of the approach is 0.84 and the recall is 0.85. },
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
pages = {300–303},
numpages = {4},
keywords = {unstructured data, preprocessing, heuristics, hierarchical clustering, mining software repositories},
location = {Hyderabad, India},
series = {MSR 2014}
}

@inproceedings{10.1145/1868321.1868324,
author = {Pr\"{a}hofer, Herbert and Schatz, Roland and Wirth, Christian},
title = {Detection of High-Level Execution Patterns in Reactive Behavior of Control Programs},
year = {2010},
isbn = {9781450301374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868321.1868324},
doi = {10.1145/1868321.1868324},
abstract = {This paper presents an approach to extract high-level patterns from traces of programmable logic control (PLC) programs recorded with a deterministic replay debugging tool. Our deterministic replay debugging works by recording an application run in real-time with minimal overhead so that it can be reproduced afterwards. In a subsequent phase, the application is replayed in offline mode to produce a more detailed trace log with additional information about the application run. A software developer can replay the program in a debugger and use debugger features to analyze the program run and locate errors. However, due to the vast amount of data and the complex behavior of reactive control programs, a normal debugger is usually only a poor support in comprehending the program behavior. In this paper we present an approach to analyze recorded program runs of PLC applications. We present a technology to visualize the reactive behavior of a program run and find recurring high-level execution patterns in long-running applications. We give an overview of possible application scenarios to support program comprehension, testing, and debugging.},
booktitle = {Proceedings of the Eighth International Workshop on Dynamic Analysis},
pages = {14–19},
numpages = {6},
keywords = {diagnostics, reactive behavior, execution patterns, PLC applications, real-time systems},
location = {Trento, Italy},
series = {WODA '10}
}

@inproceedings{10.1109/ICSSP.2019.00011,
author = {Kapur, Ritu and Sodhi, Balwinder},
title = {Towards a Knowledge Warehouse and Expert System for the Automation of SDLC Tasks},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSSP.2019.00011},
doi = {10.1109/ICSSP.2019.00011},
abstract = {Cost of a skilled and competent software developer is high, and it is desirable to minimize dependency on such costly human resources. One of the ways to minimize such costs is via automation of various software development tasks.Recent advances in Artificial Intelligence (AI) and the availability of a large volume of knowledge bearing data at various software development related venues present a ripe opportunity for building tools that can automate software development tasks. For instance, there is significant latent knowledge present in raw or unstructured data associated with items such as source files, code commit logs, defect reports, comments, and so on, available in the Open Source Software (OSS) repositories.We aim to leverage such knowledge-bearing data, the latest advances in AI and hardware to create knowledge warehouses and expert systems for the software development domain. Such tools can help in building applications for performing various software development tasks such as defect prediction, effort estimation, code review, etc.},
booktitle = {Proceedings of the International Conference on Software and System Processes},
pages = {5–8},
numpages = {4},
keywords = {automated software engineering, software maintenance, data mining, supervised learning},
location = {Montreal, Quebec, Canada},
series = {ICSSP '19}
}

@inproceedings{10.1145/3297858.3304040,
author = {Kim, Jaeho and Mathew, Ajit and Kashyap, Sanidhya and Ramanathan, Madhava Krishnan and Min, Changwoo},
title = {MV-RLU: Scaling Read-Log-Update with Multi-Versioning},
year = {2019},
isbn = {9781450362405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297858.3304040},
doi = {10.1145/3297858.3304040},
abstract = {This paper presents multi-version read-log-update (MV-RLU), an extension of the read-log-update (RLU) synchronization mechanism. While RLU has many merits including an intuitive programming model and excellent performance for read-mostly workloads, we observed that the performance of RLU significantly drops in workloads with more write operations. The core problem is that RLU manages only two versions. To overcome such limitation, we extend RLU to support multi-versioning and propose new techniques to make multi-versioning efficient. At the core of MV-RLU design is concurrent autonomous garbage collection, which prevents reclaiming invisible versions being a bottleneck, and reduces the version traversal overhead the main overhead of multi-version design. We extensively evaluate MV-RLU with the state-of-the-art synchronization mechanisms, including RCU, RLU, software transactional memory (STM), and lock-free approaches, on concurrent data structures and real-world applications (database concurrency control and in-memory key-value store). Our evaluation results show that MV-RLU significantly outperforms other techniques for a wide range of workloads with varying contention levels and data-set size.},
booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {779–792},
numpages = {14},
keywords = {multi-version, synchronization, garbage collection, concurrency control},
location = {Providence, RI, USA},
series = {ASPLOS '19}
}

@inproceedings{10.1145/3417113.3422153,
author = {Bhuiyan, Farzana Ahamed and Rahman, Akond and Morrison, Patrick},
title = {Vulnerability Discovery Strategies Used in Software Projects},
year = {2020},
isbn = {9781450381284},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417113.3422153},
doi = {10.1145/3417113.3422153},
abstract = {Malicious users can exploit undiscovered software vulnerabilities i.e., undiscovered weaknesses in software, to cause serious consequences, such as large-scale data breaches. A systematic approach that synthesizes strategies used by security testers can aid practitioners to identify latent vulnerabilities. The goal of this paper is to help practitioners identify software vulnerabilities by categorizing vulnerability discovery strategies using open source software bug reports. We categorize vulnerability discovery strategies by applying qualitative analysis on 312 OSS bug reports. Next, we quantify the frequency and evolution of the identified strategies by analyzing 1,632 OSS bug reports collected from five software projects spanning across 2009 to 2019. The five software projects are Chrome, Eclipse, Mozilla, OpenStack, and PHP.We identify four vulnerability discovery strategies: diagnostics, malicious payload construction, misconfiguration, and pernicious execution. For Eclipse and OpenStack, the most frequently used strategy is diagnostics, where security testers inspect source code and build/debug logs. For three web-related software projects namely, Chrome, Mozilla, and PHP, the most frequently occurring strategy is malicious payload construction i.e., creating malicious files, such as malicious certificates and malicious videos.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering Workshops},
pages = {13–18},
numpages = {6},
keywords = {strategy, vulnerability, empirical study, bug report, taxonomy},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/2723742.2723763,
author = {Sureka, Ashish and Kumar, Atul and Gupta, Shrinath},
title = {Ahaan: Software Process Intelligence: Mining Software Process Data for Extracting Actionable Information},
year = {2015},
isbn = {9781450334327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723742.2723763},
doi = {10.1145/2723742.2723763},
abstract = {Software Processes consist of a structured set of activities performed during creation and maintenance of software products. The construction and subsequent maintenance of a software is facilitated by several applications and tools. Some of the tools such as Issue Tracking System (ITS) and Version Control System (VCS) can be classified as Process Aware Information System (PAIS) logging data consisting of events, activities, time-stamp, user or actor and context specific information. Such events or trace data generated by information systems used during software construction (as part of the software development process) contains valuable information which can be mined for gaining useful insights and actionable information. Software Process Intelligence (SPI) is an emerging and evolving discipline involving mining and analysis of software processes. This is modeled on the lines of Business Process Intelligence (BPI), but with the focus on software processes and its applicability in software systems. In this paper, we present a generic framework for Software Process Intelligence and some of its applications.},
booktitle = {Proceedings of the 8th India Software Engineering Conference},
pages = {198–199},
numpages = {2},
keywords = {Software Process Intelligence, Business Process Intelligence (BPI), Process Mining, Automated Software Engineering, Mining Software Repositories},
location = {Bangalore, India},
series = {ISEC '15}
}

@inproceedings{10.1145/1842993.1843054,
author = {Mazzola, Luca and Mazza, Riccardo},
title = {An Infrastructure for Creating Graphical Indicators of the Learner Profile by Mashing up Different Sources},
year = {2010},
isbn = {9781450300766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842993.1843054},
doi = {10.1145/1842993.1843054},
abstract = {The procedures to collect information about users are well known in computer science till long time. They range from getting explicit information from users, required in order to enable some functionalities, to the gathering of user behaviors, collected as log files generated by software applications. In the field of Technology Enhanced Learning the creation of a user profile is necessary in order to fulfill some didactical tasks, such as measuring the degree of participation to a course, or the performance on quizzes and assignments. The task of collecting students' data is normally performed by Learning Management Systems, which also provide with a way to explore this data. Our approach extends the information that models user profiles in Learning Management Systems with data coming from other online resources, such as social network websites. In this paper, we describe our idea of opening the learners' profile, eventually for integrating it with external on-line resources. The ultimate goal of our work is to create graphical indicators for the profile of the learners that take into account internal and external user data, in order to have a more complete and comprehensive view of user behaviors in Learning Management Systems.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
pages = {329–332},
numpages = {4},
keywords = {open learner models, life long learning, MashUp},
location = {Roma, Italy},
series = {AVI '10}
}

@inproceedings{10.1145/2987386.2987406,
author = {Yeh, Chih-Wei and Yeh, Wan-Ting and Hung, Shih-Hao and Lin, Chih-Ta},
title = {Flattened Data in Convolutional Neural Networks: Using Malware Detection as Case Study},
year = {2016},
isbn = {9781450344555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2987386.2987406},
doi = {10.1145/2987386.2987406},
abstract = {Convolutional Neural Networks (CNNs) are very powerful variants of multilayer perceptron models inspired by human's brain neural system to reveal local, spatial correlation in a series of data. While CNNs are popularly used for image recognition nowadays, it is also possible to apply CNNs in other areas, for example, detection of malicious software. In this paper, we show how CNNs may be used to improve the classification of malicious software due to the high-level feature abstraction and equal-variance property against noises. Taking advantages of convolution kernels, CNNs are naturally born for pattern recognition on images only. For this application, we introduce a new transformation technique which converts a series of event logs into flattened data with two-dimensional features so that CNNs can be trained to detect malicious behaviors effectively. With the combination property and the proposed flattened input format, CNN can perform a k-skip-n-gram dimensionality reduction which learns more flexible and complex patterns comparing to the traditional solutions. Our preliminary results show that our latest CNNs-based malware detection engine reaches 93.012% prediction accuracy and 12.9% FNR under 32,000 samples of a training set. To our knowledge, this is the first paper discussing the application and effectiveness of CNNs on malware detection.},
booktitle = {Proceedings of the International Conference on Research in Adaptive and Convergent Systems},
pages = {130–135},
numpages = {6},
keywords = {malware, machine learning, dynamic analysis, convolutional neural networks, Android},
location = {Odense, Denmark},
series = {RACS '16}
}

@inproceedings{10.1145/2702123.2702495,
author = {Vigo, Markel and Jay, Caroline and Stevens, Robert},
title = {Constructing Conceptual Knowledge Artefacts: Activity Patterns in the Ontology Authoring Process},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702123.2702495},
doi = {10.1145/2702123.2702495},
abstract = {Ontologies are complex knowledge representation artefacts used widely across biomedical, media and industrial domains. They are used for defining terminologies and providing metadata, especially for linked open data, and as such their use is rapidly increasing, but so far development tools have not benefited from empirical research into the ontology authoring process. This paper presents the results of a study that identifies common activity patterns through analysis of eye-tracking data and the event logs of the popular authoring tool, Prot\'{e}g\'{e}. Informed by the activity patterns discovered, we propose design guidelines for bulk editing, efficient reasoning and increased situational awareness. Methodological implications go beyond the remit of knowledge artefacts: we establish a method for studying the usability of software designed for highly specialised complex domains.},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages = {3385–3394},
numpages = {10},
keywords = {semantic web, authoring tools, knowledge representation, activity patterns, complex domains, ontologies},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}

@inproceedings{10.1145/3339825.3394931,
author = {Montagud, Mario and De Rus, Juan Antonio and Fayos-Jordan, Rafael and Garcia-Pineda, Miguel and Segura-Garcia, Jaume},
title = {Open-Source Software Tools for Measuring Resources Consumption and DASH Metrics},
year = {2020},
isbn = {9781450368452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3339825.3394931},
doi = {10.1145/3339825.3394931},
abstract = {When designing and deploying multimedia systems, it is essential to accurately know about the necessary requirements and the Quality of Service (QoS) offered to the customers. This paper presents two open-source software tools that contribute to these key needs. The first tool is able to measure and register resources consumption metrics for any Windows program (i.e. process id), like the CPU, GPU and RAM usage. Unlike the Task Manager, which requires manual visual inspection for just a subset of these metrics, the developed tool runs on top of the Powershell to periodically measure these metrics, calculate statistics, and register them in log files. The second tool is able to measure QoS metrics from DASH streaming sessions by running on top of TShark, if a non-secure HTTP connection is used. For each DASH chunk, the tool registers: the round-trip time from request to download, the number of TCP segments and bytes, the effective bandwidth, the selected DASH representation, and the associated parameters in the MPD (e.g., resolution, bitrate). It also registers the MPD and the total amount of downloaded frames and bytes. The advantage of this second tool is that these metrics can be registered regardless of the player used, even from a device connected to the same network than the DASH player.},
booktitle = {Proceedings of the 11th ACM Multimedia Systems Conference},
pages = {261–266},
numpages = {6},
keywords = {open-source software, DASH, traffic analysis, quality of service (QoS), performance metrics},
location = {Istanbul, Turkey},
series = {MMSys '20}
}

@inproceedings{10.1145/2544137.2544167,
author = {Li, Jianjun and Wang, Zhenjiang and Wu, Chenggang and Hsu, Wei-Chung and Xu, Di},
title = {Dynamic and Adaptive Calling Context Encoding},
year = {2014},
isbn = {9781450326704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2544137.2544167},
doi = {10.1145/2544137.2544167},
abstract = {Calling context has been widely used in many software development processes such as testing, event logging, and program analysis. It plays an even more important role in data race detection and performance bottleneck analysis for multi-threaded programs. This paper presents DACCE (Dynamic and Adaptive Calling Context Encoding), an efficient runtime encoding/decoding mechanism for single-threaded and multi-threaded programs that captures dynamic calling contexts. It can dynamically encode all call paths invoked at runtime, and adjust the encodings according to program's execution behavior. In contrast to existing context encoding method, DACCE can work on incomplete call graph, and it does not require source code analysis and offline profiling to conduct context encoding. DACCE has significantly expanded the functionality and applicability of calling context with even lower runtime overhead. DACCE is very efficient based on experiments with SPEC CPU2006 and Parsec 2.1 (with about 2% of runtime overhead) and effective for all tested benchmarks.},
booktitle = {Proceedings of Annual IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {120–131},
numpages = {12},
keywords = {calling context encoding, dynamic analysis, adaptive},
location = {Orlando, FL, USA},
series = {CGO '14}
}

@inproceedings{10.1145/2581122.2544167,
author = {Li, Jianjun and Wang, Zhenjiang and Wu, Chenggang and Hsu, Wei-Chung and Xu, Di},
title = {Dynamic and Adaptive Calling Context Encoding},
year = {2014},
isbn = {9781450326704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2581122.2544167},
doi = {10.1145/2581122.2544167},
abstract = {Calling context has been widely used in many software development processes such as testing, event logging, and program analysis. It plays an even more important role in data race detection and performance bottleneck analysis for multi-threaded programs. This paper presents DACCE (Dynamic and Adaptive Calling Context Encoding), an efficient runtime encoding/decoding mechanism for single-threaded and multi-threaded programs that captures dynamic calling contexts. It can dynamically encode all call paths invoked at runtime, and adjust the encodings according to program's execution behavior. In contrast to existing context encoding method, DACCE can work on incomplete call graph, and it does not require source code analysis and offline profiling to conduct context encoding. DACCE has significantly expanded the functionality and applicability of calling context with even lower runtime overhead. DACCE is very efficient based on experiments with SPEC CPU2006 and Parsec 2.1 (with about 2% of runtime overhead) and effective for all tested benchmarks.},
booktitle = {Proceedings of Annual IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {120–131},
numpages = {12},
keywords = {calling context encoding, dynamic analysis, adaptive},
location = {Orlando, FL, USA},
series = {CGO '14}
}

@article{10.14778/3137765.3137779,
author = {Antonopoulos, Panagiotis and Kodavalla, Hanuma and Tran, Alex and Upreti, Nitish and Shah, Chaitali and Sztajno, Mirek},
title = {Resumable Online Index Rebuild in SQL Server},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137765.3137779},
doi = {10.14778/3137765.3137779},
abstract = {Azure SQL Database and the upcoming release of SQL Server enhance Online Index Rebuild to provide fault-tolerance and allow index rebuild operations to resume after a system failure or a user-initiated pause. SQL Server is the first commercial DBMS to support pause and resume functionality for index rebuilds. This is achieved by splitting the operation into incremental units of work and persisting the required state so that it can be resumed later with minimal loss of progress. At the same time, the proposed technology minimizes the log space required for the operation to succeed, making it possible to rebuild large indexes using only a small, constant amount of log space. These capabilities are critical to guarantee the reliability of these operations in an environment where a) the database sizes are increasing at a much faster pace compared to the available hardware, b) system failures are frequent in Cloud architectures using commodity hardware, c) software upgrades and other maintenance tasks are automatically handled by the Cloud platforms, introducing further unexpected failures for the users and d) most modern applications need to be available 24/7 and have very tight maintenance windows. This paper describes the design of "Resumable Online Index Rebuild" and discusses how this technology can be extended to cover more schema management operations in the future.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1742–1753},
numpages = {12}
}

@inproceedings{10.1145/2348543.2348587,
author = {Hassanieh, Haitham and Adib, Fadel and Katabi, Dina and Indyk, Piotr},
title = {Faster GPS via the Sparse Fourier Transform},
year = {2012},
isbn = {9781450311595},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2348543.2348587},
doi = {10.1145/2348543.2348587},
abstract = {GPS is one of the most widely used wireless systems. A GPS receiver has to lock on the satellite signals to calculate its position. The process of locking on the satellites is quite costly and requires hundreds of millions of hardware multiplications, leading to high power consumption. The fastest known algorithm for this problem is based on the Fourier transform and has a complexity of O(n log n), where n is the number of signal samples. This paper presents the fastest GPS locking algorithm to date. The algorithm reduces the locking complexity to O(n√(log n)). Further, if the SNR is above a threshold, the algorithm becomes linear, i.e., O(n). Our algorithm builds on recent developments in the growing area of sparse recovery. It exploits the sparse nature of the synchronization problem, where only the correct alignment between the received GPS signal and the satellite code causes their cross-correlation to spike.We further show that the theoretical gain translates into empirical gains for GPS receivers. Specifically, we built a prototype of the design using software radios and tested it on two GPS data sets collected in the US and Europe. The results show that the new algorithm reduces the median number of multiplications by 2.2x in comparison to the state of the art design, for real GPS signals.},
booktitle = {Proceedings of the 18th Annual International Conference on Mobile Computing and Networking},
pages = {353–364},
numpages = {12},
keywords = {GPS, sparse fourier transform, synchronization},
location = {Istanbul, Turkey},
series = {Mobicom '12}
}

@inproceedings{10.1145/3064176.3064179,
author = {Huang, Jiamin and Mozafari, Barzan and Wenisch, Thomas F.},
title = {Statistical Analysis of Latency Through Semantic Profiling},
year = {2017},
isbn = {9781450349383},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3064176.3064179},
doi = {10.1145/3064176.3064179},
abstract = {Most software profiling tools quantify average performance and rely on a program's control flow graph to organize and report results. However, in interactive server applications, performance predictability is often an equally important measure. Moreover, the end user is often concerned with the performance of a semantically defined interval of execution, such as a request or transaction, which may not directly map to any single function in the call graph, especially in high-performance applications that use asynchrony or event-based programming. It is difficult to distinguish functionality that lies on the critical path of a semantic interval from other activity (e.g., periodic logging or side operations) that may nevertheless appear prominent in a conventional profile. Existing profilers lack the ability to (i) aggregate results for a semantic interval and (ii) attribute its performance variance to individual functions.We propose a profiler called VProfiler that, given the source code of a software system and programmer annotations indicating the start and end of semantic intervals of interest, is able to identify the dominant sources of latency variance in a semantic context. Using a novel abstraction, called a variance tree, VProfiler analyzes the thread interleaving and deconstructs overall latency variance into variances and covariances of the execution time of individual functions. It then aggregates latency variance along a backwards path of dependence relationships among threads from the end of an interval to its start. We evaluate VProfiler's effectiveness on three popular open-source projects (MySQL, Postgres, and Apache Web Server). By identifying a few culprit functions in these complex code bases, VProfiler allows us to eliminate 27%--82% of the overall latency variance of these systems with a modest programming effort.},
booktitle = {Proceedings of the Twelfth European Conference on Computer Systems},
pages = {64–79},
numpages = {16},
keywords = {Performance, Tail Latencies, Predictability, Variance, Semantic Profiling},
location = {Belgrade, Serbia},
series = {EuroSys '17}
}

@inproceedings{10.1145/1882291.1882308,
author = {Bachmann, Adrian and Bird, Christian and Rahman, Foyzur and Devanbu, Premkumar and Bernstein, Abraham},
title = {The Missing Links: Bugs and Bug-Fix Commits},
year = {2010},
isbn = {9781605587912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1882291.1882308},
doi = {10.1145/1882291.1882308},
abstract = {Empirical studies of software defects rely on links between bug databases and program code repositories. This linkage is typically based on bug-fixes identified in developer-entered commit logs. Unfortunately, developers do not always report which commits perform bug-fixes. Prior work suggests that such links can be a biased sample of the entire population of fixed bugs. The validity of statistical hypotheses-testing based on linked data could well be affected by bias. Given the wide use of linked defect data, it is vital to gauge the nature and extent of the bias, and try to develop testable theories and models of the bias. To do this, we must establish ground truth: manually analyze a complete version history corpus, and nail down those commits that fix defects, and those that do not. This is a diffcult task, requiring an expert to compare versions, analyze changes, find related bugs in the bug database, reverse-engineer missing links, and finally record their work for use later. This effort must be repeated for hundreds of commits to obtain a useful sample of reported and unreported bug-fix commits. We make several contributions. First, we present Linkster, a tool to facilitate link reverse-engineering. Second, we evaluate this tool, engaging a core developer of the Apache HTTP web server project to exhaustively annotate 493 commits that occurred during a six week period. Finally, we analyze this comprehensive data set, showing that there are serious and consequential problems in the data.},
booktitle = {Proceedings of the Eighteenth ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {97–106},
numpages = {10},
keywords = {apache, bias, case study, manual annotation, tool},
location = {Santa Fe, New Mexico, USA},
series = {FSE '10}
}

@inproceedings{10.1145/3219819.3219914,
author = {Samel, Karan and Miao, Xu},
title = {Active Deep Learning to Tune Down the Noise in Labels},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219914},
doi = {10.1145/3219819.3219914},
abstract = {The great success of supervised learning has initiated a paradigm shift from building a deterministic software system to a probabilistic artificial intelligent system throughout the industry. The historical records in enterprise domains can potentially bootstrap the traditional business into the modern data-driven approach almost everywhere. The introduction of the Deep Neural Networks (DNNs) significantly reduces the efforts of feature engineering so that supervised learning becomes even more automated. The last bottleneck is to ensure the data quality, particularly the label quality, because the performance of supervised learning is bounded by the errors present in labels. In this paper, we present a new Active Deep Denoising (ADD) approach that first builds a DNN noise model, and then adopts an active learning algorithm to identify the optimal denoising function. We prove that under the low noise condition, we only need to query the oracle with log n examples where n is the total number in the data. We apply ADD on one enterprise application and show that it can effectively reduce 1/3 of the prediction error with only 0.1% of examples verified by the oracle.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {685–694},
numpages = {10},
keywords = {denoising, deep neural networks, active learning, classification},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.1109/TechDebt.2019.00029,
author = {Lerina, Aversano and Nardi, Laura},
title = {Investigating on the Impact of Software Clones on Technical Debt},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/TechDebt.2019.00029},
doi = {10.1109/TechDebt.2019.00029},
abstract = {Code reuse by copying a code fragment with or without modification generates duplicate copies of exact or similar code fragments in a software system, known as code clones. The debate about the harmfulness of clone in ongoing in the literature, nevertheless, it is widely recognized that clones needs special considerations during software evolution. In this paper, it is proposed a quantitative analysis of technical debt values to understand if it is higher with cloned code than those without cloned code. Moreover, changes performed on these files have been analyzed by analyzing commit logs. According to our inspection on four subject systems, the technical debt of files with cloned code is significantly higher than those without cloned code. Moreover, as expected, files with cloned code are more impacted by changes.},
booktitle = {Proceedings of the Second International Conference on Technical Debt},
pages = {108–112},
numpages = {5},
keywords = {technical debt, software clones, software evolution, software maintenance},
location = {Montreal, Quebec, Canada},
series = {TechDebt '19}
}

@inproceedings{10.1145/1982185.1982513,
author = {Jin, Hongxia and Lotspiech, Jeffrey},
title = {Efficient Traitor Tracing for Clone Attack in Content Protection},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982513},
doi = {10.1145/1982185.1982513},
abstract = {In this paper we design efficient traitor tracing scheme for a pirate clone attack against a broadcast-encryption-based content protection system. In this content protection system, each user(device) is assigned a set of secret keys. In a clone attack, pirates (legitimate users) compromise their devices, extract their secret keys and use those keys to build a clone device. The clone device allows decryption of content that is originally only accessible by legitimate devices. The pirates can sell the clone device for profits. When a clone device is recovered, a traitor tracing scheme could identify which compromised devices' (called traitors) keys are in the clone. Once the compromised keys are detected, they can be disabled for future content access. In the process of tracing traitors, a series of carefully constructed cipher text is fed into the clone device and the reaction of the clone device is observed and used to deduce which keys are contained inside the clone. The traceability of a tracing scheme is measured by the number of testing cipher texts needed to identify the traitors. The state-of-art traitor tracing schemes in the symmetric key setting achieve O(t3 log t) traceabilities for t traitors. Unfortunately the theoretically efficient polynomial traceability could convert to years' tracing time in reality. In this paper, we present a practical approach that combines traditional traitor tracing scheme design with system security engineering consideration by introducing a "software key conversion data" virtual program. This combination enables our approach to drastically improve traceability over the state-of-art traitor tracing scheme existed in applied cryptography community. The traceabilities for clone attack is improved from O(t3 log t) to O(t) which converts the tracing time from the original 15 years to 4 hours for a clone attack of 100 traitors. Our much improved traceabilities makes them ultimately adopted to use in AACS [1], the new industry content protection standard for next generation high definition DVDs.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {1544–1549},
numpages = {6},
keywords = {content protection, broadcast encryption, traitor tracing},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@inproceedings{10.1145/2556288.2557142,
author = {Lafreniere, Ben and Grossman, Tovi and Matejka, Justin and Fitzmaurice, George},
title = {Investigating the Feasibility of Extracting Tool Demonstrations from In-Situ Video Content},
year = {2014},
isbn = {9781450324731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556288.2557142},
doi = {10.1145/2556288.2557142},
abstract = {Short video demonstrations are effective resources for helping users to learn tools in feature-rich software. However manually creating demonstrations for the hundreds (or thousands) of individual features in these programs would be impractical. In this paper, we investigate the potential for identifying good tool demonstrations from within screen recordings of users performing real-world tasks. Using an instrumented image-editing application, we collected workflow video content and log data from actual end users. We then developed a heuristic for identifying demonstration clips, and had the quality of a sample set of clips evaluated by both domain experts and end users. This multi-step approach allowed us to characterize the quality of 'naturally occurring' tool demonstrations, and to derive a list of good and bad features of these videos. Finally, we conducted an initial investigation into using machine learning techniques to distinguish between good and bad demonstrations.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {4007–4016},
numpages = {10},
keywords = {in-situ usage data, help, learning, feature-rich software, video tooltips, toolclips},
location = {Toronto, Ontario, Canada},
series = {CHI '14}
}

@inproceedings{10.1145/3017680.3022384,
author = {Peveler, Matthew and Tyler, Jeramey and Breese, Samuel and Cutler, Barbara and Milanova, Ana},
title = {Submitty: An Open Source, Highly-Configurable Platform for Grading of Programming Assignments (Abstract Only)},
year = {2017},
isbn = {9781450346986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3017680.3022384},
doi = {10.1145/3017680.3022384},
abstract = {Submitty (http://submitty.org) is an open source programming assignment submission system from the Rensselaer Center for Open Source Software (RCOS) at Rensselaer Polytechnic Institute (RPI). Students can submit their code via a web interface in a variety of ways, where it is then tested with a highly configurable and customizable automated grader. Students receive immediate feedback from the grader, and can resubmit to correct errors as needed. Through an online interface, TAs can access detailed grading results and supplement the automated scores with manual grading (numeric and written feedback) of overall program structure, good use of comments, reasonable error checking, etc. and any non-programming components of the assignment. The instructor can also configure the system to allow for a configurable late day policy on a per assignment and per student basis. We currently use Submitty in eight different courses (spanning from introductory through advanced topics) serving over 1500 students and 35+ instructors and TAs each week. We will present a range of "case study" assignment configurations in a hands-on demo, going from simple through complex, using a variety of different automated grading methods including per-character and per-line output difference checkers, external unit testing frameworks (such as JUnit), memory debugging tools (Valgrind and DrMemory), code coverage (e.g., Emma), static analysis tools, and custom graders. Submitty can be customized per test case as appropriate to apply resource limits (running time, number of processes, output file size, etc.) and to display or hide from students the program output, autograding results, and testing logs.},
booktitle = {Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education},
pages = {641},
numpages = {1},
keywords = {automated grading, computer science education, learning approaches, testing, assessment},
location = {Seattle, Washington, USA},
series = {SIGCSE '17}
}

@inproceedings{10.1145/3374135.3385305,
author = {Pham, Hung Viet and Nguyen, Tam The and Vu, Phong Minh and Nguyen, Tung Thanh},
title = {A Vision on Mining Visual Logs of Software},
year = {2020},
isbn = {9781450371056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3374135.3385305},
doi = {10.1145/3374135.3385305},
abstract = {In this paper, we define visual logs of a software system as data capturing the interactions between its users and its graphic user interfaces (GUI) such as screenshots and screen recordings. We expect that mining such visual logs could be useful for many software engineering tasks: bug reproducing and debugging, automated GUI testing, user interface design, question answering of common software usages, etc. Toward that vision, we propose a core framework for mining visual logs of software. This framework focuses on detecting GUI elements and changes in visual logs, removing users' private data, recognizing user interactions with GUI elements, and learning GUI usage patterns. We also presented a small study on characteristics of GUI elements in mobile apps. The findings from this study suggest several heuristics to design techniques for recognizing GUI elements and interactions.},
booktitle = {Proceedings of the 2020 ACM Southeast Conference},
pages = {284–287},
numpages = {4},
keywords = {GUI, Mining User Interactions, Reverse Engineering, Visual Logs},
location = {Tampa, FL, USA},
series = {ACM SE '20}
}

@inproceedings{10.1145/2641580.2641592,
author = {Romo, Bilyaminu Auwal and Capiluppi, Andrea and Hall, Tracy},
title = {Filling the Gaps of Development Logs and Bug Issue Data},
year = {2014},
isbn = {9781450330169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2641580.2641592},
doi = {10.1145/2641580.2641592},
abstract = {It has been suggested that the data from bug repositories is not always in sync or complete compared to the logs detailing the actions of developers on source code.In this paper, we trace two sources of information relative to software bugs: the change logs of the actions of developers and the issues reported as bugs. The aim is to identify and quantify the discrepancies between the two sources in recording and storing the developer logs relative to bugs.Focussing on the databases produced by two mining software repository tools, CVSAnalY and Bicho, we use part of the SZZ algorithm to identify bugs and to compare how the "defects-fixing changes" are recorded in the two databases. We use a working example to show how to do so.The results indicate that there is a significant amount of information, not in sync when tracing bugs in the two databases. We, therefore, propose an automatic approach to re-align the two databases, so that the collected information is mirrored and in sync.},
booktitle = {Proceedings of The International Symposium on Open Collaboration},
pages = {1–4},
numpages = {4},
keywords = {Bug traceability, bug-fixing commits},
location = {Berlin, Germany},
series = {OpenSym '14}
}

@inproceedings{10.1145/2236584.2236589,
author = {Saxena, Mohit and Shah, Mehul A. and Harizopoulos, Stavros and Swift, Michael M. and Merchant, Arif},
title = {Hathi: Durable Transactions for Memory Using Flash},
year = {2012},
isbn = {9781450314459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2236584.2236589},
doi = {10.1145/2236584.2236589},
abstract = {Recent architectural trends---cheap, fast solid-state storage, inexpensive DRAM, and multi-core CPUs---provide an opportunity to rethink the interface between applications and persistent storage. To leverage these advances, we propose a new system architecture called Hathi that provides an in-memory transactional heap made persistent using high-speed flash drives. With Hathi, programmers can make consistent concurrent updates to in-memory data structures that survive system failures.Hathi focuses on three major design goals: ACID semantics, a simple programming interface, and fine-grained programmer control. Hathi relies on software transactional memory to provide a simple concurrent interface to in-memory data structures, and extends it with persistent logs and checkpoints to add durability.To reduce the cost of durability, Hathi uses two main techniques. First, it provides split-phase and partitioned commit interfaces, that allow programmers to overlap commit I/O with computation and to avoid unnecessary synchronization. Second, it uses partitioned logging, which reduces contention on in-memory log buffers and exploits internal SSD parallelism. We find that our implementation of Hathi can achieve 1.25 million txns/s with a single SSD.},
booktitle = {Proceedings of the Eighth International Workshop on Data Management on New Hardware},
pages = {33–38},
numpages = {6},
location = {Scottsdale, Arizona},
series = {DaMoN '12}
}

@inproceedings{10.1145/3322798.3329253,
author = {Kim, Youngsoo and Kim, Jonghyun and Kim, Ikkyun and Kim, Hyunchul},
title = {Real-Time Multi-Process Tracing Decoder Architecture},
year = {2019},
isbn = {9781450367615},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322798.3329253},
doi = {10.1145/3322798.3329253},
abstract = {Tracing is a form of logging by recording the execution information of programs. Since a large amount of data must be created and decoded in real time, a tracer composed mainly of dedicated hardware is widely used. Intel® PT records all information related to software execution from each hardware thread. When the execution of the corresponding software is completed, the accurate program flow can be indicated through the recorded trace data. The hardware trace program can be integrated into the operating system, but in the case of the Windows system, the kernel is not disclosed so tight integration is not achieved. Also, in a Windows environment, it can only trace a single process and do not provide a way to trace multiple process streams. In this paper, we propose a way of extending the PT trace program in order to overcome this shortcoming by supporting multi-process stream tracing in Windows environment.},
booktitle = {Proceedings of the ACM Workshop on Systems and Network Telemetry and Analytics},
pages = {49–52},
numpages = {4},
keywords = {software testing, multi-stream decoder, processor trace},
location = {Phoenix, AZ, USA},
series = {SNTA '19}
}

@inproceedings{10.1145/2957265.2961834,
author = {Filho, Jackson Feij\'{o} and Prata, Wilson and Oliveira, Juan},
title = {Affective-Ready, Contextual and Automated Usability Test for Mobile Software},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961834},
doi = {10.1145/2957265.2961834},
abstract = {This work proposes the use of system to perform affective-ready, contextual and automated usability tests for mobile software. Our proposal augments the traditional methods of software usability evaluation by monitoring users' location, weather conditions, moving/stationary status, data connection availability and spontaneous facial expressions automatically. This aims to identify the moment of negative and positive events. Identifying those situations and systematically associating them to the context of interaction, assisted software creators to overcome design flaws and enhancing interfaces' strengths.The validation of our approach include post-test questionnaires with test subjects. The results indicate that the automated user-context logging can be a substantial supplement to mobile software usability tests.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {638–644},
numpages = {7},
keywords = {mobile software, affective computing, usability},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/1979742.1979798,
author = {Morrison, Alistair and Brown, Owain and McMillan, Donald and Chalmers, Matthew},
title = {Informed Consent and Users' Attitudes to Logging in Large Scale Trials},
year = {2011},
isbn = {9781450302685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1979742.1979798},
doi = {10.1145/1979742.1979798},
abstract = {The HCI community has begun to use 'app store'-style software repositories as a distribution channel for research applications. A number of ethical challenges present themselves in this setting, not least that of gaining informed consent from potential participants before logging data on their use of the software. We note that standard 'terms and conditions' pages have proved unsuccessful in communicating relevant information to users, and explore further means of conveying researchers' intent and allowing opt-out mechanisms. We test the hypothesis that revealing collected information to users will affect their level of concern at being recorded and find that users are more concerned when presented with a personalised representation of recorded data, and consequently stop using the application sooner. Also described is a means of allowing between-groups experiments in such mass participation trials.},
booktitle = {CHI '11 Extended Abstracts on Human Factors in Computing Systems},
pages = {1501–1506},
numpages = {6},
keywords = {ethics, app store, user trials, mass participation},
location = {Vancouver, BC, Canada},
series = {CHI EA '11}
}

@inproceedings{10.1145/2494603.2480339,
author = {Cauchi, Abigail},
title = {Using Differential Formal Analysis for Dependable Number Entry},
year = {2013},
isbn = {9781450321389},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494603.2480339},
doi = {10.1145/2494603.2480339},
abstract = {User interfaces that employ the same display and buttons may look the same but can work very differently depending on how they are implemented. In healthcare, it is critical that interfaces that look the same are the same. Hospitals typically have many types of similar infusion pump, with different software versions, and variation between pump behavior may lead to unexpected adverse events. For example, when entering drug doses into infusion pumps that use the same display and button designs, different results may arise when pushing identical sequences of buttons. These differences arise as a result of subtle implementation differences and may lead to under-dose or over-dose errors.This work explores different implementations of a 5-key interface for entering numbers using a new user interface analysis technique, Differential Formal Analysis.Using Differential Formal Analysis different 5-key interfaces are analysed based on log data collected from 19 infusion pumps over a 3 year period from a UK hospital. The results from this analysis is domain specific to infusion pumps. A comparison is made between domain specific results and generic results from Differential Formal Analysis performed using random data.},
booktitle = {Proceedings of the 5th ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
pages = {155–158},
numpages = {4},
keywords = {stochastic simulation, medical devices, number entry, differential formal analysis},
location = {London, United Kingdom},
series = {EICS '13}
}

@inproceedings{10.5555/3172795.3172803,
author = {Tatsi, Krystalenia and Kontogiannis, Kostas},
title = {Assisting Developers towards Fault Localization by Analyzing Failure Reports},
year = {2017},
publisher = {IBM Corp.},
address = {USA},
abstract = {Large software applications encompass many components with complex interdependencies. When a failure occurs, developers usually have limited information and time in their disposal for localizing the root cause of the observed failure. The most common information developers have readily access to includes failure reports, stack traces, and event logs. In this context, a major challenge is to devise techniques that assist developers utilize this information in order to zero-in their focus on specific methods that have a high probability of containing the root cause of the observed failure. Once such an initial set of methods has been identified, other more elaborate, complex, and computationally expensive data flow analyses could be applied. In this paper, we present a technique which aims to identify such an initial set of suspicious methods by first, retrieving information from failure reports obtained from Bugzilla repositories, second by combining this information with graph models that denote actual dependencies obtained from the subject system's source code in order to create an hypothesis space and third, by applying a ranking score to identify methods that have high likelihood of containing the root cause. The technique is shown to be tractable when applied to systems with several thousands of source code methods and the results exhibit high accuracy.},
booktitle = {Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering},
pages = {56–65},
numpages = {10},
keywords = {fault localization, software repositories, DevOps, software maintenance, code analysis},
location = {Markham, Ontario, Canada},
series = {CASCON '17}
}

@inproceedings{10.1145/2110363.2110391,
author = {Gopalan, Ramya and Ant\'{o}n, Annie and Doyle, Jon},
title = {UCON<sub>LEGAL</sub>: A Usage Control Model for HIPAA},
year = {2012},
isbn = {9781450307819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110363.2110391},
doi = {10.1145/2110363.2110391},
abstract = {Developing an access control system that satisfies the requirements expressed in regulations, such as the Health Insurance Portability and Accountability Act (HIPAA), can help ensure regulatory compliance in software systems. A usage control model that specifies the rules governing information access and usage, as expressed in law, is an important step towards achieving such compliance. Software systems that handle health records must comply with regulations in the HIPAA Privacy and Security Rules. Herein, we analyze the HIPAA Privacy Rule using a grounded theory methodology coupled with an inquiry driven approach to determine the components that must be supported by a usage control model to achieve regulatory-compliant health records usage. In this paper, we propose a usage control model, UCONLEGAL, which extends UCONABC with components to model purposes, cross-references, exceptions, conditions, and logs. We also employ UCONLEGAL to show how to express the access and usage rules we identified in the HIPAA Privacy Rule. Our analysis yielded seven types of conditions specific to HIPAA that we include in UCONLEGAL; these conditions were previously unsupported by existing usage control models.},
booktitle = {Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium},
pages = {227–236},
numpages = {10},
keywords = {personal health information, access control, software requirements, legal compliance, hipaa-compliance, regulatory compliance, electronic health records, usage control},
location = {Miami, Florida, USA},
series = {IHI '12}
}

@article{10.5555/2591204.2591215,
author = {Bienkowski, Marcin and Feldmann, Anja and Grassler, Johannes and Schaffrath, Gregor and Schmid, Stefan},
title = {The Wide-Area Virtual Service Migration Problem: A Competitive Analysis Approach},
year = {2014},
issue_date = {February 2014},
publisher = {IEEE Press},
volume = {22},
number = {1},
issn = {1063-6692},
abstract = {Today's trend toward network virtualization and software-defined networking enables flexible new distributed systems where resources can be dynamically allocated and migrated to locations where they are most useful. This paper proposes a competitive analysis approach to design and reason about online algorithms that find a good tradeoff between the benefits and costs of a migratable service. A competitive online algorithm provides worst-case performance guarantees under any demand dynamics, and without any information or statistical assumptions on the demand in the future. This is attractive especially in scenarios where the demand is hard to predict and can be subject to unexpected events. As a case study, we describe a service (e.g., an SAP server or a gaming application) that uses network virtualization to improve the quality of service (QoS) experienced by thin client applications running on mobile devices. By decoupling the service from the underlying resource infrastructure, it can be migrated closer to the current client locations while taking into account migration costs. We identify the major cost factors in such a system and formalize the wide-area service migration problem. Our main contributions are a randomized and a deterministic online algorithm that achieve a competitive ratio of $O(log {n})$ in a simplified scenario, where $n$is the size of the substrate network. This is almost optimal. We complement our worst-case analysis with simulations in different specific scenarios and also sketch a migration demonstrator.},
journal = {IEEE/ACM Trans. Netw.},
month = feb,
pages = {165–178},
numpages = {14}
}

@article{10.1109/TNET.2020.3017771,
author = {Zhou, Hao and Gao, Xiaofeng and Zheng, Jiaqi and Chen, Guihai},
title = {Scheduling Relaxed Loop-Free Updates Within Tight Lower Bounds in SDNs},
year = {2020},
issue_date = {Dec. 2020},
publisher = {IEEE Press},
volume = {28},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2020.3017771},
doi = {10.1109/TNET.2020.3017771},
abstract = {We consider a fundamental update problem of avoiding forwarding loops based on the node-ordering protocol in Software Defined Networks (SDNs). Due to the distributed data plane, forwarding loops may occur during the updates and influence the network performance. The node-ordering protocol can avoid such forwarding loops by controlling the update orders of the switches and does not consume extra flow table space overhead. However, an <inline-formula> <tex-math notation="LaTeX">$Omega (n)$ </tex-math></inline-formula> lower bound on the number of rounds required by any algorithm using this protocol with loop-free constraint has been proved, where <inline-formula> <tex-math notation="LaTeX">$n$ </tex-math></inline-formula> is the number of switches in the network. To accelerate the updates, a weaker notion of loop-freedom — relaxed loop-freedom — has been introduced. Despite that, the theoretical bound of the node-ordering protocol with relaxed loop-free constraint remains unknown yet. In this article, we solve a long-standing open problem: how to derive <inline-formula> <tex-math notation="LaTeX">$omega (1)$ </tex-math></inline-formula>-round lower bound or to show that <inline-formula> <tex-math notation="LaTeX">$O(1)$ </tex-math></inline-formula>-round schedules always exist for the relaxed loop-free update problem. Specifically, we prove that any algorithm needs <inline-formula> <tex-math notation="LaTeX">$Omega (log n)$ </tex-math></inline-formula> rounds to guarantee relaxed loop freedom in the worst case. In addition, we develop a fast relaxed loop-free update algorithm named Savitar that touches the tight lower bound. For any update instance, Savitar can use at most <inline-formula> <tex-math notation="LaTeX">$2 lfloor log _{2},,n rfloor - 1$ </tex-math></inline-formula> rounds to schedule relaxed loop-free updates. Extensive experiments on Mininet using a Floodlight controller show that Savitar can significantly decrease the update time, achieve near optimal performance and save over 30% of the rounds compared with the state of the art.},
journal = {IEEE/ACM Trans. Netw.},
month = dec,
pages = {2503–2516},
numpages = {14}
}

@inproceedings{10.1145/3345629.3345630,
author = {Lenarduzzi, Valentina and Saarim\"{a}ki, Nyyti and Taibi, Davide},
title = {The Technical Debt Dataset},
year = {2019},
isbn = {9781450372336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345629.3345630},
doi = {10.1145/3345629.3345630},
abstract = {Technical Debt analysis is increasing in popularity as nowadays researchers and industry are adopting various tools for static code analysis to evaluate the quality of their code. Despite this, empirical studies on software projects are expensive because of the time needed to analyze the projects. In addition, the results are difficult to compare as studies commonly consider different projects. In this work, we propose the Technical Debt Dataset, a curated set of project measurement data from 33 Java projects from the Apache Software Foundation. In the Technical Debt Dataset, we analyzed all commits from separately defined time frames with SonarQube to collect Technical Debt information and with Ptidej to detect code smells. Moreover, we extracted all available commit information from the git logs, the refactoring applied with Refactoring Miner, and fault information reported in the issue trackers (Jira). Using this information, we executed the SZZ algorithm to identify the fault-inducing and -fixing commits. We analyzed 78K commits from the selected 33 projects, detecting 1.8M SonarQube issues, 62K code smells, 28K faults and 57K refactorings. The project analysis took more than 200 days. In this paper, we describe the data retrieval pipeline together with the tools used for the analysis. The dataset is made available through CSV files and an SQLite database to facilitate queries on the data. The Technical Debt Dataset aims to open up diverse opportunities for Technical Debt research, enabling researchers to compare results on common projects.},
booktitle = {Proceedings of the Fifteenth International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {2–11},
numpages = {10},
keywords = {Faults, SZZ, Dataset, Mining Software Repository, SonarQube, Software Quality, Technical Debt},
location = {Recife, Brazil},
series = {PROMISE'19}
}

@inproceedings{10.1145/3337821.3337887,
author = {Xu, Gaoxiang and Feng, Dan and Tan, Zhipeng and Zhang, Xinyan and Xu, Jie and Shu, Xi and Zhu, Yifeng},
title = {RFPL: A Recovery Friendly Parity Logging Scheme for Reducing Small Write Penalty of SSD RAID},
year = {2019},
isbn = {9781450362955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3337821.3337887},
doi = {10.1145/3337821.3337887},
abstract = {Parity based RAID suffers from poor small write performance due to heavy parity update overhead. The recently proposed method EPLOG constructs a new stripe with updated data chunks without updating old parity chunks. However, due to skewness of data accesses, old versions of updated data chunks often need to be kept to protect other data chunks of the same stripe. This seriously hurts the efficiency of recovering system from device failures due to the need of reconstructing the preserved old data chunks on failed devices.In this paper, we propose a Recovery Friendly Parity Logging scheme, called RFPL, which minimizes small write penalty and provides high recovery performance for SSD RAID. The key idea of RFPL is to reduce the mixture of old and new data chunks in a stripe by exploiting skewness of data accesses. RFPL constructs a new stripe with updated data chunks of the same old stripe. Since cold data chunks of the old stripe are rarely updated, it is likely that all of data chunks written to the new stripe are hot data and become old together within a short time span. This co-old of data chunks in a stripe effectively mitigates the total number of old data chunks which need to be preserved. We have implemented RFPL on a RAID-5 SSD array in Linux 4.3. Experimental results show that, compared with the Linux software RAID, RFPL reduces user I/O response time by 83.1% for normal state and 81.6% for reconstruction state. Compared with the state-of-the-art scheme EPLOG, RFPL reduces user I/O response time by 46.8% for normal state and 40.9% for reconstruction state. Our reliability analysis shows RFPL improves the mean time to data loss (MTTDL) by 9.36X and 1.44X compared with the Linux software RAID and EPLOG.},
booktitle = {Proceedings of the 48th International Conference on Parallel Processing},
articleno = {23},
numpages = {10},
keywords = {Recovery Performance, SSD RAID, Small Write Penalty, I/O Performance, Reliability},
location = {Kyoto, Japan},
series = {ICPP 2019}
}

@inproceedings{10.1145/2245276.2232069,
author = {Lee, Jongmin and Kim, Ahreum and Park, Moonju and Choi, Jongmoo and Lee, Donghee and Noh, Sam H.},
title = {Real-Time Flash Memory Storage with Janus-FTL},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2232069},
doi = {10.1145/2245276.2232069},
abstract = {Janus-FTL, a recently proposed Flash Translation Layer (FTL) software layer for flash memory storage, has the potential to reduce the worst case write time of flash memory storage by keeping hot data in the page mapping area and cold data in the block mapping area. In this paper, we present the design of the real-time Janus-FTL that guarantees worst case garbage collection cost and write response time. In the proposed FTL, each task needs to decide whether to place its data either in the page mapping area or the block mapping area. A naive optimal placement algorithm, which minimizes the worst case write time, has O(2N) time complexity for N tasks. We propose an algorithm reducing the time complexity from O(2N) to O(N log N). Through experiments, we verify that the real-time Janus-FTL reduces the worst case write time and, as a result, accommodates more real-time tasks than the previous state-of-the-art realtime page mapping FTL.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {1799–1806},
numpages = {8},
keywords = {Janus-FTL, real-time flash memory storage, page mapping FTL, real-time garbage collection},
location = {Trento, Italy},
series = {SAC '12}
}

@inproceedings{10.1145/2542050.2542091,
author = {Phi, Diep Bui and Trong, Khanh Nguyen and Nguyen, Viet Ha},
title = {A Runtime Approach for Estimating Resource Usage},
year = {2013},
isbn = {9781450324540},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2542050.2542091},
doi = {10.1145/2542050.2542091},
abstract = {In the era of information explosion, a program is necessary to be scalable. Therefore, scalability analysis becomes very important in software verification and validation. However, current approaches to empirical scalability analysis remain limitations related to the number of supported models and performance. In this paper, we propose a runtime approach for estimating the program resource usage with two aims: evaluating the program scalability and revealing potential errors. In this approach, the resource usage of a program is first observed when it is executed on inputs with different scales, the observed results are then fitted on a model of the usage according to the program's input. Comparing to other approaches, ours supports diverse models to illustrate the resource usage, i.e., linear-log, power-law, polynomial, etc. We currently focus on the computation cost and stack frames usage as two representatives of resource usage, but the approach can be extended to other kinds of resource. The experimental result shows that our approach achieves more precise estimation and better performance than other state-of-the-art approaches.},
booktitle = {Proceedings of the Fourth Symposium on Information and Communication Technology},
pages = {261–266},
numpages = {6},
keywords = {runtime approach, resource usage, scalability analysis},
location = {Danang, Vietnam},
series = {SoICT '13}
}

@inproceedings{10.1145/2786567.2792902,
author = {Filho, Jackson Feij\'{o} and Valle, Thiago and Prata, Wilson},
title = {Automated Usability Tests for Mobile Devices through Live Emotions Logging},
year = {2015},
isbn = {9781450336536},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786567.2792902},
doi = {10.1145/2786567.2792902},
abstract = {Usability tests for mobile phones software present several consistent obstacles. The issue of observing users' interaction within a controlled and supervised environment versus unsupervised field tests is a significant one. Additionally, the burden of frequently time-consuming and expensive usability tests for small software development teams are evident. This work proposes the use of a system to implement emotions logging in automated usability tests for mobile phones. We do this by efficiently, easily and cost-effectively assessing the users' affective state by evaluating their expressive reactions during a mobile software usability evaluation process. These reactions are collected using the front camera on mobile devices.},
booktitle = {Proceedings of the 17th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {636–643},
numpages = {8},
keywords = {Emotion Feedback, Mobile Software, Usability},
location = {Copenhagen, Denmark},
series = {MobileHCI '15}
}

@inproceedings{10.1145/2642937.2642988,
author = {Ohmann, Tony and Herzberg, Michael and Fiss, Sebastian and Halbert, Armand and Palyart, Marc and Beschastnikh, Ivan and Brun, Yuriy},
title = {Behavioral Resource-Aware Model Inference},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2642988},
doi = {10.1145/2642937.2642988},
abstract = {Software bugs often arise because of differences between what developers think their system does and what the system actually does. These differences frustrate debugging and comprehension efforts. We describe Perfume, an automated approach for inferring behavioral, resource-aware models of software systems from logs of their executions. These finite state machine models ease understanding of system behavior and resource use.Perfume improves on the state of the art in model inference by differentiating behaviorally similar executions that differ in resource consumption. For example, Perfume separates otherwise identical requests that hit a cache from those that miss it, which can aid understanding how the cache affects system behavior and removing cache-related bugs. A small user study demonstrates that using Perfume is more effective than using logs and another model inference tool for system comprehension. A case study on the TCP protocol demonstrates that Perfume models can help understand non-trivial protocol behavior. Perfume models capture key system properties and improve system comprehension, while being reasonably robust to noise likely to occur in real-world executions.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {19–30},
numpages = {12},
keywords = {system understanding, performance, perfume, comprehension, model inference, log analysis, debugging},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@article{10.14778/2536360.2536372,
author = {Stoica, Radu and Ailamaki, Anastasia},
title = {Improving Flash Write Performance by Using Update Frequency},
year = {2013},
issue_date = {July 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/2536360.2536372},
doi = {10.14778/2536360.2536372},
abstract = {Solid-state drives (SSDs) are quickly becoming the default storage medium as the cost of NAND flash memory continues to drop. However, flash memory introduces new challenges, as data cannot be eciently updated in-place. To overcome the technology's limitations, SSDs incorporate a software Flash Translation Layer (FTL) that implements out-of-place updates, typically by storing data in a log-structured fashion. Despite a large number of existing FTL algorithms, SSD performance, predictability, and lifetime remain an issue, especially for the write-intensive workloads specific to database applications.In this paper, we show how to design FTLs that are more efficient by using the I/O write skew to guide data placement on flash memory. We model the relationship between data placement and write performance for basic I/O write patterns and detail the most important concepts of writing to flash memory: i) the trade-o between the extra capacity available and write overhead, ii) the benefit of adapting data placement to write skew, iii) the impact of the cleaning policy, and iv) how to estimate the best achievable write performance for a given I/O workload. Based on the findings of the theoretical model, we propose a new principled data placement algorithm that can be incorporated into existing FTLs. We show the benefits of our data placement algorithm when running micro-benchmarks and real database I/O traces: our data placement algorithm reduces write overhead by 20% - 75% when compared to state-of-art techniques.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {733–744},
numpages = {12}
}

@article{10.1109/TCBB.2013.108,
author = {Todor, Andrei and Dobra, Alin and Kahveci, Tamer},
title = {Characterizing the Topology of Probabilistic Biological Networks},
year = {2013},
issue_date = {July 2013},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {10},
number = {4},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2013.108},
doi = {10.1109/TCBB.2013.108},
abstract = {Biological interactions are often uncertain events, that may or may not take place with some probability. This uncertainty leads to a massive number of alternative interaction topologies for each such network. The existing studies analyze the degree distribution of biological networks by assuming that all the given interactions take place under all circumstances. This strong and often incorrect assumption can lead to misleading results. In this paper, we address this problem and develop a sound mathematical basis to characterize networks in the presence of uncertain interactions. Using our mathematical representation, we develop a method that can accurately describe the degree distribution of such networks. We also take one more step and extend our method to accurately compute the joint-degree distributions of node pairs connected by edges. The number of possible network topologies grows exponentially with the number of uncertain interactions. However, the mathematical model we develop allows us to compute these degree distributions in polynomial time in the number of interactions. Our method works quickly even for entire protein-protein interaction (PPI) networks. It also helps us find an adequate mathematical model using MLE. We perform a comparative study of node-degree and joint-degree distributions in two types of biological networks: the classical deterministic networks and the more flexible probabilistic networks. Our results confirm that power-law and log-normal models best describe degree distributions for both probabilistic and deterministic networks. Moreover, the inverse correlation of degrees of neighboring nodes shows that, in probabilistic networks, nodes with large number of interactions prefer to interact with those with small number of interactions more frequently than expected. We also show that probabilistic networks are more robust for node-degree distribution computation than the deterministic ones. Availability: all the data sets used, the software implemented and the alignments found in this paper are available at &gt;http://bioinformatics.cise.ufl.edu/projects/probNet/.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jul,
pages = {970–983},
numpages = {14},
keywords = {network topology, degree distribution, Probabilistic biological networks, random graphs}
}

@inproceedings{10.1145/2484239.2484254,
author = {Avni, Hillel and Shavit, Nir and Suissa, Adi},
title = {Leaplist: Lessons Learned in Designing Tm-Supported Range Queries},
year = {2013},
isbn = {9781450320658},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2484239.2484254},
doi = {10.1145/2484239.2484254},
abstract = {We introduce Leaplist, a concurrent data-structure that is tailored to provide linearizable range queries. A lookup in Leaplist takes O (log n) and is comparable to a balanced binary search tree or to a Skiplist. However, in Leaplist, each node holds up-to K immutable key-value pairs, so collecting a linearizable range is K times faster than the same operation performed non-linearizably on a Skiplist.We show how software transactional memory support in a commercial compiler helped us create an efficient lock-based implementation of Leaplist. We used this STM to implement short transactions which we call Locking Transactions (LT), to acquire locks, while verifying that the state of the data-structure is legal, and combine them with a transactional Consistency Oblivious Programming (COP) [2] mechanism to enhance data structure traversals.We compare Leaplist to prior implementations of Skiplists, and show that while updates in the Leaplist are slower, lookups are somewhat faster, and for range-queries the Leaplist outperforms the Skiplist's non-linearizable range query operations by an order of magnitude. We believe that this data structure and its performance would have been impossible to obtain without the STM support.},
booktitle = {Proceedings of the 2013 ACM Symposium on Principles of Distributed Computing},
pages = {299–308},
numpages = {10},
keywords = {range-queries, transactional-memory, data-structures},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {PODC '13}
}

@inproceedings{10.1145/2479871.2479903,
author = {Wang, Miao and Holub, Viliam and Murphy, John and O'Sullivan, Patrick},
title = {Stream-Based Event Prediction Using Bayesian and Bloom Filters},
year = {2013},
isbn = {9781450316361},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2479871.2479903},
doi = {10.1145/2479871.2479903},
abstract = {Nowadays, enterprise software systems store a large amount of operational information in logs. Manually analysing these data can be time-consuming and error-prone. Although a static knowledge database eases the task to capture recurring problems, maintaining such a knowledge repository requires periodic knowledge updates by domain experts. Moreover, as the repository grows, the problem of memory efficiency will also arise.Our goal is to enable administrators to efficiently capture interesting data in a high volume stream of events in real-time. We are proposing a statistical approach for software applications to be automatically trained with a smaller dataset to efficiently predict interesting data under such conditions. The proposed solution maintains a stable memory usage by migrating keywords from a dynamic data structure to fixed sized data structures (Bloom Filter). In particular, the solution has achieved better prediction results by enhancing the Bayesian theory with belief modifiers.},
booktitle = {Proceedings of the 4th ACM/SPEC International Conference on Performance Engineering},
pages = {217–228},
numpages = {12},
keywords = {high performance, stream event prediction, statistical},
location = {Prague, Czech Republic},
series = {ICPE '13}
}

@inproceedings{10.1145/3314221.3314613,
author = {Maeng, Kiwan and Lucia, Brandon},
title = {Supporting Peripherals in Intermittent Systems with Just-in-Time Checkpoints},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314613},
doi = {10.1145/3314221.3314613},
abstract = {Batteryless energy-harvesting devices have the potential to be the foundation of applications for which batteries are infeasible. Just-In-Time checkpointing supports intermittent execution on energy-harvesting devices by checkpointing processor state right before a power failure. While effective for software execution, Just-In-Time checkpointing remains vulnerable to unrecoverable failures involving peripherals(e.g., sensors and accelerators) because checkpointing during a peripheral operation may lead to inconsistency between peripheral and program state. Additionally, a peripheral operation that uses more energy than a device can buffer never completes, causing non-termination.  This paper presents Samoyed, a Just-In-Time checkpointing system that safely supports peripherals. Samoyed correctly runs user-annotated peripheral functions by selectively disabling checkpoints and undo-logging. Samoyed guarantees progress by energy profiling, dynamic peripheral workload scaling, and a user-provided software fallback routine. Our evaluation shows that Samoyed correctly executes peripheral operations that fail with existing systems, achieving up to 122.9x speedup by using accelerators. Samoyed preserves the performance benefit of Just-In-Time checkpointing, showing 4.11x mean speedup compared to a recent possible alternative. Moreover, Samoyed’s unique ability to profile energy and to dynamically scale large peripheral operations simplifies programming.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1101–1116},
numpages = {16},
keywords = {intermittent computing, energy-harvesting},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/2687233.2687250,
author = {Polovtcev, Aleksandr and Zozulya, Aleksey and Vert, Tatiana and Krikun, Tatiana and Itsykson, Vladimir},
title = {Visualization of Parallel Program Execution for Software Behavior Analysis and Defect Detection},
year = {2014},
isbn = {9781450328890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2687233.2687250},
doi = {10.1145/2687233.2687250},
abstract = {Modern software systems are developed as very complex, heavily parallelized programs. Communication between multiple processes and threads inevitably leads to exponential growth of program state space and nondetermined program behaviour. These facts make standard error detection techniques, such as testing or static analysis, ineffective when applied to parallel programs. One way to solve the error detection problem is dynamic software visualization. In this paper we propose an approach, based on program trace analysis and vizualization. Our prototype allows building program trace logs using code instrumentation and supports 3 visualization types. Experimental results prove that proposed approch is effective and applicable to wide spectrum of software systems.},
booktitle = {Proceedings of the 10th Central and Eastern European Software Engineering Conference in Russia},
articleno = {24},
numpages = {10},
keywords = {software visualization, parallel programming, program execution trace, code instrumentation, parrallel error detection},
location = {Moscow, Russia},
series = {CEE-SECR '14}
}

@inproceedings{10.1145/2961111.2962587,
author = {Shahin, Mojtaba and Babar, Muhammad Ali and Zhu, Liming},
title = {The Intersection of Continuous Deployment and Architecting Process: Practitioners' Perspectives},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962587},
doi = {10.1145/2961111.2962587},
abstract = {Context: Development and Operations (DevOps) is an emerging software industry movement to bridge the gap between software development and operations teams. DevOps supports frequently and reliably releasing new features and products-- thus subsuming Continuous Deployment (CD) practice. Goal: This research aims at empirically exploring the potential impact of CD practice on architecting process. Method: We carried out a case study involving interviews with 16 software practitioners. Results: We have identified (1) a range of recurring architectural challenges (i.e., highly coupled monolithic architecture, team dependencies, and ever-changing operational environments and tools) and (2) five main architectural principles (i.e., small and independent deployment units, not too much focus on reusability, aggregating logs, isolating changes, and testability inside the architecture) that should be considered when an application is (re-) architected for CD practice. This study also supports that software architecture can better support operations if an operations team is engaged at an early stage of software development for taking operational aspects into considerations. Conclusion: These findings provide evidence that software architecture plays a significant role in successfully and efficiently adopting continuous deployment. The findings contribute to establish an evidential body of knowledge about the state of the art of architecting for CD practice},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {44},
numpages = {10},
keywords = {continuous deployment, DevOps, empirical study, Software architecture},
location = {Ciudad Real, Spain},
series = {ESEM '16}
}

@inproceedings{10.1145/2797022.2797033,
author = {Cui, Heming and Gu, Rui and Liu, Cheng and Yang, Junfeng},
title = {RepFrame: An Efficient and Transparent Framework for Dynamic Program Analysis},
year = {2015},
isbn = {9781450335546},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797022.2797033},
doi = {10.1145/2797022.2797033},
abstract = {Dynamic program analysis frameworks greatly improve software quality as they enable a wide range of powerful analysis tools (e.g., reliability, profiling, and logging) at runtime. However, because existing frameworks run only one actual execution for each software application, the execution is fully or partially coupled with an analysis tool in order to transfer execution states (e.g., accessed memory and thread interleavings) to the analysis tool, easily causing a prohibitive slowdown for the execution. To reduce the portions of execution states that require transfer, many frameworks require significantly carving analysis tools as well as the frameworks themselves. Thus, these frameworks significantly trade off transparency with analysis tools and allow only one type of tools to run within one execution.This paper presents RepFrame, an efficient and transparent framework that fully decouples execution and analysis by constructing multiple equivalent executions. To do so, RepFrame leverages a recent fault-tolerant technique: transparent state machine replication, which runs the same software application on a set of machines (or replicas), and ensures that all replicas see the same sequence of inputs and process these inputs with the same efficient thread interleavings automatically. In addition, this paper discusses potential directions in which REPFRAME can further strengthen existing analyses. Evaluation shows that REPFRAME is easy to run two asynchronous analysis tools together and has reasonable overhead.},
booktitle = {Proceedings of the 6th Asia-Pacific Workshop on Systems},
articleno = {13},
numpages = {9},
location = {Tokyo, Japan},
series = {APSys '15}
}

@inproceedings{10.1145/3196321.3196352,
author = {Porkol\'{a}b, Zolt\'{a}n and Brunner, Tibor},
title = {The Codecompass Comprehension Framework},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3196352},
doi = {10.1145/3196321.3196352},
abstract = {CodeCompass is an open source LLVM/Clang based tool developed by Ericsson Ltd. and the E\"{o}tv\"{o}s Lor\'{a}nd University, Budapest to help understanding large legacy software systems. Based on the LLVM/Clang compiler infrastructure, CodeCompass gives exact information on complex C/C++ language elements like overloading, inheritance, the usage of variables and types, possible uses of function pointers and the virtual functions - features that various existing tools support only partially. Steensgaard's and Andersen's pointer analysis algorithm are used to compute and visualize the use of pointers/references. The wide range of interactive visualizations extends further than the usual class and function call diagrams; architectural, component and interface diagrams are a few of the implemented graphs. To make comprehension more extensive, CodeCompass is not restricted to the source code. It also utilizes build information to explore the system architecture as well as version control information e.g. git commit history and blame view. Clang based static analysis results are also integrated to CodeCompass. Although the tool focuses mainly on C and C++, it also supports Java and Python languages.In this demo we will simulate a typical bug-fixing work flow in a C++ system. First, we show, how to use the combined text and definition based search for a fast feature location. Here we also demonstrate our log search, which can be used to locate the code source of an emitted message. When we have an approximate location of the issue, we can start a detailed investigation understanding the class relationships, function call chains (including virtual calls, and calls via function pointers), and the read/write events on individual variables. We also visualize the pointer relationships. To make the comprehension complete, we check the version control information who committed the code, when and why.This Tool demo submission is complementing our Industry track submission with the similar title. A live demo is also available at the homepage of the tool https://github.com/ericsson/codecompass.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {393–396},
numpages = {4},
keywords = {software visualization, C/C++ programming language, code comprehension},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@inproceedings{10.5555/3105427.3105433,
author = {Haraldsson, Saemundur O. and Woodward, John R. and Brownlee, Alexander I. E.},
title = {The Use of Automatic Test Data Generation for Genetic Improvement in a Live System},
year = {2017},
isbn = {9781538627891},
publisher = {IEEE Press},
abstract = {In this paper we present a bespoke live system in commercial use that has been implemented with self-improving properties. During business hours it provides overview and control for many specialists to simultaneously schedule and observe the rehabilitation process for multiple clients. However in the evening, after the last user logs out, it starts a self-analysis based on the day's recorded interactions and the self-improving process. It uses Search Based Software Testing (SBST) techniques to generate test data for Genetic Improvement (GI) to fix any bugs if exceptions have been recorded. The system has already been under testing for 4 months and demonstrates the effectiveness of simple test data generation and the power of GI for improving live code.},
booktitle = {Proceedings of the 10th International Workshop on Search-Based Software Testing},
pages = {28–31},
numpages = {4},
keywords = {real world application, test data generation, search based software engineering, bug fixing},
location = {Buenos Aires, Argentina},
series = {SBST '17}
}

@inproceedings{10.1145/3428029.3428564,
author = {Prasad, Prajish and Iyer, Sridhar},
title = {Inferring Students’ Tracing Behaviors from Interaction Logs of a Learning Environment for Software Design Comprehension},
year = {2020},
isbn = {9781450389211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428029.3428564},
doi = {10.1145/3428029.3428564},
abstract = { VeriSIM is a learning environment which helps software engineering students comprehend a given design, by enabling them to trace different scenarios in the design. Learners trace different scenarios by constructing a state diagram. In VeriSIM, learners go through four challenges which help them progressively trace scenarios in the design. VeriSIM provides learners affordances to add, edit, delete states, as well as “run” a given state diagram. All such learner interactions are logged in the system. In this paper, we analyze the interaction logs of 12 students, in order to infer certain behavior patterns. The preliminary analysis of these patterns shows that the order of the challenges help learners effectively trace scenarios.},
booktitle = {Koli Calling '20: Proceedings of the 20th Koli Calling International Conference on Computing Education Research},
articleno = {39},
numpages = {2},
keywords = {UML modelling, interaction log analysis, design tracing},
location = {Koli, Finland},
series = {Koli Calling '20}
}

@article{10.1145/3331157,
author = {Aiordachioae, Adrian and Vatavu, Radu-Daniel},
title = {Life-Tags: A Smartglasses-Based System for Recording and Abstracting Life with Tag Clouds},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {EICS},
url = {https://doi.org/10.1145/3331157},
doi = {10.1145/3331157},
abstract = {We introduce "Life-Tags," a wearable, smartglasses-based system for abstracting life in the form of clouds of tags and concepts automatically extracted from snapshots of the visual reality recorded by wearable video cameras. Life-Tags summarizes users' life experiences using word clouds, highlighting the "executive summary" of what the visual experience felt like for the smartglasses user during some period of time, such as a specific day, week, month, or the last hour. In this paper, we focus on (i) design criteria and principles of operation for Life-Tags, such as its first-person, eye-level perspective for recording life, passive logging mode, and privacy-oriented operation, as well as on (ii) technical and engineering aspects for implementing Life-Tags, such as the block architecture diagram highlighting devices, software modules, third-party services, and dataflows. We also conduct a technical evaluation of Life-Tags and report results from a controlled experiment that generated 21,600 full HD snapshots from six indoor and outdoor scenarios, representative of everyday life activities, such as walking, eating, traveling, etc., with a total of 180 minutes of recorded life to abstract with tag clouds. Our experimental results and Life-Tags prototype inform design and engineering of future life abstracting systems based on smartglasses and wearable video cameras to ensure effective generation of rich clouds of concepts, reflective of the visual experience of the smartglasses user.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jun,
articleno = {15},
numpages = {22},
keywords = {experiment, word clouds, automatic tag extraction, wearable video cameras, wearable computing, tag clouds, smartglasses, lifelogging}
}

@inproceedings{10.1145/3183440.3190334,
author = {Gulzar, Muhammad Ali},
title = {Interactive and Automated Debugging for Big Data Analytics},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3190334},
doi = {10.1145/3183440.3190334},
abstract = {An abundance of data in many disciplines of science, engineering, national security, health care, and business has led to the emerging field of Big Data Analytics that run in a cloud computing environment. To process massive quantities of data in the cloud, developers leverage Data-Intensive Scalable Computing (DISC) systems such as Google's MapReduce, Hadoop, and Spark.Currently, developers do not have easy means to debug DISC applications. The use of cloud computing makes application development feel more like batch jobs and the nature of debugging is therefore post-mortem. Developers of big data applications write code that implements a data processing pipeline and test it on their local workstation with a small sample data, downloaded from a TB-scale data warehouse. They cross fingers and hope that the program works in the expensive production cloud. When a job fails or they get a suspicious result, data scientists spend hours guessing at the source of the error, digging through post-mortem logs. In such cases, the data scientists may want to pinpoint the root cause of errors by investigating a subset of corresponding input records.The vision of my work is to provide interactive, real-time and automated debugging services for big data processing programs in modern DISC systems with minimum performance impact. My work investigates the following research questions in the context of big data analytics: (1) What are the necessary debugging primitives for interactive big data processing? (2) What scalable fault localization algorithms are needed to help the user to localize and characterize the root causes of errors? (3) How can we improve testing efficiency during iterative development of DISC applications by reasoning the semantics of dataflow operators and user-defined functions used inside dataflow operators in tandem?To answer these questions, we synthesize and innovate ideas from software engineering, big data systems, and program analysis, and coordinate innovations across the software stack from the user-facing API all the way down to the systems infrastructure.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {509–511},
numpages = {3},
keywords = {data-intensive scalable computing (DISC), automated debugging, big data, and data cleaning, debugging and testing, fault localization, test minimization, data provenance},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@article{10.1145/2499947.2499951,
author = {Hundhausen, Christopher D. and Agrawal, Anukrati and Agarwal, Pawan},
title = {Talking about Code: Integrating Pedagogical Code Reviews into Early Computing Courses},
year = {2013},
issue_date = {August 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
url = {https://doi.org/10.1145/2499947.2499951},
doi = {10.1145/2499947.2499951},
abstract = {Given the increasing importance of soft skills in the computing profession, there is good reason to provide students with more opportunities to learn and practice those skills in undergraduate computing courses. Toward that end, we have developed an active learning approach for computing education called the Pedagogical Code Review (PCR). Inspired by the code inspection process used in the software industry, a PCR is a collaborative activity in which a small team of students, led by a trained moderator: (a) walk through segments of each other's programming solutions, (b) check the code against a list of best coding practices, and (c) discuss and log issues that arise. To evaluate the viability and effectiveness of this approach, we conducted a series of four mixed-method empirical studies of various implementations of PCRs in CS1 courses at Washington State University. The first study validated the viability of the PCR activity. Using a quasi-experimental design, the final three studies evaluated two alternative implementations of PCRs—face-to-face and online. Our results provide evidence that PCRs can promote positive attitudinal shifts, and hone skills in critical review, teamwork, and communication. Based on our findings, we present a set of best practices for implementing PCRs.},
journal = {ACM Trans. Comput. Educ.},
month = aug,
articleno = {14},
numpages = {28},
keywords = {pedagogical code reviews, Studio-based learning and instruction, computer-supported collaborative learning}
}

@article{10.1145/3291057,
author = {Wu, Song and Zhou, Fang and Gao, Xiang and Jin, Hai and Ren, Jinglei},
title = {Dual-Page Checkpointing: An Architectural Approach to Efficient Data Persistence for In-Memory Applications},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3291057},
doi = {10.1145/3291057},
abstract = {Data persistence is necessary for many in-memory applications. However, the disk-based data persistence largely slows down in-memory applications. Emerging non-volatile memory (NVM) offers an opportunity to achieve in-memory data persistence at the DRAM-level performance. Nevertheless, NVM typically requires a software library to operate NVM data, which brings significant overhead.This article demonstrates that a hardware-based high-frequency checkpointing mechanism can be used to achieve efficient in-memory data persistence on NVM. To maintain checkpoint consistency, traditional logging and copy-on-write techniques incur excessive NVM writes that impair both performance and endurance of NVM; recent work attempts to solve the issue but requires a large amount of metadata in the memory controller. Hence, we design a new dual-page checkpointing system, which achieves low metadata cost and eliminates most excessive NVM writes at the same time. It breaks the traditional trade-off between metadata space cost and extra data writes. Our solution outperforms the state-of-the-art NVM software libraries by 13.6\texttimes{} in throughput, and leads to 34% less NVM wear-out and 1.28\texttimes{} higher throughput than state-of-the-art hardware checkpointing solutions, according to our evaluation with OLTP, graph computing, and machine-learning workloads.},
journal = {ACM Trans. Archit. Code Optim.},
month = jan,
articleno = {57},
numpages = {27},
keywords = {crash consistency, non-volatile memory (NVM), Data persistence, checkpointing}
}

@inproceedings{10.1145/3352460.3358321,
author = {Gupta, Siddharth and Daglis, Alexandros and Falsafi, Babak},
title = {Distributed Logless Atomic Durability with Persistent Memory},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358321},
doi = {10.1145/3352460.3358321},
abstract = {Datacenter operators have started deploying Persistent Memory (PM), leveraging its combination of fast access and persistence for significant performance gains. A key challenge for PM-aware software is to maintain high performance while achieving atomic durability. The latter typically requires the use of logging, which introduces considerable overhead with additional CPU cycles, write traffic, and ordering requirements. In this paper, we exploit the data multiversioning inherent in the memory hierarchy to achieve atomic durability without logging. Our design, LAD, relies on persistent buffering space at the memory controllers (MCs)---already present in modern CPUs---to speculatively accumulate all of a transaction's updates before they are all atomically committed to PM. LAD employs an on-chip distributed commit protocol in hardware to manage the distributed speculative state each transaction accumulates across multiple MCs. We demonstrate that LAD is a practical design relying on modest hardware modifications to provide atomically durable transactions, while delivering up to 80% of ideal---i.e., PM-oblivious software's---performance.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {466–478},
numpages = {13},
keywords = {Atomic Durability, Persistent Memory, Logging, Atomicity},
location = {Columbus, OH, USA},
series = {MICRO '52}
}

@article{10.1145/2109196.2109197,
author = {Graefe, Goetz},
title = {A Survey of B-Tree Logging and Recovery Techniques},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {0362-5915},
url = {https://doi.org/10.1145/2109196.2109197},
doi = {10.1145/2109196.2109197},
abstract = {B-trees have been ubiquitous in database management systems for several decades, and they serve in many other storage systems as well. Their basic structure and their basic operations are well understood including search, insertion, and deletion. However, implementation of transactional guarantees such as all-or-nothing failure atomicity and durability in spite of media and system failures seems to be difficult. High-performance techniques such as pseudo-deleted records, allocation-only logging, and transaction processing during crash recovery are widely used in commercial B-tree implementations but not widely understood. This survey collects many of these techniques as a reference for students, researchers, system architects, and software developers. Central in this discussion are physical data independence, separation of logical database contents and physical representation, and the concepts of user transactions and system transactions. Many of the techniques discussed are applicable beyond B-trees.},
journal = {ACM Trans. Database Syst.},
month = mar,
articleno = {1},
numpages = {35}
}

@inproceedings{10.1145/2591062.2591107,
author = {Ohmann, Tony and Thai, Kevin and Beschastnikh, Ivan and Brun, Yuriy},
title = {Mining Precise Performance-Aware Behavioral Models from Existing Instrumentation},
year = {2014},
isbn = {9781450327688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591062.2591107},
doi = {10.1145/2591062.2591107},
abstract = { Software bugs often arise from differences between what developers envision their system does and what that system actually does. When faced with such conceptual inconsistencies, debugging can be very difficult. Inferring and presenting developers with accurate behavioral models of the system implementation can help developers reconcile their view of the system with reality and improve system quality.  We present Perfume, a model-inference algorithm that improves on the state of the art by using performance information to differentiate otherwise similar-appearing executions and to remove false positives from the inferred models. Perfume uses a system's runtime execution logs to infer a concise, precise, and predictive finite state machine model that describes both observed executions and executions that have not been observed but that the system can likely generate. Perfume guides the model inference process by mining temporal performance-constrained properties from the logs, ensuring precision of the model's predictions. We describe the model inference process and demonstrate how it improves precision over the state of the art. },
booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
pages = {484–487},
numpages = {4},
keywords = {Model inference, log analysis, Perfume, performance},
location = {Hyderabad, India},
series = {ICSE Companion 2014}
}

@inproceedings{10.1145/2187836.2187880,
author = {Bursztein, Elie and Soman, Chinmay and Boneh, Dan and Mitchell, John C.},
title = {SessionJuggler: Secure Web Login from an Untrusted Terminal Using Session Hijacking},
year = {2012},
isbn = {9781450312295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2187836.2187880},
doi = {10.1145/2187836.2187880},
abstract = {We use modern features of web browsers to develop a secure login system from an untrusted terminal. The system, called Session Juggler, requires no server-side changes and no special software on the terminal beyond a modern web browser. This important property makes adoption much easier than with previous proposals. With Session Juggler users never enter their long term credential on the untrusted terminal. Instead, users log in to a web site using a smartphone app and then transfer the entire session, including cookies and all other session state, to the untrusted terminal. We show that Session Juggler works on all the Alexa top 100 sites except eight. Of those eight, five failures were due to the site enforcing IP session binding. We also show that Session Juggler works flawlessly with Facebook connect. Beyond login, Session Juggler also provides a secure logout mechanism where the trusted phone is used to kill the session. To validate the session juggling concept we conducted a number of web site surveys that are of independent interest. First, we survey how web sites bind a session token to a specific device and show that most use fairly basic techniques that are easily defeated. Second, we survey how web sites handle logout and show that many popular sites surprisingly do not properly handle logout requests.},
booktitle = {Proceedings of the 21st International Conference on World Wide Web},
pages = {321–330},
numpages = {10},
keywords = {mobile, malware, https, android, cookie, session, session hijhacking},
location = {Lyon, France},
series = {WWW '12}
}

@inproceedings{10.1145/3093338.3093351,
author = {Liu, Fang Cherry and Xu, Weijia and Belgin, Mehmet and Huang, Ruizhu and Fleischer, Blake C.},
title = {Insights into Research Computing Operations Using Big Data-Powered Log Analysis},
year = {2017},
isbn = {9781450352727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3093338.3093351},
doi = {10.1145/3093338.3093351},
abstract = {Research computing centers provide a wide variety of services including large-scale computing resources, data storage, high-speed interconnect and scientific software repositories to facilitate continuous competitive research. Efficient management of these complex resources and services, as well as ensuring their fair use by a large number of researchers from different scientific domains are key to a center's success. Almost all research centers use monitoring services based on real time data gathered from systems and services, but often lack tools to perform a deeper analysis on large volumes of historical logs for identifying insightful trends from recurring events. The size of collected data can be massive, posing significant challenges for the use of conventional tools for this kind of analysis. This paper describes a big data pipeline based on Hadoop and Spark technologies, developed in close collaboration between TACC and Georgia Tech. This data pipeline is capable of processing large volumes of data collected from schedulers using PBSTools, making it possible to run a deep analysis in minutes as opposed to hours with conventional tools. Our component-based pipeline design adds the flexibility of plugging in different components, as well as promotes data reuse. Using this data pipeline, we demonstrate the process of formulating several critical operational questions around researcher behavior, systems health, operational aspects and software usage trends, all of which are critical factors in determining solutions and strategies for efficient management of research computing centers.},
booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact},
articleno = {31},
numpages = {8},
keywords = {big data, hadoop, log analysis, spark},
location = {New Orleans, LA, USA},
series = {PEARC17}
}

@article{10.5555/2188385.2343708,
author = {Yuan, Guo-Xun and Ho, Chia-Hua and Lin, Chih-Jen},
title = {An Improved GLMNET for L1-Regularized Logistic Regression},
year = {2012},
issue_date = {3/1/2012},
publisher = {JMLR.org},
volume = {13},
number = {null},
issn = {1532-4435},
abstract = {Recently, Yuan et al. (2010) conducted a comprehensive comparison on software for L1-regularized classification. They concluded that a carefully designed coordinate descent implementation CDN is the fastest among state-of-the-art solvers. In this paper, we point out that CDN is less competitive on loss functions that are expensive to compute. In particular, CDN for logistic regression is much slower than CDN for SVM because the logistic loss involves expensive exp/log operations.In optimization, Newton methods are known to have fewer iterations although each iteration costs more. Because solving the Newton sub-problem is independent of the loss calculation, this type of methods may surpass CDN under some circumstances. In L1-regularized classification, GLMNET by Friedman et al. is already a Newton-type method, but experiments in Yuan et al. (2010) indicated that the existing GLMNET implementation may face difficulties for some largescale problems. In this paper, we propose an improved GLMNET to address some theoretical and implementation issues. In particular, as a Newton-type method, GLMNET achieves fast local convergence, but may fail to quickly obtain a useful solution. By a careful design to adjust the effort for each iteration, our method is efficient for both loosely or strictly solving the optimization problem. Experiments demonstrate that our improved GLMNET ismore efficient than CDN for L1-regularized logistic regression.},
journal = {J. Mach. Learn. Res.},
month = jun,
pages = {1999–2030},
numpages = {32},
keywords = {support vector machines, linear classification, logistic regression, L1 regularization, optimization methods}
}

@article{10.14778/3317315.3317321,
author = {Won, Youjip and Kim, Sundoo and Yun, Juseong and Tuan, Dam Quang and Seo, Jiwon},
title = {DASH: Database Shadowing for Mobile DBMS},
year = {2019},
issue_date = {March 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {7},
issn = {2150-8097},
url = {https://doi.org/10.14778/3317315.3317321},
doi = {10.14778/3317315.3317321},
abstract = {In this work, we propose Database Shadowing, or DASH, which is a new crash recovery technique for SQLite DBMS. DASH is a hybrid mixture of classical shadow paging and logging. DASH addresses four major issues in the current SQLite journal modes: the performance and write amplification issues of the rollback mode and the storage space requirement and tail latency issues of the WAL mode. DASH exploits two unique characteristics of SQLite: the database files are small and the transactions are entirely serialized. DASH consists of three key ingredients Aggregate Update, Atomic Exchange and Version Reset. Aggregate Update eliminates the redundant write overhead and the requirement to maintain multiple snapshots both of which are inherent in the out-of-place update. Atomic Exchange resolves the overhead of updating the locations of individual database pages exploiting order-preserving nature of the metadata update operation in modern filesystem. Version Reset makes the result of the Atomic Exchange durable without relying on expensive filesystem journaling. The salient aspect of DASH lies in its simplicity and compatibility with the legacy. DASH does not require any modifications in the underlying filesystem or the database organization. It requires only 451 LOC to implement. In Cyclomatic Complexity score, which represents software complexity, DASH renders 33% lower (simpler) mark than PERSIST and WAL modes of SQLite. We implement DASH for SQLite on Android and extensively evaluate it on widely used smartphone devices. DASH yields 4x performance gain over PERSIST mode (default journaling mode). Compared to WAL mode (the fastest journaling mode), DASH uses only 2.5% of the storage space on average. The transaction latency of DASH at 99.9% is one fourth of that of WAL mode.},
journal = {Proc. VLDB Endow.},
month = mar,
pages = {793–806},
numpages = {14}
}

@article{10.5555/2503308.2343708,
author = {Yuan, Guo-Xun and Ho, Chia-Hua and Lin, Chih-Jen},
title = {An Improved GLMNET for L1-Regularized Logistic Regression},
year = {2012},
issue_date = {January 2012},
publisher = {JMLR.org},
volume = {13},
number = {1},
issn = {1532-4435},
abstract = {Recently, Yuan et al. (2010) conducted a comprehensive comparison on software for L1-regularized classification. They concluded that a carefully designed coordinate descent implementation CDN is the fastest among state-of-the-art solvers. In this paper, we point out that CDN is less competitive on loss functions that are expensive to compute. In particular, CDN for logistic regression is much slower than CDN for SVM because the logistic loss involves expensive exp/log operations.In optimization, Newton methods are known to have fewer iterations although each iteration costs more. Because solving the Newton sub-problem is independent of the loss calculation, this type of methods may surpass CDN under some circumstances. In L1-regularized classification, GLMNET by Friedman et al. is already a Newton-type method, but experiments in Yuan et al. (2010) indicated that the existing GLMNET implementation may face difficulties for some largescale problems. In this paper, we propose an improved GLMNET to address some theoretical and implementation issues. In particular, as a Newton-type method, GLMNET achieves fast local convergence, but may fail to quickly obtain a useful solution. By a careful design to adjust the effort for each iteration, our method is efficient for both loosely or strictly solving the optimization problem. Experiments demonstrate that our improved GLMNET ismore efficient than CDN for L1-regularized logistic regression.},
journal = {J. Mach. Learn. Res.},
month = jun,
pages = {1999–2030},
numpages = {32},
keywords = {linear classification, L1 regularization, logistic regression, optimization methods, support vector machines}
}

@inproceedings{10.1145/1999320.1999377,
author = {Collins, Brendan},
title = {Performance vs. Productivity in the Context of ArcGIS Server 10},
year = {2011},
isbn = {9781450306812},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1999320.1999377},
doi = {10.1145/1999320.1999377},
abstract = {While many programming languages excel in their ability to execute commands quickly, others embody a greater focus on programmer productivity and clear syntax. In ESRI's GIS software package ArcGIS, Python is now the choice language for many GIS Analysts as an alternative to the more complex ArcObjects library. ArcObjects is written in C#, Visual Basic, Java, or C++, all more difficult languages to learn than Python, but also much faster. In modern web mapping, ArcGIS Python scripts are now making their way onto the server, sometimes at the expense of application performance and stability.I have explored the idea of code performance vs. programmer productivity in the context of ArcGIS Server by writing several web-based geoprocessing services in both Python and C# ArcObjects. The goal was to identify the classes of tools which are best developed using one technology or the other, either based on performance or ease of development. From the outset, I made the assumption that under equal circumstances, it is easier to develop a service in Python, but that C# will always execute faster.The different geoprocessing services were divided into three categories: raster-based, vector-based, and server utilities. The services had different inputs and outputs ranging from text to polygons to zip files. Multi-Mechanize web performance and load testing framework was used to automate requests and make testing repeatable. Multi-Mechanize is an open source testing framework written in python which assisted in replaying requests, logging responses, and compiling statistics. Using this framework, I was able to make an assessment of the exact types of geoprocessing services which should be built using python, and which should be avoided.},
booktitle = {Proceedings of the 2nd International Conference on Computing for Geospatial Research &amp; Applications},
articleno = {56},
numpages = {1},
location = {Washington, DC, USA},
series = {COM.Geo '11}
}

@inproceedings{10.1145/2987550.2987579,
author = {Shin, Ji-Yong and Balakrishnan, Mahesh and Marian, Tudor and Szefer, Jakub and Weatherspoon, Hakim},
title = {Towards Weakly Consistent Local Storage Systems},
year = {2016},
isbn = {9781450345255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2987550.2987579},
doi = {10.1145/2987550.2987579},
abstract = {Heterogeneity is a fact of life for modern storage servers. For example, a server may spread terabytes of data across many different storage media, ranging from magnetic disks, DRAM, NAND-based solid state drives (SSDs), as well as hybrid drives that package various combinations of these technologies. It follows that access latencies to data can vary hugely depending on which media the data resides on. At the same time, modern storage systems naturally retain older versions of data due to the prevalence of log-structured designs and caches in software and hardware layers. In a sense, a contemporary storage system is very similar to a small-scale distributed system, opening the door to consistency/performance trade-offs. In this paper, we propose a class of local storage systems called StaleStores that support relaxed consistency, returning stale data for better performance. We describe several examples of StaleStores, and show via emulations that serving stale data can improve access latency by between 35% and 20X. We describe a particular StaleStore called Yogurt, a weakly consistent local block storage system. Depending on the application's consistency requirements (e.g. bounded staleness, mono-tonic reads, read-my-writes, etc.), Yogurt queries the access costs for different versions of data within tolerable staleness bounds and returns the fastest version. We show that a distributed key-value store running on top of Yogurt obtains a 6X speed-up for access latency by trading off consistency and performance within individual storage servers.},
booktitle = {Proceedings of the Seventh ACM Symposium on Cloud Computing},
pages = {294–306},
numpages = {13},
keywords = {Weak consistency, local storage},
location = {Santa Clara, CA, USA},
series = {SoCC '16}
}

@inproceedings{10.1145/1831708.1831726,
author = {Jiang, Zhen Ming},
title = {Automated Analysis of Load Testing Results},
year = {2010},
isbn = {9781605588230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1831708.1831726},
doi = {10.1145/1831708.1831726},
abstract = {Many software systems must be load tested to ensure that they can scale up while maintaining functional and performance requirements. Current industrial practices for checking the results of a load test remain ad hoc, involving high level checks. Few research efforts are devoted to the automated analysis of load testing results, mainly due to the limited access to large scale systems for use as case studies. Automated and systematic load testing analysis is going to be much needed, as many services have been offered online to an increasing number of users. This dissertation proposes automated approaches to detect functional and performance problems in a load test by mining the recorded load testing data (execution logs and performance metrics). Case studies show that our approaches scale well to large enterprise systems and output high precision results that help analysts detect load testing problems.},
booktitle = {Proceedings of the 19th International Symposium on Software Testing and Analysis},
pages = {143–146},
numpages = {4},
keywords = {dynamic analysis, load testing, software mining},
location = {Trento, Italy},
series = {ISSTA '10}
}

@article{10.1145/3178540,
author = {Anagnostopoulos, Evangelos and Emiris, Ioannis Z. and Psarros, Ioannis},
title = {Randomized Embeddings with Slack and High-Dimensional Approximate Nearest Neighbor},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1549-6325},
url = {https://doi.org/10.1145/3178540},
doi = {10.1145/3178540},
abstract = {Approximate nearest neighbor search (ϵ-ANN) in high dimensions has been mainly addressed by Locality Sensitive Hashing (LSH), which has complexity with polynomial dependence in dimension, sublinear query time, but subquadratic space requirement. We introduce a new “low-quality” embedding for metric spaces requiring that, for some query, there exists an approximate nearest neighbor among the pre-images of its k&gt; 1 approximate nearest neighbors in the target space. In Euclidean spaces, we employ random projections to a dimension inversely proportional to k.Our approach extends to the decision problem with witness of checking whether there exists an approximate near neighbor; this also implies a solution for ϵ-ANN. After dimension reduction, we store points in a uniform grid of side length ϵ /√ d′, where d′ is the reduced dimension. Given a query, we explore cells intersecting the unit ball around the query. This data structure requires linear space and query time in O(d nρ), ρ ≈ 1-ϵ2i&gt;/log(1ϵ), where n denotes input cardinality and d space dimension. Bounds are improved for doubling subsets via r-nets.We present our implementation for ϵ-ANN in C++ and experiments for d≤ 960, n≤ 106, using synthetic and real datasets, which confirm the theoretical analysis and, typically, yield better practical performance. We compare to FALCONN, the state-of-the-art implementation of multi-probe LSH: our prototype software is essentially comparable in terms of preprocessing, query time, and storage usage.},
journal = {ACM Trans. Algorithms},
month = apr,
articleno = {18},
numpages = {21},
keywords = {randomized embeddings, experimental study, doubling dimension, curse of dimensionality, Johnson-Lindenstrauss Lemma, Approximate nearest neighbor}
}

@inproceedings{10.1145/2854946.2854949,
author = {Selvarajah, Vinesha},
title = {Information Retrieval Behaviors among EBM Practitioners When Performing Evidence Based Medicine},
year = {2016},
isbn = {9781450337519},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2854946.2854949},
doi = {10.1145/2854946.2854949},
abstract = {This paper reveals the preliminary analysis of how EBM practitioners retrieve EBM related information during their clinical rounds at a public teaching hospital in Malaysia. We focused on studying EBM practitioners' searching behaviors during EBM process. However, in this paper we only present analysis for query issuing pattern and result viewing behavior of the retrieval process. We collected our preliminary data using Morae's Key-Logging software that captures participants' information searching behavior. We recorded 30 preliminary search sessions via convenience sampling and analyzed our data using descriptive statistics. Our findings indicate that 90% of the search sessions successfully returned results that the participants searched for. Among the queries issued, 24.47% were ineffective queries. Result viewing behaviors revealed 56 online sources were clicked where 33.33% belonged to PubMed, Medscape and UpToDate. Our result shows higher average query length and the number of medical terms used in queries compared to previous studies. We also found that result viewing behavior using multiple tabs confuses the participants and are more likely to cause overlooking of online resources on some of the opened tabs. The outcome of our study leads to better initiatives on query issuing and result viewing strategies for EBM related searches to improve the EBM process among practitioners.},
booktitle = {Proceedings of the 2016 ACM on Conference on Human Information Interaction and Retrieval},
pages = {369–371},
numpages = {3},
keywords = {nicu, search behaviors, interactive information retrieval, practitioners, evidence based medicine, malaysia},
location = {Carrboro, North Carolina, USA},
series = {CHIIR '16}
}

@inproceedings{10.1145/2527031.2527038,
author = {Smith, Ashley and O'Hara, Kieron and Lewis, Paul},
title = {Visualising the Past: Annotating a Life with Linked Open Data},
year = {2011},
isbn = {9781450308557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2527031.2527038},
doi = {10.1145/2527031.2527038},
abstract = {This paper introduces a novel lifelogging system. The novelty of our system is that it uses open linked data to reduce the burden on the user to an absolute minimum by combining the user's private logs with data obtained automatically from the web. This in turn is annotated with data generated by the latest version of the author's portable 'life annotation' software to generate a detailed, interactive, user-friendly visual representation of the user's life experiences with little or no manual work required. This paper covers several key parts of the system that make it work. Specifically, the analysis of linked data, entity comparison with known objects and events from the user's own logs in order to determine relevance, and careful annotation of the user's data ensuring that the data's provenance is stored alongside the annotations generated.},
booktitle = {Proceedings of the 3rd International Web Science Conference},
articleno = {16},
numpages = {8},
location = {Koblenz, Germany},
series = {WebSci '11}
}

@article{10.1145/2674632.2674642,
author = {Parashar, Anshu and Chhabra, Jitender Kumar},
title = {Measuring Change-Readiness of Classes by Mining Change-History},
year = {2014},
issue_date = {November 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/2674632.2674642},
doi = {10.1145/2674632.2674642},
abstract = {The change-history of a software system records how the system evolved over time. The change-history can tell which components (classes) are changed together, i.e. are change-coupled. In this paper, we propose two metrics for quantifying change-coupling among classes on the basis of their change-history and then propose another measure to compute a change-readiness index of the classes. The change-readiness of a class is measured as how much the class is ready to change. In this paper, we define change-history-based metrics, demonstrate computation of these measures by example and validate them theoretically. The SVNSearch subversion web-based utility has been used to mine the change logs of the EGit project of Eclipse. Further, change-readiness is computed for the classes of EGit and our findings are discussed. It has been found that the metrics based on change-history can be helpful to predict the future change behavior of the classes.},
journal = {SIGSOFT Softw. Eng. Notes},
month = dec,
pages = {1–5},
numpages = {5},
keywords = {Mining Software Repositories, Change-Coupling, Changeability}
}

@article{10.1145/3387909,
author = {Polyvyanyy, Artem and Solti, Andreas and Weidlich, Matthias and Ciccio, Claudio Di and Mendling, Jan},
title = {Monotone Precision and Recall Measures for Comparing Executions and Specifications of Dynamic Systems},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3387909},
doi = {10.1145/3387909},
abstract = {The behavioural comparison of systems is an important concern of software engineering research. For example, the areas of specification discovery and specification mining are concerned with measuring the consistency between a collection of execution traces and a program specification. This problem is also tackled in process mining with the help of measures that describe the quality of a process specification automatically discovered from execution logs. Though various measures have been proposed, it was recently demonstrated that they neither fulfil essential properties, such as monotonicity, nor can they handle infinite behaviour. In this article, we address this research problem by introducing a new framework for the definition of behavioural quotients. We prove that corresponding quotients guarantee desired properties that existing measures have failed to support. We demonstrate the application of the quotients for capturing precision and recall measures between a collection of recorded executions and a system specification. We use a prototypical implementation of these measures to contrast their monotonic assessment with measures that have been defined in prior research.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {17},
numpages = {41},
keywords = {coverage, fitness, entropy, precision, process mining, recall, System comparison, behavioural comparison, behavioural analysis, conformance checking}
}

@inproceedings{10.1145/2996758.2996764,
author = {Mirsky, Yisroel and Shabtai, Asaf and Rokach, Lior and Shapira, Bracha and Elovici, Yuval},
title = {SherLock vs Moriarty: A Smartphone Dataset for Cybersecurity Research},
year = {2016},
isbn = {9781450345736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2996758.2996764},
doi = {10.1145/2996758.2996764},
abstract = {In this paper we describe and share with the research community, a significant smartphone dataset obtained from an ongoing long-term data collection experiment. The dataset currently contains 10 billion data records from 30 users collected over a period of 1.6 years and an additional 20 users for 6 months (totaling 50 active users currently participating in the experiment).The experiment involves two smartphone agents: SherLock and Moriarty. SherLock collects a wide variety of software and sensor data at a high sample rate. Moriarty perpetrates various attacks on the user and logs its activities, thus providing labels for the SherLock dataset.The primary purpose of the dataset is to help security professionals and academic researchers in developing innovative methods of implicitly detecting malicious behavior in smartphones. Specifically, from data obtainable without superuser (root) privileges. To demonstrate possible uses of the dataset, we perform a basic malware analysis and evaluate a method of continuous user authentication.},
booktitle = {Proceedings of the 2016 ACM Workshop on Artificial Intelligence and Security},
pages = {1–12},
numpages = {12},
keywords = {machine learning, continuous authentication, malware, smartphone dataset, forensics, anomaly detection},
location = {Vienna, Austria},
series = {AISec '16}
}

@article{10.1145/3440757,
author = {Zhao, Guoliang and Hassan, Safwat and Zou, Ying and Truong, Derek and Corbin, Toby},
title = {Predicting Performance Anomalies in Software Systems at Run-Time},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3440757},
doi = {10.1145/3440757},
abstract = {High performance is a critical factor to achieve and maintain the success of a software system. Performance anomalies represent the performance degradation issues (e.g., slowing down in system response times) of software systems at run-time. Performance anomalies can cause a dramatically negative impact on users’ satisfaction. Prior studies propose different approaches to detect anomalies by analyzing execution logs and resource utilization metrics after the anomalies have happened. However, the prior detection approaches cannot predict the anomalies ahead of time; such limitation causes an inevitable delay in taking corrective actions to prevent performance anomalies from happening. We propose an approach that can predict performance anomalies in software systems and raise anomaly warnings in advance. Our approach uses a Long-Short Term Memory neural network to capture the normal behaviors of a software system. Then, our approach predicts performance anomalies by identifying the early deviations from the captured normal system behaviors. We conduct extensive experiments to evaluate our approach using two real-world software systems (i.e., Elasticsearch and Hadoop). We compare the performance of our approach with two baselines. The first baseline is one state-to-the-art baseline called Unsupervised Behavior Learning. The second baseline predicts performance anomalies by checking if the resource utilization exceeds pre-defined thresholds. Our results show that our approach can predict various performance anomalies with high precision (i.e., 97–100%) and recall (i.e., 80–100%), while the baselines achieve 25–97% precision and 93–100% recall. For a range of performance anomalies, our approach can achieve sufficient lead times that vary from 20 to 1,403 s (i.e., 23.4 min). We also demonstrate the ability of our approach to predict the performance anomalies that are caused by real-world performance bugs. For predicting performance anomalies that are caused by real-world performance bugs, our approach achieves 95–100% precision and 87–100% recall, while the baselines achieve 49–83% precision and 100% recall. The obtained results show that our approach outperforms the existing anomaly prediction approaches and is able to predict performance anomalies in real-world systems.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {33},
numpages = {33},
keywords = {LSTM neural network, Performance anomaly prediction, software systems}
}

@inproceedings{10.5555/2387880.2387910,
author = {Attariyan, Mona and Chow, Michael and Flinn, Jason},
title = {X-Ray: Automating Root-Cause Diagnosis of Performance Anomalies in Production Software},
year = {2012},
isbn = {9781931971966},
publisher = {USENIX Association},
address = {USA},
abstract = {Troubleshooting the performance of production software is challenging. Most existing tools, such as profiling, tracing, and logging systems, reveal what events occurred during performance anomalies. However, users of such toolsmust infer why these events occurred; e.g., that their execution was due to a root cause such as a specific input request or configuration setting. Such inference often requires source code and detailed application knowledge that is beyond system administrators and end users.This paper introduces performance summarization, a technique for automatically diagnosing the root causes of performance problems. Performance summarization instruments binaries as applications execute. It first attributes performance costs to each basic block. It then uses dynamic information flow tracking to estimate the likelihood that a block was executed due to each potential root cause. Finally, it summarizes the overall cost of each potential root cause by summing the per-block cost multiplied by the cause-specific likelihood over all basic blocks. Performance summarization can also be performed differentially to explain performance differences between two similar activities. X-ray is a tool that implements performance summarization. Our results show that X-ray accurately diagnoses 17 performance issues in Apache, lighttpd, Postfix, and PostgreSQL, while adding 2.3% average runtime overhead.},
booktitle = {Proceedings of the 10th USENIX Conference on Operating Systems Design and Implementation},
pages = {307–320},
numpages = {14},
location = {Hollywood, CA, USA},
series = {OSDI'12}
}

@article{10.1145/2700527,
author = {Whittinghill, David and Herring, Donald Grant},
title = {A Comparison of Text-Annotated vs. Purely Visio-Spatial Instructions for Video Game Input Sequences},
year = {2017},
issue_date = {Summer 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
url = {https://doi.org/10.1145/2700527},
doi = {10.1145/2700527},
abstract = {Animated tutorials for controller input sequences in video games will be easier for developers to localize, and therefore more universally accessible across multiple nationalities, if they are designed to rely on purely visio-spatial, text-free communication. However, it has hitherto been unclear whether a lack of text in such tutorials may be associated with lower effectiveness in overall teaching outcomes, given that text may augment the available information. With this in mind, we conducted a between-subjects study in which 42 subjects each played one of two versions of a custom, stand-alone game control tutorial designed to teach character moves for an action/fighting game; both versions contained animated visuals, but one also included onscreen text instructions. We recorded players’ performances in each version by measuring elapsed times, failure counts, demonstration replay counts, and skip counts using software-based logging and combined each player’s data in these categories to create an overall effectiveness index to measure and compare teaching efficacy associated with each version. We compared the means of our data between the version groups using t-tests with results suggesting that people playing the non-text-annotated version performed at least as well, all around, as those playing the text-annotated version and even better in the areas of elapsed time, failure count, and overall score. Though a significant co-factor—player skill/experience—is likely an influence that should be further delineated in future studies, our findings clearly demonstrated that text-free game control tutorials are as good as, if not better than, text-annotated ones.},
journal = {Comput. Entertain.},
month = apr,
articleno = {3},
numpages = {19},
keywords = {controller, training, annotations, teaching, tutorial, user interface, input device, visual spatial, Games}
}

@inproceedings{10.1145/3368691.3368734,
author = {Malkawe, Rabia and Qasaimeh, Malik and Ghanim, Firas and Ababneh, Mohammad},
title = {Toward an Early Assessment for Ransomware Attack Vulnerabilities},
year = {2019},
isbn = {9781450372848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368691.3368734},
doi = {10.1145/3368691.3368734},
abstract = {In 1989 the first extortion for currency using a Trojan called AIDS was logged in history, revealing a new threat to data and resources of individuals and organizations, globally known as ransomware. Ransomware is a type of malicious software that has the ability to duplicate itself on a network, affecting devices by locking access to these devices or encrypting data where only the attacker has the decryption key to restore this affected data. This gives the attacker the privilege to demand an amount of money, or a ransom, in order to send the decryption key to the victim, or unlock their device. This paper aims to propose a matrix of measures used to assess the vulnerability of IoT devices or systems to the threat of ransomware, which allows different entities to be prepared for this kind of attack, through shedding the light on ransomware as a major issue threatening today's modern life, from smart cars, smart homes, electronic health records, to planted heart pacemakers; by presenting some previous ransomware attack cases, presenting and discussing some of the current ransomware detection or prevention techniques.},
booktitle = {Proceedings of the Second International Conference on Data Science, E-Learning and Information Systems},
articleno = {43},
numpages = {7},
keywords = {IoT security, vulnerabilities mitigation, Ransomware attack, security assessment},
location = {Dubai, United Arab Emirates},
series = {DATA '19}
}

@inproceedings{10.1145/3302424.3303983,
author = {Wang, Xiaohao and Yuan, Yifan and Zhou, You and Coats, Chance C. and Huang, Jian},
title = {Project Almanac: A Time-Traveling Solid-State Drive},
year = {2019},
isbn = {9781450362818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302424.3303983},
doi = {10.1145/3302424.3303983},
abstract = {Preserving the history of storage states is critical to ensuring system reliability and security. It facilitates system functions such as debugging, data recovery, and forensics. Existing software-based approaches like data journaling, logging, and backups not only introduce performance and storage cost, but also are vulnerable to malware attacks, as adversaries can obtain kernel privileges to terminate or destroy them.In this paper, we present Project Almanac, which includes (1) a time-travel solid-state drive (SSD) named TimeSSD that retains a history of storage states in hardware for a window of time, and (2) a toolkit named TimeKits that provides storage-state query and rollback functions. TimeSSD tracks the history of storage states in the hardware device, without relying on explicit backups, by exploiting the property that the flash retains old copies of data when they are updated or deleted. We implement TimeSSD with a programmable SSD and develop TimeKits for several typical system applications. Experiments, with a variety of real-world case studies, demonstrate that TimeSSD can retain all the storage states for eight weeks, with negligible performance overhead, while providing the device-level time-travel property.},
booktitle = {Proceedings of the Fourteenth EuroSys Conference 2019},
articleno = {13},
numpages = {16},
keywords = {solid-state drive, firmware-isolated logging, time travel},
location = {Dresden, Germany},
series = {EuroSys '19}
}

@inproceedings{10.1145/1822327.1822331,
author = {No\"{e}l, Cyprien},
title = {Extensible Software Transactional Memory},
year = {2010},
isbn = {9781605589015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1822327.1822331},
doi = {10.1145/1822327.1822331},
abstract = {XSTM is a software transactional memory that can be extended by pluggable components. Extensions can access transactions read and write sets through an API, and process them e.g., for logging, change notification, state persistence or replication. This project explores ways to make memory transactions useful beyond thread synchronization. We describe in particular an application architecture enabled by extensions which aims to combine some of the strengths of shared state and of the Actor model. Shared state offers developers the modeling power of object orientation, and avoids the overhead of copying memory between components. The Actor model offers safety and composability when writing parallel and distributed applications. Our second design goal is to make memory transactions easy to reason about and safe to use. Opacity is achieved using a Multi Version Concurrency Control design where transactions are view-isolated, i.e., run in stable and consistent snapshots of the full memory. Transactions never encounter inconsistent data, do not abort while partially executed, and global progress is guaranteed. The programming model is further simplified by enforcing strong atomicity at the type system level, as transactional objects accessors require an ambient transaction. Finally, our design offers interesting performance characteristics by avoiding mutable shared state. Data is either mutable but private to a thread, or shared but immutable. This allows transactions to run without synchronization (no memory fence) between start and commit, which are themselves implemented in a lock-free way using O(1) memory fences and compare-and-swaps. We describe working implementations on the JVM and CLR for the STM and some extensions.},
booktitle = {Proceedings of the Third C* Conference on Computer Science and Software Engineering},
pages = {23–34},
numpages = {12},
keywords = {synchronization, composability, transactional memory, concurrent programming, object replication, software architecture},
location = {Montr\'{e}al, Quebec, Canada},
series = {C3S2E '10}
}

@inproceedings{10.1145/2901378.2901379,
author = {Lange, Patrick and Weller, Rene and Zachmann, Gabriel},
title = {GraphPool: A High Performance Data Management For 3D Simulations},
year = {2016},
isbn = {9781450337427},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901378.2901379},
doi = {10.1145/2901378.2901379},
abstract = {We present a new graph-based approach called GraphPool for the generation, management and distribution of simulation states for 3D simulation applications. Currently, relational databases are often used for this task in simulation applications. In contrast, our approach combines novel wait-free nested hash map techniques with traditional graphs which results in a schema-less, in-memory, highly efficient data management. Our GraphPool stores static and dynamic parts of a simulation model, distributes changes caused by the simulation and logs the simulation run. Even more, the GraphPool supports sophisticated query types of traditional relational databases. As a consequence, our GraphPool overcomes the associated drawbacks of relational database technology for sophisticated 3D simulation applications. Our GraphPool has several advantages compared to other state-of-the-art decentralized methods, such as persistence for simulation state over time, object identification, standardized interfaces for software components as well as a consistent world model for the overall simulation system. We tested our approach in a synthetic benchmark scenario but also in real-world use cases. The results show that it outperforms state-of-the-art relational databases by several orders of magnitude.},
booktitle = {Proceedings of the 2016 ACM SIGSIM Conference on Principles of Advanced Discrete Simulation},
pages = {23–33},
numpages = {11},
keywords = {graph database, nested hash maps, 3D simulation system, simulation database},
location = {Banff, Alberta, Canada},
series = {SIGSIM-PADS '16}
}

@inproceedings{10.1145/2854038.2854062,
author = {Hawkins, Byron and Demsky, Brian and Taylor, Michael B.},
title = {BlackBox: Lightweight Security Monitoring for COTS Binaries},
year = {2016},
isbn = {9781450337786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2854038.2854062},
doi = {10.1145/2854038.2854062},
abstract = { After a software system is compromised, it can be difficult to understand what vulnerabilities attackers exploited. Any information residing on that machine cannot be trusted as attackers may have tampered with it to cover their tracks. Moreover, even after an exploit is known, it can be difficult to determine whether it has been used to compromise a given machine. Aviation has long-used black boxes to better understand the causes of accidents, enabling improvements that reduce the likelihood of future accidents. Many attacks introduce abnormal control flows to compromise systems. In this paper, we present BlackBox, a monitoring system for COTS software. Our techniques enable BlackBox to efficiently monitor unexpected and potentially harmful control flow in COTS binaries. BlackBox constructs dynamic profiles of an application's typical control flows to filter the vast majority of expected control flow behavior, leaving us with a manageable amount of data that can be logged across the network to remote devices. Modern applications make extensive use of dynamically generated code, some of which varies greatly between executions. We introduce support for code generators that can detect security-sensitive behaviors while allowing BlackBox to avoid logging the majority of ordinary behaviors. We have implemented BlackBox in DynamoRIO. We evaluate the runtime overhead of BlackBox, and show that it can effectively monitor recent versions of Microsoft Office and Google Chrome. We show that in ROP, COOP, and state- of-the-art JIT injection attacks, BlackBox logs the pivotal actions by which the attacker takes control, and can also blacklist those actions to prevent repeated exploits. },
booktitle = {Proceedings of the 2016 International Symposium on Code Generation and Optimization},
pages = {261–272},
numpages = {12},
keywords = {Control Flow Integrity, Program Monitoring, Binary Rewriting, Dynamic Code Generation, Software Security},
location = {Barcelona, Spain},
series = {CGO '16}
}

@inproceedings{10.1145/1879141.1879202,
author = {Qiu, Tongqing and Ge, Zihui and Pei, Dan and Wang, Jia and Xu, Jun},
title = {What Happened in My Network: Mining Network Events from Router Syslogs},
year = {2010},
isbn = {9781450304832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1879141.1879202},
doi = {10.1145/1879141.1879202},
abstract = {Router syslogs are messages that a router logs to describe a wide range of events observed by it. They are considered one of the most valuable data sources for monitoring network health and for trou- bleshooting network faults and performance anomalies. However, router syslog messages are essentially free-form text with only a minimal structure, and their formats vary among different vendors and router OSes. Furthermore, since router syslogs are aimed for tracking and debugging router software/hardware problems, they are often too low-level from network service management perspectives. Due to their sheer volume (e.g., millions per day in a large ISP network), detailed router syslog messages are typically examined only when required by an on-going troubleshooting investigation or when given a narrow time range and a specific router under suspicion. Automated systems based on router syslogs on the other hand tend to focus on a subset of the mission critical messages (e.g., relating to network fault) to avoid dealing with the full diversity and complexity of syslog messages. In this project, we design a Sys-logDigest system that can automatically transform and compress such low-level minimally-structured syslog messages into meaningful and prioritized high-level network events, using powerful data mining techniques tailored to our problem domain. These events are three orders of magnitude fewer in number and have much better usability than raw syslog messages. We demonstrate that they provide critical input to network troubleshooting, and net- work health monitoring and visualization.},
booktitle = {Proceedings of the 10th ACM SIGCOMM Conference on Internet Measurement},
pages = {472–484},
numpages = {13},
keywords = {syslog, troubleshooting},
location = {Melbourne, Australia},
series = {IMC '10}
}

@article{10.1145/1924475.1924498,
author = {Liu, Jingjing},
title = {Personalizing Information Retrieval Using Task Stage, Topic Knowledge, and Task Products},
year = {2011},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {2},
issn = {0163-5840},
url = {https://doi.org/10.1145/1924475.1924498},
doi = {10.1145/1924475.1924498},
abstract = {Personalization of information retrieval tailors search towards individual users to meet their particular information needs by taking into account information about users and their contexts, often through implicit sources of evidence such as user behaviors and contextual factors. The current study looks particularly at users' dwelling behavior, measured by the time that they spend on documents; and several contextual factors: the stage of users' work tasks, task type, users' knowledge of task topics, to explore whether or not taking account of task stage, task type, and topic knowledge could help predict document usefulness from the time that users spend on the documents. This study also investigates whether or not expanding queries with important terms extracted from task products and useful pages improves search performance. To these ends, a controlled lab experiment was conducted with 24 student participants, each coming three times in a two-week period to work on three subtasks in a general work task. Data were collected by logging software that recorded user-system interaction and questionnaires that elicited users' background information and perceptions on a number of aspects. Observations in the study and examinations of the data found that the time users spent on documents could have three different types: total display time, total dwell time, and decision time, which had different roles in working as a reliable indicator of document usefulness. Task stage was found to help interpret certain types of time as reliable indicators of document usefulness in certain task types, so was topic knowledge, and the latter played a more significant role when both were available. This study contributes to a better understanding of how information seeking behaviors, specifically, time that users spend on documents, can be used as implicit evidence of document usefulness, as well as how contextual factors of task stage, topic knowledge, and task type can help interpret time as an indicator of usefulness. These findings have theoretical and practical implications for using behaviors and contextual factors in the development of personalization systems. Future studies are suggested on making use of these findings as well as research on related issues.},
journal = {SIGIR Forum},
month = jan,
pages = {87},
numpages = {1}
}

@inproceedings{10.1109/ISCA.2018.00042,
author = {Jun, Sang-Woo and Wright, Andy and Zhang, Sizhuo and Xu, Shuotao and Arvind},
title = {GraFboost: Using Accelerated Flash Storage for External Graph Analytics},
year = {2018},
isbn = {9781538659847},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2018.00042},
doi = {10.1109/ISCA.2018.00042},
abstract = {We describe GraFBoost, a flash-based architecture with hardware acceleration for external analytics of multi-terabyte graphs. We compare the performance of GraFBoost with 1 GB of DRAM against various state-of-the-art graph analytics software including FlashGraph, running on a 32-thread Xeon server with 128 GB of DRAM. We demonstrate that despite the relatively small amount of DRAM, GraFBoost achieves high performance with very large graphs no other system can handle, and rivals the performance of the fastest software platforms on sizes of graphs that existing platforms can handle. Unlike in-memory and semi-external systems, GraFBoost uses a constant amount of memory for all problems, and its performance decreases very slowly as graph sizes increase, allowing GraFBoost to scale to much larger problems than possible with existing systems while using much less resources on a single-node system.The key component of GraFBoost is the sort-reduce accelerator, which implements a novel method to sequentialize fine-grained random accesses to flash storage. The sort-reduce accelerator logs random update requests and then uses hardware-accelerated external sorting with interleaved reduction functions. GraFBoost also stores newly updated vertex values generated in each superstep of the algorithm lazily with the old vertex values to further reduce I/O traffic.We evaluate the performance of GraFBoost for PageRank, breadth-first search and betweenness centrality on our FPGA-based prototype (Xilinx VC707 with 1 GB DRAM and 1 TB flash) and compare it to other graph processing systems including a pure software implementation of GrapFBoost.},
booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
pages = {411–424},
numpages = {14},
keywords = {sort-reduce, hardware acceleration, FPGA, graph analytics, flash storage},
location = {Los Angeles, California},
series = {ISCA '18}
}

@inproceedings{10.1109/SST.2019.00015,
author = {Seiler, Marcus and H\"{u}bner, Paul and Paech, Barbara},
title = {Comparing Traceability through Information Retrieval, Commits, Interaction Logs, and Tags},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SST.2019.00015},
doi = {10.1109/SST.2019.00015},
abstract = {Traceability is used to follow and understand the relationships between various software engineering artifacts such as requirements and source code. Comprehensive traceability of software engineering artifacts is important to ensure that a software can be further developed or maintained. Traceability links are often created manually by using commit ids or retrospectively by using information retrieval (IR). [Question/Problem] However, creating traceability links manually is costly and it is error-prone in retrospect. As part of our ongoing research on feature management where traceability is also of interest, we use a lightweight tagging approach to relate artifacts. We want to investigate how such a tag-based approach compares to approaches using commit ids, interaction logs (IL), and IR for creating traceability links. [Principal ideas/results] We conducted a case study in which students applied the tag-based, the IL-based, and the commit-based approach. We transformed the tags to traceability links and compared them with the commit-based and IL-based approach as well as with IR-based approaches, using a gold standard. We applied different improvements. [Contribution] This is the first study comparing four completely different traceability approaches in one project. The combination of IL and commit ids shows the best precision and recall but is intrusive. The other approaches differ less in precision and recall and both need improvement for practical application. We discuss the benefits and drawbacks of the different approaches and state implications for research and practice.},
booktitle = {Proceedings of the 10th International Workshop on Software and Systems Traceability},
pages = {21–28},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {SST '19}
}

@inproceedings{10.1145/2835022.2835038,
author = {Hurvitz, Philip M.},
title = {GPS and Accelerometer Time Stamps: Proper Data Handling and Avoiding Pitfalls},
year = {2015},
isbn = {9781450339735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2835022.2835038},
doi = {10.1145/2835022.2835038},
abstract = {Introduction.Maintaining correct time stamp values is critical for management and analysis of time-indexed data, including GPS and accelerometry.Methods.Objective location and activity data were collected for one week periods in four projects, for 1551 subjects, using Qstarz BT1000 GPS data loggers and Actigraph GT3X accelerometers, in King County, WA. Data collection spanned September, 2011 to April, 2014, with analysis in May, 2014. GPS data were exported using QTravel and Actilife software, and processed under Windows XP and 7, using Microsoft Excel and Access, as well as two free and open-source applications, the R statistical programming environment and the database package, PostgreSQL.Results and conclusion.Time stamp errors resulted from the export of GPS data from QTravel and accelerometry data from Actilife for files spanning the Daylight Savings Time (DST) transition. QTravel has a specific problem in its interaction with Windows' default time zone settings. Excel and Access did not properly handle either time zone or DST, whereas both R and PostgreSQL did. Practices for preventing time stamp errors from Actigraph accelerometers and Qstarz GPS units are presented.},
booktitle = {Proceedings of the 1st International ACM SIGSPATIAL Workshop on Smart Cities and Urban Analytics},
pages = {94–100},
numpages = {7},
keywords = {time zone, Actigraph, GPS, Qstarz, accelerometry, daylight savings time},
location = {Bellevue, WA, USA},
series = {UrbanGIS'15}
}

@article{10.1145/3446391,
author = {Izadpanah, Ramin and Peterson, Christina and Solihin, Yan and Dechev, Damian},
title = {PETRA: Persistent Transactional Non-Blocking Linked Data Structures},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1544-3566},
url = {https://doi.org/10.1145/3446391},
doi = {10.1145/3446391},
abstract = {Emerging byte-addressable Non-Volatile Memories (NVMs) enable persistent memory where process state can be recovered after crashes. To enable applications to rely on persistent data, durable data structures with failure-atomic operations have been proposed. However, they lack the ability to allow users to execute a sequence of operations as transactions. Meanwhile, persistent transactional memory (PTM) has been proposed by adding durability to Software Transactional Memory (STM). However, PTM suffers from high performance overheads and low scalability due to false aborts, logging, and ordering constraints on persistence.In this article, we propose PETRA, a new approach for constructing persistent transactional linked data structures. PETRA natively supports transactions, but unlike PTM, relies on the high-level information from the data structure semantics. This gives PETRA unique advantages in the form of high performance and high scalability. Our experimental results using various benchmarks demonstrate the scalability of PETRA in all workloads and transaction sizes. PETRA outperforms the state-of-the-art PTMs by an order of magnitude in transactions of size greater than one, and demonstrates superior performance in transactions of size one.},
journal = {ACM Trans. Archit. Code Optim.},
month = mar,
articleno = {23},
numpages = {26},
keywords = {transactional data structure, non-volatile memory, concurrency, durability, non-blocking data structure, Persistent memory}
}

@inproceedings{10.1145/3217871.3217872,
author = {Brown, Andy and Tuor, Aaron and Hutchinson, Brian and Nichols, Nicole},
title = {Recurrent Neural Network Attention Mechanisms for Interpretable System Log Anomaly Detection},
year = {2018},
isbn = {9781450358651},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3217871.3217872},
doi = {10.1145/3217871.3217872},
abstract = {Deep learning has recently demonstrated state-of-the art performance on key tasks related to the maintenance of computer systems, such as intrusion detection, denial of service attack detection, hardware and software system failures, and malware detection. In these contexts, model interpretability is vital for administrator and analyst to trust and act on the automated analysis of machine learning models. Deep learning methods have been criticized as black box oracles which allow limited insight into decision factors. In this work we seek to bridge the gap between the impressive performance of deep learning models and the need for interpretable model introspection. To this end we present recurrent neural network (RNN) language models augmented with attention for anomaly detection in system logs. Our methods are generally applicable to any computer system and logging source. By incorporating attention variants into our RNN language models we create opportunities for model introspection and analysis without sacrificing state-of-the art performance. We demonstrate model performance and illustrate model interpretability on an intrusion detection task using the Los Alamos National Laboratory (LANL) cyber security dataset, reporting upward of 0.99 area under the receiver operator characteristic curve despite being trained only on a single day's worth of data.},
booktitle = {Proceedings of the First Workshop on Machine Learning for Computing Systems},
articleno = {1},
numpages = {8},
keywords = {Recurrent Neural Networks, Anomaly detection, Online Training, Attention, Interpretable Machine Learning, System Log Analysis},
location = {Tempe, AZ, USA},
series = {MLCS'18}
}

@inproceedings{10.1145/3323165.3323201,
author = {Alistarh, Dan and Nadiradze, Giorgi and Koval, Nikita},
title = {Efficiency Guarantees for Parallel Incremental Algorithms under Relaxed Schedulers},
year = {2019},
isbn = {9781450361842},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323165.3323201},
doi = {10.1145/3323165.3323201},
abstract = {Several classic problems in graph processing and computational geometry are solved via incremental algorithms, which split computation into a series of small tasks acting on shared state, which gets updated progressively. While the sequential variant of such algorithms usually specifies a fixed (but sometimes random) order in which the tasks should be performed, a standard approach to parallelizing such algorithms is to relax this constraint to allow for out-of-order parallel execution. This is the case for parallel implementations of Dijkstra's single-source shortest-paths (SSSP) algorithm, and for parallel Delaunay mesh triangulation. While many software frameworks parallelize incremental computation in this way, it is still not well understood whether this relaxed ordering approach can still provide any complexity guarantees. In this paper, we address this problem, and analyze the efficiency guarantees provided by a range of incremental algorithms when parallelized via relaxed schedulers. We show that, for algorithms such as Delaunay mesh triangulation and sorting by insertion, schedulers with a maximum relaxation factor of k in terms of the maximum priority inversion allowed will introduce a maximum amount of wasted work of O(\l{}og n poly(k)), where n is the number of tasks to be executed. For SSSP, we show that the additional work is O(poly(k), dmax / wmin), where dmax is the maximum distance between two nodes, and wmin is the minimum such distance. In practical settings where n &gt;&gt; k, this suggests that the overheads of relaxation will be outweighed by the improved scalability of the relaxed scheduler. On the negative side, we provide lower bounds showing that certain algorithms will inherently incur a non-trivial amount of wasted work due to scheduler relaxation, even for relatively benign relaxed schedulers.},
booktitle = {The 31st ACM Symposium on Parallelism in Algorithms and Architectures},
pages = {145–154},
numpages = {10},
keywords = {shortest path algorithms, incremental algorithms, relaxed data structures},
location = {Phoenix, AZ, USA},
series = {SPAA '19}
}

@inproceedings{10.1145/2000172.2000180,
author = {Kim, Youngil and Taylor, Keith and Dunbar, Carson and Walker, Brenton and Mundur, Padma},
title = {Reality vs Emulation: Running Real Mobility Traces on a Mobile Wireless Testbed},
year = {2011},
isbn = {9781450307420},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000172.2000180},
doi = {10.1145/2000172.2000180},
abstract = {Laboratory-based mobile wireless testbeds such as MeshTest and the CMU Wireless Emulator are powerful platforms that allow users to perform controlled, repeatable, mobile wireless experiments in the lab. Unfortunately such systems can only accommodate 10-15 nodes in an experiment. We have designed and built a scalable wireless testbed that uses software virtualization and live migration to facilitate experiments involving intermittently connected networks with many multiples of the number of physical nodes available on such a testbed.In this paper, we share our experience with using real traces from the DieselNet/DOME project on our testbed. While this trace provide GPS coordinates for where the contact occurs, we needed to deduce the overall mobility from using GPS logs for individual buses. Replicating the bus mobility on our testbed we recreate contact events and compare those to DOME traces. We also survey other traces we can use on VMT and evaluate how challenging they would be to run on the system.},
booktitle = {Proceedings of the 3rd ACM International Workshop on MobiArch},
pages = {23–28},
numpages = {6},
keywords = {performance analysis, delay-tolerant networks, traces, mobile wireless testbeds},
location = {Bethesda, Maryland, USA},
series = {HotPlanet '11}
}

@inproceedings{10.1145/2380116.2380130,
author = {Chi, Pei-Yu and Ahn, Sally and Ren, Amanda and Dontcheva, Mira and Li, Wilmot and Hartmann, Bj\"{o}rn},
title = {MixT: Automatic Generation of Step-by-Step Mixed Media Tutorials},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380130},
doi = {10.1145/2380116.2380130},
abstract = {Users of complex software applications often learn concepts and skills through step-by-step tutorials. Today, these tutorials are published in two dominant forms: static tutorials composed of images and text that are easy to scan, but cannot effectively describe dynamic interactions; and video tutorials that show all manipulations in detail, but are hard to navigate. We hypothesize that a mixed tutorial with static instructions and per-step videos can combine the benefits of both formats. We describe a comparative study of static, video, and mixed image manipulation tutorials with 12 participants and distill design guidelines for mixed tutorials. We present MixT, a system that automatically generates step-by-step mixed media tutorials from user demonstrations. MixT segments screencapture video into steps using logs of application commands and input events, applies video compositing techniques to focus on salient infor-mation, and highlights interactions through mouse trails. Informal evaluation suggests that automatically generated mixed media tutorials were as effective in helping users complete tasks as tutorials that were created manually.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {93–102},
numpages = {10},
keywords = {video, instructions, software tutorials},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2578153.2578211,
author = {Busjahn, Teresa and Bednarik, Roman and Schulte, Carsten},
title = {What Influences Dwell Time during Source Code Reading? Analysis of Element Type and Frequency as Factors},
year = {2014},
isbn = {9781450327510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2578153.2578211},
doi = {10.1145/2578153.2578211},
abstract = {While knowledge about reading behavior in natural-language text is abundant, little is known about the visual attention distribution when reading source code of computer programs. Yet, this knowledge is important for teaching programming skills as well as designing IDEs and programming languages. We conducted a study in which 15 programmers with various expertise read short source codes and recorded their eye movements. In order to study attention distribution on code elements, we introduced the following procedure: First we (pre)-processed the eye movement data using log-transformation. Taking into account the word lengths, we then analyzed the time spent on different lexical elements. It shows that most attention is oriented towards understanding of identifiers, operators, keywords and literals, relatively little reading time is spent on separators. We further inspected the attention on keywords and provide a description of the gaze on these primary building blocks for any formal language. The analysis indicates that approaches from research on natural-language text reading can be applied to source code as well, however not without review.},
booktitle = {Proceedings of the Symposium on Eye Tracking Research and Applications},
pages = {335–338},
numpages = {4},
keywords = {code reading, program comprehension, eye tracking},
location = {Safety Harbor, Florida},
series = {ETRA '14}
}

@inproceedings{10.1145/3403782.3403803,
author = {Liu, Huaiquan and Chen, Yunzhi and Zhu, Xing and Zhang, Mengzhi and Li, Wen},
title = {Identification of Key Genes and Pathways Associated with Chronic Obstructive Pulmonary Disease Based on Bioinformatics Analysis},
year = {2020},
isbn = {9781450377096},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3403782.3403803},
doi = {10.1145/3403782.3403803},
abstract = {Objective: Chronic obstructive pulmonary disease (COPD) is recognized as a common preventable respiratory disease whose character is incompletely reversible airflow limitation. However, its specific pathogenesis is still not fully understood. Here we sought to identify novel biomarkers or targets for COPD.Methods: We performed expression profiling through gene arrays contained lung tissue data of 14 COPD patients and 5 non-smokers from the Gene Expression Omnibus (GEO) database. The differentially expressed genes (DEGs) are identified by using the GEO2R tool with standard of the |log(FC)|&gt;2 and P&lt;0.05. Then the DAVID 6.8 databases were used to do the Gene Ontology (GO) enrichment analysis and Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway analysis of the DEGs. The String 11.0 database was applied for constructing the protein-protein interaction (PPI) network. We used Cytoscape 3.6.0 software to create a PPI interaction module.Results: A total of 133 DEGs were discovered from GEO2R, including 78 up-regulated genes and 55 down-regulated genes. GO enrichment analysis showed that up-regulated genes were involved in cell chemotaxis, cytokine activity and external stimuli response, the down-regulated genes had influenced on membrane organization and neurogenesis, and the all DEGs were mainly involved in cell chemotaxis, cytokine activity, external stimuli response and response to bacterium. KEGG pathway analysis showed that up-regulated genes were mainly involved in Toll-like receptors(TLR), phosphoinositide-3-kinase/protein kinase (PI3K-Akt), fork head box class O (FoxO) and tumor necrosis factor (TNF), and down-regulated genes mainly had influenced on estrogen and cell adhesion molecules (CAMs) signaling pathways, and all DEGs were mainly involved in interaction with cytokine and cytokine receptor, Chemokine and IL-17 signaling pathway. A total of 4 hub genes (degree≥9) were obtained from the PPI network, including interleukin6 (IL6), CC chemokine ligand 20 (CCL20), secreted phosphoprotein 1 (SPP1) and epithelial neutrophil activating peptide-78 (CXCL5). When we used the MCODE plug-in in Cytoscape 3.6.0 software, we found an important gene group. Through KEGG analysis of those gene groups, there are signaling pathways stated out, including immune response, inflammatory response, reninangiotensin system, tyrosine kinase-signal transducers and transcriptional activators (Jak-STAT), phagosome and others.Conclusion: This study indicates that IL6, CCL20, SPP1, CXCL5 and the pathways involved were crucial to the COPD pathophysiology. These key genes and pathways are valuable for further investigations. This study provides a better understanding of the gene expression profile in COPD pathophysiology.},
booktitle = {Proceedings of the Fourth International Conference on Biological Information and Biomedical Engineering},
articleno = {21},
numpages = {7},
keywords = {Differentially expressed genes, Chronic obstructive pulmonary disease, Bioinformatics},
location = {Chengdu, China},
series = {BIBE2020}
}

@inproceedings{10.1145/3123939.3124552,
author = {Ding, Caiwen and Liao, Siyu and Wang, Yanzhi and Li, Zhe and Liu, Ning and Zhuo, Youwei and Wang, Chao and Qian, Xuehai and Bai, Yu and Yuan, Geng and Ma, Xiaolong and Zhang, Yipeng and Tang, Jian and Qiu, Qinru and Lin, Xue and Yuan, Bo},
title = {C<span class="smallcaps SmallerCapital">ir</span>CNN: Accelerating and Compressing Deep Neural Networks Using Block-Circulant Weight Matrices},
year = {2017},
isbn = {9781450349529},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123939.3124552},
doi = {10.1145/3123939.3124552},
abstract = {Large-scale deep neural networks (DNNs) are both compute and memory intensive. As the size of DNNs continues to grow, it is critical to improve the energy efficiency and performance while maintaining accuracy. For DNNs, the model size is an important factor affecting performance, scalability and energy efficiency. Weight pruning achieves good compression ratios but suffers from three drawbacks: 1) the irregular network structure after pruning, which affects performance and throughput; 2) the increased training complexity; and 3) the lack of rigirous guarantee of compression ratio and inference accuracy.To overcome these limitations, this paper proposes CirCNN, a principled approach to represent weights and process neural networks using block-circulant matrices. CirCNN utilizes the Fast Fourier Transform (FFT)-based fast multiplication, simultaneously reducing the computational complexity (both in inference and training) from O(n2) to O(n log n) and the storage complexity from O(n2) to O(n), with negligible accuracy loss. Compared to other approaches, CirCNN is distinct due to its mathematical rigor: the DNNs based on CirCNN can converge to the same "effectiveness" as DNNs without compression. We propose the CirCNN architecture, a universal DNN inference engine that can be implemented in various hardware/software platforms with configurable network architecture (e.g., layer type, size, scales, etc.). In CirCNN architecture: 1) Due to the recursive property, FFT can be used as the key computing kernel, which ensures universal and small-footprint implementations. 2) The compressed but regular network structure avoids the pitfalls of the network pruning and facilitates high performance and throughput with highly pipelined and parallel design. To demonstrate the performance and energy efficiency, we test CirCNN in FPGA, ASIC and embedded processors. Our results show that CirCNN architecture achieves very high energy efficiency and performance with a small hardware footprint. Based on the FPGA implementation and ASIC synthesis results, CirCNN achieves 6 - 102X energy efficiency improvements compared with the best state-of-the-art results.},
booktitle = {Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {395–408},
numpages = {14},
keywords = {FPGA, acceleration, compression, block-circulant matrix, deep learning},
location = {Cambridge, Massachusetts},
series = {MICRO-50 '17}
}

@inproceedings{10.1145/2034773.2034783,
author = {Shivers, Olin and Turon, Aaron J.},
title = {Modular Rollback through Control Logging: A Pair of Twin Functional Pearls},
year = {2011},
isbn = {9781450308656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2034773.2034783},
doi = {10.1145/2034773.2034783},
abstract = {We present a technique, based on the use of first-class control operators, enabling programs to maintain and invoke rollback logs for sequences of reversible effects. Our technique is modular, in that it provides complete separation between some library of effectful operations, and a client, "driver" program which invokes and rolls back sequences of these operations. In particular, the checkpoint mechanism, which is entirely encapsulated within the effect library, logs not only the library's effects, but also the client's control state. Thus, logging and rollback can be almost completely transparent to the client code.This separation of concerns manifests itself nicely when we must implement software with sophisticated error handling. We illustrate with two examples that exploit the architecture to disentangle some core parsing task from its error management. The parser code is completely separate from the error-correction code, although the two components are deeply intertwined at run time.},
booktitle = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming},
pages = {58–68},
numpages = {11},
keywords = {error repair, checkpoint, delimited control},
location = {Tokyo, Japan},
series = {ICFP '11}
}

@article{10.1145/2034574.2034783,
author = {Shivers, Olin and Turon, Aaron J.},
title = {Modular Rollback through Control Logging: A Pair of Twin Functional Pearls},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {9},
issn = {0362-1340},
url = {https://doi.org/10.1145/2034574.2034783},
doi = {10.1145/2034574.2034783},
abstract = {We present a technique, based on the use of first-class control operators, enabling programs to maintain and invoke rollback logs for sequences of reversible effects. Our technique is modular, in that it provides complete separation between some library of effectful operations, and a client, "driver" program which invokes and rolls back sequences of these operations. In particular, the checkpoint mechanism, which is entirely encapsulated within the effect library, logs not only the library's effects, but also the client's control state. Thus, logging and rollback can be almost completely transparent to the client code.This separation of concerns manifests itself nicely when we must implement software with sophisticated error handling. We illustrate with two examples that exploit the architecture to disentangle some core parsing task from its error management. The parser code is completely separate from the error-correction code, although the two components are deeply intertwined at run time.},
journal = {SIGPLAN Not.},
month = sep,
pages = {58–68},
numpages = {11},
keywords = {delimited control, error repair, checkpoint}
}

@inproceedings{10.1145/2382456.2382505,
author = {Felthousen, Mat},
title = {Reducing Costs, Improving Service, and Extending the Life of Computers with Solid-State Drives},
year = {2012},
isbn = {9781450314947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2382456.2382505},
doi = {10.1145/2382456.2382505},
abstract = {Due to compliance concerns, during the summer of 2011 the University of Rochester began requiring that all individuals authenticate in order to use the network. Benchmarking with other institutions indicated that Windows 7 took two to three minutes from the time someone logged on to the point the machine was fully responsive and a browser could be used. We had optimized the machines such that the time had been reduced to about a minute and a half, but students complained about how long the logon process took. At the same time, budget constraints forced extending the three-year replacement cycle for computers, and the older machines were both slower and less reliable, usually due to hard drive failures. Student leaders indicated that they needed strategically placed computers, referred to as 'kiosks', to be optimized for logon time so that students could have immediate access to email and the web between classes. They would also accept a reduced suite of applications on kiosks if it meant faster logon times. There are less than twenty kiosks across campus, compared to more than 550 public machines total. Kiosks are ideal for evaluating experimental software or hardware, as changes made on them would not affect classes or other production uses. We installed solid state drives in kiosks with a minimal suite of applications. The time from logon to having a fully functional browser dropped to fifteen seconds, and the performance of our oldest PCs exceeded a new PC with a regular hard drive. The lifespan of the PCs could potentially be extended by years. Feedback has been so positive that we are now evaluating switching all machines to solid state technology. This paper will discuss the results of our performance testing of solid state drives, including the performance gains in all types of applications, and the costs involved; both in implementing solid state drives on already deployed PCs, and avoided due to not having to replace PCs as often.},
booktitle = {Proceedings of the 40th Annual ACM SIGUCCS Conference on User Services},
pages = {199–202},
numpages = {4},
keywords = {life cycle management, solid state drive},
location = {Memphis, Tennessee, USA},
series = {SIGUCCS '12}
}

@inproceedings{10.1109/ICSE-SEIP.2017.12,
author = {Wu, Jingzheng and Liu, Shen and Ji, Shouling and Yang, Mutian and Luo, Tianyue and Wu, Yanjun and Wang, Yongji},
title = {Exception beyond Exception: Crashing Android System by Trapping in "UncaughtException"},
year = {2017},
isbn = {9781538627174},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP.2017.12},
doi = {10.1109/ICSE-SEIP.2017.12},
abstract = {Android is characterized as a complicated open source software stack created for a wide array of phones with different form of factors, whose latest release has over one hundred million lines of code. Such code is mainly developed with the Java language, which builds complicated logic and brings implicit information flows among components and the inner framework. By studying the source code of system service interfaces, we discovered an unknown type of code flaw, which is named uncaughtException flaw, caused by unwell implemented exceptions that could crash the system and be further vulnerable to system level Denial-of-Service (DoS) attacks. We found that exceptions are used to handle the errors and other exceptional events but sometimes they would kill some critical system services exceptionally. We designed and implemented ExHunter, a new tool for automatic detection of this uncaughtException flaw by dynamically reflecting service interfaces, continuously fuzzing parameters and verifying the running logs. On 11 new popular Android phones, ExHunter extracted 1045 system services, reflected 758 suspicious functions, discovered 132 uncaughtException flaws which have never been known before and generated 275 system DoS attack exploitations. The results showed that: (1) almost every type of Android phone suffers from this flaw; (2) the flaws are different from phone by phone; and (3) all the vulnerabilities can be exploited by direct/indirect trapping. To mitigate uncaughtException flaws, we further developed ExCatcher to re-catch the exceptions. Finally, we informed four leading Android phones manufactures and provided secure improvements in their commercial phones.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering: Software Engineering in Practice Track},
pages = {283–292},
numpages = {10},
keywords = {DoS attack, Android system service, exception, vulnerability},
location = {Buenos Aires, Argentina},
series = {ICSE-SEIP '17}
}

@inproceedings{10.1145/2814270.2814292,
author = {Biswas, Swarnendu and Zhang, Minjia and Bond, Michael D. and Lucia, Brandon},
title = {Valor: Efficient, Software-Only Region Conflict Exceptions},
year = {2015},
isbn = {9781450336895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814270.2814292},
doi = {10.1145/2814270.2814292},
abstract = { Data races complicate programming language semantics, and a data race is often a bug. Existing techniques detect data races and define their semantics by detecting conflicts between synchronization-free regions (SFRs). However, such techniques either modify hardware or slow programs dramatically, preventing always-on use today. This paper describes Valor, a sound, precise, software-only region conflict detection analysis that achieves high performance by eliminating the costly analysis on each read operation that prior approaches require. Valor instead logs a region's reads and lazily detects conflicts for logged reads when the region ends. As a comparison, we have also developed FastRCD, a conflict detector that leverages the epoch optimization strategy of the FastTrack data race detector. We evaluate Valor, FastRCD, and FastTrack, showing that Valor dramatically outperforms FastRCD and FastTrack. Valor is the first region conflict detector to provide strong semantic guarantees for racy program executions with under 2X slowdown. Overall, Valor advances the state of the art in always-on support for strong behavioral guarantees for data races. },
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {241–259},
numpages = {19},
keywords = {dynamic analysis, region serializability, conflict exceptions, data races},
location = {Pittsburgh, PA, USA},
series = {OOPSLA 2015}
}

@article{10.1145/2858965.2814292,
author = {Biswas, Swarnendu and Zhang, Minjia and Bond, Michael D. and Lucia, Brandon},
title = {Valor: Efficient, Software-Only Region Conflict Exceptions},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/2858965.2814292},
doi = {10.1145/2858965.2814292},
abstract = { Data races complicate programming language semantics, and a data race is often a bug. Existing techniques detect data races and define their semantics by detecting conflicts between synchronization-free regions (SFRs). However, such techniques either modify hardware or slow programs dramatically, preventing always-on use today. This paper describes Valor, a sound, precise, software-only region conflict detection analysis that achieves high performance by eliminating the costly analysis on each read operation that prior approaches require. Valor instead logs a region's reads and lazily detects conflicts for logged reads when the region ends. As a comparison, we have also developed FastRCD, a conflict detector that leverages the epoch optimization strategy of the FastTrack data race detector. We evaluate Valor, FastRCD, and FastTrack, showing that Valor dramatically outperforms FastRCD and FastTrack. Valor is the first region conflict detector to provide strong semantic guarantees for racy program executions with under 2X slowdown. Overall, Valor advances the state of the art in always-on support for strong behavioral guarantees for data races. },
journal = {SIGPLAN Not.},
month = oct,
pages = {241–259},
numpages = {19},
keywords = {region serializability, dynamic analysis, data races, conflict exceptions}
}

@inproceedings{10.1145/2462456.2483967,
author = {LiKamWa, Robert and Liu, Yunxin and Lane, Nicholas D. and Zhong, Lin},
title = {MoodScope: Building a Mood Sensor from Smartphone Usage Patterns},
year = {2013},
isbn = {9781450316729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2462456.2483967},
doi = {10.1145/2462456.2483967},
abstract = {We present MoodScope, a software system which infers the mood of its user based on how the smartphone is used. Similar to smartphone sensors that measure acceleration, light, and other physical properties, MoodScope is a "sensor" that measures the mental state of the user and provides mood as an important input to context-aware computing. We run a formative statistical study with smartphone-logged data collected from 32 participants over two months. Through the study, we find that by analyzing communication history and application usage patterns, we can statistically infer a user's daily mood average with an accuracy of 93% after a two-month training period. Motivated by these results, we build a service, MoodScope, which analyzes usage history to act as a sensor of the user's mood.},
booktitle = {Proceeding of the 11th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {465–466},
numpages = {2},
keywords = {affective computing, machine learning, mobile systems, smartphone usage, mood},
location = {Taipei, Taiwan},
series = {MobiSys '13}
}

@inproceedings{10.1145/3373376.3378483,
author = {Krishnan, R. Madhava and Kim, Jaeho and Mathew, Ajit and Fu, Xinwei and Demeri, Anthony and Min, Changwoo and Kannan, Sudarsun},
title = {Durable Transactional Memory Can Scale with Timestone},
year = {2020},
isbn = {9781450371025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373376.3378483},
doi = {10.1145/3373376.3378483},
abstract = {Non-volatile main memory (NVMM) technologies promise byte addressability and near-DRAM access that allows developers to build persistent applications with common load and store instructions. However, it is difficult to realize these promises because NVMM software should also provide crash consistency while providing high performance, and scalability. Durable transactional memory (DTM) systems address these challenges. However, none of them scale beyond 16 cores. The poor scalability either stems from the underlying STM layer or from employing limited write parallelism (single writer or dual version). In addition, other fundamental issues with guaranteeing crash consistency are high write amplification and memory footprint in existing approaches. To address these challenges, we propose TimeStone: a highly scalable DTM system with low write amplification and minimal memory footprint. TimeStone uses a novel multi-layered hybrid logging technique, called TOC logging, to guarantee crash consistency. Also, TimeStone further relies on Multi-Version Concurrency Control (MVCC) mechanism to achieve high scalability and to support different isolation levels on the same data set. Our evaluation of TimeStone against the state-of-the-art DTM systems shows that it significantly outperforms other systems for a wide range of workloads with varying data-set size and contention levels, up to 112 hardware threads. In addition, with our TOC logging, TimeStone achieves a write amplification of less than 1, while existing DTM systems suffer from 2\texttimes{}-6\texttimes{} overhead.},
booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {335–349},
numpages = {15},
keywords = {logging, transactional memory, write amplification, multi-version, scalability},
location = {Lausanne, Switzerland},
series = {ASPLOS '20}
}

@inproceedings{10.1145/1879211.1879247,
author = {Maheswara, Gowritharan and Bradbury, Jeremy S. and Collins, Christopher},
title = {TIE: An Interactive Visualization of Thread Interleavings},
year = {2010},
isbn = {9781450300285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1879211.1879247},
doi = {10.1145/1879211.1879247},
abstract = {Multi-core processors have become increasingly prevalent, driving a software shift toward concurrent programs which best utilize these processors. Testing and debugging concurrent programs is difficult due to the many different ways threads can interleave. One solution to testing concurrent software is to use tools, such as NASA's Java PathFinder (JPF), to explore the thread interleaving space. Although tools such as JPF provide comprehensive data about program errors, the data is generally in the form of bulk text logs, which provide little support for common analysis tasks, such as finding common and rare error states. In this paper, we present an interactive visualization tool, TIE, that integrates with JPF to enhance concurrency testing and debugging.},
booktitle = {Proceedings of the 5th International Symposium on Software Visualization},
pages = {215–216},
numpages = {2},
keywords = {debugging, concurrency, testing, model checking, visualization},
location = {Salt Lake City, Utah, USA},
series = {SOFTVIS '10}
}

@inproceedings{10.1145/2462456.2464449,
author = {LiKamWa, Robert and Liu, Yunxin and Lane, Nicholas D. and Zhong, Lin},
title = {MoodScope: Building a Mood Sensor from Smartphone Usage Patterns},
year = {2013},
isbn = {9781450316729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2462456.2464449},
doi = {10.1145/2462456.2464449},
abstract = {We report a first-of-its-kind smartphone software system, MoodScope, which infers the mood of its user based on how the smartphone is used. Compared to smartphone sensors that measure acceleration, light, and other physical properties, MoodScope is a "sensor" that measures the mental state of the user and provides mood as an important input to context-aware computing. We run a formative statistical mood study with smartphone-logged data collected from 32 participants over two months. Through the study, we find that by analyzing communication history and application usage patterns, we can statistically infer a user's daily mood average with an initial accuracy of 66%, which gradu-ally improves to an accuracy of 93% after a two-month personal-ized training period. Motivated by these results, we build a service, MoodScope, which analyzes usage history to act as a sensor of the user's mood. We provide a MoodScope API for developers to use our system to create mood-enabled applications. We further create and deploy a mood-sharing social application.},
booktitle = {Proceeding of the 11th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {389–402},
numpages = {14},
keywords = {mobile systems, smartphone usage, affective computing, mood, machine learning},
location = {Taipei, Taiwan},
series = {MobiSys '13}
}

@inproceedings{10.1145/3274694.3274710,
author = {Oprea, Alina and Li, Zhou and Norris, Robin and Bowers, Kevin},
title = {MADE: Security Analytics for Enterprise Threat Detection},
year = {2018},
isbn = {9781450365697},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274694.3274710},
doi = {10.1145/3274694.3274710},
abstract = {Enterprises are targeted by various malware activities at a staggering rate. To counteract the increased sophistication of cyber attacks, most enterprises deploy within their perimeter a number of security technologies, including firewalls, antivirus software, and web proxies, as well as specialized teams of security analysts forming Security Operations Centers (SOCs).In this paper we address the problem of detecting malicious activity in enterprise networks and prioritizing the detected activities according to their risk. We design a system called MADE using machine learning applied to data extracted from security logs. MADE leverages an extensive set of features for enterprise malicious communication and uses supervised learning in a novel way for prioritization, rather than detection, of enterprise malicious activities. MADE has been deployed in a large enterprise and used by SOC analysts. Over one month, MADE successfully prioritizes the most risky domains contacted by enterprise hosts, achieving a precision of 97% in 100 detected domains, at a very small false positive rate. We also demonstrate MADE's ability to identify new malicious activities (18 out of 100) overlooked by state-of-the-art security technologies.},
booktitle = {Proceedings of the 34th Annual Computer Security Applications Conference},
pages = {124–136},
numpages = {13},
location = {San Juan, PR, USA},
series = {ACSAC '18}
}

@inproceedings{10.1145/3372297.3423345,
author = {Hiller, Jens and Amann, Johanna and Hohlfeld, Oliver},
title = {The Boon and Bane of Cross-Signing: Shedding Light on a Common Practice in Public Key Infrastructures},
year = {2020},
isbn = {9781450370899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372297.3423345},
doi = {10.1145/3372297.3423345},
abstract = {Public Key Infrastructures (PKIs) with their trusted Certificate Authorities (CAs) provide the trust backbone for the Internet: CAs sign certificates which prove the identity of servers, applications, or users. To be trusted by operating systems and browsers, a CA has to undergo lengthy and costly validation processes. Alternatively, trusted CAs can cross-sign other CAs to extend their trust to them. In this paper, we systematically analyze the present and past state of cross-signing in the Web PKI. Our dataset (derived from passive TLS monitors and public CT logs) encompasses more than 7 years and 225 million certificates with 9.3 billion trust paths. We show benefits and risks of cross-signing. We discuss the difficulty of revoking trusted CA certificates where, worrisome, cross-signing can result in valid trust paths to remain after revocation; a problem for non-browser software that often blindly trusts all CA certificates and ignores revocations. However, cross-signing also enables fast bootstrapping of new CAs, e.g., Let's Encrypt, and achieves a non-disruptive user experience by providing backward compatibility. In this paper, we propose new rules and guidance for cross-signing to preserve its positive potential while mitigating its risks.},
booktitle = {Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1289–1306},
numpages = {18},
keywords = {cross certification, PKI, TLS, SSL, cross-signing, X.509},
location = {Virtual Event, USA},
series = {CCS '20}
}

@inproceedings{10.1145/3342195.3387527,
author = {Park, SeongJae and McKenney, Paul E. and Dufour, Laurent and Yeom, Heon Y.},
title = {An HTM-Based Update-Side Synchronization for RCU on NUMA Systems},
year = {2020},
isbn = {9781450368827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342195.3387527},
doi = {10.1145/3342195.3387527},
abstract = {Read-copy update (RCU) can provide ideal scalability for read-mostly workloads, but some believe that it provides only poor performance for updates. This belief is due to the lack of RCU-centric update synchronization mechanisms. RCU instead works with a range of update-side mechanisms, such as locking. In fact, many developers embrace simplicity by using global locking. Logging, hardware transactional memory, or fine-grained locking can provide better scalability, but each of these approaches has limitations, such as imposing overhead on readers or poor scalability on non-uniform memory access (NUMA) systems, mainly due to their lack of NUMA-aware design principles.This paper introduces an RCU extension (RCX) that provides highly scalable RCU updates on NUMA systems while retaining RCU's read-side benefits. RCX is a software-based synchronization mechanism combining hardware transactional memory (HTM) and traditional locking based on our NUMA-aware design principles for RCU. Micro-bench-marks on a NUMA system having 144 hardware threads show RCX has up to 22.6 times better performance and up to 145 times lower HTM abort rates compared to a state-of-the-art RCU/HTM combination. To demonstrate the effectiveness and applicability of RCX, we have applied RCX to parallelize some of the Linux kernel memory management system and an in-memory database system. The optimized kernel and the database show up to 24 and 17 times better performance compared to the original version, respectively.},
booktitle = {Proceedings of the Fifteenth European Conference on Computer Systems},
articleno = {37},
numpages = {15},
keywords = {transactional memory, operating systems, RCU, synchronization, parallel programming},
location = {Heraklion, Greece},
series = {EuroSys '20}
}

@inproceedings{10.1145/3362744.3363344,
author = {Stucchi, Diego and Susella, Ruggero and Fragneto, Pasqualina and Rossi, Beatrice},
title = {Secure and Effective Implementation of an IOTA Light Node Using STM32},
year = {2019},
isbn = {9781450370127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3362744.3363344},
doi = {10.1145/3362744.3363344},
abstract = {A major challenge in networked sensor systems and other IoT environments is addressing security. Vulnerabilities in those systems arise from poor physical security, unauthenticated devices, insecure firmware updates, insecure communication, and data corruption. In recent times Distributed Ledger Technologies (DLTs), of which Blockchain is an instance, have been identified as a possible solution to some of these issues. The blokchain model genetically ensures decentralized security and privacy, and therefore could provide IoT systems with a trusted infrastructure for securely logging data or exchanging tokens without the necessity, and costs, of central servers. Blockchain is no panacea, either. IoT devices that get connected to a blockchain network must still be secured, in particular they must protect the confidentiality of the keys. This requires the embedded microcontroller to execute only authenticated firmware, with protections against software attacks, such as buffer overflows, and resistance against side-channel attacks. In addition, as confirmed from the scarcity of implementations reported in the literature, it is still not clear whether blockchain protocols can be implemented efficiently on resource-constrained IoT devices. In this work, also supported by a Demo, we show an example of secure IoT device that enables the functionalities of IOTA, a DLT specifically designed for the use in the IoT. In particular, we present a Light Node based on STM32 that implements all the cryptographic functions, IOTA specific operations and communication functions required to successfully publish transactions in the IOTA distributed ledger. Our implementations on microcontrollers (ARM Cortex-M) performs up to 22 times faster in terms of cycles and up to 4 times faster in absolute time with respect to the state-of-the-art implementation on a Raspberry PI 3B. Our Light Node also ensures protection of the stored private data and guarantees secure firmware update thanks to a suitable configuration of some security features provided by STM32 microcontrollers.},
booktitle = {Proceedings of the 2nd Workshop on Blockchain-Enabled Networked Sensor},
pages = {28–29},
numpages = {2},
keywords = {Security, STM32, IOTA, Internet of Things, Distributed Ledger Technology, Networked Systems},
location = {New York, NY, USA},
series = {BlockSys'19}
}

@inproceedings{10.1145/2611264.2611267,
author = {Lee, Youngki and Balan, Rajesh Krishina},
title = {The Case for Human-Centric Personal Analytics},
year = {2014},
isbn = {9781450328258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2611264.2611267},
doi = {10.1145/2611264.2611267},
abstract = {The rich context provided by smartphones has enabled many new context-aware applications. However, these applications still need to provide their own mechanisms to interpret low-level sensing data and generate high-level user states. In this paper, we propose the idea of building a personal analytics (PA) layer that will use inputs from multiple lower layer sources, such as sensor data (accelerometers, gyroscopes, etc.), phone data (call logs, application activity, etc.), and online sources (Twitter, Facebook posts, etc.) to generate high-level user contextual states (such as emotions, preferences, and engagements). Developers can then use the PA layer to easily build a new set of interesting and compelling applications. We describe several scenarios enabled by this new layer and present a proposed software architecture. We end with a description of some of the key research challenges that need to be solved to achieve this goal.},
booktitle = {Proceedings of the 2014 Workshop on Physical Analytics},
pages = {25–29},
numpages = {5},
keywords = {personal analytics, human centric contexts},
location = {Bretton Woods, New Hampshire, USA},
series = {WPA '14}
}

@inproceedings{10.5555/2872518.3251222,
author = {Berendt, Bettina and Hollink, Laura and Luczak-Roesch, Markus},
title = {Session Details: USEWOD'16},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {It is our great pleasure to welcome you to the 6th International Workshop on Usage Analysis and the Web of Data (USEWOD), associated with WWW 2016.The workshop is dedicated to the diverse ecosystem of Web of Data access mechanisms. From academic to government data, from complex SPARQL queries to Linked Data Fragments, from DBpedia to Wikidata: mthe data sources on the Web of Data and the ways in which these sources can be created and consumed vary greatly and raise fundamental questions.The call for papers attracted submissions from United States, Canada, Asia, and Europe. The program committee reviewed and accepted the following: Venue or Track Reviewed -4 Accepted - 2.Additionally, we decided to publish an invited paper that presents a particularly interesting crossdisciplinary perspective on the Web of Data and outline current research directions and challenges in an extended 'Message from the USEWOD Chairs'.The USEWOD 2016 Research Dataset As in previous years, a standard research dataset of usage data from well-recognized Web-of-Data datasets has been published to promote reproducible research on the workshop themes. A particular highlight of this year's dataset is overlapping usage data from the official DBpedia servers as well as the Linked Data Fragments interface to DBpedia and Wikidata. This dataset allows researchers to study alternative Web of Data usage mechanisms in an unprecedented way and could therefore become a unique resource of great importance for the field. For more information on the datasets released in previous years, please see http://usewod.org/data-sets.html. Special thanks got to Open Link Software for providing us with DBpedia logs as well as Ruben Verborgh for access to Linked Data Fragments usage data.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}


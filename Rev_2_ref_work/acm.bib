@article{10.1145/3460345,
author = {He, Shilin and He, Pinjia and Chen, Zhuangbin and Yang, Tianyi and Su, Yuxin and Lyu, Michael R.},
title = {A Survey on Automated Log Analysis for Reliability Engineering},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3460345},
doi = {10.1145/3460345},
abstract = {Logs are semi-structured text generated by logging statements in software source code.
In recent decades, software logs have become imperative in the reliability assurance
mechanism of many software systems, because they are often the only data available
that record software runtime information. As modern software is evolving into a large
scale, the volume of logs has increased rapidly. To enable effective and efficient
usage of modern software logs in reliability engineering, a number of studies have
been conducted on automated log analysis. This survey presents a detailed overview
of automated log analysis research, including how to automate and assist the writing
of logging statements, how to compress logs, how to parse logs into structured event
templates, and how to employ logs to detect anomalies, predict failures, and facilitate
diagnosis. Additionally, we survey work that releases open-source toolkits and datasets.
Based on the discussion of the recent advances, we present several promising future
directions toward real-world and next-generation automated log analysis.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {130},
numpages = {37},
keywords = {log compression, log analysis, Log, logging, log parsing, log mining}
}

@inproceedings{10.1145/3465481.3470018,
author = {Eckel, Michael and Riemann, Tim},
title = {Userspace Software Integrity Measurement},
year = {2021},
isbn = {9781450390514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465481.3470018},
doi = {10.1145/3465481.3470018},
abstract = {Todays computing systems are more interconnected and sophisticated than ever before.
Especially in healthcare 4.0, services and infrastructures rely on cyber-physical
systemss (CPSess) and Internet of Things (IoT) devices. This adds to the complexity
of these highly connected systems and their manageability. Even worse, the variety
of emerging cyber attacks is becoming more severe and sophisticated, making healthcare
one of the most important sectors with major security risks. The development of appropriate
countermeasures constitutes one of the most complex and difficult challenges in cyber
security research. Research areas include, among others, anomaly detection, network
security, multi-layer event detection, cyber resiliency, and integrity protection.
Securing the integrity of software running on a device is a desirable protection goal
in the context of systems security. With a Trusted Platform Module (TPM), measured
boot, and remote attestation there exist technologies to ensure that a system has
booted up correctly and runs only authentic software. The Linux Integrity Measurement
Architecture (IMA) extends these principles into the operating systems (OSes), measuring
native binaries before they are loaded. However, interpreted language files, such
as Java classes and Python scripts, are not considered executables and are not measured
as such. Contemporary OSess ship with many of these and it is vital to consider them
as security-critical as native binaries. In this paper, we introduce Userspace Software
Integrity Measurement (USIM) for the Linux OSes. Userspace Software Integrity Measurement
(USIM) enables interpreters to measure, log, and irrevocably anchor critical events
in the TPM. We develop a software library in C which provides TPM-based measurement
functionality as well as the USIM service, which provides concurrent access handling
to the TPM based event logging. Further, we develop and implement a concept to realize
highly frequent event logging on the slow TPM. We integrate this library into the
Java Virtual Machine (JVM) to measure Java classes and show that it can be easily
integrated into other interpreters. With performance measurements we demonstrate that
our contribution is feasible and that overhead is negligible. },
booktitle = {The 16th International Conference on Availability, Reliability and Security},
articleno = {138},
numpages = {11},
keywords = {integrity verification, Trusted Computing, Systems security},
location = {Vienna, Austria},
series = {ARES 2021}
}

@inproceedings{10.1145/3458305.3478449,
author = {Liu, Shengmei and Claypool, Mark},
title = {EvLag: A Tool for Monitoring and Lagging Linux Input Devices},
year = {2021},
isbn = {9781450384346},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458305.3478449},
doi = {10.1145/3458305.3478449},
abstract = {Understanding the effects of latency on interaction is important for building software,
such as computer games, that perform well over a range of system configurations. Unfortunately,
user studies evaluating latency must each write their own code to add latency to user
input and, even worse, must limit themselves to open source applications. To address
these shortcomings, this paper presents EvLag, a tool for adding latency to user input
devices in Linux. EvLag provides a custom amount of latency for each device regardless
of the application being run, enabling user studies for systems and software that
cannot be modified (e.g., commercial games). Evaluation shows EvLag has low overhead
and accurately adds the expected amount of latency to user input. In addition, EvLag
can log user input events for post study analysis with several utilities provided
to facilitate output event parsing.},
booktitle = {Proceedings of the 12th ACM Multimedia Systems Conference},
pages = {281–286},
numpages = {6},
keywords = {delay, lag, latency, game},
location = {Istanbul, Turkey},
series = {MMSys '21}
}

@inproceedings{10.1145/3468264.3468613,
author = {Wang, Zehao and Zhang, Haoxiang and Chen, Tse-Hsun (Peter) and Wang, Shaowei},
title = {Would You like a Quick Peek? Providing Logging Support to Monitor Data Processing in Big Data Applications},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468613},
doi = {10.1145/3468264.3468613},
abstract = {To analyze large-scale data efficiently, developers have created various big data
processing frameworks (e.g., Apache Spark). These big data processing frameworks provide
abstractions to developers so that they can focus on implementing the data analysis
logic. In traditional software systems, developers leverage logging to monitor applications
and record intermediate states to assist workload understanding and issue diagnosis.
However, due to the abstraction and the peculiarity of big data frameworks, there
is currently no effective monitoring approach for big data applications. In this paper,
we first manually study 1,000 randomly sampled Spark-related questions on Stack Overflow
to study their root causes and the type of information, if recorded, that can assist
developers with motioning and diagnosis. Then, we design an approach, DPLOG, which
assists developers with monitoring Spark applications. DPLOG leverages statistical
sampling to minimize performance overhead and provides intermediate information and
hint/warning messages for each data processing step of a chained method pipeline.
We evaluate DPLOG on six benchmarking programs and find that DPLOG has a relatively
small overhead (i.e., less than 10% increase in response time when processing 5GB
data) compared to without using DPLOG, and reduce the overhead by over 500% compared
to the baseline. Our user study with 20 developers shows that DPLOG can reduce the
needed time to debug big data applications by 63% and the participants give DPLOG
an average of 4.85/5 for its usefulness. The idea of DPLOG may be applied to other
big data processing frameworks, and our study sheds light on future research opportunities
in assisting developers with monitoring big data applications.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {516–526},
numpages = {11},
keywords = {Apache Spark, Logging, Monitoring},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3466752.3480102,
author = {Zeng, Jianping and Choi, Jongouk and Fu, Xinwei and Shreepathi, Ajay Paddayuru and Lee, Dongyoon and Min, Changwoo and Jung, Changhee},
title = {ReplayCache: Enabling Volatile Cachesfor Energy Harvesting Systems},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480102},
doi = {10.1145/3466752.3480102},
abstract = {Energy harvesting systems have shown their unique benefit of ultra-long operation
time without maintenance and are expected to be more prevalent in the era of Internet
of Things. However, due to the batteryless nature, they suffer unpredictable frequent
power outages. They thus require a lightweight mechanism for crash consistency since
saving/restoring checkpoints across the outages can limit forward progress by consuming
hard-won energy. For the reason, energy harvesting systems have been designed with
a non-volatile memory (NVM) only. The use of a volatile data cache has been assumed
to be not viable or at least challenging due to the difficulty to ensure cacheline
persistence. In this paper, we propose ReplayCache, a software-only crash consistency
scheme that enables commodity energy harvesting systems to exploit a volatile data
cache. ReplayCache does not have to ensure the persistence of dirty cachelines or
record their logs at run time. Instead, ReplayCache recovery runtime re-executes the
potentially unpersisted stores in the wake of power failure to restore the consistent
NVM state, from which interrupted program can safely resume. To support store replay
during recovery, ReplayCache partitions program into a series of regions in a way
that store operand registers remain intact within each region, and checkpoints all
registers just before power failure using the crash consistency mechanism of the commodity
systems. For performance, ReplayCache enables region-level persistence that allows
the stores in a region to be asynchronously persisted until the region ends, exploiting
ILP. The evaluation with 23 benchmark applications show that compared to the baseline
with no caches, ReplayCache can achieve about 10.72x and 8.5x-8.9x speedup (on geometric
mean) for the scenarios without and with power outages, respectively. },
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {170–182},
numpages = {13},
location = {Virtual Event, Greece},
series = {MICRO '21}
}

@inproceedings{10.1145/3430895.3460877,
author = {Ludwig, Sabrina and Rausch, Andreas and Deutscher, Viola and Seifried, J\"{u}rgen},
title = {Problem Solving Analytics (PSA) in the Web-Based Office Simulation LUCA},
year = {2021},
isbn = {9781450382151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430895.3460877},
doi = {10.1145/3430895.3460877},
abstract = {Open-ended e-learning environments allow for explorative behaviour in challenging
scenarios and hence, foster problem-solving competences. The web-based office simulation
LUCA (funded by the German Federal Ministry of Education and Research) addresses the
domain-specific competences of students in commercial vocational education and training
(VET). The office simulation provides authentic office tools such as a spreadsheet
application and an ERP software to solve complex work scenarios. These scenarios are
implemented via the "LUCA Editor" and can contain automated assistance based on evidence
rules of certain behaviours ("scaffolding"). The real-time analysis of the resulting
log files enables the analysis of individual problem-solving behaviour ("Problem Solving
Analytics", PSA). Teachers and trainers can monitor their students' problem-solving
efforts in the "LUCA Dashboard", where they can also provide individual assistance
via a chat tool. In our contribution, the scientific foundations of PSA will be outlined,
followed by a demonstration of the latest prototype of LUCA and visitors' interaction
with the software. LUCA's alpha version will be released in September of this year
and will be available for practitioners in vocational schools and companies.},
booktitle = {Proceedings of the Eighth ACM Conference on Learning @ Scale},
pages = {363–364},
numpages = {2},
keywords = {automated feedback, log data analysis, computer-based office simulation, open-ended learning environment, vocational education, learning analytics},
location = {Virtual Event, Germany},
series = {L@S '21}
}

@inproceedings{10.1145/3463274.3463349,
author = {Zieglmeier, Valentin and Daiqui, Gabriel Loyola},
title = {GDPR-Compliant Use of Blockchain for Secure Usage Logs},
year = {2021},
isbn = {9781450390538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463274.3463349},
doi = {10.1145/3463274.3463349},
abstract = { The unique properties of blockchain enable central requirements of distributed secure
logging: Immutability, integrity, and availability. Especially when providing transparency
about data usages, a blockchain-based secure log can be beneficial, as no trusted
third party is required. Yet, with data governed by privacy legislation such as the
GDPR or CCPA, the core advantage of immutability becomes a liability. After a rightful
request, an individual’s personal data need to be rectified or deleted, which is impossible
in an immutable blockchain. To solve this issue, we exploit a legal property of pseudonymized
data: They are only regarded personal data if they can be associated with an individual’s
identity. We make use of this fact by presenting P3, a pseudonym provisioning system
for secure usage logs including a protocol for recording new usages. For each new
block, a one-time transaction pseudonym is generated. The pseudonym generation algorithm
guarantees unlinkability and enables proof of ownership. These properties enable GDPR-compliant
use of blockchain, as data subjects can exercise their legal rights with regards to
their personal data. The new-usage protocol ensures non-repudiation, and therefore
accountability and liability. Most importantly, our approach does not require a trusted
third party and is independent of the utilized blockchain software.},
booktitle = {Evaluation and Assessment in Software Engineering},
pages = {313–320},
numpages = {8},
keywords = {GDPR, Blockchain, Logging, Privacy, Anonymity},
location = {Trondheim, Norway},
series = {EASE 2021}
}

@article{10.1145/3493499.3493502,
author = {Hoshino, Shinji and Arahori, Yoshitaka and Gondow, Katsuhiko},
title = {Postmortem Accurate IR-Level State Recovery for Deployed Concurrent Programs},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {1559-6915},
url = {https://doi.org/10.1145/3493499.3493502},
doi = {10.1145/3493499.3493502},
abstract = {Debugging failures of deployed concurrent software is important for quality assurance.
However, such failures are difficult to debug because their behavior is non-deterministic
and limited information can be obtained with conventional means. Reverse debuggers
such as REPT [11] assists with debugging by recovering data values before the failure.
This is achieved by using a hardware-tracer to log control-flow information, then
using the information and a conventional coredump to recover data values via reverse-execution
at machine-level. REPT's algorithm for data value recovery is reliable and fast. But
the implementation cost is high because of its dependence on architecture. Applying
REPT to more abstract IR (Intermediate Representation) level instructions to counter
this yielded limited results with low accuracy compared to the original x86_64 implementation.
The main reason for this is that the stack layout is abstracted at IR-level.In this
paper, we present STRAB (State Recovery at Abstract-level), a collection of our proposed
methods to solve these problems. STRAB works in two phases. First, the data values
in the coredump are lifted from machine-level to IR-level using rich debug information
(DWARF3) and a novel technique we call mid-recovery lifting, the latter helping to
recover more heap data values at IR-level. Second, our novel hybrid memory location
resolution reduces the accuracy loss due to the abstracted stack layout at IR-level.Experimental
results on a variety of real-world concurrent programs show that STRAB has significantly
higher accuracy compared to REPT at IR-level (+40% on average) with only minor slowdowns
(x2.7 on average), while also achieving architecture-independence.},
journal = {SIGAPP Appl. Comput. Rev.},
month = oct,
pages = {33–48},
numpages = {16},
keywords = {concurrent programming, reverse debugging, state recovery, intermediate representation, data value inference}
}

@inproceedings{10.1145/3461002.3473066,
author = {Fortz, Sophie},
title = {LIFTS: Learning Featured Transition Systems},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473066},
doi = {10.1145/3461002.3473066},
abstract = {This PhD project aims to automatically learn transition systems capturing the behaviour
of a whole family of software-based systems. Reasoning at the family level yields
important economies of scale and quality improvements for a broad range of systems
such as software product lines, adaptive and configurable systems. Yet, to fully benefit
from the above advantages, a model of the system family's behaviour is necessary.
Such a model is often prohibitively expensive to create manually due to the number
of variants. For large long-lived systems with outdated specifications or for systems
that continuously adapt, the modelling cost is even higher. Therefore, this PhD proposes
to automate the learning of such models from existing artefacts. To advance research
at a fundamental level, our learning target are Featured Transition Systems (FTS),
an abstract formalism that can be used to provide a pivot semantics to a range of
variability-aware state-based modelling languages. The main research questions addressed
by this PhD project are: (1) Can we learn variability-aware models efficiently? (2)
Can we learn FTS in a black-box fashion? (i.e., with access to execution logs but
not to source code); (3) Can we learn FTS in a white/grey-box testing fashion? (i.e.,
with access to source code); and (4) How do the proposed techniques scale in practice?},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {1–6},
numpages = {6},
keywords = {software product lines, model learning, active automata learning, featured transition systems, variability mining},
location = {Leicester, United Kindom},
series = {SPLC '21}
}


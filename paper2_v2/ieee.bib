@ARTICLE{9392377,
author={Yao, Kundi and Sayagh, Mohammed and Shang, Weiyi and Hassan, Ahmed E.},
journal={IEEE Transactions on Software Engineering}, title={Improving State-of-the-art Compression Techniques for Log Management Tools},
year={2021},
volume={},
number={},
pages={1-1},
abstract={Log data records important runtime information about the running of a software system for different purposes including performance assurance, capacity planning, and anomaly detection. Log management tools such as ELK Stack and Splunk are widely adopted to manage and leverage log data in order to assist DevOps in real-time log analytics and decision making. To enable fast queries and to save storage space, such tools split log data into small blocks (e.g., 16KB), then index and compress each block separately. Previous log compression studies focus on improving the compression of either large-sized log files or log streams, without considering improving the compression of small log blocks (the actual compression need by modern log management tools). The evaluation of four state-of-the-art compression approaches (e.g., Logzip, a variation of Logzip by pre-extracting log templates named Logzip-E, LogArchive and Cowic) indicates that these approaches do not perform well on small log blocks. In fact, the compressed blocks that are preprocessed using Logzip, Logzip-E, LogArchive or Cowic are even larger (on median 1.3 times, 1.5 times, 0.2 times or 6.6 times) than the compressed blocks without any preprocessing. Hence, we propose an approach named LogBlock to preprocess small log blocks before compressing them with a general compressor such as gzip, deflate and lz4, which are widely adopted by log management tools. Logblock reduces the repetitiveness of logs by preprocessing the log headers and rearranging the log content leading to an improved compression ratio for a log file. Our evaluation on 16 log files shows that, for 16KB to 128KB block sizes, the compressed blocks by LogBlock are on median 5% to 21% smaller than the same compressed blocks without preprocessing (outperforming the state-of-the-art compression approaches). LogBlock achieves both a higher compression ratio (a median of 1.7 to 8.4 times, 1.9 to 10.0 times, 1.3 to 1.9 times and 6.2 to 11.4 times) and a faster compression speed (a median of 30.8 to 49.7 times, 42.6 to 53.8 times, 4.5 to 6.0 times and 2.5 to 4.0 times) than Logzip, Logzip-E, LogArchive and Cowic. LogBlock can help improve the storage efficiency of log management tools.},
keywords={Tools;Indexes;IP networks;Software systems;Runtime;Monitoring;Message systems;Software log compression;Software logging;Log management tools F},
doi={10.1109/TSE.2021.3069958},
ISSN={1939-3520},
month={},}
@INPROCEEDINGS{9463117,
author={Gholamian, Sina and Ward, Paul A. S.},
booktitle={2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)}, title={On the Naturalness and Localness of Software Logs},
year={2021},
volume={},
number={},
pages={155-166},
abstract={Logs are an essential part of the development and maintenance of large and complex software systems as they contain rich information pertaining to the dynamic content and state of the system. As such, developers and practitioners rely heavily on the logs to monitor their systems. In parallel, the increasing volume and scale of the logs, due to the growing complexity of modern software systems, renders the traditional way of manual log inspection insurmountable. Consequently, to handle large volumes of logs efficiently and effectively, various prior research aims to automate the analysis of log files. Thus, in this paper, we begin with the hypothesis that log files are natural and local and these attributes can be applied for automating log analysis tasks. We guide our research with six research questions with regards to the naturalness and localness of the log files, and present a case study on anomaly detection and introduce a tool for anomaly detection, called ANALOG, to demonstrate how our new findings facilitate the automated analysis of logs.},
keywords={Manuals;Tools;Maintenance engineering;Inspection;Software systems;Complexity theory;Task analysis;software systems;logging statements;log files;entropy;natural language processing;naturalness;localness;natural language processing (NLP);anomaly detection},
doi={10.1109/MSR52588.2021.00028},
ISSN={2574-3864},
month={May},}
@INPROCEEDINGS{9402068,
author={Li, Zhenhao and Li, Heng and Chen, Tse-Hsun and Shang, Weiyi},
booktitle={2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)}, title={DeepLV: Suggesting Log Levels Using Ordinal Based Neural Networks},
year={2021},
volume={},
number={},
pages={1461-1472},
abstract={Developers write logging statements to generate logs that provide valuable runtime information for debugging and maintenance of software systems. Log level is an important component of a logging statement, which enables developers to control the information to be generated at system runtime. However, due to the complexity of software systems and their runtime behaviors, deciding a proper log level for a logging statement is a challenging task. For example, choosing a higher level (e.g., error) for a trivial event may confuse end users and increase system maintenance overhead, while choosing a lower level (e.g., trace) for a critical event may prevent the important execution information to be conveyed opportunely. In this paper, we tackle the challenge by first conducting a preliminary manual study on the characteristics of log levels. We find that the syntactic context of the logging statement and the message to be logged might be related to the decision of log levels, and log levels that are further apart in order (e.g., trace and error) tend to have more differences in their characteristics. Based on this, we then propose a deep-learning based approach that can leverage the ordinal nature of log levels to make suggestions on choosing log levels, by using the syntactic context and message features of the logging statements extracted from the source code. Through an evaluation on nine large-scale open source projects, we find that: 1) our approach outperforms the state-of-the-art baseline approaches; 2) we can further improve the performance of our approach by enlarging the training data obtained from other systems; 3) our approach also achieves promising results on cross-system suggestions that are even better than the baseline approaches on within-system suggestions. Our study highlights the potentials in suggesting log levels to help developers make informed logging decisions.},
keywords={Runtime;Manuals;Syntactics;Maintenance engineering;Software systems;Feature extraction;Task analysis;logs;deep learning;log level;empirical study},
doi={10.1109/ICSE43902.2021.00131},
ISSN={1558-1225},
month={May},}
@INPROCEEDINGS{9463118,
author={Cândido, Jeanderson and Haesen, Jan and Aniche, Maurício and van Deursen, Arie},
booktitle={2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)}, title={An Exploratory Study of Log Placement Recommendation in an Enterprise System},
year={2021},
volume={},
number={},
pages={143-154},
abstract={Logging is a development practice that plays an important role in the operations and monitoring of complex systems. Developers place log statements in the source code and use log data to understand how the system behaves in production. Unfortunately, anticipating where to log during development is challenging. Previous studies show the feasibility of leveraging machine learning to recommend log placement despite the data imbalance since logging is a fraction of the overall code base. However, it remains unknown how those techniques apply to an industry setting, and little is known about the effect of imbalanced data and sampling techniques. In this paper, we study the log placement problem in the code base of Adyen, a large-scale payment company. We analyze 34,526 Java files and 309,527 methods that sum up +2M SLOC. We systematically measure the effectiveness of five models based on code metrics, explore the effect of sampling techniques, understand which features models consider to be relevant for the prediction, and evaluate whether we can exploit 388,086 methods from 29 Apache projects to learn where to log in an industry setting. Our best performing model achieves 79% of balanced accuracy, 81% of precision, 60% of recall. While sampling techniques improve recall, they penalize precision at a prohibitive cost. Experiments with open-source data yield under-performing models over Adyen's test set; nevertheless, they are useful due to their low rate of false positives. Our supporting scripts and tools are available to the community.},
keywords={Industries;Measurement;Training;Biological system modeling;Training data;Machine learning;Production;Log Placement;Log Recommendation;Logging Practices;Supervised Learning},
doi={10.1109/MSR52588.2021.00027},
ISSN={2574-3864},
month={May},}
@INPROCEEDINGS{9458872,
author={Vervaet, Arthur},
booktitle={2021 IEEE 37th International Conference on Data Engineering (ICDE)}, title={MoniLog: An Automated Log-Based Anomaly Detection System for Cloud Computing Infrastructures},
year={2021},
volume={},
number={},
pages={2739-2743},
abstract={Within today's large-scale systems, one anomaly can impact millions of users. Detecting such events in real-time is essential to maintain the quality of services. It allows the monitoring team to prevent or diminish the impact of a failure. Logs are a core part of software development and maintenance, by recording detailed information at runtime. Such log data are universally available in nearly all computer systems. They enable developers as well as system maintainers to monitor and dissect anomalous events. For Cloud computing companies and large online platforms in general, growth is linked to the scaling potential. Automatizing the anomaly detection process is a promising way to ensure the scalability of monitoring capacities regarding the increasing volume of logs generated by modern systems. In this paper, we will introduce MoniLog, a distributed approach to detect real-time anomalies within large-scale environments. It aims to detect sequential and quantitative anomalies within a multi-source log stream. MoniLog is designed to structure a log stream and perform the monitoring of anomalous sequences. Its output classifier learns from the administrator's actions to label and evaluate the criticality level of anomalies.},
keywords={Cloud computing;Three-dimensional displays;Runtime;Scalability;Quality of service;Maintenance engineering;Real-time systems;Anomaly Detection;Log Analysis;Log Instability;Log Parsing},
doi={10.1109/ICDE51399.2021.00317},
ISSN={2375-026X},
month={April},}
@ARTICLE{8840982,
author={Liu, Zhongxin and Xia, Xin and Lo, David and Xing, Zhenchang and Hassan, Ahmed E. and Li, Shanping},
journal={IEEE Transactions on Software Engineering}, title={Which Variables Should I Log?},
year={2021},
volume={47},
number={9},
pages={2012-2031},
abstract={Developers usually depend on inserting logging statements into the source code to collect system runtime information. Such logged information is valuable for software maintenance. A logging statement usually prints one or more variables to record vital system status. However, due to the lack of rigorous logging guidance and the requirement of domain-specific knowledge, it is not easy for developers to make proper decisions about which variables to log. To address this need, in this work, we propose an approach to recommend logging variables for developers during development by learning from existing logging statements. Different from other prediction tasks in software engineering, this task has two challenges: 1) Dynamic labels – different logging statements have different sets of accessible variables, which means in this task, the set of possible labels of each sample is not the same. 2) Out-of-vocabulary words – identifiers’ names are not limited to natural language words and the test set usually contains a number of program tokens which are out of the vocabulary built from the training set and cannot be appropriately mapped to word embeddings. To deal with the first challenge, we convert this task into a representation learning problem instead of a multi-label classification problem. Given a code snippet which lacks a logging statement, our approach first leverages a neural network with an RNN (recurrent neural network) layer and a self-attention layer to learn the proper representation of each program token, and then predicts whether each token should be logged through a unified binary classifier based on the learned representation. To handle the second challenge, we propose a novel method to map program tokens into word embeddings by making use of the pre-trained word embeddings of natural language tokens. We evaluate our approach on 9 large and high-quality Java projects. Our evaluation results show that the average MAP of our approach is over 0.84, outperforming random guess and an information-retrieval-based method by large margins.},
keywords={Task analysis;Recurrent neural networks;Tools;Compounds;Vocabulary;Software maintenance;Log;logging variable;word embedding;representation learning},
doi={10.1109/TSE.2019.2941943},
ISSN={1939-3520},
month={Sep.},}
@ARTICLE{9360483,
author={Li, Zhenhao and Chen, Tse-Hsun Peter and Yang, Jinqiu and Shang, Weiyi},
journal={IEEE Transactions on Software Engineering}, title={Studying Duplicate Logging Statements and Their Relationships with Code Clones},
year={2021},
volume={},
number={},
pages={1-1},
abstract={Developers rely on software logs for a variety of tasks, such as debugging, testing, program comprehension, verification, and performance analysis. Despite the importance of logs, prior studies show that there is no industrial standard on how to write logging statements. In this paper, we focus on studying duplicate logging statements, which are logging statements that have the same static text message. Such duplications in the text message are potential indications of logging code smells, which may affect developers understanding of the dynamic view of the system. We manually studied over 4K duplicate logging statements and their surrounding code in five large-scale open source systems: Hadoop, CloudStack, Elasticsearch, Cassandra, and Flink. We uncovered five patterns of duplicate logging code smells. For each instance of the duplicate logging code smell, we further manually identify the potentially problematic (i.e., require fixes) and justifiable (i.e., do not require fixes) cases. Then, we contact developers to verify our manual study result. We integrated our manual study result and developers feedback into our automated static analysis tool, DLFinder, which automatically detects problematic duplicate logging code smells. We evaluated DLFinder on the five manually studied systems and three additional systems: Camel, Kafka and Wicket. In total, combining the results of DLFinder and our manual analysis, we reported 91 problematic duplicate logging code smell instances to developers and all of them have been fixed. We further study the relationship between duplicate logging statements, including the problematic instances of duplicate logging code smells, and code clones. We find that 83% of the duplicate logging code smell instances reside in cloned code, but 17% of them reside in micro-clones that are difficult to detect using automated clone detection tools. We also find that more than half of the duplicate logging statements reside in cloned code snippets, and a large portion of them reside in very short code blocks which may not be effectively detected by existing code clone detection tools. Our study shows that, in addition to general source code that implements the business logic, code clones may also result in bad logging practices that could increase maintenance difficulties.},
keywords={Cloning;Manuals;Tools;Static analysis;Maintenance engineering;Java;Cloud computing;log;code smell;duplicate log;code clone;static analysis;empirical study},
doi={10.1109/TSE.2021.3060918},
ISSN={1939-3520},
month={},}
@INPROCEEDINGS{9548437,
author={Abbasli, Nazrin and Ganiz, Murat Can},
booktitle={2021 International Conference on INnovations in Intelligent SysTems and Applications (INISTA)}, title={Log and Execution Trace Analytics System},
year={2021},
volume={},
number={},
pages={1-7},
abstract={Log files are available on every computer system. They automatically record important run time events of operating systems or software applications. They are mainly used to find the root cause of failures. Analyzing such log files allows us to detect anomalies, problems and improve the system. Since the log files are usually unstructured or semi-structured, the important task of log analysis is to parse usually immense amount of log message strings into the human readable and actionable reports. In this paper, we propose an implementation of a machine learning based log parsing system using Named Entity Recognition which is the process of identifying and categorizing entities in the text. Our approach makes use of Bidirectional Encoder Representations from Transformers (BERT) to extract entities. The paper reports the results of experiments on two benchmark; macOS and Linux OS datasets.},
keywords={Training;Adaptation models;Analytical models;Technological innovation;Text recognition;Linux;Bit error rate;Log Parsing;Log Analysing;NER;BERT;Drain;Tagtog},
doi={10.1109/INISTA52262.2021.9548437},
ISSN={},
month={Aug},}
@ARTICLE{9442364,
author={Locke, Steven and Li, Heng and Chen, Tse-Hsun Peter and Shang, Weiyi and Liu, Wei},
journal={IEEE Transactions on Software Engineering}, title={LogAssist: Assisting Log Analysis Through Log Summarization},
year={2021},
volume={},
number={},
pages={1-1},
abstract={Logs contain valuable information about the runtime behaviors of software systems. Thus, practitioners rely on logs for various tasks such as debugging, system comprehension, and anomaly detection. However, due to the unstructured nature and large size of logs, there are several challenges that practitioners face with log analysis. In this paper, we propose a novel approach called LogAssist that tackles these challenges and assists practitioners with log analysis. LogAssist provides an organized and concise view of logs by first grouping logs into event sequences (i.e., workflows), which better illustrate the system runtime execution paths. Then, LogAssist compresses the log events in workflows by hiding consecutive events and applying n-gram modeling to identify common event sequences. We evaluated LogAssist on the logs that are generated by two open-source and one enterprise system. We find that LogAssist can reduce the number of log events that practitioners need to investigate by up to 99%. Through a user study with 19 participants, we also find that LogAssist can assist practitioners by reducing the needed time on log analysis tasks by an average of 40%. The participants also rated LogAssist an average of 4.53 out of 5 for improving their experiences of performing log analysis. Finally, we document our experiences and lessons learned from developing and adopting LogAssist in practice. We believe that LogAssist and our reported experiences may lay the basis for future analysis and interactive exploration on logs.},
keywords={Task analysis;Runtime;Tools;Testing;Faces;Anomaly detection;Software systems;Log analysis;log compression;n-gram modeling;log abstraction;workflow characterization;log reduction},
doi={10.1109/TSE.2021.3083715},
ISSN={1939-3520},
month={},}
@INPROCEEDINGS{9402230,
author={Rand, Jared and Miranskyy, Andriy},
booktitle={2021 IEEE/ACM 43rd International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER)}, title={On Automatic Parsing of Log Records},
year={2021},
volume={},
number={},
pages={41-45},
abstract={Software log analysis helps to maintain the health of software solutions and ensure compliance and security. Existing software systems consist of heterogeneous components emitting logs in various formats. A typical solution is to unify the logs using manually built parsers, which is laborious. Instead, we explore the possibility of automating the parsing task by employing machine translation (MT). We create a tool that generates synthetic Apache log records which we used to train recurrent-neural-network-based MT models. Models' evaluation on real-world logs shows that the models can learn Apache log format and parse individual log records. The median relative edit distance between an actual real-world log record and the MT prediction is less than or equal to 28%. Thus, we show that log parsing using an MT approach is promising.},
keywords={Tools;Software systems;Security;Machine translation;Task analysis;Software engineering},
doi={10.1109/ICSE-NIER52604.2021.00017},
ISSN={},
month={May},}
@INPROCEEDINGS{9460930,
author={Harty, Julian and Zhang, Haonan and Wei, Lili and Pascarella, Luca and Aniche, Maurício and Shang, Weiyi},
booktitle={2021 IEEE/ACM 8th International Conference on Mobile Software Engineering and Systems (MobileSoft)}, title={Logging Practices with Mobile Analytics: An Empirical Study on Firebase},
year={2021},
volume={},
number={},
pages={56-60},
abstract={Software logs are of great value in both industrial and open-source projects. Mobile analytics logging enables developers to collect logs remotely from their apps running on end user devices at the cost of recording and transmitting logs across the Internet to a centralised infrastructure.This paper makes a first step in characterising logging practices with a widely adopted mobile analytics logging library, namely Firebase Analytics. We provide an empirical evaluation of the use of Firebase Analytics in 57 open-source Android applications by studying the evolution of code-bases to understand: a) the needs-in-common that push practitioners to adopt logging practices on mobile devices, and b) the differences in the ways developers use local and remote logging.Our results indicate mobile analytics logs are less pervasive and less maintained than traditional logging code. Based on our analysis, we believe logging using mobile analytics is more user centered compared to traditional logging, where the latter is mainly used to record information for debugging purposes.},
keywords={Debugging;Mobile handsets;Libraries;Internet;Interviews;Open source software;Monitoring;mobile analytics;mobile software development;logging engineering;software monitoring;empirical software engineering},
doi={10.1109/MobileSoft52590.2021.00013},
ISSN={},
month={May},}
@ARTICLE{8291718,
author={Pei, Jisheng and Wen, Lijie and Yang, Hedong and Wang, Jianmin and Ye, Xiaojun},
journal={IEEE Transactions on Services Computing}, title={Estimating Global Completeness of Event Logs: A Comparative Study},
year={2021},
volume={14},
number={2},
pages={441-457},
abstract={Event logs are the basis of process mining techniques and tools that extract process behavior information for better understanding and optimization of business processes. While it has been widely realized that the degree of completeness of event logs may largely determine the effectiveness of these techniques, how to estimate the completeness of event logs has not yet been fully addressed. This is mainly because ground-truth process models are usually unknown. To attack this problem, we pay a closer look to several concepts and implicit assumptions in the log completeness estimation problem and characterize it as a special case of the species estimation problem in the field of statistics. Although species estimation is still an open problem, a number of statistic models and techniques with approximate solutions have been available. To investigate the relevance of these methods for event log completeness estimation, we have designed and conducted a wide scope of empirical study and quantitative experiments on both real-world and synthesized event logs to compare the performance of these methods. In addition, the completeness estimation of several important and widely used real-world events logs are reported for the first time together with some best practice experience learned through this research.},
keywords={Estimation;Task analysis;Measurement;Computational modeling;Algorithm design and analysis;Prediction algorithms;Quality and metrics;business process management;process mining;event logs;completeness},
doi={10.1109/TSC.2018.2805912},
ISSN={1939-1374},
month={March},}
@INPROCEEDINGS{9559017,
author={Duman, Atahan and Soğukpinar, İbrahim},
booktitle={2021 6th International Conference on Computer Science and Engineering (UBMK)}, title={Deep Learning Based Event Correlation Analysis in Information Systems},
year={2021},
volume={},
number={},
pages={209-214},
abstract={Information systems and applications provide indispensable services at every stage of life, enabling us to carry out our activities more effectively and efficiently. Today, information technology systems produce many alarm and event records. These produced records often have a relationship with each other, and when this relationship is captured correctly, many interruptions that will harm institutions can be prevented before they occur. For example, an increase in the disk I/O speed of a server or a problem may cause the business software running on that server to slow down and cause different results in this slowness. Here, an institution’s accurate analysis and management of all event records, and rule-based analysis of the resulting records in certain time periods and depending on certain rules will ensure efficient and effective management of millions of alarms. In addition, it will be possible to prevent possible problems by removing the relationships between events. Events that occur in IT systems are a kind of footprint. It is also vital to keep a record of the events in question, and when necessary, these event records can be analyzed to analyze the efficiency of the systems, harmful interferences, system failure tendency, etc. By understanding the undesirable situations such as taking the necessary precautions, possible losses can be prevented. In this study, the model developed for fault prediction in systems by performing event log analysis in information systems is explained and the experimental results obtained are given.},
keywords={Deep learning;Computer science;Correlation;Computational modeling;Predictive models;Software;Servers;Deep Learning;Event Log;Cyber Security;Neural Networks;LSTM;GRU;RNN;Attention Mechanism},
doi={10.1109/UBMK52708.2021.9559017},
ISSN={2521-1641},
month={Sep.},}
@INPROCEEDINGS{9507157,
author={Zhukova, Nataly A. and Kulikov, Igor A. and Utkin, Nikolay Yu.},
booktitle={2021 XXIV International Conference on Soft Computing and Measurements (SCM)}, title={The Method for Searching Patterns In Log Files of Telecommunication Devices for Monitoring their State},
year={2021},
volume={},
number={},
pages={124-126},
abstract={The article introduces a new method for searching patterns in log files of telecommunication devices for monitoring their states. An overview of the methods used in practice is presented, the significance of log files as a source of operational data provided by monitoring systems is shown. Usage of the results of processing log files allows consider operational data within the models of the telecommunication networks. Based on the real log files received from the network devices used in telecommunication networks, patterns that characterize their behavior were found. The Phyton application for searching and analyzing log files, generating operational data packets and adding them to the telecommunications networks models has been developed.},
keywords={Analytical models;Cable TV;Communications technology;Data models;Software;Real-time systems;Task analysis;telecommunication network;knowledge graph;analysis of log files;patterns},
doi={10.1109/SCM52931.2021.9507157},
ISSN={},
month={May},}
@INPROCEEDINGS{9431022,
author={Martí, Felip and Forkan, Abdur Rahim Mohammad and Jayaraman, Prem Prakash and McCarthy, Chris and Ghaderi, Hadi},
booktitle={2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)}, title={LogLiDAR: An Internet of Things Solution for Counting and Scaling Logs},
year={2021},
volume={},
number={},
pages={413-415},
abstract={Accurate counting and measurement of logs (known as log scaling) stacked in piles, is an integral part of the wood log supply chain. Currently, these tasks are manual, and hence labour intensive and prone to human errors. In this paper, we present LogLiDAR an IoT sensing-based solution for counting and scaling logs using LiDAR (Light Detection and Ranging) images. LogLiDAR incorporates an interactive user interface to explore log counting and scaling. The underlying system for processing LiDAR images is developed using a pipeline of point cloud library (PCL) algorithms. This work is the first attempt to develop an IoT-based (LiDAR) solution for counting and scaling logs for the log industry.},
keywords={Laser radar;Three-dimensional displays;Conferences;Software algorithms;Supply chains;Pipelines;User interfaces;Wood Log;Internet of Things;Log Scaling;Point Cloud;LiDAR;Visualisation},
doi={10.1109/PerComWorkshops51409.2021.9431022},
ISSN={},
month={March},}
@INPROCEEDINGS{9474393,
author={Catovic, Armin and Cartwright, Carolyn and Gebreyesus, Yasmin Tesfaldet and Ferlin, Simone},
booktitle={2021 IEEE/ACM 1st Workshop on AI Engineering - Software Engineering for AI (WAIN)}, title={Linnaeus: A highly reusable and adaptable ML based log classification pipeline},
year={2021},
volume={},
number={},
pages={11-18},
abstract={Logs are a common way to record detailed run-time information in software. As modern software systems evolve in scale and complexity, logs have become indispensable to understanding the internal states of the system. At the same time however, manually inspecting logs has become impractical. In recent times, there has been more emphasis on statistical and machine learning (ML) based methods for analyzing logs. While the results have shown promise, most of the literature focuses on algorithms and state-of-the-art (SOTA), while largely ignoring the practical aspects. In this paper we demonstrate our end-to-end log classification pipeline, Linnaeus. Besides showing the more traditional ML flow, we also demonstrate our solutions for adaptability and re-use, integration towards large scale software development processes, and how we cope with lack of labelled data. We hope Linnaeus can serve as a blueprint for, and inspire the integration of, various ML based solutions in other large scale industrial settings.},
keywords={Machine learning algorithms;Conferences;Pipelines;Software algorithms;Machine learning;Software systems;Complexity theory;log classification;machine learning;anomaly detection;troubleshooting;automation},
doi={10.1109/WAIN52551.2021.00008},
ISSN={},
month={May},}
@INPROCEEDINGS{9402368,
author={Yang, Lin and Chen, Junjie and Wang, Zan and Wang, Weijing and Jiang, Jiajun and Dong, Xuyuan and Zhang, Wenbin},
booktitle={2021 IEEE/ACM 43rd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)}, title={PLELog: Semi-Supervised Log-Based Anomaly Detection via Probabilistic Label Estimation},
year={2021},
volume={},
number={},
pages={230-231},
abstract={PLELog is a novel approach for log-based anomaly detection via probabilistic label estimation. It is designed to effectively detect anomalies in unlabeled logs and meanwhile avoid the manual labeling effort for training data generation. We use semantic information within log events as fixed-length vectors and apply HDBSCAN to automatically clustering log sequences. After that, we also propose a Probabilistic Label Estimation approach to reduce the noises introduced by error labeling and put "labeled" instances into an attention-based GRU network for training. We conducted an empirical study to evaluate the effectiveness of PLELog on two open-source log data (i.e., HDFS and BGL). The results demonstrate the effectiveness of PLELog. In particular, PLELog has been applied to two real-world systems from a university and a large corporation, further demonstrating its practicability.},
keywords={Training;Estimation;Training data;Probabilistic logic;Labeling;Anomaly detection;Software engineering},
doi={10.1109/ICSE-Companion52605.2021.00106},
ISSN={2574-1926},
month={May},}
@INPROCEEDINGS{9370654,
author={Jain, Shubham and de Buitléir, Amy and Fallon, Enda},
booktitle={2021 23rd International Conference on Advanced Communication Technology (ICACT)}, title={An Extensible Parsing Pipeline for Unstructured Data Processing},
year={2021},
volume={},
number={},
pages={312-318},
abstract={Network monitoring and diagnostics systems depict the running system's state and generate enormous amounts of unstructured data through log files, print statements, and other reports. It is not feasible to manually analyze all these files due to limited resources and the need to develop custom parsers to convert unstructured data into desirable file formats. Prior research focuses on rule-based and relationship-based parsing methods to parse unstructured data into structured file formats; these methods are labor-intensive and need large annotated datasets. This paper presents an unsupervised text processing pipeline that analyses such text files, removes extraneous information, identifies tabular components, and parses them into a structured file format. The proposed approach is resilient to changes in the data structure, does not require training data, and is domain-independent. We experiment and compare topic modeling and clustering approaches to verify the accuracy of the proposed technique. Our findings indicate that combining similarity and clustering algorithms to identify data components had better accuracy than topic modeling.},
keywords={Pipelines;Training data;Machine learning;Metadata;Data mining;Monitoring;Text processing;Unsupervised Data Mining;Information Extraction;Clustering;Topic Modeling},
doi={10.23919/ICACT51234.2021.9370654},
ISSN={1738-9445},
month={Feb},}
@ARTICLE{9190056,
author={Liu, Cong},
journal={IEEE Transactions on Automation Science and Engineering}, title={Discovery and Quality Evaluation of Software Component Behavioral Models},
year={2021},
volume={18},
number={4},
pages={1538-1549},
abstract={Tremendous amounts of execution data are collected during software execution. These data provide rich information for software runtime behavior comprehension. The unstructured execution data may be too complex, involving multiple interleaved components and so on. Applying existing process discovery techniques results in spaghetti-like models with no clear structure and no valuable information that can be easily understood by end users. In this article, we start with the observation that a software system is composed of a group of components, and we use this information to decompose the problem into smaller independent ones by discovering a behavioral model per component. To this end, we first distill a software event log for each component from the raw software execution data. Then, we construct the hierarchical software event log by recursively applying caller-and-callee relation detection. Next, component behavioral models, represented as hierarchical Petri nets, are discovered by recursively applying existing process discovery techniques. To measure the quality of discovered models against the execution data, we transform hierarchical Petri nets to flat ones, and the quality metrics, e.g., fitness, precision, and complexity, are applied. All proposed approaches have been implemented in the open-source process mining toolkit ProM. Through the experimental evaluation using both synthetic software systems and open-source software systems, we illustrate that the proposed approach facilitates the discovery of more understandable and high-quality software behavioral models. Note to Practitioners—Software execution data record rich information on the runtime behavior of software systems. Discovering an overall behavioral model for the whole software system typically results in an extremely complicated model that hinders further comprehension and usage. With the observation that a software system is composed of a set of interacting components, this article considers the problem of discovering a behavioral model per component. The proposed techniques are implemented in the open-source process mining tool ProM, and experimental evaluations using both synthetic and real-life software systems have indicated their applicability. The proposed approaches are readily applicable to industrial-size software behavior comprehension.},
keywords={Software systems;Data mining;Petri nets;Process modeling;Component behavioral model;component instance identification;hierarchical Petri nets;process discovery;software execution data},
doi={10.1109/TASE.2020.3008897},
ISSN={1558-3783},
month={Oct},}
@INPROCEEDINGS{9442035,
author={Deepa, B and Ramesh, K},
booktitle={2021 7th International Conference on Advanced Computing and Communication Systems (ICACCS)}, title={Production Level Data Pipeline Environment for Machine Learning Models},
year={2021},
volume={1},
number={},
pages={404-407},
abstract={Machine learning field has a plethora of options to help diagnose various medical ailments. These models and algorithms seldomly form production level tools as the designs are compromised at the implementation level. The compromise is in the form of hardcoded file paths, variables, and development in a local environment. To offer scalable, deployable and platform independent code, the machine learning models should be implemented using best software practices from the initial design phase. Whenever it is required to analyze a big amount of data, loading the complete data adds latency. This latency in analysis is commonly seen in log files and health records [1]. This paper discusses the best practices for writing production level code for an example of epileptic seizure prediction. The design, analysis and visualization is done using Python language. The packages `kedro' and `kedro-viz' are discussed in detail for electroencephalograms (EEG) readings dataset available on UCI's (University of California, Irvine) machine learning repository [2]. The packages are used to create data pipelines for developing production level code. This paper is a preliminary effort to demonstrate the basics of designing production level models including pipelines taking an example of epileptic seizure prediction.},
keywords={Computational modeling;Pipelines;Software algorithms;Production;Machine learning;Predictive models;Writing;kedro;kedro-viz;python;data pipelines;machine learning model;production level code;epileptic seizure prediction},
doi={10.1109/ICACCS51430.2021.9442035},
ISSN={2575-7288},
month={March},}
@INPROCEEDINGS{9529498,
author={Jarry, Richard and Kobayashi, Satoru and Fukuda, Kensuke},
booktitle={2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)}, title={A Quantitative Causal Analysis for Network Log Data},
year={2021},
volume={},
number={},
pages={1437-1442},
abstract={Data logs from network devices are primary data to understand the current status of operational networks. However, since many and heterogeneous devices generate network logs, extracting information on the network status from such logs is not an easy task in network operation, e.g., root cause analysis of network events. Though multi-variate time-series based log analyses extract correlation structure of the logs, identifying causality of the network logs is still a complex and challenging problem. The state of the art algorithm called the PC algorithm had been applied to network log analysis, but it has two fundamental limitations; (1) Generated graphs still have many undirected edges, and (2) Edges have no weight (whether plausible causality or not). To overcome these two limitations, in this paper, we rely on MixedLiNGAM to network log analysis; This algorithm produces weighted DAGs from a set of multivariate log time series. In order to show the effectiveness of the proposed method, we apply MixedLiNGAM to a set of syslog data collected at a research and education network in Japan, and then compare output causal graphs generated by MixedLiNGAM and the PC algorithm. Our result demonstrates that obtained weighted directional edges help better understand the root cause of the network events.},
keywords={Root cause analysis;Correlation;Conferences;Time series analysis;Software algorithms;Software;Inference algorithms;Log analysis;Network logs;Causal inference},
doi={10.1109/COMPSAC51774.2021.00213},
ISSN={0730-3157},
month={July},}
@ARTICLE{9565919,
author={Kim, Kyoung-Sook and Pham, Dinh-Lam and Kim, Kwanghoon Pio},
journal={IEEE Access}, title={ρ-Algorithm: A SICN-Oriented Process Mining Framework},
year={2021},
volume={9},
number={},
pages={139852-139875},
abstract={This paper devises an algorithmic process mining framework characterized by the mathematical process model of structured information control nets (SICN) and the concept of mass-driven $\rho $ -function as a decision-making criterion of structural process patterns. In order to prove the functional correctness of the proposed algorithmic framework, this paper also implements all the related algorithms as a process mining system and carries out an operational experiment on a typical synthetic dataset of process enactment event logs prepared and released in the 4TU Centre for Research Data. The core contribution of the paper is just the algorithmic framework development named as the $\rho $ -Algorithm, which ought to be a novel approach not only for mining all the primitive process patterns, such as linear (sequential), disjunctive (selective-OR), conjunctive (parallel-AND), and repetitive (iterative-LOOP) process patterns, with perfectly keeping the structural properties of matched pairing and proper nesting, but also for reasonably discovering structured (even unstructured) information control nets from such IEEE XES-formatted datasets of process enactment event logs. The mining functionality of the $\rho $ -Algorithm is made up of three stepwise algorithms: STEP-1, STEP-2 and STEP-3 algorithms, and these algorithms are formally described as an algorithmic framework supported by the conceptual process mining architecture with a series of theoretical concepts with the temporal work-case model and the temporal loop-case model. Finally, we validate the functional correctness as well as the discovery perfectness of the proposed algorithmic framework named as $\rho $ -Algorithm by deploying the implemented $\rho $ -Algorithm on a synthetic, non-noise and IEEE XES-formatted dataset of process enactment event logs recorded from the 10,000 work-cases with 113 activities of the Petrinet-oriented process model named as the Large Bank Transaction Process Model.},
keywords={Task analysis;Process control;Mathematical models;Data mining;Organizations;Standards organizations;Analytical models;Structured information control net;process mining;process reengineering;process analyzing;process discovery and rediscovery;process enactment event log datasets},
doi={10.1109/ACCESS.2021.3119011},
ISSN={2169-3536},
month={},}
@ARTICLE{9497654,
author={Guo, Yi and Guo, Shunan and Jin, Zhuochen and Kaul, Smiti and Gotz, David and Cao, Nan},
journal={IEEE Transactions on Visualization and Computer Graphics}, title={A Survey on Visual Analysis of Event Sequence Data},
year={2021},
volume={},
number={},
pages={1-1},
abstract={Event sequence data record series of discrete events in the time order of occurrence. They are commonly observed in a variety of applications ranging from electronic health records to network logs, with the characteristics of large-scale, high-dimensional and heterogeneous. This high complexity of event sequence data makes it difficult for analysts to manually explore and find patterns, resulting in ever-increasing needs for computational and perceptual aids from visual analytics techniques to extract and communicate insights from event sequence datasets. In this paper, we review the state-of-the-art visual analytics approaches, characterize them with our proposed design space, and categorize them based on analytical tasks and applications.},
keywords={Data visualization;Visual analytics;Task analysis;Data mining;Sequences;Pipelines;Medical diagnostic imaging;Visual Analysis;Event Sequences;Visualization},
doi={10.1109/TVCG.2021.3100413},
ISSN={1941-0506},
month={},}
@INPROCEEDINGS{9402087,
author={Gonzalez, Danielle and Zimmermann, Thomas and Godefroid, Patrice and Schaefer, Max},
booktitle={2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)}, title={Anomalicious: Automated Detection of Anomalous and Potentially Malicious Commits on GitHub},
year={2021},
volume={},
number={},
pages={258-267},
abstract={Security is critical to the adoption of open source software (OSS), yet few automated solutions currently exist to help detect and prevent maliciouscontributions from infecting open source repositories. On GitHub, a primary host of OSS, repositories contain not only code but also a wealth of commitrelated and contextual metadata - whatifthismetadatacouldbeusedtoautomaticallyidentifymaliciousOSScontributions? In this work, we show how to use only commit logs and repository metadata to automatically detect anomalous and potentially malicious commits. We identify and evaluate several relevant factors which can be automatically computed from this data, such as the modification of sensitive files, outlier change properties, or a lack of trust in the commit's author. Our tool, Anomalicious, automatically computes these factors and considers them holistically using a rule-based decision model. In an evaluation on a data set of 15 malware-infected repositories, Anomalicious showed promising results and identified 53.33% of malicious commits, while flagging less than 1% of commits for most repositories. Additionally, the tool found other interesting anomalies that are not related to malicious commits in an analysis of repositories with no known malicious commits.},
keywords={Ecosystems;Tools;Metadata;Security;Open source software;Software engineering;Software development management;anomaly detection;malicious commits;supply chain attacks},
doi={10.1109/ICSE-SEIP52600.2021.00035},
ISSN={},
month={May},}
@ARTICLE{9293262,
author={Meng, Zhaoyi and Xiong, Yan and Huang, Wenchao and Miao, Fuyou and Huang, Jianmeng},
journal={IEEE Transactions on Information Forensics and Security}, title={AppAngio: Revealing Contextual Information of Android App Behaviors by API-Level Audit Logs},
year={2021},
volume={16},
number={},
pages={1912-1927},
abstract={Android users are now suffering severe threats from unwanted behaviors of various apps. The analysis of apps' audit logs is one of the essential methods for the security analysts of various companies to unveil the underlying maliciousness within apps. We propose and implement AppAngio, a novel system that reveals contextual information in Android app behaviors by API-level audit logs. Our goal is to help security analysts understand how the target apps worked and facilitate the identification of the maliciousness within apps. The key module of AppAngio is identifying the path matched with the logs on the app's control-flow graphs (CFGs). The challenge, however, is that the limited-quantity logs may incur high computational complexity in the log matching, where there are a large number of candidates caused by the coupling relation of successive logs. To address the challenge, we propose a divide and conquer strategy that precisely positions the nodes matched with log records on the corresponding CFGs and connects the nodes with as few backtracks as possible. Our experiments show that AppAngio reveals contextual information of behaviors in real-world apps. Moreover, the revealed results assist the analysts in identifying the maliciousness of app behaviors and complement existing analysis schemes. Meanwhile, AppAngio incurs negligible performance overhead on the real device in the experiments.},
keywords={Runtime;Security;Computational complexity;Performance evaluation;Malware;Feature extraction;Couplings;Contextual reveal;log matching;divide and conquer;Android security},
doi={10.1109/TIFS.2020.3044867},
ISSN={1556-6021},
month={},}
@ARTICLE{9397337,
author={Chen, An Ran and Chen, Tse-Hsun Peter and Wang, Shaowei},
journal={IEEE Transactions on Software Engineering}, title={Pathidea: Improving Information Retrieval-Based Bug Localization by Re-Constructing Execution Paths Using Logs},
year={2021},
volume={},
number={},
pages={1-1},
abstract={To assist developers with debugging and analyzing bug reports, researchers have proposed information retrieval-based bug localization (IRBL) approaches. IRBL approaches leverage the textual information in bug reports as queries to generate a ranked list of potential buggy files that may need further investigation. Although IRBL approaches have shown promising results, most prior research only leverages the textual information that is visible in bug reports, such as bug description or title. However, in addition to the textual description of the bug, developers also often attach logs in bug reports. Logs provide important information that can be used to re-construct the system execution paths when an issue happens and assist developers with debugging. In this paper, we propose an IRBL approach, Pathidea, which leverages logs in bug reports to re-construct execution paths and helps improve the results of bug localization. Pathidea uses static analysis to create a file-level call graph, and re-constructs the call paths from the reported logs. We evaluate Pathidea on eight open source systems, with a total of 1,273 bug reports that contain logs. We find that Pathidea achieves a high recall (up to 51.9% for Top@5). On average, Pathidea achieves an improvement that varies from 8% to 21% and 5% to 21% over BRTracer in terms of Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) across studied systems, respectively. Moreover, we find that the re-constructed execution paths can also complement other IRBL approaches by providing a 10% and 8% improvement in terms of MAP and MRR, respectively. Finally, we conduct a parameter sensitivity analysis and provide recommendations on setting the parameter values when applying Pathidea.},
keywords={Computer bugs;Location awareness;Debugging;Static analysis;Information retrieval;History;Tools;bug localization;log;bug report;information retrieval},
doi={10.1109/TSE.2021.3071473},
ISSN={1939-3520},
month={},}
@ARTICLE{9536968,
author={Sun, Mengyu and Zhou, Zhangbing and Xue, Xiao and Zhang, Wenbo and Hung, Patrick C. K.},
journal={IEEE Internet of Things Journal}, title={Service Configuration Optimization in Edge-Cloud Networks Leveraging Log Analysis},
year={2021},
volume={},
number={},
pages={1-1},
abstract={The edge-cloud collaboration network is promising to support complex requirements with temporal constraints, where a requirement can be achieved through the composition of computation-demanding and delay-sensitive services. In this setting, most services should be optimally configured at the network edge, in order to decreasing service response latency and reducing network resource consumption. To address this challenge, this paper proposes an optimal service configuration mechanism, where temporal constraints between services are mined from event logs through our temporal interval discovery mechanism. Service configuration is formulated as a constrained multi-objective optimization problem, which is solved by our improved non-dominated sorting genetic algorithm II. Extensive experiments are conducted, and evaluation results demonstrate that our approach can find the close-to-optimal service configuration in comparison with the state-of-the-art techniques in terms of delay sensitivity and energy efficiency, especially when edge nodes can co-host a relatively large number of services.},
keywords={Business;Cloud computing;Internet of Things;Task analysis;Optimization;Delays;Registers;Service Configuration Optimization;Edge-Cloud Networks;Temporal Constraints;Log Analysis.},
doi={10.1109/JIOT.2021.3112609},
ISSN={2327-4662},
month={},}
@INPROCEEDINGS{9456495,
author={ShuangChang, Feng and Pengzhao, Zhang and Wenhao, Shen and Jie, Chen},
booktitle={2021 IEEE International Conference on Artificial Intelligence and Industrial Design (AIID)}, title={Research on elevator speed governor calibration software based on image processing},
year={2021},
volume={},
number={},
pages={517-520},
abstract={As a momentous device to ensure the safety of the elevator operation, the speed governor plays an important protective role of the safety control components in the event of overspeed and rope breaking. In order to judge the action of the speed governor, this article research and develop a software which carries out color processing, statistics and detection on the images according to the photos of the governor. The software can achieve picture browsing and reading function, image processing and parameter adjustment function, statistical results display function and log output function. Through processing the image and outputting the situation of the governor, the software provides professional reference of the elevator safety for the inspector. By comparing the obtained electric action speed with the specified value in the standard, the inspector can judge whether the operation speed of the speed governor meets the requirements.},
keywords={Image color analysis;Conferences;Software;Elevators;Safety;Calibration;Artificial intelligence;elevator inspection;speed governor;image processing},
doi={10.1109/AIID51893.2021.9456495},
ISSN={},
month={May},}
@INPROCEEDINGS{9486307,
author={Steverson, Kai and Carlin, Caleb and Mullin, Jonathan and Ahiskali, Metin},
booktitle={2021 International Conference on Military Communication and Information Systems (ICMCIS)}, title={Cyber Intrusion Detection using Natural Language Processing on Windows Event Logs},
year={2021},
volume={},
number={},
pages={1-7},
abstract={This paper applies deep learning and natural language processing to Windows Event Logs for the purpose of detecting cyber attacks. Data is collected from an emulated network that models an enterprise network. The network experiences a cyber attack that uses a spear phishing email and the eternal blue exploit to spread botnet malware. A machine learning anomaly detection algorithm is constructed using the transformer model and self-supervised training. The model is able to detect both the compromised devices as well as attack timing with near perfect precision and recall. These results suggest that this approach could function as the detection portion of an autonomous end point defense system wherein each device is able to react independently to potential intrusions.},
keywords={Training;Military communication;Machine learning algorithms;Phishing;Intrusion detection;Natural language processing;Malware;cybersecurity;machine learning;artificial intelligence;natural language processing;botnet},
doi={10.1109/ICMCIS52405.2021.9486307},
ISSN={},
month={May},}
@INPROCEEDINGS{9402069,
author={Henkel, Jordan and Silva, Denini and Teixeira, Leopoldo and d’Amorim, Marcelo and Reps, Thomas},
booktitle={2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)}, title={Shipwright: A Human-in-the-Loop System for Dockerfile Repair},
year={2021},
volume={},
number={},
pages={1148-1160},
abstract={Docker is a tool for lightweight OS-level virtualization. Docker images are created by performing a build, controlled by a source-level artifact called a Dockerfile. We studied Dockerfiles on GitHub, and-to our great surprise-found that over a quarter of the examined Dockerfiles failed to build (and thus to produce images). To address this problem, we propose SHIPWRIGHT, a human-in-the-loop system for finding repairs to broken Dockerfiles. SHIPWRIGHT uses a modified version of the BERT language model to embed build logs and to cluster broken Dockerfiles. Using these clusters and a search-based procedure, we were able to design 13 rules for making automated repairs to Dockerfiles. With the aid of SHIPWRIGHT, we submitted 45 pull requests (with a 42.2% acceptance rate) to GitHub projects with broken Dockerfiles. Furthermore, in a "time-travel" analysis of broken Dockerfiles that were later fixed, we found that SHIPWRIGHT proposed repairs that were equivalent to human-authored patches in 22.77% of the cases we studied. Finally, we compared our work with recent, state-of-the-art, static Dockerfile analyses, and found that, while static tools detected possible build-failure-inducing issues in 20.6-33.8% of the files we examined, SHIPWRIGHT was able to detect possible issues in 73.25% of the files and, additionally, provide automated repairs for 18.9% of the files.},
keywords={Bit error rate;Maintenance engineering;Tools;Virtualization;Software development management;Software engineering;Docker;DevOps;Repair},
doi={10.1109/ICSE43902.2021.00106},
ISSN={1558-1225},
month={May},}
@ARTICLE{9399143,
author={Pham, Dinh-Lam and Ahn, Hyun and Kim, Kyoung-Sook and Kim, Kwanghoon Pio},
journal={IEEE Access}, title={Process-Aware Enterprise Social Network Prediction and Experiment Using LSTM Neural Network Models},
year={2021},
volume={9},
number={},
pages={57922-57940},
abstract={Process mining that exploits system event logs provides significant information regarding operating events in an organization. By discovering process models and analyzing social network metrics created throughout the operation of the information system, we can better understand the roles of performers and characteristics of activities, and more easily predict what will occur in the next operation of a system. By using accurate and valuable predicted information, we can create effective environments, provide suitable materials to perform activities better, and facilitate more efficient operations. In this study, we apply the long short-term memory, a variant of the recurrent neural network, to predict the enterprise social networks that are formed through information regarding a business system's operation. More precisely, we apply the multivariate multi-step long short-term memory model to predict not only the next activity and next performer, but also all the variants of a process-aware enterprise social network based on the next performer predictions using a probability threshold. Furthermore, we conduct an experimental evaluation on the real-life event logs and compare our results with some related researches. The results indicate that our approach creates a useful model to predict an enterprise social network and provides metrics to improve the operation of an information system based on the predicted information.},
keywords={Predictive models;Mathematical model;Social networking (online);Training;Recurrent neural networks;Modulation;Data models;Process mining;long short-term memory neural network;process-aware enterprise social network;next event prediction},
doi={10.1109/ACCESS.2021.3071789},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{9478232,
author={Shobhit and Bera, Padmalochan},
booktitle={2021 International Conference on Cyber Situational Awareness, Data Analytics and Assessment (CyberSA)}, title={ModCGAN: A Multimodal Approach to Detect New Malware},
year={2021},
volume={},
number={},
pages={1-2},
abstract={Advancements in deep learning have enabled the development of effective solutions to different real-world problems. In this paper, we propose a deep learning model for malware detection in computing and network systems. The architecture of our proposed system consists of three modules, namely Image auto-encoder, Text auto-encoder, and GAN. Any incoming application should go through a proxy to create a log file. The log files that contain API call sequences are used to generate image and text representations that capture the dynamic behavior of the malware. Our system integrates the features from image and text auto-encoder into a single feature vector. The feature vector extracted is individually used to classify whether the log file is from a malware application. We have created a new dataset containing different newly generated malware of similar distribution. In the detection phase, the incoming log file is converted into a single feature vector by the same method as in generation. Based on the similarity index between the newly generated malware and incoming file, log files are classified as malware or benign. Thus, the newly generated dataset serves as a comparison base for the classification of log files. We have evaluated our system through a test dataset by comparing it with the newly generated malware.},
keywords={Deep learning;Data analysis;Computational modeling;Computer architecture;Feature extraction;Generative adversarial networks;Malware;Dynamic analysis;types of attacks;autoencoders;generative adversarial networks;malware generation;similarity calculation},
doi={10.1109/CyberSA52016.2021.9478232},
ISSN={},
month={June},}
@INPROCEEDINGS{9473923,
author={Wu, Qiulin and Zhou, You and Wu, Fei and Wang, Ke and Lv, Hao and Wan, Jiguang and Xie, Changsheng},
booktitle={2021 Design, Automation Test in Europe Conference Exhibition (DATE)}, title={SW-WAL: Leveraging Address Remapping of SSDs to Achieve Single-Write Write-Ahead Logging},
year={2021},
volume={},
number={},
pages={802-807},
abstract={Write-ahead logging (WAL) has been widely used to provide transactional atomicity in databases, such as SQLite and MySQL/InnoDB. However, the WAL introduces duplicate writes, where changes are recorded in the WAL file and then written to the database file, called checkpointing writes. On the other hand, NAND flash-based SSDs, which have an inherent indirection software layer, called flash translation layer (FTL), become commonplace in modern storage systems. Innovative SSD designs have been proposed to eliminate the WAL overheads by exploiting the FTL, such as providing an atomic write interface or utilizing its address remapping. However, these designs introduce significant performance overheads of maintaining and persisting extra transactional information to guarantee the transactional atomicity or mapping consistency. In this paper, we propose single-write WAL (SW-WAL), a novel cross-layer design, to eliminate WAL-induced duplicate writes on SSDs with minimal overheads. The SSD exposes an address remapping interface to the host, through which the checkpointing writes can be completed without conducting real data writes. To ensure the transactional atomicity and mapping consistency, we make the SSD aware of the transactional writes to the WAL file. Specifically, when transactional data are written to the WAL file, both transactional and mapping semantics are delivered from the host to the SSD and persisted in relevant flash pages as housekeeping metadata without any extra overheads. We implement a prototype of SW-WAL, which runs a popular database SQLite on an emulated NVMe SSD. Experimental results show that SW-WAL improves the database performance by up to 62% compared with original SQLite that bears the WAL overheads and up to 32% compared with the state-of-the-art design that eliminates the WAL overheads.},
keywords={Checkpointing;Databases;Atomic layer deposition;System performance;Semantics;Prototypes;Metadata},
doi={10.23919/DATE51398.2021.9473923},
ISSN={1558-1101},
month={Feb},}
@ARTICLE{9069446,
author={Ma, Chenlin and Shen, Zhaoyan and Wang, Jihe and Wang, Yi and Chen, Renhai and Guan, Yong and Shao, Zili},
journal={IEEE Transactions on Computers}, title={Tiler: An Autonomous Region-Based Scheme for SMR Storage},
year={2021},
volume={70},
number={2},
pages={291-304},
abstract={Shingled Magnetic Recording (SMR) Disks are adopted as a high-density, non-volatile media that significantly precedes conventional disks in both the storage capacity and cost. However, inefficient read-modify-writes (RMWs) greatly challenge the management of SMR disks. This article for the first time presents an approach called Tiler to manage SMR disks by dividing the physical space into small autonomous regions (ARs). Each AR can manage its space allocation, address mapping, and cleaning independently. By managing these ARs in a log-structured way, RMWs can be avoided; besides, ARs can also help update data when the adjacent tracks contain no valid data. Tiler is capable of partitioning a large-scale cleaning into self-contained-small-scale cleaning and thus, the data that need to be relocated are limited inside independent ARs, which further minimizes the performance overhead. Our experimental results show that Tiler can shorten the overall system response time by 50.21 percent and reduce the cleaning time by 90.24 percent on average.},
keywords={Cleaning;Writing;Computer science;Magnetic recording;Memory management;Resource management;Time factors;Shingled magnetic recording;cache management;garbage collection},
doi={10.1109/TC.2020.2988004},
ISSN={1557-9956},
month={Feb},}
@INPROCEEDINGS{9529762,
author={Amir-Mohammadian, Sepehr and Zowj, Afsoon Yousefi},
booktitle={2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)}, title={Towards Concurrent Audit Logging in Microservices},
year={2021},
volume={},
number={},
pages={1357-1362},
abstract={Information-algebraic models have been shown as effective semantic frameworks in the specification of audit logging requirements. These semantic frameworks underlie implementation models that formally guarantee correctness of audit logs. Recently, one such implementation model has been proposed for concurrent systems. In this paper, we study the deployment of an instrumentation tool based on this implementation model, aiming at microservices-based applications that are built by Java Spring framework. This tool instruments these applications according to a given logging specification, described in JSON. A set of events in one or more microservices may necessitate the generation of log entries in a specific microservice. Instrumentation of an application enables different microservices of that application to concurrently generate audit logs.},
keywords={Java;Logic programming;Instruments;Conferences;Semantics;Restful API;Tools;Audit logs;concurrent systems;microservices;programming languages;security},
doi={10.1109/COMPSAC51774.2021.00191},
ISSN={0730-3157},
month={July},}
@INPROCEEDINGS{9478117,
author={Maurya, Vinod Kumar and Chaudhari, Swati and Sirohi, Deepak Kumar and Tomar, Shailendra and Rajan, Alpana and Rawat, Anil},
booktitle={2021 2nd International Conference on Secure Cyber Computing and Communications (ICSCCC)}, title={Inimitable Approach to Detect amp; Quarantine Botnet Malware Infections in Network},
year={2021},
volume={},
number={},
pages={238-243},
abstract={Internet connected organizational networks are susceptible to malware attacks. Best of the existing anti malware solutions are known to detect only 80% of the malwares. Latest heuristic algorithms based antimalware solutions can also fail at times in resource constrained environments. Thus organizations have to formulate additional strategies to tackle malware attacks which escape detection by standard end point security systems. Special categories of malware called botnets are the most prominent and dangerous forms, which can cause large scale damage if not quarantined timely. These days, organizations mostly use proxy servers to access the Internet, thus traces of botnets using HTTP based communication channels can be extracted from proxy logs. We experimented with the Squid proxy log analysis techniques for detection of such botnets and have developed a system which provides potential solution to quarantine malware infected PCs in an organization. In this paper, we present the design, development and analysis of complete system. The system incorporates necessary functions to provide complete life cycle management of such PCs, starting from isolation to reinstatement in the production network. The system has been implemented in our organization having 2500 active users and two squid proxy servers. It has been observed that CPU utilization on the proxy servers is reduced by about 20% and the size of access log files by about 25%, which is a big gain for organizations with large number of users.},
keywords={Botnet;Standards organizations;Organizations;Malware;Servers;Security;Communication networks;Anti malware;Heuristic;Squid Proxy;Access log;Botnet;TCP_DENIED;quarantine},
doi={10.1109/ICSCCC51823.2021.9478117},
ISSN={},
month={May},}
@INPROCEEDINGS{9499524,
author={Kiperberg, Michael and Amit, Guy and Yeshooroon, Amir and Zaidenberg, Nezer J.},
booktitle={2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)}, title={Efficient DLP-visor: An efficient hypervisor-based DLP},
year={2021},
volume={},
number={},
pages={344-355},
abstract={Many organization consider insider threat for data theft to be one of the most severe threats. An insider may also leak sensitive information without malicious intent (as a result of social engineering) Data leakage prevention (DLP) systems attempt to prevent intentional or accidental disclosure of sensitive information by monitoring the content or the context in which the information is transferred, for example, in a file system, an email server, instant messengers. We present a context-sensitive DLP system, called Efficient DLP-Visor. We implemented DLP-visor as a thin hypervisor capable of intercepting system calls in Windows operating systems equipped with Kernel Patch Protection. By intercepting system calls that govern the file system, inter-process communications, networking, system register and system clipboard, DLP-Visor guarantees that sensitive information can never leave a predefined set of directories. The performance overhead of Efficient DLP-Visor (7.2%) allows its deployment in real-world applications. Efficient DLP-visor logs were improved for better detection and logging of a DLP event. On idle time Efficient DLP-visor deletes most of the data log while maintaining the important data of leaks and attack.},
keywords={Cloud computing;Virtual machine monitors;File systems;Organizations;Registers;Servers;Kernel},
doi={10.1109/CCGrid51090.2021.00044},
ISSN={},
month={May},}
@ARTICLE{9067076,
author={Koo, Jinhyung and Chung, Chanwoo and Arvind and Lee, Sungjin},
journal={IEEE Transactions on Computers}, title={A Case for Application-Managed Flash},
year={2021},
volume={70},
number={2},
pages={240-254},
abstract={We propose a new I/O architecture for NAND flash-based SSDs, called application-managed flash (AMF) and present two case studies to show its usefulness. In a typical SSD controller, an intermediate software layer, called the flash translation layer (FTL), is employed between NAND flash chips and a host interface. The main responsibility of an FTL is to provide interoperability with conventional HDDs, but this interoperability comes at the cost of extra hardware resources and degraded I/O performance. The proposed AMF refactors the flash storage architecture so that an SSD controller exposes append-only segments, which do not permit overwriting. This refactoring dramatically improves performance of applications and reduces hardware costs by allowing applications to directly manage flash storage with minimal supports from the SSD controller. In order to understand the benefits of AMF, we study two popular applications: a log-structured file system (F2FS) and a key-value store (RocksDB). Our experiments show that the DRAM in the flash controller is reduced by 128X and the performances of the file system and the key-value store improve by 80 and 54 percent, respectively, over conventional SSDs.},
keywords={Computer architecture;Hardware;Software;Media;Geometry;Interoperability;Random access memory;NAND flash;solid-state disks;file system;key-value store;flash translation layer},
doi={10.1109/TC.2020.2987569},
ISSN={1557-9956},
month={Feb},}
@INPROCEEDINGS{9460479,
author={Gugnani, Shashank and Li, Tianxi and Lu, Xiaoyi},
booktitle={2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, title={NVMe-CR: A Scalable Ephemeral Storage Runtime for Checkpoint/Restart with NVMe-over-Fabrics},
year={2021},
volume={},
number={},
pages={172-181},
abstract={Emerging SSDs with NVMe-over-Fabrics (NVMf) support provide new opportunities to significantly improve the performance of IO-intensive HPC applications. However, state-of-the-art parallel filesystems can not extract the best possible performance from fast NVMe SSDs and are not designed for latency-critical ephemeral IO tasks, such as checkpoint/restart. In this paper, we propose a powerful abstraction called microfs to peel away unnecessary software layers and eliminate namespace coordination. Building upon this abstraction, we present the design of NVMe-CR, a scalable ephemeral storage runtime for clusters with disaggregated compute and storage. NVMe-CR proposes techniques like metadata provenance, log record coalescing, and logically isolated shared device access, built around the microfs abstraction, to reduce the overhead of writing millions of concurrent checkpoint files. NVMe-CR utilizes high-density allflash arrays accessible via NVMf to absorb bursty checkpoint IO and increase the progress rates of applications obliviously. Using the ECP CoMD application as a use case, results show that our runtime can achieve near perfect (> 0.96) efficiency at 448 processes and reduce checkpoint overhead by as much as 2x compared to state-of-the-art storage systems.},
keywords={Distributed processing;Runtime;Buildings;Writing;Metadata;Software;Task analysis;Checkpoint/Restart;NVMe;NVMf;Exascale},
doi={10.1109/IPDPS49936.2021.00026},
ISSN={1530-2075},
month={May},}
@ARTICLE{8979375,
author={Ezeme, Okwudili M. and Azim, Akramul and Mahmoud, Qusay H.},
journal={IEEE Transactions on Emerging Topics in Computing}, title={PESKEA: Anomaly Detection Framework for Profiling Kernel Event Attributes in Embedded Systems},
year={2021},
volume={9},
number={2},
pages={957-971},
abstract={In the software development life cycle, we use the execution traces of a given application to examine the behavior of the software when an error occurs or to monitor the software performance and compliance. However, this type of application trace analysis focuses on checking the performance of the software against its design goals. Conversely, the operating system (OS) sits between the application and the hardware, and traces logged from this layer capture the behavior of the embedded system and not just the application. Hence, an analysis of the kernel events captures the system-wide performance of the embedded system. Consequently, we present a feature-based anomaly detection framework called PESKEA, which exploits the statistical variance of the features in the execution traces of an embedded OS to perform trace classification, and subsequently, anomaly detection. We test PESKEA with two public datasets we refer to as Dataset I and Dataset II. On Dataset I, PESKEA results show a 3 to 6 percent improvement in the true positive rate (TPR) of Dataset I compared to the previous work tested on this dataset, and scores between 88.37 to 100 percent in Dataset II. We hope to test PESKEA on non-UAV embedded control application datasets in future work.},
keywords={Anomaly detection;Embedded systems;Feature extraction;Monitoring;Kernel;Hardware;Context modeling;anomaly detection framework;embedded operating system;machine learning},
doi={10.1109/TETC.2020.2971251},
ISSN={2168-6750},
month={April},}
@ARTICLE{9378537,
author={Ahn, Hyun and Kim, Kwanghoon Pio},
journal={IEEE Access}, title={Organizational Closeness Centralities of Workflow-Supported Performer-to-Activity Affiliation Networks},
year={2021},
volume={9},
number={},
pages={48555-48582},
abstract={A workflow model specifies execution sequences of the associated activities and their affiliated relationships with roles, performers, invoked-applications, and relevant data. These affiliated relationships exhibit a series of valuable human-centered organizational knowledge and are utilized for exploring human resource's work patterns. This paper focuses not only on a specific type of affiliated relationships between performers and activities, in particular, which forms a performer-to-activity affiliation network, but also on a specific type of analysis techniques, which builds a closeness centrality measurement approach for quantifying the degrees of farnesses between performers as well as between activities. In other words, this paper investigates a series of formal approaches for building organizational closeness centrality measurement techniques on the specific type of affiliation networks. The investigation mainly deploys two types of algorithmic formalisms along with an operational example, which are measuring performer-centered organizational closeness centralities and activity-centered organizational closeness centralities, respectively. In order to validate the deployed algorithmic equations, the paper carries out a couple of operational experiments; One is on an ICN-based workflow package and the other is on a discovered workflow model mined from a dataset of workflow event logs. Summarily, this paper devises a series of algorithms and equations for measuring closeness centralities of activities, verify the devised algorithms and their related equations along with operational examples, and discuss the ultimate implications of these analysis techniques of organizational closeness centrality measurements as the performer-to-activity affiliation networking knowledge in workflow-supported organizations.},
keywords={Social networking (online);Knowledge engineering;Visualization;Organizations;Analytical models;Mathematical model;Particle measurements;Workflow process;performer-activity affiliations;information control net;organizational social network;workflow-supported affiliation network;closeness centrality;workflow intelligence;human resource management},
doi={10.1109/ACCESS.2021.3065925},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{9397123,
author={Kumar, Amit and Sivak Kumar, M. and Namdeo, Varsha},
booktitle={2021 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS)}, title={A Regression-based Hybrid Machine Learning Technique to Enhance the Database Migration in Cloud Environment},
year={2021},
volume={},
number={},
pages={149-155},
abstract={The report of cloud computing in recent years has prompted circumstances that usually has lead to numerous advancements & novel mechanisms. The technologies available in the cloud have been prevalent for businesses as well as people who understand that cloud computing is a significant problem, even though they don't know why. We present a methodology that accurately assesses the migration cost, relocation length with cloud operating cost of the social databases, and upgraded the execution. The first step in our approach is to acquire workloads and structure models for moving the database from the database logs as well as from schemes. The second step uses these models to perform a discrete form of event simulation for estimated costs and time. We have implemented the software tools that simplify our approach in both phases. A comprehensive review contrasts our approach to the effects of real-world cloud data migration.},
keywords={Cloud computing;Databases;Computational modeling;Machine learning;Tools;Software reliability;Software tools;Cloud Computing;Database Migration;Hybrid HGSA;Enterprise Systems},
doi={10.1109/ICCCIS51004.2021.9397123},
ISSN={},
month={Feb},}
@INPROCEEDINGS{9418136,
author={Perera, Akalanka and Rathnayaka, Shanith and Perera, N. D. and Madushanka, W.W. and Senarathne, Amila Nuwan},
booktitle={2021 6th International Conference for Convergence in Technology (I2CT)}, title={The Next Gen Security Operation Center},
year={2021},
volume={},
number={},
pages={1-9},
abstract={Due to the evolving Cyber threat landscape, Cyber criminals have found new and ingenious ways of breaching defenses in networks. Due to the sheer destruction these threat actors can cause to an organization, most modern-day organizations have focused their attention towards protecting their critical infrastructure and sensitive information through multiple methods. The main defense against both internal and external threats to an organization has been the implementation of the Security Operations Center (SOC) which is responsible for monitoring, analyzing and mitigating incoming threats. At the heart of the Security Operations Center, lies the Security Information and Event Management system (SIEM) which is utilized by SOC analysts as the centralized point where all security notifications from various security technologies including firewalls, IPS/IDS and Anti-Virus logs are collected and visualized. The effective operation of SOC in an organization is dependent on how well the SIEM filters log events and generates actual alerts. Here lies the major problem faced by SOC analysts in detecting threats. If proper alert correlation is not accomplished, analysts would have to deal with too much alert noise due to a high false positive count. This would ultimately cause analysts to miss critical security incidents, thus causing severe implications to the organization's security. The performance of a SIEM can be enhanced through adding various functionalities such as Threat Hunting, Threat Intelligence and malware identification and prevention in order to reduce false positive alarms, threat framework and machine learning which would increase the accuracy and efficiency of the overall Security Operations process of an organization. Even though many products which provide these additional functionalities exist in the current market, they can be too expensive for smaller scale organizations to handle. Our aim is to make security operations deliverable to any organization regardless of the size and scale without any financial implications and enhance its functionalities with the aid of Advanced Machine Learning Techniques.},
keywords={Clustering algorithms;Machine learning;Artificial neural networks;Companies;Tools;Security;Feeds;K-Means;Natural Language Processing (NLP);Machine Learning algorithm;Threat Intelligence;Artificial Neural Network (ANN);Security Operation Center (SOC);security incident and event management (SIEM)},
doi={10.1109/I2CT51068.2021.9418136},
ISSN={},
month={April},}
@ARTICLE{8823943,
author={Mu, Dongliang and Du, Yunlan and Xu, Jianhao and Xu, Jun and Xing, Xinyu and Mao, Bing and Liu, Peng},
journal={IEEE Transactions on Software Engineering}, title={POMP++: Facilitating Postmortem Program Diagnosis with Value-Set Analysis},
year={2021},
volume={47},
number={9},
pages={1929-1942},
abstract={With the emergence of hardware-assisted processor tracing, execution traces can be logged with lower runtime overhead and integrated into the core dump. In comparison with an ordinary core dump, such a new post-crash artifact provides software developers and security analysts with more clues to a program crash. However, existing works only rely on the resolved runtime information, which leads to the limitation in data flow recovery within long execution traces. In this work, we propose POMP++, an automated tool to facilitate the analysis of post-crash artifacts. More specifically, POMP++ introduces a reverse execution mechanism to construct the data flow that a program followed prior to its crash. Furthermore, POMP++ utilizes Value-set Analysis, which helps to verify memory alias relation, to improve the ability of data flow recovery. With the restored data flow, POMP++ then performs backward taint analysis and highlights program statements that actually contribute to the crash. We have implemented POMP++ for Linux system on x86-32 platform, and tested it against various crashes resulting from 31 distinct real-world security vulnerabilities. The evaluation shows that, our work can pinpoint the root causes in 29 cases, increase the number of recovered memory addresses by 12 percent and reduce the execution time by 60 percent compared with existing reverse execution. In short, POMP++ can accurately and efficiently pinpoint program statements that truly contribute to the crashes, making failure diagnosis significantly convenient.},
keywords={Computer crashes;Software;Security;Core dumps;Registers;Runtime;Tools;Postmortem program diagnosis;failure diagnosis;reverse execution;value-set analysis},
doi={10.1109/TSE.2019.2939528},
ISSN={1939-3520},
month={Sep.},}
@ARTICLE{9066943,
author={Wu, Sung-Ming and Lin, Kai-Hsiang and Chang, Li-Pin},
journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, title={Integrating LSM Trees With Multichip Flash Translation Layer for Write-Efficient KVSSDs},
year={2021},
volume={40},
number={1},
pages={87-100},
abstract={Log-structured-merge (LSM) trees are a highly write-optimized data structure for lightweight, high-performance key-value (KV) stores. Furthermore, solid-state drives (SSDs) are a crucial component for I/O acceleration. Conventional LSM-over-SSD designs involve multiple software layers, including the LSM tree, host file system, and flash translation layer (FTL), which introduce cascading write amplifications. To manage the write amplifications from different layers, we propose KVSSDs, which are a close integration of LSM trees and the FTL. KVSSDs exploit the FTL mapping mechanism to implement copy-free compaction of LSM trees, and they enables direct data allocation in flash memory for efficient garbage collection. Our design also uses a fine-grained, dynamic striping policy to fully exploit the rich internal parallelism of multichip SSDs. The experimental results indicated that our LSM-SSD integrated design reduced the write amplification by 86% and improved the throughput by 383% compared with a conventional LSM-over-SSD design.},
keywords={Compaction;Software;Parallel processing;Throughput;Writing;Resource management;Integrated design;Flash memory;key-value (KV) store;log-structured-merge (LSM) tree;solid-state drive (SSD)},
doi={10.1109/TCAD.2020.2987781},
ISSN={1937-4151},
month={Jan},}
@INPROCEEDINGS{9502454,
author={Létourneau, Louis-Simon and El Jabri, Chaymae and Frappier, Marc and Tardif, Pierre-Martin and Lépine, Guy and Boisvert, Guillaume},
booktitle={2021 51st Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W)}, title={Statistical Approach For Cloud Security: Microsoft Office 365 audit logs case study},
year={2021},
volume={},
number={},
pages={15-18},
abstract={Detecting abnormal user interaction with a computer system is paramount to prevent unauthorized access. With the growth in the use of cloud services, both from a personal and business perspective, cloud service accounts are a profitable target for cyber attacks. This work is a practical attempt to improve SaaS security through accessible and adaptable solutions. We used kernel density estimation in order to classify events from Microsoft audit logs. We were able to model the active hours of each user within an organization and then detect when an action was made outside of these hours.},
keywords={Conferences;Estimation;Organizations;Machine learning;Bandwidth;Random variables;Kernel;anomaly detection;SaaS security;kernel density estimation;unsupervised learning},
doi={10.1109/DSN-W52860.2021.00014},
ISSN={2325-6664},
month={June},}
@INPROCEEDINGS{9505224,
author={Bichhawat, Abhishek and Fredrikson, Matt and Yang, Jean},
booktitle={2021 IEEE 34th Computer Security Foundations Symposium (CSF)}, title={Automating Audit with Policy Inference},
year={2021},
volume={},
number={},
pages={1-16},
abstract={The risk posed by high-profile data breaches has raised the stakes for adhering to data access policies for many organizations, but the complexity of both the policies themselves and the applications that must obey them raises significant challenges. To mitigate this risk, fine-grained audit of access to private data has become common practice, but this is a costly, time-consuming, and error-prone process.We propose an approach for automating much of the work required for fine-grained audit of private data access. Starting from the assumption that the auditor does not have an explicit, formal description of the correct policy, but is able to decide whether a given policy fragment is partially correct, our approach gradually infers a policy from audit log entries. When the auditor determines that a proposed policy fragment is appropriate, it is added to the system’s mechanized policy, and future log entries to which the fragment applies can be dealt with automatically. We prove that for a general class of attribute-based data policies, this inference process satisfies a monotonicity property which implies that eventually, the mechanized policy will comprise the full set of access rules, and no further manual audit is necessary. Finally, we evaluate this approach using a case study involving synthetic electronic medical records and the HIPAA rule, and show that the inferred mechanized policy quickly converges to the full, stable rule, significantly reducing the amount of effort needed to ensure compliance in a practical setting.},
keywords={Organizations;Medical services;Manuals;Data breach;Inference algorithms;Complexity theory;Electronic medical records;Auditing;policy inference;logic;access control},
doi={10.1109/CSF51468.2021.00001},
ISSN={2374-8303},
month={June},}
@INPROCEEDINGS{9529370,
author={Dass, Shuvalaxmi and Datta, Prerit and Namin, Akbar Siami},
booktitle={2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)}, title={Attack Prediction using Hidden Markov Model},
year={2021},
volume={},
number={},
pages={1695-1702},
abstract={It is important to predict any adversarial attacks and their types to enable effective defense systems. Often it is hard to label such activities as malicious ones without adequate analytical reasoning. We propose the use of Hidden Markov Model (HMM) to predict the family of related attacks. Our proposed model is based on the observations often agglomerated in the form of log files and from the target or the victim’s perspective. We have built an HMM-based prediction model and implemented our proposed approach using Viterbi algorithm, which generates a sequence of states corresponding to stages of a particular attack. As a proof of concept and also to demonstrate the performance of the model, we have conducted a case study on predicting a family of attacks called Action Spoofing.},
keywords={Viterbi algorithm;Computational modeling;Conferences;Software algorithms;Hidden Markov models;Predictive models;Prediction algorithms;Hidden Markov Model;Viterbi algorithm;At-tack prediction;Attack family;Action spoofing},
doi={10.1109/COMPSAC51774.2021.00253},
ISSN={0730-3157},
month={July},}
@INBOOK{9514772,
author={Dalbhanjan, Ronald and Chatterjee, Sudipta and Gogoi, Rajdeep and Pathak, Tanuj and Sahay, Shivam},
booktitle={Implementing Enterprise Cybersecurity with Open-Source Software and Standard Architecture}, title={3 Implementation of Honeypot, NIDs, and HIDs Technologies in SOC Environment},
year={2021},
volume={},
number={},
pages={51-66},
abstract={The cybersecurity industry is often disinclined to adopt new technologies due to perceived complications, assumed dependencies, and unclear information about the benefits. Putting the right information security architecture into practice within an organization can be an intimidating challenge. Many organizations have enforced a security information and event management (SIEM) system to comply with the logging requirements of various security standards, only to find that it does not meet their information security expectations. They do not get the benefit of the value they spend on the proprietary SIEM tools. The solution should be simple, affordable, and maintainable with readily available resources and open-source products. The aim of this study is to understand honeypot technologies, networkbased intrusion detection systems (NIDS) and host-based intrusion detection systems (HIDS), and their implementation in a scalable security operation center (SOC) environment with the help of open-source tools which would include monitoring and investigation. Based on our learning, we have designed a virtualized SOC environment protected with firewall solution like pfSense, threat hunting solution like Security Onion which can be used for monitoring network traffic both internally and externally, further integrated with honeypot technology, i.e., T-Pot for better security enhancements. Threat intelligence information from this study is used to prepare, prevent, and identify cyber threats looking to take advantage of valuable resources. Lastly, conclusions and recommendations from our study will provide the best practices for implementing effective defense tools for various micro, small and medium enterprises (MSME) with affordable budgets.},
keywords={},
doi={},
ISSN={},
publisher={River Publishers},
isbn={9788770224222},
url={https://ieeexplore-ieee-org.proxy.lib.uwaterloo.ca/document/9514772},}
@INPROCEEDINGS{9441338,
author={Patel, Mansi and Prabhu, S Raja and Agrawal, Animesh Kumar},
booktitle={2021 8th International Conference on Computing for Sustainable Global Development (INDIACom)}, title={Network Traffic Analysis for Real-Time Detection of Cyber Attacks},
year={2021},
volume={},
number={},
pages={642-646},
abstract={Preventing the cyberattacks has been a concern for any organization. In this research, the authors propose a novel method to detect cyberattacks by monitoring and analyzing the network traffic. It was observed that the various log files that are created in the server does not contain all the relevant traces to detect a cyberattack. Hence, the HTTP traffic to the web server was analyzed to detect any potential cyberattacks. To validate the research, a web server was simulated using the Opensource Damn Vulnerable Web Application (DVWA) and the cyberattacks were simulated as per the OWASP standards. A python program was scripted that captured the network traffic to the DVWA server. This traffic was analyzed in real-time by reading the various HTTP parameters viz., URLs, Get / Post methods and the dependencies. The results were found to be encouraging as all the simulated attacks in real-time could be successfully detected. This work can be used as a template by various organizations to prevent any insider threat by monitoring the internal HTTP traffic.},
keywords={Standards organizations;Telecommunication traffic;Organizations;Real-time systems;Web servers;Monitoring;Cyberattack;Network forensics;network traffic;insider attack;DVWA;Python},
doi={10.1109/INDIACom51348.2021.00113},
ISSN={},
month={March},}
@INPROCEEDINGS{9529379,
author={Hernandez, Julio and McKenna, Lucy and Brennan, Rob},
booktitle={2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)}, title={TIKD: A Trusted Integrated Knowledge Dataspace For Sensitive Healthcare Data Sharing},
year={2021},
volume={},
number={},
pages={1855-1860},
abstract={This paper presents the Trusted Integrated Knowledge Dataspace (TIKD), a new dataspace, based on linked data technologies and trusted data sharing, that supports integrated knowledge graphs for sensitive application environments such as healthcare. State-of-the-art shared dataspaces do not consider sensitive data and privacy-aware log records as part of their solutions, defining only how to access data. TIKD complements dataspace security approaches through trusted data sharing that considers personal data handling, data privileges, pseudonymization of user activity logging, and privacy-aware data interlinking services. TIKD was implemented on the Access Risk Knowledge (ARK) Platform, a socio-technical risk governance system, and deployed as part of the ARK-Virus Project which aims to govern the risk management of Personal Protection Equipment (PPE) across a group of collaborating healthcare institutions. The ARK Platform was evaluated, both before and after implementing the TIKD, using the ISO 27001 Gap Analysis Tool (GAT) which determines compliance with the information security standard. The results of the evaluation indicated that compliance with ISO 27001 increased from 50% to 85%. The evaluation also provided a set of recommended actions to meet the remaining requirements of the ISO 27001 standard. TIKD provides a collaborative environment, based on knowledge graph integration and GDPR-compliant personal data handling, as part of the data security infrastructure. As a result of this work, a new trusted data security methodology, based on personal data handling, data privileges, access control context specification, and privacy-aware data interlinking, was developed using a knowledge graph approach.},
keywords={Access control;ISO Standards;Data handling;Data security;Linked data;Collaboration;Medical services;Dataspace;Knowledge Graph;Trusted Data;Personal Data Handling},
doi={10.1109/COMPSAC51774.2021.00280},
ISSN={0730-3157},
month={July},}
@ARTICLE{9316994,
author={Yeshchenko, Anton and Di Ciccio, Claudio and Mendling, Jan and Polyvyanyy, Artem},
journal={IEEE Transactions on Visualization and Computer Graphics}, title={Visual Drift Detection for Sequence Data Analysis of Business Processes},
year={2021},
volume={},
number={},
pages={1-1},
abstract={Event sequence data is increasingly available in various application domains, such as business process management, software engineering, or medical pathways. Processes in these domains are typically represented as process diagrams or flow charts. So far, various techniques have been developed for automatically generating such diagrams from event sequence data. An open challenge is the visual analysis of drift phenomena when processes change over time. In this paper, we address this research gap. Our contribution is a system for fine-granular process drift detection and corresponding visualizations for event logs of executed business processes. We evaluated our system both on synthetic and real-world data. On synthetic logs, we achieved an average F-score of 0.96 and outperformed all the state-of-the-art methods. On real-world logs, we identified all types of process drifts in a comprehensive manner. Finally, we conducted a user study highlighting that our visualizations are easy to use and useful as perceived by process mining experts. In this way, our work contributes to research on process mining, event sequence analysis, and visualization of temporal data.},
keywords={Business;Data visualization;Data mining;Visualization;Erbium;Antibiotics;Guidelines;Sequence data;Visualization;Temporal data;Process mining;Process drifts;Declarative process models},
doi={10.1109/TVCG.2021.3050071},
ISSN={1941-0506},
month={},}
@INPROCEEDINGS{9511610,
author={Anang Othman, Nurul Amira binti and Zolkapli, Maizatul and Hairuddin, Muhammad Asraf and Hassan, Harnani and Manut, Azrif and Zoolfakar, Ahmad Sabirin and Abdullah, Noor Ezan},
booktitle={2021 IEEE Regional Symposium on Micro and Nanoelectronics (RSM)}, title={The Development of IoT-based Solar Battery Monitoring System},
year={2021},
volume={},
number={},
pages={34-37},
abstract={The solar photovoltaic (PV) system has become the key attraction for the generation of clean, renewable electricity. Nevertheless, performance varies due to different parameters and environmental factors. Therefore, a remote and real- performance is required to evaluate its performance. The Internet of Things (IoT) in the monitoring of the solar PV system has been implemented and its efficiency has been studied. The monitoring system uses real-time measured values to display power, current, voltage, temperature, and light intensity graphs, and it is also easy to track and view the database file to analyze the history of the collected data. The design work was divided into two main sections, hardware, and software. The hardware includes the creation of major units such as the power supply unit, the control unit and the sensor units while the software includes the database system and pulled the data out into private webpage and visualization on Grafana dashboard. This paper aims to create an IoT-based Solar Battery Monitoring System using two microcontrollers, Arduino UNO and NodeMCU. The data obtained will be stored in the local database and can be viewed through a personal web page that serves as a data log and through a visualization tool using Grafana. Throughout the system, users can easily track their solar PV system over the internet.},
keywords={Temperature measurement;Temperature sensors;Voltage measurement;Power measurement;Current measurement;Real-time systems;Software;solar photovoltaic system;Internet of Things;MySQL;Grafana},
doi={10.1109/RSM52397.2021.9511610},
ISSN={2639-4642},
month={Aug},}
@INPROCEEDINGS{9558948,
author={Başer, Melike and Güven, Ebu Yusuf and Aydın, Muhammed Ali},
booktitle={2021 6th International Conference on Computer Science and Engineering (UBMK)}, title={SSH and Telnet Protocols Attack Analysis Using Honeypot Technique : *Analysis of SSH AND TELNET Honeypot},
year={2021},
volume={},
number={},
pages={806-811},
abstract={Generally, the defense measures taken against new cyber-attack methods are insufficient for cybersecurity risk management. Contrary to classical attack methods, the existence of undiscovered attack types called’ zero-day attacks’ can invalidate the actions taken. It is possible with honeypot systems to implement new security measures by recording the attacker’s behavior. The purpose of the honeypot is to learn about the methods and tools used by the attacker or malicious activity. In particular, it allows us to discover zero-day attack types and develop new defense methods for them. Attackers have made protocols such as SSH (Secure Shell) and Telnet, which are widely used for remote access to devices, primary targets. In this study, SSHTelnet honeypot was established using Cowrie software. Attackers attempted to connect, and attackers record their activity after providing access. These collected attacker log records and files uploaded to the system are published on Github to other researchers1. We shared the observations and analysis results of attacks on SSH and Telnet protocols with honeypot.},
keywords={Protocols;Information security;Tools;Particle measurements;Safety;Risk management;IP networks;SSH;TELNET;Honeypot;Cowrie;Cyber Attack},
doi={10.1109/UBMK52708.2021.9558948},
ISSN={2521-1641},
month={Sep.},}
@INPROCEEDINGS{9468212,
author={Ude, Okechukwu and Swar, Bobby},
booktitle={2021 4th IEEE International Conference on Industrial Cyber-Physical Systems (ICPS)}, title={Securing Remote Access Networks Using Malware Detection Tools for Industrial Control Systems},
year={2021},
volume={},
number={},
pages={166-171},
abstract={With their role as an integral part of its infrastructure, Industrial Control Systems (ICS) are a vital part of every nation's industrial development drive. Despite several significant advancements - such as controlled-environment agriculture, automated train systems, and smart homes, achieved in critical infrastructure sectors through the integration of Information Systems (IS) and remote capabilities with ICS, the fact remains that these advancements have introduced vulnerabilities that were previously either nonexistent or negligible, one being Remote Access Trojans (RATs). Present RAT detection methods either focus on monitoring network traffic or studying event logs on host systems. This research's objective is the detection of RATs by comparing actual utilized system capacity to reported utilized system capacity. To achieve the research objective, open-source RAT detection methods were identified and analyzed, a GAP-analysis approach was used to identify the deficiencies of each method, after which control algorithms were developed into source code for the solution.},
keywords={Integrated circuits;Industrial control;Telecommunication traffic;Smart homes;Rats;Tools;Object recognition;Industrial control systems;operational technology;remote access trojans;malware detection;Purdue control hierarchy;hash calculation;confidentiality;integrity;availability},
doi={10.1109/ICPS49255.2021.9468212},
ISSN={},
month={May},}
@INPROCEEDINGS{9464587,
author={Kriaa, Siwar and Chaabane, Yahia},
booktitle={2021 12th International Conference on Information and Communication Systems (ICICS)}, title={SecKG: Leveraging attack detection and prediction using knowledge graphs},
year={2021},
volume={},
number={},
pages={112-119},
abstract={Advanced persistent threats targeting sensitive corporations, are becoming today stealthier and more complex, coordinating different attacks steps and lateral movements, and trying to stay undetected for long time. Classical security solutions that rely on signature-based detection can be easily thwarted by malware using obfuscation and encryption techniques. More recent solutions are using machine learning approaches for detecting outliers. Nevertheless, the majority of them reason on tabular unstructured data which can lead to missing obvious conclusions. We propose in this paper a novel approach that leverages a combination of both knowledge graphs and machine learning techniques to detect and predict attacks. Using Cyber Threat Intelligence (CTI), we built a knowledge graph that processes event logs in order to not only detect attack techniques, but also learn how to predict them.},
keywords={Industries;Scalability;Focusing;Machine learning;Predictive models;Real-time systems;Malware;knowledge graphs;cyber threat intelligence;attack modeling;attack detection;attack prediction},
doi={10.1109/ICICS52457.2021.9464587},
ISSN={2573-3346},
month={May},}
@ARTICLE{9380153,
author={Venkatesh, Sushma and Ramachandra, Raghavendra and Raja, Kiran and Busch, Christoph},
journal={IEEE Transactions on Technology and Society}, title={Face Morphing Attack Generation and Detection: A Comprehensive Survey},
year={2021},
volume={2},
number={3},
pages={128-145},
abstract={Face recognition has been successfully deployed in real-time applications, including secure applications such as border control. The vulnerability of face recognition systems (FRSs) to various kinds of attacks (both direct and indirect attacks) and face morphing attacks has received great interest from the biometric community. The goal of a morphing attack is to subvert an FRS at an automatic border control (ABC) gate by presenting an electronic machine-readable travel document (eMRTD) or e-passport that is obtained based on a morphed face image. Since the application process for an e-passport in the majority of countries requires a passport photograph to be presented by the applicant, a malicious actor and an accomplice can generate a morphed face image to obtain the e-passport. An e-passport with a morphed face image can be used by both the malicious actor and the accomplice to cross a border, as the morphed face image can be verified against both of them. This can result in a significant threat, as a malicious actor can cross the border without revealing the trace of his/her criminal background, while the details of the accomplice are recorded in the log of the access control system. This survey aims to present a systematic overview of the progress made in the area of face morphing in terms of both morph generation and morph detection. In this article, we describe and illustrate various aspects of face morphing attacks, including different techniques for generating morphed face images and state-of-the-art morph attack detection (MAD) algorithms based on a stringent taxonomy as well as the availability of public databases, which allow us to benchmark new MAD algorithms in a reproducible manner. The outcomes of competitions and benchmarking, vulnerability assessments, and performance evaluation metrics are also provided in a comprehensive manner. Furthermore, we discuss the open challenges and potential future areas that need to be addressed in the evolving field of biometrics.},
keywords={Face recognition;Faces;Benchmark testing;Open source software;Tools;Taxonomy;Strain;Attack detection;biometrics;face recognition;morphing attack;vulnerability},
doi={10.1109/TTS.2021.3066254},
ISSN={2637-6415},
month={Sep.},}